/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 17454
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17554, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:36, 36.23s/it]Extractor Estimating: 2it [00:38, 15.97s/it]Extractor Estimating: 3it [00:38,  8.97s/it]Extractor Estimating: 4it [00:39,  5.68s/it]Extractor Estimating: 5it [00:39,  3.86s/it]Extractor Estimating: 6it [00:40,  2.78s/it]Extractor Estimating: 7it [00:41,  2.06s/it]Extractor Estimating: 8it [00:41,  1.61s/it]Extractor Estimating: 9it [00:42,  1.28s/it]Extractor Estimating: 10it [00:43,  1.08s/it]Extractor Estimating: 11it [00:43,  1.06it/s]Extractor Estimating: 12it [00:44,  1.19it/s]Extractor Estimating: 13it [00:44,  1.30it/s]Extractor Estimating: 14it [00:45,  1.39it/s]Extractor Estimating: 15it [00:46,  1.41it/s]Extractor Estimating: 16it [00:46,  1.46it/s]Extractor Estimating: 17it [00:47,  1.50it/s]Extractor Estimating: 18it [00:48,  1.54it/s]Extractor Estimating: 19it [00:48,  1.55it/s]Extractor Estimating: 20it [00:49,  1.65it/s]Extractor Estimating: 21it [00:49,  1.67it/s]Extractor Estimating: 22it [00:50,  1.29it/s]Extractor Estimating: 23it [00:51,  1.39it/s]Extractor Estimating: 24it [00:52,  1.48it/s]Extractor Estimating: 25it [00:52,  1.56it/s]Extractor Estimating: 26it [00:53,  1.52it/s]Extractor Estimating: 27it [00:54,  1.53it/s]Extractor Estimating: 28it [00:55,  1.15it/s]Extractor Estimating: 29it [00:56,  1.01s/it]Extractor Estimating: 30it [00:57,  1.09it/s]Extractor Estimating: 31it [00:58,  1.19it/s]Extractor Estimating: 32it [00:58,  1.27it/s]Extractor Estimating: 33it [00:59,  1.36it/s]Extractor Estimating: 34it [01:00,  1.36it/s]Extractor Estimating: 35it [01:00,  1.39it/s]Extractor Estimating: 36it [01:01,  1.44it/s]Extractor Estimating: 37it [01:02,  1.46it/s]Extractor Estimating: 38it [01:02,  1.51it/s]Extractor Estimating: 39it [01:03,  1.54it/s]Extractor Estimating: 40it [01:04,  1.50it/s]Extractor Estimating: 41it [01:04,  1.52it/s]Extractor Estimating: 42it [01:05,  1.54it/s]Extractor Estimating: 43it [01:05,  1.50it/s]Extractor Estimating: 44it [01:06,  1.54it/s]Extractor Estimating: 45it [01:07,  1.52it/s]Extractor Estimating: 46it [01:07,  1.52it/s]Extractor Estimating: 47it [01:08,  1.53it/s]Extractor Estimating: 48it [01:09,  1.52it/s]Extractor Estimating: 49it [01:09,  1.57it/s]Extractor Estimating: 50it [01:10,  1.59it/s]Extractor Estimating: 51it [01:11,  1.60it/s]Extractor Estimating: 52it [01:11,  1.56it/s]Extractor Estimating: 53it [01:12,  1.23it/s]Extractor Estimating: 54it [01:13,  1.34it/s]Extractor Estimating: 55it [01:14,  1.46it/s]Extractor Estimating: 56it [01:14,  1.55it/s]Extractor Estimating: 57it [01:15,  1.59it/s]Extractor Estimating: 58it [01:15,  1.60it/s]Extractor Estimating: 59it [01:16,  1.58it/s]Extractor Estimating: 60it [01:17,  1.63it/s]Extractor Estimating: 61it [01:17,  1.63it/s]Extractor Estimating: 62it [01:18,  1.62it/s]Extractor Estimating: 63it [01:18,  1.59it/s]Extractor Estimating: 64it [01:19,  1.58it/s]Extractor Estimating: 65it [01:20,  1.59it/s]Extractor Estimating: 66it [01:20,  1.65it/s]Extractor Estimating: 67it [01:21,  1.57it/s]Extractor Estimating: 68it [01:22,  1.61it/s]Extractor Estimating: 69it [01:22,  1.57it/s]Extractor Estimating: 70it [01:23,  1.62it/s]Extractor Estimating: 71it [01:23,  1.62it/s]Extractor Estimating: 72it [01:24,  1.61it/s]Extractor Estimating: 73it [01:25,  1.54it/s]Extractor Estimating: 74it [01:25,  1.55it/s]Extractor Estimating: 75it [01:26,  1.56it/s]Extractor Estimating: 76it [01:27,  1.58it/s]Extractor Estimating: 77it [01:27,  1.53it/s]Extractor Estimating: 78it [01:28,  1.55it/s]Extractor Estimating: 79it [01:29,  1.54it/s]Extractor Estimating: 80it [01:29,  1.52it/s]Extractor Estimating: 81it [01:30,  1.52it/s]Extractor Estimating: 82it [01:31,  1.53it/s]Extractor Estimating: 83it [01:31,  1.53it/s]Extractor Estimating: 84it [01:32,  1.52it/s]Extractor Estimating: 85it [01:33,  1.54it/s]Extractor Estimating: 86it [01:33,  1.57it/s]Extractor Estimating: 87it [01:34,  1.47it/s]Extractor Estimating: 88it [01:35,  1.50it/s]Extractor Estimating: 89it [01:35,  1.57it/s]Extractor Estimating: 90it [01:36,  1.57it/s]Extractor Estimating: 91it [01:37,  1.46it/s]Extractor Estimating: 92it [01:37,  1.48it/s]Extractor Estimating: 93it [01:38,  1.51it/s]Extractor Estimating: 94it [01:38,  1.57it/s]Extractor Estimating: 95it [01:39,  1.58it/s]Extractor Estimating: 96it [01:40,  1.56it/s]Extractor Estimating: 97it [01:40,  1.55it/s]Extractor Estimating: 98it [01:41,  1.58it/s]Extractor Estimating: 99it [01:42,  1.55it/s]Extractor Estimating: 100it [01:42,  1.56it/s]Extractor Estimating: 101it [01:43,  1.59it/s]Extractor Estimating: 102it [01:44,  1.59it/s]Extractor Estimating: 103it [01:44,  1.55it/s]Extractor Estimating: 104it [01:45,  1.46it/s]Extractor Estimating: 105it [01:46,  1.40it/s]Extractor Estimating: 106it [01:46,  1.49it/s]Extractor Estimating: 107it [01:47,  1.54it/s]Extractor Estimating: 108it [01:48,  1.57it/s]Extractor Estimating: 109it [01:48,  1.64it/s]Extractor Estimating: 110it [01:49,  1.61it/s]Extractor Estimating: 111it [01:49,  1.64it/s]Extractor Estimating: 112it [01:50,  1.63it/s]Extractor Estimating: 113it [01:51,  1.66it/s]Extractor Estimating: 114it [01:51,  1.64it/s]Extractor Estimating: 115it [01:52,  1.63it/s]Extractor Estimating: 116it [01:52,  1.64it/s]Extractor Estimating: 117it [01:53,  1.60it/s]Extractor Estimating: 118it [01:54,  1.56it/s]Extractor Estimating: 119it [01:54,  1.59it/s]Extractor Estimating: 120it [01:55,  1.62it/s]Extractor Estimating: 121it [01:56,  1.60it/s]Extractor Estimating: 122it [01:56,  1.60it/s]Extractor Estimating: 123it [01:57,  1.60it/s]Extractor Estimating: 124it [01:57,  1.63it/s]Extractor Estimating: 125it [01:58,  1.65it/s]Extractor Estimating: 126it [01:59,  1.64it/s]Extractor Estimating: 127it [01:59,  1.61it/s]Extractor Estimating: 128it [02:00,  1.60it/s]Extractor Estimating: 129it [02:01,  1.56it/s]Extractor Estimating: 130it [02:02,  1.13it/s]Extractor Estimating: 131it [02:03,  1.24it/s]Extractor Estimating: 132it [02:03,  1.29it/s]Extractor Estimating: 133it [02:04,  1.31it/s]Extractor Estimating: 134it [02:05,  1.35it/s]Extractor Estimating: 135it [02:05,  1.39it/s]Extractor Estimating: 136it [02:06,  1.44it/s]Extractor Estimating: 137it [02:07,  1.46it/s]Extractor Estimating: 138it [02:07,  1.51it/s]Extractor Estimating: 139it [02:08,  1.51it/s]Extractor Estimating: 140it [02:09,  1.48it/s]Extractor Estimating: 141it [02:09,  1.51it/s]Extractor Estimating: 142it [02:10,  1.51it/s]Extractor Estimating: 143it [02:11,  1.47it/s]Extractor Estimating: 144it [02:11,  1.51it/s]Extractor Estimating: 145it [02:12,  1.56it/s]Extractor Estimating: 146it [02:13,  1.53it/s]Extractor Estimating: 147it [02:13,  1.46it/s]Extractor Estimating: 148it [02:14,  1.48it/s]Extractor Estimating: 149it [02:15,  1.50it/s]Extractor Estimating: 150it [02:15,  1.51it/s]Extractor Estimating: 151it [02:16,  1.54it/s]Extractor Estimating: 152it [02:17,  1.54it/s]Extractor Estimating: 153it [02:17,  1.59it/s]Extractor Estimating: 154it [02:18,  1.55it/s]Extractor Estimating: 155it [02:18,  1.60it/s]Extractor Estimating: 156it [02:19,  1.55it/s]Extractor Estimating: 157it [02:20,  1.54it/s]Extractor Estimating: 158it [02:20,  1.62it/s]Extractor Estimating: 159it [02:21,  1.59it/s]Extractor Estimating: 160it [02:22,  1.54it/s]Extractor Estimating: 161it [02:22,  1.56it/s]Extractor Estimating: 162it [02:23,  1.66it/s]Extractor Estimating: 163it [02:23,  1.71it/s]Extractor Estimating: 164it [02:24,  1.69it/s]Extractor Estimating: 165it [02:25,  1.62it/s]Extractor Estimating: 166it [02:25,  1.63it/s]Extractor Estimating: 167it [02:26,  1.61it/s]Extractor Estimating: 168it [02:26,  1.63it/s]Extractor Estimating: 169it [02:27,  1.69it/s]Extractor Estimating: 170it [02:28,  1.59it/s]Extractor Estimating: 171it [02:28,  1.59it/s]Extractor Estimating: 172it [02:29,  1.58it/s]Extractor Estimating: 173it [02:30,  1.61it/s]Extractor Estimating: 174it [02:30,  1.60it/s]Extractor Estimating: 175it [02:31,  1.62it/s]Extractor Estimating: 176it [02:31,  1.61it/s]Extractor Estimating: 177it [02:32,  1.42it/s]Extractor Estimating: 178it [02:33,  1.40it/s]Extractor Estimating: 179it [02:34,  1.45it/s]Extractor Estimating: 180it [02:34,  1.53it/s]Extractor Estimating: 181it [02:35,  1.56it/s]Extractor Estimating: 182it [02:36,  1.57it/s]Extractor Estimating: 183it [02:36,  1.53it/s]Extractor Estimating: 184it [02:37,  1.48it/s]Extractor Estimating: 185it [02:38,  1.45it/s]Extractor Estimating: 186it [02:38,  1.44it/s]Extractor Estimating: 187it [02:39,  1.45it/s]Extractor Estimating: 188it [02:40,  1.52it/s]Extractor Estimating: 189it [02:40,  1.47it/s]Extractor Estimating: 190it [02:41,  1.49it/s]Extractor Estimating: 191it [02:42,  1.54it/s]Extractor Estimating: 192it [02:42,  1.50it/s]Extractor Estimating: 193it [02:43,  1.42it/s]Extractor Estimating: 194it [02:44,  1.50it/s]Extractor Estimating: 195it [02:45,  1.39it/s]Extractor Estimating: 196it [02:45,  1.38it/s]Extractor Estimating: 197it [02:46,  1.44it/s]Extractor Estimating: 198it [02:47,  1.46it/s]Extractor Estimating: 199it [02:47,  1.49it/s]Extractor Estimating: 200it [02:48,  1.54it/s]Extractor Estimating: 201it [02:48,  1.56it/s]Extractor Estimating: 202it [02:49,  1.54it/s]Extractor Estimating: 203it [02:50,  1.47it/s]Extractor Estimating: 204it [02:50,  1.51it/s]Extractor Estimating: 205it [02:51,  1.52it/s]Extractor Estimating: 206it [02:52,  1.54it/s]Extractor Estimating: 207it [02:52,  1.57it/s]Extractor Estimating: 208it [02:53,  1.57it/s]Extractor Estimating: 209it [02:54,  1.52it/s]Extractor Estimating: 210it [02:54,  1.47it/s]Extractor Estimating: 211it [02:55,  1.53it/s]Extractor Estimating: 212it [02:56,  1.54it/s]Extractor Estimating: 213it [02:56,  1.49it/s]Extractor Estimating: 214it [02:57,  1.44it/s]Extractor Estimating: 215it [02:58,  1.48it/s]Extractor Estimating: 216it [02:58,  1.47it/s]Extractor Estimating: 217it [02:59,  1.48it/s]Extractor Estimating: 218it [03:00,  1.44it/s]Extractor Estimating: 219it [03:01,  1.41it/s]Extractor Estimating: 220it [03:01,  1.45it/s]Extractor Estimating: 221it [03:02,  1.45it/s]Extractor Estimating: 222it [03:03,  1.45it/s]Extractor Estimating: 223it [03:03,  1.44it/s]Extractor Estimating: 224it [03:04,  1.48it/s]Extractor Estimating: 225it [03:05,  1.49it/s]Extractor Estimating: 226it [03:05,  1.47it/s]Extractor Estimating: 227it [03:06,  1.46it/s]Extractor Estimating: 228it [03:07,  1.47it/s]Extractor Estimating: 229it [03:07,  1.48it/s]Extractor Estimating: 230it [03:08,  1.46it/s]Extractor Estimating: 231it [03:09,  1.47it/s]Extractor Estimating: 232it [03:09,  1.50it/s]Extractor Estimating: 233it [03:10,  1.52it/s]Extractor Estimating: 234it [03:12,  1.10s/it]Extractor Estimating: 235it [03:13,  1.02it/s]Extractor Estimating: 236it [03:14,  1.13it/s]Extractor Estimating: 237it [03:14,  1.25it/s]Extractor Estimating: 238it [03:15,  1.31it/s]Extractor Estimating: 239it [03:15,  1.39it/s]Extractor Estimating: 240it [03:16,  1.38it/s]Extractor Estimating: 241it [03:17,  1.45it/s]Extractor Estimating: 242it [03:17,  1.50it/s]Extractor Estimating: 243it [03:18,  1.53it/s]Extractor Estimating: 244it [03:19,  1.54it/s]Extractor Estimating: 245it [03:19,  1.54it/s]Extractor Estimating: 246it [03:20,  1.53it/s]Extractor Estimating: 247it [03:21,  1.53it/s]Extractor Estimating: 248it [03:21,  1.54it/s]Extractor Estimating: 249it [03:22,  1.52it/s]Extractor Estimating: 250it [03:23,  1.51it/s]Extractor Estimating: 251it [03:23,  1.51it/s]Extractor Estimating: 252it [03:24,  1.53it/s]Extractor Estimating: 253it [03:25,  1.38it/s]Extractor Estimating: 254it [03:25,  1.40it/s]Extractor Estimating: 255it [03:26,  1.44it/s]Extractor Estimating: 256it [03:27,  1.43it/s]Extractor Estimating: 257it [03:28,  1.40it/s]Extractor Estimating: 258it [03:28,  1.41it/s]Extractor Estimating: 259it [03:29,  1.42it/s]Extractor Estimating: 260it [03:30,  1.41it/s]Extractor Estimating: 261it [03:30,  1.41it/s]Extractor Estimating: 262it [03:31,  1.40it/s]Extractor Estimating: 263it [03:32,  1.38it/s]Extractor Estimating: 264it [03:32,  1.44it/s]Extractor Estimating: 265it [03:33,  1.46it/s]Extractor Estimating: 266it [03:34,  1.50it/s]Extractor Estimating: 267it [03:34,  1.49it/s]Extractor Estimating: 268it [03:35,  1.49it/s]Extractor Estimating: 269it [03:36,  1.51it/s]Extractor Estimating: 270it [03:37,  1.45it/s]Extractor Estimating: 271it [03:37,  1.48it/s]Extractor Estimating: 272it [03:38,  1.46it/s]Extractor Estimating: 273it [03:38,  1.52it/s]Extractor Estimating: 274it [03:39,  1.48it/s]Extractor Estimating: 275it [03:40,  1.48it/s]Extractor Estimating: 276it [03:40,  1.54it/s]Extractor Estimating: 277it [03:41,  1.53it/s]Extractor Estimating: 278it [03:42,  1.14it/s]Extractor Estimating: 279it [03:43,  1.25it/s]Extractor Estimating: 280it [03:44,  1.32it/s]Extractor Estimating: 281it [03:44,  1.39it/s]Extractor Estimating: 282it [03:45,  1.44it/s]Extractor Estimating: 283it [03:46,  1.51it/s]Extractor Estimating: 284it [03:46,  1.58it/s]Extractor Estimating: 285it [03:47,  1.52it/s]Extractor Estimating: 286it [03:48,  1.53it/s]Extractor Estimating: 287it [03:48,  1.51it/s]Extractor Estimating: 288it [03:49,  1.55it/s]Extractor Estimating: 289it [03:49,  1.56it/s]Extractor Estimating: 290it [03:50,  1.60it/s]Extractor Estimating: 291it [03:51,  1.63it/s]Extractor Estimating: 292it [03:51,  1.63it/s]Extractor Estimating: 293it [03:52,  1.61it/s]Extractor Estimating: 294it [03:53,  1.59it/s]Extractor Estimating: 295it [03:53,  1.61it/s]Extractor Estimating: 296it [03:54,  1.63it/s]Extractor Estimating: 297it [03:54,  1.59it/s]Extractor Estimating: 298it [03:55,  1.62it/s]Extractor Estimating: 299it [03:56,  1.63it/s]Extractor Estimating: 300it [03:56,  1.61it/s]Extractor Estimating: 301it [03:57,  1.61it/s]Extractor Estimating: 302it [03:58,  1.54it/s]Extractor Estimating: 303it [03:58,  1.54it/s]Extractor Estimating: 304it [03:59,  1.56it/s]Extractor Estimating: 305it [03:59,  1.58it/s]Extractor Estimating: 306it [04:00,  1.52it/s]Extractor Estimating: 307it [04:01,  1.50it/s]Extractor Estimating: 308it [04:02,  1.49it/s]Extractor Estimating: 309it [04:02,  1.54it/s]Extractor Estimating: 310it [04:03,  1.57it/s]Extractor Estimating: 311it [04:03,  1.59it/s]Extractor Estimating: 312it [04:04,  1.58it/s]Extractor Estimating: 313it [04:05,  1.60it/s]Extractor Estimating: 314it [04:05,  1.61it/s]Extractor Estimating: 315it [04:06,  1.60it/s]Extractor Estimating: 316it [04:06,  1.60it/s]Extractor Estimating: 317it [04:07,  1.60it/s]Extractor Estimating: 318it [04:08,  1.57it/s]Extractor Estimating: 319it [04:08,  1.57it/s]Extractor Estimating: 320it [04:09,  1.64it/s]Extractor Estimating: 321it [04:10,  1.61it/s]Extractor Estimating: 322it [04:10,  1.64it/s]Extractor Estimating: 323it [04:11,  1.66it/s]Extractor Estimating: 324it [04:11,  1.61it/s]Extractor Estimating: 325it [04:12,  1.56it/s]Extractor Estimating: 326it [04:13,  1.45it/s]Extractor Estimating: 327it [04:13,  1.55it/s]Extractor Estimating: 328it [04:14,  1.64it/s]Extractor Estimating: 329it [04:15,  1.57it/s]Extractor Estimating: 330it [04:15,  1.59it/s]Extractor Estimating: 331it [04:16,  1.51it/s]Extractor Estimating: 332it [04:17,  1.53it/s]Extractor Estimating: 333it [04:17,  1.59it/s]Extractor Estimating: 334it [04:18,  1.63it/s]Extractor Estimating: 335it [04:18,  1.65it/s]Extractor Estimating: 336it [04:19,  1.69it/s]Extractor Estimating: 337it [04:20,  1.70it/s]Extractor Estimating: 338it [04:20,  1.68it/s]Extractor Estimating: 339it [04:21,  1.61it/s]Extractor Estimating: 340it [04:21,  1.62it/s]Extractor Estimating: 341it [04:22,  1.62it/s]Extractor Estimating: 342it [04:23,  1.71it/s]Extractor Estimating: 343it [04:23,  1.67it/s]Extractor Estimating: 344it [04:24,  1.62it/s]Extractor Estimating: 345it [04:25,  1.60it/s]Extractor Estimating: 346it [04:25,  1.58it/s]Extractor Estimating: 347it [04:26,  1.62it/s]Extractor Estimating: 348it [04:26,  1.63it/s]Extractor Estimating: 349it [04:27,  1.58it/s]Extractor Estimating: 350it [04:28,  1.58it/s]Extractor Estimating: 351it [04:28,  1.60it/s]Extractor Estimating: 352it [04:29,  1.59it/s]Extractor Estimating: 353it [04:30,  1.58it/s]Extractor Estimating: 354it [04:30,  1.58it/s]Extractor Estimating: 355it [04:31,  1.54it/s]Extractor Estimating: 356it [04:31,  1.59it/s]Extractor Estimating: 357it [04:32,  1.62it/s]Extractor Estimating: 358it [04:33,  1.60it/s]Extractor Estimating: 359it [04:33,  1.61it/s]Extractor Estimating: 360it [04:34,  1.57it/s]Extractor Estimating: 361it [04:35,  1.60it/s]Extractor Estimating: 362it [04:35,  1.68it/s]Extractor Estimating: 363it [04:36,  1.71it/s]Extractor Estimating: 364it [04:36,  1.75it/s]Extractor Estimating: 365it [04:37,  1.74it/s]Extractor Estimating: 366it [04:37,  1.65it/s]Extractor Estimating: 367it [04:38,  1.66it/s]Extractor Estimating: 368it [04:39,  1.59it/s]Extractor Estimating: 369it [04:39,  1.62it/s]Extractor Estimating: 370it [04:40,  1.65it/s]Extractor Estimating: 371it [04:41,  1.67it/s]Extractor Estimating: 372it [04:41,  1.65it/s]Extractor Estimating: 373it [04:42,  1.67it/s]Extractor Estimating: 374it [04:42,  1.69it/s]Extractor Estimating: 375it [04:43,  1.68it/s]Extractor Estimating: 376it [04:43,  1.68it/s]Extractor Estimating: 377it [04:44,  1.68it/s]Extractor Estimating: 378it [04:45,  1.73it/s]Extractor Estimating: 379it [04:45,  1.73it/s]Extractor Estimating: 380it [04:46,  1.69it/s]Extractor Estimating: 381it [04:46,  1.66it/s]Extractor Estimating: 382it [04:47,  1.67it/s]Extractor Estimating: 383it [04:48,  1.65it/s]Extractor Estimating: 384it [04:48,  1.63it/s]Extractor Estimating: 385it [04:49,  1.60it/s]Extractor Estimating: 386it [04:50,  1.60it/s]Extractor Estimating: 387it [04:50,  1.65it/s]Extractor Estimating: 388it [04:51,  1.68it/s]Extractor Estimating: 389it [04:51,  1.72it/s]Extractor Estimating: 390it [04:52,  1.65it/s]Extractor Estimating: 391it [04:53,  1.65it/s]Extractor Estimating: 392it [04:53,  1.65it/s]Extractor Estimating: 393it [04:54,  1.68it/s]Extractor Estimating: 394it [04:54,  1.70it/s]Extractor Estimating: 395it [04:55,  1.69it/s]Extractor Estimating: 396it [04:55,  1.68it/s]Extractor Estimating: 397it [04:56,  1.63it/s]Extractor Estimating: 398it [04:57,  1.63it/s]Extractor Estimating: 399it [04:57,  1.63it/s]Extractor Estimating: 400it [04:58,  1.62it/s]Extractor Estimating: 401it [04:59,  1.61it/s]Extractor Estimating: 402it [04:59,  1.55it/s]Extractor Estimating: 403it [05:00,  1.57it/s]Extractor Estimating: 404it [05:01,  1.56it/s]Extractor Estimating: 405it [05:01,  1.58it/s]Extractor Estimating: 406it [05:02,  1.58it/s]Extractor Estimating: 407it [05:02,  1.56it/s]Extractor Estimating: 408it [05:03,  1.55it/s]Extractor Estimating: 409it [05:04,  1.63it/s]Extractor Estimating: 410it [05:04,  1.61it/s]Extractor Estimating: 411it [05:05,  1.61it/s]Extractor Estimating: 412it [05:06,  1.53it/s]Extractor Estimating: 413it [05:06,  1.64it/s]Extractor Estimating: 414it [05:07,  1.64it/s]Extractor Estimating: 415it [05:07,  1.59it/s]Extractor Estimating: 416it [05:08,  1.57it/s]Extractor Estimating: 417it [05:09,  1.57it/s]Extractor Estimating: 418it [05:09,  1.63it/s]Extractor Estimating: 419it [05:10,  1.48it/s]Extractor Estimating: 420it [05:11,  1.52it/s]Extractor Estimating: 421it [05:11,  1.58it/s]Extractor Estimating: 422it [05:12,  1.54it/s]Extractor Estimating: 423it [05:13,  1.54it/s]Extractor Estimating: 424it [05:13,  1.57it/s]Extractor Estimating: 425it [05:14,  1.57it/s]Extractor Estimating: 426it [05:14,  1.61it/s]Extractor Estimating: 427it [05:15,  1.58it/s]Extractor Estimating: 428it [05:16,  1.58it/s]Extractor Estimating: 429it [05:16,  1.60it/s]Extractor Estimating: 430it [05:17,  1.55it/s]Extractor Estimating: 431it [05:18,  1.59it/s]Extractor Estimating: 432it [05:18,  1.65it/s]Extractor Estimating: 433it [05:19,  1.67it/s]Extractor Estimating: 434it [05:19,  1.64it/s]Extractor Estimating: 435it [05:20,  1.62it/s]Extractor Estimating: 436it [05:21,  1.60it/s]Extractor Estimating: 437it [05:21,  1.63it/s]Extractor Estimating: 438it [05:22,  1.62it/s]Extractor Estimating: 439it [05:23,  1.63it/s]Extractor Estimating: 440it [05:23,  1.61it/s]Extractor Estimating: 441it [05:24,  1.66it/s]Extractor Estimating: 442it [05:24,  1.62it/s]Extractor Estimating: 443it [05:25,  1.63it/s]Extractor Estimating: 444it [05:26,  1.64it/s]Extractor Estimating: 445it [05:26,  1.63it/s]Extractor Estimating: 446it [05:27,  1.65it/s]Extractor Estimating: 447it [05:27,  1.67it/s]Extractor Estimating: 448it [05:28,  1.64it/s]Extractor Estimating: 449it [05:29,  1.71it/s]Extractor Estimating: 450it [05:29,  1.63it/s]Extractor Estimating: 451it [05:30,  1.59it/s]Extractor Estimating: 452it [05:31,  1.51it/s]Extractor Estimating: 453it [05:32,  1.38it/s]Extractor Estimating: 454it [05:32,  1.41it/s]Extractor Estimating: 455it [05:33,  1.47it/s]Extractor Estimating: 456it [05:33,  1.49it/s]Extractor Estimating: 457it [05:34,  1.49it/s]Extractor Estimating: 458it [05:35,  1.49it/s]Extractor Estimating: 459it [05:35,  1.47it/s]Extractor Estimating: 460it [05:36,  1.46it/s]Extractor Estimating: 461it [05:37,  1.47it/s]Extractor Estimating: 462it [05:38,  1.46it/s]Extractor Estimating: 463it [05:38,  1.49it/s]Extractor Estimating: 464it [05:39,  1.48it/s]Extractor Estimating: 465it [05:40,  1.45it/s]Extractor Estimating: 466it [05:40,  1.44it/s]Extractor Estimating: 467it [05:41,  1.44it/s]Extractor Estimating: 468it [05:42,  1.45it/s]Extractor Estimating: 469it [05:42,  1.45it/s]Extractor Estimating: 470it [05:43,  1.50it/s]Extractor Estimating: 471it [05:44,  1.44it/s]Extractor Estimating: 472it [05:44,  1.41it/s]Extractor Estimating: 473it [05:45,  1.43it/s]Extractor Estimating: 474it [05:46,  1.46it/s]Extractor Estimating: 475it [05:46,  1.49it/s]Extractor Estimating: 476it [05:47,  1.52it/s]Extractor Estimating: 477it [05:48,  1.54it/s]Extractor Estimating: 478it [05:48,  1.54it/s]Extractor Estimating: 479it [05:49,  1.62it/s]Extractor Estimating: 480it [05:49,  1.63it/s]Extractor Estimating: 481it [05:50,  1.66it/s]Extractor Estimating: 482it [05:51,  1.61it/s]Extractor Estimating: 483it [05:51,  1.57it/s]Extractor Estimating: 484it [05:52,  1.57it/s]Extractor Estimating: 485it [05:53,  1.61it/s]Extractor Estimating: 486it [05:53,  1.54it/s]Extractor Estimating: 487it [05:54,  1.53it/s]Extractor Estimating: 488it [05:55,  1.60it/s]Extractor Estimating: 489it [05:55,  1.65it/s]Extractor Estimating: 490it [05:56,  1.61it/s]Extractor Estimating: 491it [05:56,  1.66it/s]Extractor Estimating: 492it [05:57,  1.60it/s]Extractor Estimating: 493it [05:58,  1.60it/s]Extractor Estimating: 494it [05:58,  1.54it/s]Extractor Estimating: 495it [05:59,  1.61it/s]Extractor Estimating: 496it [06:00,  1.58it/s]Extractor Estimating: 497it [06:00,  1.59it/s]Extractor Estimating: 498it [06:01,  1.59it/s]Extractor Estimating: 499it [06:02,  1.44it/s]Extractor Estimating: 500it [06:02,  1.61it/s]Extractor Estimating: 500it [06:02,  1.38it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9972 mean pseudo reward: 0.9479813999901722
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 31047
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31147, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31147, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.283, loss:3121.0307
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.970, loss:2325.9702
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.971, loss:1845.3274
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.967, loss:1716.5482
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.964, loss:1586.8818
>> valid entity prec:0.6037, rec:0.5601, f1:0.5811
>> valid relation prec:0.3765, rec:0.0912, f1:0.1468
>> valid relation with NER prec:0.3765, rec:0.0912, f1:0.1468
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.240, loss:1441.9032
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.982, loss:1399.7730
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 0.975, loss:1360.6396
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.981, loss:1200.4600
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.976, loss:1224.1356
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5892, rec:0.5302, f1:0.5581
>> valid relation prec:0.2500, rec:0.0690, f1:0.1082
>> valid relation with NER prec:0.2500, rec:0.0690, f1:0.1082
g_step 1100, step 268, avg_time 2.222, loss:1129.1124
g_step 1200, step 368, avg_time 0.972, loss:1111.5140
g_step 1300, step 52, avg_time 0.977, loss:1061.6958
g_step 1400, step 152, avg_time 0.981, loss:1043.6974
g_step 1500, step 252, avg_time 0.975, loss:1003.4425
>> valid entity prec:0.5838, rec:0.5591, f1:0.5712
>> valid relation prec:0.1986, rec:0.0656, f1:0.0986
>> valid relation with NER prec:0.1986, rec:0.0656, f1:0.0986
g_step 1600, step 352, avg_time 2.220, loss:1016.2411
g_step 1700, step 36, avg_time 0.972, loss:993.1715
g_step 1800, step 136, avg_time 0.966, loss:957.4526
g_step 1900, step 236, avg_time 0.980, loss:954.1701
g_step 2000, step 336, avg_time 0.978, loss:949.0933
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5937, rec:0.5224, f1:0.5558
>> valid relation prec:0.1980, rec:0.0679, f1:0.1011
>> valid relation with NER prec:0.1980, rec:0.0679, f1:0.1011
g_step 2100, step 20, avg_time 2.218, loss:946.2625
g_step 2200, step 120, avg_time 0.986, loss:914.7494
g_step 2300, step 220, avg_time 0.972, loss:893.3121
g_step 2400, step 320, avg_time 0.976, loss:875.2471
g_step 2500, step 4, avg_time 0.963, loss:897.5883
>> valid entity prec:0.6231, rec:0.5072, f1:0.5592
>> valid relation prec:0.1743, rec:0.0538, f1:0.0822
>> valid relation with NER prec:0.1743, rec:0.0538, f1:0.0822
g_step 2600, step 104, avg_time 2.230, loss:813.3794
g_step 2700, step 204, avg_time 0.976, loss:860.1365
g_step 2800, step 304, avg_time 0.980, loss:850.5219
g_step 2900, step 404, avg_time 0.969, loss:869.1589
g_step 3000, step 88, avg_time 0.973, loss:799.9601
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6076, rec:0.5134, f1:0.5565
>> valid relation prec:0.1544, rec:0.0446, f1:0.0692
>> valid relation with NER prec:0.1544, rec:0.0446, f1:0.0692
g_step 3100, step 188, avg_time 2.223, loss:814.5609
g_step 3200, step 288, avg_time 0.980, loss:807.2108
g_step 3300, step 388, avg_time 0.976, loss:818.1382
g_step 3400, step 72, avg_time 0.971, loss:754.1679
g_step 3500, step 172, avg_time 0.967, loss:746.0050
>> valid entity prec:0.6018, rec:0.5212, f1:0.5586
>> valid relation prec:0.1731, rec:0.0610, f1:0.0902
>> valid relation with NER prec:0.1731, rec:0.0610, f1:0.0902
g_step 3600, step 272, avg_time 2.230, loss:776.3276
g_step 3700, step 372, avg_time 0.977, loss:797.6721
g_step 3800, step 56, avg_time 0.963, loss:761.5913
g_step 3900, step 156, avg_time 0.974, loss:743.7760
g_step 4000, step 256, avg_time 0.974, loss:738.5295
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6174, rec:0.5117, f1:0.5596
>> valid relation prec:0.2278, rec:0.0871, f1:0.1261
>> valid relation with NER prec:0.2278, rec:0.0871, f1:0.1261
g_step 4100, step 356, avg_time 2.237, loss:763.0083
g_step 4200, step 40, avg_time 0.973, loss:715.2646
g_step 4300, step 140, avg_time 0.981, loss:705.4059
g_step 4400, step 240, avg_time 0.977, loss:715.7926
g_step 4500, step 340, avg_time 0.972, loss:708.0464
>> valid entity prec:0.5918, rec:0.5094, f1:0.5475
>> valid relation prec:0.1535, rec:0.0621, f1:0.0885
>> valid relation with NER prec:0.1535, rec:0.0621, f1:0.0885
g_step 4600, step 24, avg_time 2.227, loss:717.4063
g_step 4700, step 124, avg_time 0.971, loss:668.1291
g_step 4800, step 224, avg_time 0.968, loss:665.8457
g_step 4900, step 324, avg_time 0.982, loss:694.5801
g_step 5000, step 8, avg_time 0.978, loss:703.6215
learning rate was adjusted to 0.0008
>> valid entity prec:0.6045, rec:0.4729, f1:0.5306
>> valid relation prec:0.1137, rec:0.0365, f1:0.0553
>> valid relation with NER prec:0.1137, rec:0.0365, f1:0.0553
g_step 5100, step 108, avg_time 2.237, loss:642.7158
g_step 5200, step 208, avg_time 0.977, loss:653.6212
g_step 5300, step 308, avg_time 0.972, loss:651.7749
g_step 5400, step 408, avg_time 0.963, loss:664.6498
g_step 5500, step 92, avg_time 0.969, loss:610.7842
>> valid entity prec:0.5812, rec:0.5149, f1:0.5460
>> valid relation prec:0.1172, rec:0.0469, f1:0.0670
>> valid relation with NER prec:0.1172, rec:0.0469, f1:0.0670
g_step 5600, step 192, avg_time 2.222, loss:648.6535
g_step 5700, step 292, avg_time 0.985, loss:632.5424
g_step 5800, step 392, avg_time 0.978, loss:646.0978
g_step 5900, step 76, avg_time 0.975, loss:591.4039
g_step 6000, step 176, avg_time 0.973, loss:603.9770
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5566, rec:0.5052, f1:0.5297
>> valid relation prec:0.1168, rec:0.0460, f1:0.0660
>> valid relation with NER prec:0.1168, rec:0.0460, f1:0.0660
g_step 6100, step 276, avg_time 2.229, loss:635.3342
g_step 6200, step 376, avg_time 0.974, loss:618.5588
g_step 6300, step 60, avg_time 0.979, loss:592.2768
g_step 6400, step 160, avg_time 0.980, loss:581.5566
g_step 6500, step 260, avg_time 0.979, loss:607.0028
>> valid entity prec:0.6100, rec:0.4910, f1:0.5441
>> valid relation prec:0.1336, rec:0.0549, f1:0.0778
>> valid relation with NER prec:0.1336, rec:0.0549, f1:0.0778
g_step 6600, step 360, avg_time 2.214, loss:583.2332
g_step 6700, step 44, avg_time 0.986, loss:570.0436
g_step 6800, step 144, avg_time 0.974, loss:548.3990
g_step 6900, step 244, avg_time 0.984, loss:572.8085
g_step 7000, step 344, avg_time 0.969, loss:562.5235
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5195, rec:0.4915, f1:0.5051
>> valid relation prec:0.0732, rec:0.0383, f1:0.0502
>> valid relation with NER prec:0.0732, rec:0.0383, f1:0.0502
g_step 7100, step 28, avg_time 2.220, loss:559.7972
g_step 7200, step 128, avg_time 0.977, loss:515.5225
g_step 7300, step 228, avg_time 0.973, loss:525.6924
g_step 7400, step 328, avg_time 0.975, loss:551.5387
g_step 7500, step 12, avg_time 0.966, loss:559.0694
>> valid entity prec:0.5880, rec:0.5150, f1:0.5491
>> valid relation prec:0.1347, rec:0.0621, f1:0.0850
>> valid relation with NER prec:0.1347, rec:0.0621, f1:0.0850
g_step 7600, step 112, avg_time 2.233, loss:496.1231
g_step 7700, step 212, avg_time 0.970, loss:517.0014
g_step 7800, step 312, avg_time 0.977, loss:532.9889
g_step 7900, step 412, avg_time 0.987, loss:566.1057
g_step 8000, step 96, avg_time 0.975, loss:492.3449
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5147, rec:0.5498, f1:0.5317
>> valid relation prec:0.1066, rec:0.0564, f1:0.0737
>> valid relation with NER prec:0.1066, rec:0.0564, f1:0.0737
g_step 8100, step 196, avg_time 2.232, loss:499.6436
g_step 8200, step 296, avg_time 0.977, loss:520.6672
g_step 8300, step 396, avg_time 0.978, loss:521.5472
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 23:56:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 23:56:19 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_23-56-18_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 23:56:20 - WARNING - datasets.builder -   Using custom data configuration default-3889b877c77aac84
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-3889b877c77aac84/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 23:56:22,405 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:56:22,442 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 23:56:22,466 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:56:22,467 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 23:56:22,582 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:22,637 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:22,637 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:22,637 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:22,637 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:22,637 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:22,637 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 23:56:23,011 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 23:56:34,293 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 23:56:34,304 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-3889b877c77aac84/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 23:56:34 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1456f6567950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  3.05ba/s] 18%|        | 2/11 [00:00<00:02,  3.84ba/s] 27%|       | 3/11 [00:00<00:01,  4.13ba/s] 36%|      | 4/11 [00:00<00:01,  4.28ba/s] 45%|     | 5/11 [00:01<00:01,  4.37ba/s] 55%|    | 6/11 [00:01<00:01,  4.39ba/s] 64%|   | 7/11 [00:01<00:00,  4.42ba/s] 73%|  | 8/11 [00:01<00:00,  4.45ba/s] 82%| | 9/11 [00:02<00:00,  4.49ba/s] 91%| | 10/11 [00:02<00:00,  4.49ba/s]100%|| 11/11 [00:02<00:00,  4.73ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.20ba/s] 50%|     | 2/4 [00:00<00:00,  3.87ba/s] 75%|  | 3/4 [00:00<00:00,  4.13ba/s]100%|| 4/4 [00:00<00:00,  5.27ba/s]100%|| 4/4 [00:00<00:00,  4.62ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  5.90ba/s] 27%|       | 3/11 [00:00<00:00,  8.63ba/s] 45%|     | 5/11 [00:00<00:00,  9.61ba/s] 64%|   | 7/11 [00:00<00:00, 10.02ba/s] 73%|  | 8/11 [00:00<00:00,  7.40ba/s] 91%| | 10/11 [00:01<00:00,  8.35ba/s]100%|| 11/11 [00:01<00:00,  9.27ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  6.40ba/s] 75%|  | 3/4 [00:00<00:00,  8.92ba/s]100%|| 4/4 [00:00<00:00, 10.11ba/s]
[INFO|trainer.py:414] 2023-08-27 23:56:39,958 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 23:56:39,969 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 23:56:39,969 >>   Num examples = 10022
[INFO|trainer.py:1149] 2023-08-27 23:56:39,969 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 23:56:39,969 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 23:56:39,969 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 23:56:39,969 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 23:56:39,969 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:54,  3.34it/s]  0%|          | 2/785 [00:00<03:50,  3.39it/s]  0%|          | 3/785 [00:00<03:48,  3.42it/s]  1%|          | 4/785 [00:01<03:47,  3.43it/s]  1%|          | 5/785 [00:01<03:46,  3.44it/s]  1%|          | 6/785 [00:01<03:46,  3.44it/s]  1%|          | 7/785 [00:02<03:45,  3.45it/s]  1%|          | 8/785 [00:02<03:45,  3.45it/s]  1%|          | 9/785 [00:02<03:44,  3.45it/s]  1%|         | 10/785 [00:02<03:44,  3.45it/s]  1%|         | 11/785 [00:03<03:44,  3.45it/s]  2%|         | 12/785 [00:03<03:43,  3.45it/s]  2%|         | 13/785 [00:03<03:43,  3.45it/s]  2%|         | 14/785 [00:04<03:43,  3.45it/s]  2%|         | 15/785 [00:04<03:42,  3.45it/s]  2%|         | 16/785 [00:04<03:42,  3.45it/s]  2%|         | 17/785 [00:04<03:42,  3.45it/s]  2%|         | 18/785 [00:05<03:42,  3.45it/s]  2%|         | 19/785 [00:05<03:41,  3.45it/s]  3%|         | 20/785 [00:05<03:41,  3.45it/s]  3%|         | 21/785 [00:06<03:41,  3.45it/s]  3%|         | 22/785 [00:06<03:41,  3.45it/s]  3%|         | 23/785 [00:06<03:46,  3.36it/s]  3%|         | 24/785 [00:06<03:44,  3.38it/s]  3%|         | 25/785 [00:07<03:43,  3.41it/s]  3%|         | 26/785 [00:07<03:42,  3.42it/s]  3%|         | 27/785 [00:07<03:41,  3.43it/s]  4%|         | 28/785 [00:08<03:40,  3.43it/s]  4%|         | 29/785 [00:08<03:39,  3.44it/s]  4%|         | 30/785 [00:08<03:39,  3.44it/s]  4%|         | 31/785 [00:09<03:39,  3.44it/s]  4%|         | 32/785 [00:09<03:38,  3.44it/s]  4%|         | 33/785 [00:09<03:38,  3.44it/s]  4%|         | 34/785 [00:09<03:38,  3.44it/s]  4%|         | 35/785 [00:10<03:37,  3.44it/s]  5%|         | 36/785 [00:10<03:37,  3.45it/s]  5%|         | 37/785 [00:10<03:37,  3.44it/s]  5%|         | 38/785 [00:11<03:36,  3.44it/s]  5%|         | 39/785 [00:11<03:36,  3.45it/s]  5%|         | 40/785 [00:11<03:36,  3.45it/s]  5%|         | 41/785 [00:11<03:35,  3.45it/s]  5%|         | 42/785 [00:12<03:35,  3.45it/s]  5%|         | 43/785 [00:12<03:35,  3.45it/s]  6%|         | 44/785 [00:12<03:34,  3.45it/s]  6%|         | 45/785 [00:13<03:34,  3.45it/s]  6%|         | 46/785 [00:13<03:34,  3.45it/s]  6%|         | 47/785 [00:13<03:34,  3.45it/s]  6%|         | 48/785 [00:13<03:33,  3.45it/s]  6%|         | 49/785 [00:14<03:33,  3.45it/s]  6%|         | 50/785 [00:14<03:33,  3.45it/s]  6%|         | 51/785 [00:14<03:32,  3.45it/s]  7%|         | 52/785 [00:15<03:32,  3.45it/s]  7%|         | 53/785 [00:15<03:32,  3.45it/s]  7%|         | 54/785 [00:15<03:31,  3.45it/s]  7%|         | 55/785 [00:15<03:31,  3.45it/s]  7%|         | 56/785 [00:16<03:31,  3.45it/s]  7%|         | 57/785 [00:16<03:31,  3.45it/s]  7%|         | 58/785 [00:16<03:30,  3.45it/s]  8%|         | 59/785 [00:17<03:30,  3.45it/s]  8%|         | 60/785 [00:17<03:30,  3.45it/s]  8%|         | 61/785 [00:17<03:29,  3.45it/s]  8%|         | 62/785 [00:18<03:29,  3.45it/s]  8%|         | 63/785 [00:18<03:29,  3.45it/s]  8%|         | 64/785 [00:18<03:29,  3.45it/s]  8%|         | 65/785 [00:18<03:38,  3.30it/s]  8%|         | 66/785 [00:19<03:35,  3.34it/s]  9%|         | 67/785 [00:19<03:33,  3.37it/s]  9%|         | 68/785 [00:19<03:31,  3.39it/s]  9%|         | 69/785 [00:20<03:30,  3.40it/s]  9%|         | 70/785 [00:20<03:29,  3.42it/s]  9%|         | 71/785 [00:20<03:28,  3.42it/s]  9%|         | 72/785 [00:20<03:27,  3.43it/s]  9%|         | 73/785 [00:21<03:27,  3.44it/s]  9%|         | 74/785 [00:21<03:26,  3.44it/s] 10%|         | 75/785 [00:22<04:56,  2.39it/s] 10%|         | 76/785 [00:22<04:29,  2.63it/s] 10%|         | 77/785 [00:22<04:09,  2.84it/s] 10%|         | 78/785 [00:23<03:56,  2.99it/s] 10%|         | 79/785 [00:23<03:46,  3.12it/s] 10%|         | 80/785 [00:23<03:39,  3.21it/s] 10%|         | 81/785 [00:23<03:35,  3.27it/s] 10%|         | 82/785 [00:24<03:31,  3.32it/s] 11%|         | 83/785 [00:24<03:28,  3.36it/s] 11%|         | 84/785 [00:24<03:27,  3.38it/s] 11%|         | 85/785 [00:25<03:25,  3.40it/s] 11%|         | 86/785 [00:25<03:24,  3.42it/s] 11%|         | 87/785 [00:25<03:23,  3.42it/s] 11%|         | 88/785 [00:26<03:23,  3.43it/s] 11%|        | 89/785 [00:26<03:22,  3.43it/s] 11%|        | 90/785 [00:26<03:22,  3.44it/s] 12%|        | 91/785 [00:26<03:22,  3.44it/s] 12%|        | 92/785 [00:27<03:21,  3.44it/s] 12%|        | 93/785 [00:27<03:21,  3.44it/s] 12%|        | 94/785 [00:27<03:20,  3.44it/s] 12%|        | 95/785 [00:28<03:20,  3.44it/s] 12%|        | 96/785 [00:28<03:20,  3.44it/s] 12%|        | 97/785 [00:28<03:19,  3.44it/s] 12%|        | 98/785 [00:28<03:19,  3.44it/s] 13%|        | 99/785 [00:29<03:19,  3.44it/s] 13%|        | 100/785 [00:29<03:19,  3.44it/s] 13%|        | 101/785 [00:29<03:19,  3.44it/s] 13%|        | 102/785 [00:30<03:18,  3.44it/s] 13%|        | 103/785 [00:30<03:18,  3.44it/s] 13%|        | 104/785 [00:30<03:18,  3.44it/s] 13%|        | 105/785 [00:30<03:17,  3.44it/s] 14%|        | 106/785 [00:31<03:17,  3.44it/s] 14%|        | 107/785 [00:31<03:17,  3.44it/s] 14%|        | 108/785 [00:31<03:16,  3.44it/s] 14%|        | 109/785 [00:32<03:16,  3.44it/s] 14%|        | 110/785 [00:32<03:16,  3.44it/s] 14%|        | 111/785 [00:32<03:16,  3.44it/s] 14%|        | 112/785 [00:33<03:15,  3.44it/s] 14%|        | 113/785 [00:33<03:15,  3.44it/s] 15%|        | 114/785 [00:33<03:15,  3.44it/s] 15%|        | 115/785 [00:33<03:14,  3.44it/s] 15%|        | 116/785 [00:34<03:14,  3.44it/s] 15%|        | 117/785 [00:34<03:14,  3.44it/s] 15%|        | 118/785 [00:34<03:13,  3.44it/s] 15%|        | 119/785 [00:35<03:13,  3.44it/s] 15%|        | 120/785 [00:35<03:12,  3.45it/s] 15%|        | 121/785 [00:35<03:12,  3.44it/s] 16%|        | 122/785 [00:35<03:12,  3.45it/s] 16%|        | 123/785 [00:36<03:12,  3.44it/s] 16%|        | 124/785 [00:36<03:11,  3.44it/s] 16%|        | 125/785 [00:36<03:18,  3.32it/s] 16%|        | 126/785 [00:37<03:16,  3.36it/s] 16%|        | 127/785 [00:37<03:14,  3.38it/s] 16%|        | 128/785 [00:37<03:13,  3.40it/s] 16%|        | 129/785 [00:37<03:12,  3.41it/s] 17%|        | 130/785 [00:38<03:11,  3.42it/s] 17%|        | 131/785 [00:38<03:10,  3.43it/s] 17%|        | 132/785 [00:38<03:10,  3.43it/s] 17%|        | 133/785 [00:39<03:09,  3.43it/s] 17%|        | 134/785 [00:39<03:09,  3.44it/s] 17%|        | 135/785 [00:39<03:09,  3.44it/s] 17%|        | 136/785 [00:40<03:14,  3.34it/s] 17%|        | 137/785 [00:40<03:12,  3.37it/s] 18%|        | 138/785 [00:40<03:10,  3.39it/s] 18%|        | 139/785 [00:40<03:09,  3.40it/s] 18%|        | 140/785 [00:41<03:08,  3.42it/s] 18%|        | 141/785 [00:41<03:08,  3.42it/s] 18%|        | 142/785 [00:41<03:07,  3.43it/s] 18%|        | 143/785 [00:42<03:07,  3.43it/s] 18%|        | 144/785 [00:42<03:06,  3.43it/s] 18%|        | 145/785 [00:42<03:06,  3.43it/s] 19%|        | 146/785 [00:42<03:06,  3.43it/s] 19%|        | 147/785 [00:43<03:09,  3.36it/s] 19%|        | 148/785 [00:43<03:08,  3.38it/s] 19%|        | 149/785 [00:43<03:07,  3.40it/s] 19%|        | 150/785 [00:44<03:06,  3.41it/s] 19%|        | 151/785 [00:44<03:05,  3.41it/s] 19%|        | 152/785 [00:44<03:05,  3.42it/s] 19%|        | 153/785 [00:45<03:04,  3.42it/s] 20%|        | 154/785 [00:45<03:04,  3.43it/s] 20%|        | 155/785 [00:45<03:03,  3.43it/s] 20%|        | 156/785 [00:45<03:03,  3.43it/s] 20%|        | 157/785 [00:46<02:44,  3.82it/s][INFO|trainer.py:2140] 2023-08-27 23:57:26,085 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:57:26,085 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-27 23:57:26,085 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.44it/s][A
  3%|         | 12/435 [00:00<00:08, 48.90it/s][A
  4%|         | 17/435 [00:00<00:08, 47.41it/s][A
  5%|         | 22/435 [00:00<00:08, 46.42it/s][A
  6%|         | 27/435 [00:00<00:08, 46.04it/s][A
  7%|         | 32/435 [00:00<00:08, 45.62it/s][A
  9%|         | 37/435 [00:00<00:08, 45.40it/s][A
 10%|         | 42/435 [00:00<00:08, 44.86it/s][A
 11%|         | 47/435 [00:01<00:08, 44.42it/s][A
 12%|        | 52/435 [00:01<00:08, 44.16it/s][A
 13%|        | 57/435 [00:01<00:08, 44.29it/s][A
 14%|        | 62/435 [00:01<00:08, 44.38it/s][A
 15%|        | 67/435 [00:01<00:08, 44.44it/s][A
 17%|        | 72/435 [00:01<00:08, 44.56it/s][A
 18%|        | 77/435 [00:01<00:08, 44.60it/s][A
 19%|        | 82/435 [00:01<00:07, 44.71it/s][A
 20%|        | 87/435 [00:01<00:07, 44.59it/s][A
 21%|        | 92/435 [00:02<00:07, 44.42it/s][A
 22%|       | 97/435 [00:02<00:07, 44.11it/s][A
 23%|       | 102/435 [00:02<00:07, 44.32it/s][A
 25%|       | 107/435 [00:02<00:07, 44.46it/s][A
 26%|       | 112/435 [00:02<00:07, 44.66it/s][A
 27%|       | 117/435 [00:02<00:07, 44.70it/s][A
 28%|       | 122/435 [00:02<00:06, 44.85it/s][A
 29%|       | 127/435 [00:02<00:06, 44.79it/s][A
 30%|       | 132/435 [00:02<00:06, 44.69it/s][A
 31%|      | 137/435 [00:03<00:06, 44.49it/s][A
 33%|      | 142/435 [00:03<00:07, 41.58it/s][A
 34%|      | 147/435 [00:03<00:06, 42.52it/s][A
 35%|      | 152/435 [00:03<00:06, 43.29it/s][A
 36%|      | 157/435 [00:03<00:06, 43.83it/s][A
 37%|      | 162/435 [00:03<00:06, 44.19it/s][A
 38%|      | 167/435 [00:03<00:06, 44.43it/s][A
 40%|      | 172/435 [00:03<00:05, 44.44it/s][A
 41%|      | 177/435 [00:03<00:05, 44.42it/s][A
 42%|     | 182/435 [00:04<00:05, 44.11it/s][A
 43%|     | 187/435 [00:04<00:05, 44.18it/s][A
 44%|     | 192/435 [00:04<00:05, 44.41it/s][A
 45%|     | 197/435 [00:04<00:05, 44.53it/s][A
 46%|     | 202/435 [00:04<00:05, 44.69it/s][A
 48%|     | 207/435 [00:04<00:05, 44.77it/s][A
 49%|     | 212/435 [00:04<00:05, 44.59it/s][A
 50%|     | 217/435 [00:04<00:05, 40.12it/s][A
 51%|    | 223/435 [00:05<00:04, 42.90it/s][A
 52%|    | 228/435 [00:05<00:04, 43.37it/s][A
 54%|    | 233/435 [00:05<00:04, 43.71it/s][A
 55%|    | 238/435 [00:05<00:04, 44.06it/s][A
 56%|    | 243/435 [00:05<00:04, 44.43it/s][A
 57%|    | 248/435 [00:05<00:04, 44.58it/s][A
 58%|    | 253/435 [00:05<00:04, 44.61it/s][A
 59%|    | 258/435 [00:05<00:03, 44.57it/s][A
 60%|    | 263/435 [00:05<00:03, 44.31it/s][A
 62%|   | 268/435 [00:06<00:03, 44.30it/s][A
 63%|   | 273/435 [00:06<00:03, 44.41it/s][A
 64%|   | 278/435 [00:06<00:03, 43.54it/s][A
 65%|   | 283/435 [00:06<00:03, 44.00it/s][A
 66%|   | 288/435 [00:06<00:03, 44.33it/s][A
 67%|   | 293/435 [00:06<00:03, 44.57it/s][A
 69%|   | 298/435 [00:06<00:03, 44.60it/s][A
 70%|   | 303/435 [00:06<00:02, 44.65it/s][A
 71%|   | 308/435 [00:06<00:02, 44.69it/s][A
 72%|  | 313/435 [00:07<00:02, 44.54it/s][A
 73%|  | 318/435 [00:07<00:02, 44.40it/s][A
 74%|  | 323/435 [00:07<00:02, 44.45it/s][A
 75%|  | 328/435 [00:07<00:02, 44.56it/s][A
 77%|  | 333/435 [00:07<00:02, 44.67it/s][A
 78%|  | 338/435 [00:07<00:02, 44.70it/s][A
 79%|  | 343/435 [00:07<00:02, 44.76it/s][A
 80%|  | 348/435 [00:07<00:01, 44.83it/s][A
 81%|  | 353/435 [00:07<00:01, 44.73it/s][A
 82%| | 358/435 [00:08<00:01, 44.56it/s][A
 83%| | 363/435 [00:08<00:01, 44.57it/s][A
 85%| | 368/435 [00:08<00:01, 44.64it/s][A
 86%| | 373/435 [00:08<00:01, 44.61it/s][A
 87%| | 378/435 [00:08<00:01, 44.61it/s][A
 88%| | 383/435 [00:08<00:01, 44.52it/s][A
 89%| | 388/435 [00:08<00:01, 44.74it/s][A
 90%| | 393/435 [00:08<00:00, 44.66it/s][A
 91%|| 398/435 [00:08<00:00, 44.73it/s][A
 93%|| 403/435 [00:09<00:00, 44.65it/s][A
 94%|| 408/435 [00:09<00:00, 44.63it/s][A
 95%|| 413/435 [00:09<00:00, 42.98it/s][A
 96%|| 418/435 [00:09<00:00, 43.56it/s][A
 97%|| 423/435 [00:09<00:00, 43.86it/s][A
 98%|| 428/435 [00:09<00:00, 44.09it/s][A
100%|| 433/435 [00:09<00:00, 44.32it/s][A                                                 
                                                 [A 20%|        | 157/785 [00:55<02:44,  3.82it/s]
100%|| 435/435 [00:09<00:00, 44.32it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:57:36,182 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-27 23:57:36,362 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:57:39,252 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:57:39,380 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:57:39,454 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-157/special_tokens_map.json
 20%|        | 158/785 [01:05<1:03:24,  6.07s/it] 20%|        | 159/785 [01:06<45:20,  4.35s/it]   20%|        | 160/785 [01:06<32:36,  3.13s/it] 21%|        | 161/785 [01:06<23:42,  2.28s/it] 21%|        | 162/785 [01:06<17:28,  1.68s/it] 21%|        | 163/785 [01:07<13:06,  1.27s/it] 21%|        | 164/785 [01:07<10:03,  1.03it/s] 21%|        | 165/785 [01:07<07:56,  1.30it/s] 21%|        | 166/785 [01:08<06:26,  1.60it/s] 21%|       | 167/785 [01:08<05:24,  1.91it/s] 21%|       | 168/785 [01:08<04:40,  2.20it/s] 22%|       | 169/785 [01:08<04:10,  2.46it/s] 22%|       | 170/785 [01:09<03:51,  2.65it/s] 22%|       | 171/785 [01:09<03:35,  2.85it/s] 22%|       | 172/785 [01:09<03:23,  3.01it/s] 22%|       | 173/785 [01:10<03:15,  3.13it/s] 22%|       | 174/785 [01:10<03:10,  3.21it/s] 22%|       | 175/785 [01:10<03:05,  3.28it/s] 22%|       | 176/785 [01:10<03:02,  3.33it/s] 23%|       | 177/785 [01:11<03:00,  3.36it/s] 23%|       | 178/785 [01:11<02:59,  3.39it/s] 23%|       | 179/785 [01:11<02:57,  3.41it/s] 23%|       | 180/785 [01:12<02:57,  3.42it/s] 23%|       | 181/785 [01:12<03:00,  3.35it/s] 23%|       | 182/785 [01:12<02:58,  3.37it/s] 23%|       | 183/785 [01:13<02:57,  3.39it/s] 23%|       | 184/785 [01:13<02:56,  3.41it/s] 24%|       | 185/785 [01:13<02:55,  3.42it/s] 24%|       | 186/785 [01:13<02:54,  3.42it/s] 24%|       | 187/785 [01:14<02:54,  3.43it/s] 24%|       | 188/785 [01:14<02:53,  3.43it/s] 24%|       | 189/785 [01:14<02:53,  3.44it/s] 24%|       | 190/785 [01:15<02:53,  3.44it/s] 24%|       | 191/785 [01:15<02:52,  3.44it/s] 24%|       | 192/785 [01:15<02:52,  3.44it/s] 25%|       | 193/785 [01:15<02:52,  3.44it/s] 25%|       | 194/785 [01:16<02:51,  3.44it/s] 25%|       | 195/785 [01:16<02:51,  3.44it/s] 25%|       | 196/785 [01:16<02:51,  3.44it/s] 25%|       | 197/785 [01:17<02:59,  3.28it/s] 25%|       | 198/785 [01:17<02:56,  3.32it/s] 25%|       | 199/785 [01:17<02:54,  3.36it/s] 25%|       | 200/785 [01:18<02:52,  3.38it/s] 26%|       | 201/785 [01:18<02:51,  3.40it/s] 26%|       | 202/785 [01:18<02:50,  3.42it/s] 26%|       | 203/785 [01:18<02:51,  3.39it/s] 26%|       | 204/785 [01:19<02:50,  3.40it/s] 26%|       | 205/785 [01:19<02:49,  3.41it/s] 26%|       | 206/785 [01:19<02:49,  3.42it/s] 26%|       | 207/785 [01:20<02:48,  3.43it/s] 26%|       | 208/785 [01:20<02:56,  3.26it/s] 27%|       | 209/785 [01:20<02:53,  3.32it/s] 27%|       | 210/785 [01:20<02:51,  3.35it/s] 27%|       | 211/785 [01:21<02:49,  3.38it/s] 27%|       | 212/785 [01:21<02:48,  3.40it/s] 27%|       | 213/785 [01:22<04:26,  2.15it/s] 27%|       | 214/785 [01:22<03:55,  2.42it/s] 27%|       | 215/785 [01:23<03:34,  2.66it/s] 28%|       | 216/785 [01:23<03:19,  2.85it/s] 28%|       | 217/785 [01:23<03:15,  2.90it/s] 28%|       | 218/785 [01:23<03:06,  3.04it/s] 28%|       | 219/785 [01:24<02:59,  3.15it/s] 28%|       | 220/785 [01:24<02:54,  3.24it/s] 28%|       | 221/785 [01:24<02:51,  3.29it/s] 28%|       | 222/785 [01:25<02:49,  3.33it/s] 28%|       | 223/785 [01:25<02:47,  3.36it/s] 29%|       | 224/785 [01:25<02:45,  3.39it/s] 29%|       | 225/785 [01:25<02:44,  3.40it/s] 29%|       | 226/785 [01:26<02:43,  3.41it/s] 29%|       | 227/785 [01:26<02:43,  3.42it/s] 29%|       | 228/785 [01:26<02:50,  3.26it/s] 29%|       | 229/785 [01:27<02:47,  3.31it/s] 29%|       | 230/785 [01:27<02:45,  3.35it/s] 29%|       | 231/785 [01:27<02:44,  3.38it/s] 30%|       | 232/785 [01:28<02:42,  3.40it/s] 30%|       | 233/785 [01:28<02:41,  3.41it/s] 30%|       | 234/785 [01:28<02:41,  3.42it/s] 30%|       | 235/785 [01:28<02:40,  3.42it/s] 30%|       | 236/785 [01:29<02:40,  3.43it/s] 30%|       | 237/785 [01:29<02:39,  3.43it/s] 30%|       | 238/785 [01:29<02:39,  3.43it/s] 30%|       | 239/785 [01:30<02:44,  3.31it/s] 31%|       | 240/785 [01:30<02:42,  3.35it/s] 31%|       | 241/785 [01:30<02:41,  3.37it/s] 31%|       | 242/785 [01:31<02:39,  3.39it/s] 31%|       | 243/785 [01:31<02:39,  3.41it/s] 31%|       | 244/785 [01:31<02:38,  3.42it/s] 31%|       | 245/785 [01:31<02:37,  3.42it/s] 31%|      | 246/785 [01:32<02:37,  3.43it/s] 31%|      | 247/785 [01:32<02:36,  3.43it/s] 32%|      | 248/785 [01:32<02:36,  3.43it/s] 32%|      | 249/785 [01:33<02:36,  3.44it/s] 32%|      | 250/785 [01:33<02:40,  3.33it/s] 32%|      | 251/785 [01:33<02:38,  3.36it/s] 32%|      | 252/785 [01:33<02:37,  3.38it/s] 32%|      | 253/785 [01:34<02:36,  3.40it/s] 32%|      | 254/785 [01:34<02:35,  3.41it/s] 32%|      | 255/785 [01:34<02:34,  3.42it/s] 33%|      | 256/785 [01:35<02:34,  3.43it/s] 33%|      | 257/785 [01:35<02:33,  3.43it/s] 33%|      | 258/785 [01:35<02:33,  3.44it/s] 33%|      | 259/785 [01:35<02:32,  3.44it/s] 33%|      | 260/785 [01:36<02:32,  3.44it/s] 33%|      | 261/785 [01:36<02:36,  3.34it/s] 33%|      | 262/785 [01:36<02:35,  3.37it/s] 34%|      | 263/785 [01:37<02:33,  3.39it/s] 34%|      | 264/785 [01:37<02:32,  3.41it/s] 34%|      | 265/785 [01:37<02:32,  3.42it/s] 34%|      | 266/785 [01:38<02:31,  3.42it/s] 34%|      | 267/785 [01:38<02:31,  3.43it/s] 34%|      | 268/785 [01:38<02:30,  3.43it/s] 34%|      | 269/785 [01:38<02:30,  3.43it/s] 34%|      | 270/785 [01:39<02:30,  3.43it/s] 35%|      | 271/785 [01:39<02:29,  3.44it/s] 35%|      | 272/785 [01:39<02:33,  3.35it/s] 35%|      | 273/785 [01:40<02:31,  3.37it/s] 35%|      | 274/785 [01:40<02:30,  3.39it/s] 35%|      | 275/785 [01:40<02:29,  3.40it/s] 35%|      | 276/785 [01:40<02:29,  3.41it/s] 35%|      | 277/785 [01:41<02:28,  3.42it/s] 35%|      | 278/785 [01:41<02:28,  3.42it/s] 36%|      | 279/785 [01:41<02:27,  3.42it/s] 36%|      | 280/785 [01:42<02:27,  3.43it/s] 36%|      | 281/785 [01:42<02:26,  3.43it/s] 36%|      | 282/785 [01:42<02:26,  3.43it/s] 36%|      | 283/785 [01:43<02:30,  3.33it/s] 36%|      | 284/785 [01:43<02:28,  3.36it/s] 36%|      | 285/785 [01:43<02:27,  3.38it/s] 36%|      | 286/785 [01:43<02:26,  3.40it/s] 37%|      | 287/785 [01:44<02:26,  3.41it/s] 37%|      | 288/785 [01:44<02:25,  3.41it/s] 37%|      | 289/785 [01:44<02:24,  3.42it/s] 37%|      | 290/785 [01:45<02:24,  3.42it/s] 37%|      | 291/785 [01:45<02:24,  3.43it/s] 37%|      | 292/785 [01:45<02:23,  3.43it/s] 37%|      | 293/785 [01:45<02:23,  3.43it/s] 37%|      | 294/785 [01:46<02:23,  3.43it/s] 38%|      | 295/785 [01:46<02:22,  3.43it/s] 38%|      | 296/785 [01:46<02:22,  3.43it/s] 38%|      | 297/785 [01:47<02:22,  3.43it/s] 38%|      | 298/785 [01:47<02:21,  3.43it/s] 38%|      | 299/785 [01:47<02:21,  3.43it/s] 38%|      | 300/785 [01:48<02:26,  3.32it/s] 38%|      | 301/785 [01:48<02:24,  3.35it/s] 38%|      | 302/785 [01:48<02:23,  3.37it/s] 39%|      | 303/785 [01:48<02:22,  3.39it/s] 39%|      | 304/785 [01:49<02:21,  3.40it/s] 39%|      | 305/785 [01:49<02:20,  3.41it/s] 39%|      | 306/785 [01:49<02:20,  3.41it/s] 39%|      | 307/785 [01:50<02:19,  3.42it/s] 39%|      | 308/785 [01:50<02:19,  3.42it/s] 39%|      | 309/785 [01:50<02:18,  3.43it/s] 39%|      | 310/785 [01:50<02:18,  3.43it/s] 40%|      | 311/785 [01:51<02:18,  3.43it/s] 40%|      | 312/785 [01:51<02:17,  3.43it/s] 40%|      | 313/785 [01:51<02:17,  3.43it/s] 40%|      | 314/785 [01:52<02:03,  3.81it/s][INFO|trainer.py:2140] 2023-08-27 23:58:31,983 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:58:31,983 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-27 23:58:31,983 >>   Batch size = 8
{'eval_loss': 0.9270541071891785, 'eval_runtime': 9.8154, 'eval_samples_per_second': 354.24, 'eval_steps_per_second': 44.318, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.44it/s][A
  3%|         | 12/435 [00:00<00:08, 48.69it/s][A
  4%|         | 17/435 [00:00<00:08, 46.94it/s][A
  5%|         | 22/435 [00:00<00:08, 46.12it/s][A
  6%|         | 27/435 [00:00<00:08, 45.59it/s][A
  7%|         | 32/435 [00:00<00:09, 42.26it/s][A
  9%|         | 37/435 [00:00<00:09, 43.04it/s][A
 10%|         | 42/435 [00:00<00:09, 43.33it/s][A
 11%|         | 47/435 [00:01<00:08, 43.63it/s][A
 12%|        | 52/435 [00:01<00:08, 43.93it/s][A
 13%|        | 57/435 [00:01<00:08, 44.22it/s][A
 14%|        | 62/435 [00:01<00:08, 44.37it/s][A
 15%|        | 67/435 [00:01<00:08, 44.49it/s][A
 17%|        | 72/435 [00:01<00:08, 44.25it/s][A
 18%|        | 77/435 [00:01<00:08, 44.39it/s][A
 19%|        | 82/435 [00:01<00:07, 44.52it/s][A
 20%|        | 87/435 [00:01<00:07, 44.51it/s][A
 21%|        | 92/435 [00:02<00:07, 44.48it/s][A
 22%|       | 97/435 [00:02<00:07, 44.66it/s][A
 23%|       | 102/435 [00:02<00:07, 44.60it/s][A
 25%|       | 107/435 [00:02<00:07, 44.79it/s][A
 26%|       | 112/435 [00:02<00:07, 44.64it/s][A
 27%|       | 117/435 [00:02<00:07, 44.65it/s][A
 28%|       | 122/435 [00:02<00:07, 44.70it/s][A
 29%|       | 127/435 [00:02<00:06, 44.61it/s][A
 30%|       | 132/435 [00:02<00:06, 44.64it/s][A
 31%|      | 137/435 [00:03<00:06, 44.61it/s][A
 33%|      | 142/435 [00:03<00:06, 44.70it/s][A
 34%|      | 147/435 [00:03<00:06, 44.69it/s][A
 35%|      | 152/435 [00:03<00:06, 44.73it/s][A
 36%|      | 157/435 [00:03<00:06, 44.68it/s][A
 37%|      | 162/435 [00:03<00:06, 44.70it/s][A
 38%|      | 167/435 [00:03<00:06, 41.39it/s][A
 40%|      | 172/435 [00:03<00:06, 42.52it/s][A
 41%|      | 177/435 [00:03<00:05, 43.09it/s][A
 42%|     | 182/435 [00:04<00:05, 43.64it/s][A
 43%|     | 187/435 [00:04<00:05, 44.00it/s][A
 44%|     | 192/435 [00:04<00:05, 44.16it/s][A
 45%|     | 197/435 [00:04<00:05, 44.35it/s][A
 46%|     | 202/435 [00:04<00:05, 44.34it/s][A
 48%|     | 207/435 [00:04<00:05, 44.14it/s][A
 49%|     | 212/435 [00:04<00:05, 44.24it/s][A
 50%|     | 217/435 [00:04<00:04, 44.43it/s][A
 51%|     | 222/435 [00:05<00:04, 44.55it/s][A
 52%|    | 227/435 [00:05<00:04, 44.71it/s][A
 53%|    | 232/435 [00:05<00:04, 44.67it/s][A
 54%|    | 237/435 [00:05<00:04, 44.72it/s][A
 56%|    | 242/435 [00:05<00:04, 44.66it/s][A
 57%|    | 247/435 [00:05<00:04, 44.52it/s][A
 58%|    | 252/435 [00:05<00:04, 44.47it/s][A
 59%|    | 257/435 [00:05<00:03, 44.50it/s][A
 60%|    | 262/435 [00:05<00:03, 44.52it/s][A
 61%|   | 267/435 [00:06<00:03, 44.73it/s][A
 63%|   | 272/435 [00:06<00:03, 44.64it/s][A
 64%|   | 277/435 [00:06<00:03, 44.82it/s][A
 65%|   | 282/435 [00:06<00:03, 44.74it/s][A
 66%|   | 287/435 [00:06<00:03, 44.77it/s][A
 67%|   | 292/435 [00:06<00:03, 44.58it/s][A
 68%|   | 297/435 [00:06<00:03, 44.50it/s][A
 69%|   | 302/435 [00:06<00:03, 43.59it/s][A
 71%|   | 307/435 [00:06<00:02, 44.01it/s][A
 72%|  | 312/435 [00:07<00:02, 44.25it/s][A
 73%|  | 317/435 [00:07<00:02, 44.42it/s][A
 74%|  | 322/435 [00:07<00:02, 44.56it/s][A
 75%|  | 327/435 [00:07<00:02, 44.63it/s][A
 76%|  | 332/435 [00:07<00:02, 44.64it/s][A
 77%|  | 337/435 [00:07<00:02, 44.59it/s][A
 79%|  | 342/435 [00:07<00:02, 44.36it/s][A
 80%|  | 347/435 [00:07<00:01, 44.36it/s][A
 81%|  | 352/435 [00:07<00:01, 44.48it/s][A
 82%| | 357/435 [00:08<00:01, 44.66it/s][A
 83%| | 362/435 [00:08<00:01, 44.73it/s][A
 84%| | 367/435 [00:08<00:01, 44.81it/s][A
 86%| | 372/435 [00:08<00:01, 44.79it/s][A
 87%| | 377/435 [00:08<00:01, 44.81it/s][A
 88%| | 382/435 [00:08<00:01, 44.65it/s][A
 89%| | 387/435 [00:08<00:01, 44.46it/s][A
 90%| | 392/435 [00:08<00:00, 44.45it/s][A
 91%|| 397/435 [00:08<00:00, 44.44it/s][A
 92%|| 402/435 [00:09<00:00, 44.56it/s][A
 94%|| 407/435 [00:09<00:00, 44.74it/s][A
 95%|| 412/435 [00:09<00:00, 44.76it/s][A
 96%|| 417/435 [00:09<00:00, 44.86it/s][A
 97%|| 422/435 [00:09<00:00, 44.78it/s][A
 98%|| 427/435 [00:09<00:00, 44.67it/s][A
 99%|| 432/435 [00:09<00:00, 44.65it/s][A                                                 
                                                 [A 40%|      | 314/785 [02:01<02:03,  3.81it/s]
100%|| 435/435 [00:09<00:00, 44.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:58:41,943 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-27 23:58:42,092 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:58:44,813 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:58:44,900 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:58:44,935 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-314/special_tokens_map.json
 40%|      | 315/785 [02:11<46:39,  5.96s/it] 40%|      | 316/785 [02:11<33:21,  4.27s/it] 40%|      | 317/785 [02:11<23:59,  3.08s/it] 41%|      | 318/785 [02:12<17:26,  2.24s/it] 41%|      | 319/785 [02:12<12:52,  1.66s/it] 41%|      | 320/785 [02:12<09:40,  1.25s/it] 41%|      | 321/785 [02:13<07:26,  1.04it/s] 41%|      | 322/785 [02:13<05:52,  1.31it/s] 41%|      | 323/785 [02:13<04:47,  1.61it/s] 41%|     | 324/785 [02:13<04:01,  1.91it/s] 41%|     | 325/785 [02:14<03:29,  2.20it/s] 42%|     | 326/785 [02:14<03:06,  2.46it/s] 42%|     | 327/785 [02:14<02:53,  2.65it/s] 42%|     | 328/785 [02:15<02:41,  2.83it/s] 42%|     | 329/785 [02:15<02:33,  2.98it/s] 42%|     | 330/785 [02:15<02:27,  3.09it/s] 42%|     | 331/785 [02:16<02:22,  3.18it/s] 42%|     | 332/785 [02:16<02:19,  3.24it/s] 42%|     | 333/785 [02:16<02:17,  3.29it/s] 43%|     | 334/785 [02:16<02:15,  3.32it/s] 43%|     | 335/785 [02:17<02:14,  3.34it/s] 43%|     | 336/785 [02:17<02:13,  3.36it/s] 43%|     | 337/785 [02:17<02:12,  3.37it/s] 43%|     | 338/785 [02:18<02:16,  3.28it/s] 43%|     | 339/785 [02:18<02:14,  3.31it/s] 43%|     | 340/785 [02:18<02:13,  3.33it/s] 43%|     | 341/785 [02:18<02:12,  3.35it/s] 44%|     | 342/785 [02:19<02:11,  3.36it/s] 44%|     | 343/785 [02:19<02:10,  3.38it/s] 44%|     | 344/785 [02:19<02:10,  3.38it/s] 44%|     | 345/785 [02:20<02:10,  3.38it/s] 44%|     | 346/785 [02:20<02:09,  3.39it/s] 44%|     | 347/785 [02:20<02:09,  3.39it/s] 44%|     | 348/785 [02:21<02:08,  3.39it/s] 44%|     | 349/785 [02:21<02:08,  3.39it/s] 45%|     | 350/785 [02:21<02:08,  3.39it/s] 45%|     | 351/785 [02:21<02:13,  3.25it/s] 45%|     | 352/785 [02:22<02:20,  3.07it/s] 45%|     | 353/785 [02:22<02:16,  3.16it/s] 45%|     | 354/785 [02:22<02:13,  3.23it/s] 45%|     | 355/785 [02:23<02:11,  3.28it/s] 45%|     | 356/785 [02:23<02:09,  3.31it/s] 45%|     | 357/785 [02:23<02:15,  3.16it/s] 46%|     | 358/785 [02:24<02:12,  3.23it/s] 46%|     | 359/785 [02:24<02:09,  3.28it/s] 46%|     | 360/785 [02:24<02:08,  3.31it/s] 46%|     | 361/785 [02:25<02:07,  3.33it/s] 46%|     | 362/785 [02:25<02:06,  3.35it/s] 46%|     | 363/785 [02:25<02:05,  3.36it/s] 46%|     | 364/785 [02:25<02:04,  3.37it/s] 46%|     | 365/785 [02:26<02:04,  3.38it/s] 47%|     | 366/785 [02:26<02:03,  3.38it/s] 47%|     | 367/785 [02:26<02:08,  3.26it/s] 47%|     | 368/785 [02:27<02:06,  3.30it/s] 47%|     | 369/785 [02:27<02:05,  3.33it/s] 47%|     | 370/785 [02:27<02:03,  3.35it/s] 47%|     | 371/785 [02:28<02:03,  3.36it/s] 47%|     | 372/785 [02:28<02:02,  3.37it/s] 48%|     | 373/785 [02:28<02:02,  3.38it/s] 48%|     | 374/785 [02:28<02:01,  3.38it/s] 48%|     | 375/785 [02:29<02:01,  3.38it/s] 48%|     | 376/785 [02:29<02:00,  3.38it/s] 48%|     | 377/785 [02:29<02:00,  3.38it/s] 48%|     | 378/785 [02:30<02:06,  3.22it/s] 48%|     | 379/785 [02:30<02:04,  3.27it/s] 48%|     | 380/785 [02:30<02:02,  3.31it/s] 49%|     | 381/785 [02:31<02:01,  3.33it/s] 49%|     | 382/785 [02:31<02:00,  3.35it/s] 49%|     | 383/785 [02:31<01:59,  3.36it/s] 49%|     | 384/785 [02:31<01:58,  3.37it/s] 49%|     | 385/785 [02:32<01:58,  3.38it/s] 49%|     | 386/785 [02:32<01:57,  3.38it/s] 49%|     | 387/785 [02:32<01:57,  3.38it/s] 49%|     | 388/785 [02:33<01:57,  3.39it/s] 50%|     | 389/785 [02:33<02:04,  3.19it/s] 50%|     | 390/785 [02:33<02:01,  3.25it/s] 50%|     | 391/785 [02:34<01:59,  3.29it/s] 50%|     | 392/785 [02:34<01:58,  3.32it/s] 50%|     | 393/785 [02:34<01:57,  3.34it/s] 50%|     | 394/785 [02:34<01:56,  3.35it/s] 50%|     | 395/785 [02:35<01:56,  3.36it/s] 50%|     | 396/785 [02:35<01:55,  3.37it/s] 51%|     | 397/785 [02:35<01:54,  3.38it/s] 51%|     | 398/785 [02:36<01:54,  3.38it/s] 51%|     | 399/785 [02:36<02:02,  3.15it/s] 51%|     | 400/785 [02:36<01:59,  3.22it/s] 51%|     | 401/785 [02:37<01:57,  3.27it/s] 51%|     | 402/785 [02:37<01:56,  3.30it/s] 51%|    | 403/785 [02:37<01:54,  3.32it/s] 51%|    | 404/785 [02:37<01:53,  3.34it/s] 52%|    | 405/785 [02:38<01:53,  3.36it/s] 52%|    | 406/785 [02:38<01:52,  3.36it/s] 52%|    | 407/785 [02:38<01:52,  3.37it/s] 52%|    | 408/785 [02:39<01:51,  3.37it/s] 52%|    | 409/785 [02:39<01:55,  3.27it/s] 52%|    | 410/785 [02:39<01:53,  3.30it/s] 52%|    | 411/785 [02:40<01:52,  3.32it/s] 52%|    | 412/785 [02:40<01:51,  3.34it/s] 53%|    | 413/785 [02:40<01:50,  3.36it/s] 53%|    | 414/785 [02:40<01:50,  3.36it/s] 53%|    | 415/785 [02:41<01:49,  3.37it/s] 53%|    | 416/785 [02:41<01:49,  3.37it/s] 53%|    | 417/785 [02:41<01:49,  3.38it/s] 53%|    | 418/785 [02:42<01:48,  3.38it/s] 53%|    | 419/785 [02:42<01:48,  3.38it/s] 54%|    | 420/785 [02:42<01:50,  3.31it/s] 54%|    | 421/785 [02:43<01:49,  3.33it/s] 54%|    | 422/785 [02:43<01:48,  3.35it/s] 54%|    | 423/785 [02:43<01:47,  3.36it/s] 54%|    | 424/785 [02:43<01:47,  3.36it/s] 54%|    | 425/785 [02:44<01:46,  3.37it/s] 54%|    | 426/785 [02:44<01:46,  3.37it/s] 54%|    | 427/785 [02:44<01:45,  3.38it/s] 55%|    | 428/785 [02:45<01:45,  3.38it/s] 55%|    | 429/785 [02:45<01:45,  3.38it/s] 55%|    | 430/785 [02:45<01:44,  3.38it/s] 55%|    | 431/785 [02:46<01:48,  3.25it/s] 55%|    | 432/785 [02:46<01:47,  3.29it/s] 55%|    | 433/785 [02:46<01:46,  3.31it/s] 55%|    | 434/785 [02:46<01:45,  3.33it/s] 55%|    | 435/785 [02:47<01:44,  3.35it/s] 56%|    | 436/785 [02:47<01:43,  3.36it/s] 56%|    | 437/785 [02:47<01:43,  3.37it/s] 56%|    | 438/785 [02:48<01:42,  3.37it/s] 56%|    | 439/785 [02:48<01:42,  3.37it/s] 56%|    | 440/785 [02:48<01:42,  3.38it/s] 56%|    | 441/785 [02:48<01:41,  3.38it/s] 56%|    | 442/785 [02:49<01:45,  3.26it/s] 56%|    | 443/785 [02:49<01:43,  3.30it/s] 57%|    | 444/785 [02:49<01:42,  3.32it/s] 57%|    | 445/785 [02:50<01:41,  3.34it/s] 57%|    | 446/785 [02:50<01:41,  3.35it/s] 57%|    | 447/785 [02:50<01:40,  3.36it/s] 57%|    | 448/785 [02:51<01:40,  3.37it/s] 57%|    | 449/785 [02:51<01:39,  3.37it/s] 57%|    | 450/785 [02:51<01:39,  3.38it/s] 57%|    | 451/785 [02:51<01:38,  3.38it/s] 58%|    | 452/785 [02:52<01:38,  3.38it/s] 58%|    | 453/785 [02:52<01:38,  3.38it/s] 58%|    | 454/785 [02:52<01:37,  3.38it/s] 58%|    | 455/785 [02:53<01:37,  3.38it/s] 58%|    | 456/785 [02:53<01:37,  3.38it/s] 58%|    | 457/785 [02:53<01:37,  3.38it/s] 58%|    | 458/785 [02:54<01:36,  3.38it/s] 58%|    | 459/785 [02:54<01:39,  3.29it/s] 59%|    | 460/785 [02:54<01:37,  3.32it/s] 59%|    | 461/785 [02:54<01:37,  3.33it/s] 59%|    | 462/785 [02:55<01:36,  3.35it/s] 59%|    | 463/785 [02:55<01:36,  3.35it/s] 59%|    | 464/785 [02:55<01:35,  3.36it/s] 59%|    | 465/785 [02:56<01:34,  3.37it/s] 59%|    | 466/785 [02:56<01:34,  3.37it/s] 59%|    | 467/785 [02:56<01:34,  3.37it/s] 60%|    | 468/785 [02:57<01:33,  3.37it/s] 60%|    | 469/785 [02:57<01:33,  3.38it/s] 60%|    | 470/785 [02:57<01:37,  3.24it/s] 60%|    | 471/785 [02:57<01:26,  3.63it/s][INFO|trainer.py:2140] 2023-08-27 23:59:37,842 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:59:37,842 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-27 23:59:37,843 >>   Batch size = 8
{'eval_loss': 0.9264002442359924, 'eval_runtime': 9.8023, 'eval_samples_per_second': 354.713, 'eval_steps_per_second': 44.377, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.67it/s][A
  3%|         | 12/435 [00:00<00:08, 48.64it/s][A
  4%|         | 17/435 [00:00<00:08, 47.22it/s][A
  5%|         | 22/435 [00:00<00:08, 46.26it/s][A
  6%|         | 27/435 [00:00<00:08, 45.66it/s][A
  7%|         | 32/435 [00:00<00:08, 45.29it/s][A
  9%|         | 37/435 [00:00<00:08, 44.97it/s][A
 10%|         | 42/435 [00:00<00:08, 44.68it/s][A
 11%|         | 47/435 [00:01<00:08, 44.58it/s][A
 12%|        | 52/435 [00:01<00:08, 44.59it/s][A
 13%|        | 57/435 [00:01<00:08, 44.66it/s][A
 14%|        | 62/435 [00:01<00:08, 44.55it/s][A
 15%|        | 67/435 [00:01<00:08, 44.55it/s][A
 17%|        | 72/435 [00:01<00:08, 44.46it/s][A
 18%|        | 77/435 [00:01<00:08, 44.30it/s][A
 19%|        | 82/435 [00:01<00:07, 44.31it/s][A
 20%|        | 87/435 [00:01<00:07, 44.07it/s][A
 21%|        | 92/435 [00:02<00:07, 44.33it/s][A
 22%|       | 97/435 [00:02<00:07, 44.53it/s][A
 23%|       | 102/435 [00:02<00:07, 44.64it/s][A
 25%|       | 107/435 [00:02<00:07, 44.81it/s][A
 26%|       | 112/435 [00:02<00:07, 44.82it/s][A
 27%|       | 117/435 [00:02<00:07, 41.61it/s][A
 28%|       | 122/435 [00:02<00:07, 42.62it/s][A
 29%|       | 127/435 [00:02<00:07, 43.24it/s][A
 30%|       | 132/435 [00:02<00:06, 43.56it/s][A
 31%|      | 137/435 [00:03<00:06, 43.82it/s][A
 33%|      | 142/435 [00:03<00:06, 44.11it/s][A
 34%|      | 147/435 [00:03<00:06, 44.34it/s][A
 35%|      | 152/435 [00:03<00:06, 44.52it/s][A
 36%|      | 157/435 [00:03<00:06, 44.29it/s][A
 37%|      | 162/435 [00:03<00:06, 44.41it/s][A
 38%|      | 167/435 [00:03<00:06, 44.55it/s][A
 40%|      | 172/435 [00:03<00:05, 44.67it/s][A
 41%|      | 177/435 [00:03<00:05, 44.70it/s][A
 42%|     | 182/435 [00:04<00:05, 44.57it/s][A
 43%|     | 187/435 [00:04<00:05, 44.73it/s][A
 44%|     | 192/435 [00:04<00:05, 44.66it/s][A
 45%|     | 197/435 [00:04<00:05, 44.61it/s][A
 46%|     | 202/435 [00:04<00:05, 44.40it/s][A
 48%|     | 207/435 [00:04<00:05, 44.43it/s][A
 49%|     | 212/435 [00:04<00:05, 44.57it/s][A
 50%|     | 217/435 [00:04<00:04, 44.69it/s][A
 51%|     | 222/435 [00:04<00:04, 44.72it/s][A
 52%|    | 227/435 [00:05<00:04, 44.75it/s][A
 53%|    | 232/435 [00:05<00:04, 44.74it/s][A
 54%|    | 237/435 [00:05<00:04, 44.77it/s][A
 56%|    | 242/435 [00:05<00:04, 44.68it/s][A
 57%|    | 247/435 [00:05<00:04, 44.49it/s][A
 58%|    | 252/435 [00:05<00:04, 44.18it/s][A
 59%|    | 257/435 [00:05<00:04, 44.38it/s][A
 60%|    | 262/435 [00:05<00:03, 44.52it/s][A
 61%|   | 267/435 [00:05<00:03, 44.61it/s][A
 63%|   | 272/435 [00:06<00:03, 44.64it/s][A
 64%|   | 277/435 [00:06<00:03, 44.63it/s][A
 65%|   | 282/435 [00:06<00:03, 44.58it/s][A
 66%|   | 287/435 [00:06<00:03, 44.50it/s][A
 67%|   | 292/435 [00:06<00:03, 44.39it/s][A
 68%|   | 297/435 [00:06<00:03, 44.43it/s][A
 69%|   | 302/435 [00:06<00:02, 44.65it/s][A
 71%|   | 307/435 [00:06<00:02, 44.68it/s][A
 72%|  | 312/435 [00:06<00:02, 44.77it/s][A
 73%|  | 317/435 [00:07<00:02, 44.72it/s][A
 74%|  | 322/435 [00:07<00:02, 44.70it/s][A
 75%|  | 327/435 [00:07<00:02, 44.64it/s][A
 76%|  | 332/435 [00:07<00:02, 44.54it/s][A
 77%|  | 337/435 [00:07<00:02, 44.37it/s][A
 79%|  | 342/435 [00:07<00:02, 44.43it/s][A
 80%|  | 347/435 [00:07<00:01, 44.53it/s][A
 81%|  | 352/435 [00:07<00:01, 44.65it/s][A
 82%| | 357/435 [00:08<00:01, 44.75it/s][A
 83%| | 362/435 [00:08<00:01, 44.70it/s][A
 84%| | 367/435 [00:08<00:01, 44.73it/s][A
 86%| | 372/435 [00:08<00:01, 44.75it/s][A
 87%| | 377/435 [00:08<00:01, 44.51it/s][A
 88%| | 382/435 [00:08<00:01, 44.51it/s][A
 89%| | 387/435 [00:08<00:01, 44.04it/s][A
 90%| | 392/435 [00:08<00:00, 44.29it/s][A
 91%|| 397/435 [00:08<00:00, 44.42it/s][A
 92%|| 402/435 [00:09<00:00, 44.53it/s][A
 94%|| 407/435 [00:09<00:00, 44.63it/s][A
 95%|| 412/435 [00:09<00:00, 44.61it/s][A
 96%|| 417/435 [00:09<00:00, 44.66it/s][A
 97%|| 422/435 [00:09<00:00, 44.47it/s][A
 98%|| 427/435 [00:09<00:00, 44.41it/s][A
 99%|| 432/435 [00:09<00:00, 44.45it/s][A                                                 
                                                 [A 60%|    | 471/785 [03:07<01:26,  3.63it/s]
100%|| 435/435 [00:09<00:00, 44.45it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:59:47,715 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-27 23:59:47,844 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:59:50,699 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:59:50,841 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:59:50,889 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-471/special_tokens_map.json
 60%|    | 472/785 [03:18<34:04,  6.53s/it] 60%|    | 473/785 [03:19<24:16,  4.67s/it] 60%|    | 474/785 [03:19<17:23,  3.36s/it] 61%|    | 475/785 [03:19<12:35,  2.44s/it] 61%|    | 476/785 [03:20<09:13,  1.79s/it] 61%|    | 477/785 [03:20<06:53,  1.34s/it] 61%|    | 478/785 [03:20<05:15,  1.03s/it] 61%|    | 479/785 [03:21<04:06,  1.24it/s] 61%|    | 480/785 [03:21<03:18,  1.54it/s] 61%|   | 481/785 [03:21<02:45,  1.84it/s] 61%|   | 482/785 [03:22<02:43,  1.85it/s] 62%|   | 483/785 [03:22<02:40,  1.89it/s] 62%|   | 484/785 [03:22<02:17,  2.18it/s] 62%|   | 485/785 [03:23<02:02,  2.45it/s] 62%|   | 486/785 [03:23<01:51,  2.68it/s] 62%|   | 487/785 [03:23<01:43,  2.87it/s] 62%|   | 488/785 [03:24<01:38,  3.02it/s] 62%|   | 489/785 [03:24<01:34,  3.14it/s] 62%|   | 490/785 [03:24<01:31,  3.22it/s] 63%|   | 491/785 [03:25<01:32,  3.18it/s] 63%|   | 492/785 [03:25<01:29,  3.26it/s] 63%|   | 493/785 [03:25<01:28,  3.31it/s] 63%|   | 494/785 [03:25<01:26,  3.35it/s] 63%|   | 495/785 [03:26<01:25,  3.37it/s] 63%|   | 496/785 [03:26<01:25,  3.39it/s] 63%|   | 497/785 [03:26<01:24,  3.41it/s] 63%|   | 498/785 [03:27<01:24,  3.41it/s] 64%|   | 499/785 [03:27<01:23,  3.42it/s] 64%|   | 500/785 [03:27<01:23,  3.43it/s]                                                  64%|   | 500/785 [03:27<01:23,  3.43it/s] 64%|   | 501/785 [03:27<01:22,  3.43it/s] 64%|   | 502/785 [03:28<01:25,  3.32it/s] 64%|   | 503/785 [03:28<01:24,  3.35it/s] 64%|   | 504/785 [03:28<01:23,  3.38it/s] 64%|   | 505/785 [03:29<01:22,  3.40it/s] 64%|   | 506/785 [03:29<01:21,  3.41it/s] 65%|   | 507/785 [03:29<01:21,  3.42it/s] 65%|   | 508/785 [03:30<01:20,  3.42it/s] 65%|   | 509/785 [03:30<01:20,  3.43it/s] 65%|   | 510/785 [03:30<01:20,  3.43it/s] 65%|   | 511/785 [03:30<01:19,  3.43it/s] 65%|   | 512/785 [03:31<01:19,  3.44it/s] 65%|   | 513/785 [03:31<01:25,  3.19it/s] 65%|   | 514/785 [03:31<01:23,  3.26it/s] 66%|   | 515/785 [03:32<01:21,  3.31it/s] 66%|   | 516/785 [03:32<01:20,  3.35it/s] 66%|   | 517/785 [03:32<01:19,  3.38it/s] 66%|   | 518/785 [03:33<01:18,  3.40it/s] 66%|   | 519/785 [03:33<01:17,  3.41it/s] 66%|   | 520/785 [03:33<01:17,  3.42it/s] 66%|   | 521/785 [03:33<01:17,  3.42it/s] 66%|   | 522/785 [03:34<01:16,  3.43it/s] 67%|   | 523/785 [03:34<01:16,  3.44it/s] 67%|   | 524/785 [03:34<01:19,  3.29it/s] 67%|   | 525/785 [03:35<01:18,  3.33it/s] 67%|   | 526/785 [03:35<01:16,  3.36it/s] 67%|   | 527/785 [03:35<01:16,  3.38it/s] 67%|   | 528/785 [03:35<01:15,  3.40it/s] 67%|   | 529/785 [03:36<01:15,  3.41it/s] 68%|   | 530/785 [03:36<01:14,  3.42it/s] 68%|   | 531/785 [03:36<01:14,  3.43it/s] 68%|   | 532/785 [03:37<01:13,  3.43it/s] 68%|   | 533/785 [03:37<01:13,  3.43it/s] 68%|   | 534/785 [03:37<01:13,  3.43it/s] 68%|   | 535/785 [03:38<01:14,  3.35it/s] 68%|   | 536/785 [03:38<01:13,  3.38it/s] 68%|   | 537/785 [03:38<01:13,  3.39it/s] 69%|   | 538/785 [03:38<01:12,  3.41it/s] 69%|   | 539/785 [03:39<01:12,  3.41it/s] 69%|   | 540/785 [03:39<01:11,  3.42it/s] 69%|   | 541/785 [03:39<01:11,  3.43it/s] 69%|   | 542/785 [03:40<01:10,  3.43it/s] 69%|   | 543/785 [03:40<01:10,  3.43it/s] 69%|   | 544/785 [03:40<01:10,  3.44it/s] 69%|   | 545/785 [03:40<01:09,  3.44it/s] 70%|   | 546/785 [03:41<01:11,  3.32it/s] 70%|   | 547/785 [03:41<01:10,  3.36it/s] 70%|   | 548/785 [03:41<01:10,  3.38it/s] 70%|   | 549/785 [03:42<01:09,  3.40it/s] 70%|   | 550/785 [03:42<01:08,  3.41it/s] 70%|   | 551/785 [03:42<01:08,  3.42it/s] 70%|   | 552/785 [03:42<01:08,  3.43it/s] 70%|   | 553/785 [03:43<01:07,  3.43it/s] 71%|   | 554/785 [03:43<01:07,  3.43it/s] 71%|   | 555/785 [03:43<01:07,  3.43it/s] 71%|   | 556/785 [03:44<01:06,  3.43it/s] 71%|   | 557/785 [03:44<01:07,  3.38it/s] 71%|   | 558/785 [03:44<01:06,  3.40it/s] 71%|   | 559/785 [03:45<01:06,  3.41it/s] 71%|  | 560/785 [03:45<01:05,  3.42it/s] 71%|  | 561/785 [03:45<01:05,  3.43it/s] 72%|  | 562/785 [03:45<01:04,  3.43it/s] 72%|  | 563/785 [03:46<01:04,  3.44it/s] 72%|  | 564/785 [03:46<01:04,  3.43it/s] 72%|  | 565/785 [03:46<01:03,  3.44it/s] 72%|  | 566/785 [03:47<01:03,  3.44it/s] 72%|  | 567/785 [03:47<01:03,  3.44it/s] 72%|  | 568/785 [03:47<01:06,  3.26it/s] 72%|  | 569/785 [03:48<01:05,  3.32it/s] 73%|  | 570/785 [03:48<01:04,  3.35it/s] 73%|  | 571/785 [03:48<01:03,  3.38it/s] 73%|  | 572/785 [03:48<01:02,  3.40it/s] 73%|  | 573/785 [03:49<01:02,  3.41it/s] 73%|  | 574/785 [03:49<01:01,  3.42it/s] 73%|  | 575/785 [03:49<01:01,  3.43it/s] 73%|  | 576/785 [03:50<01:00,  3.43it/s] 74%|  | 577/785 [03:50<01:00,  3.44it/s] 74%|  | 578/785 [03:50<01:00,  3.44it/s] 74%|  | 579/785 [03:50<01:02,  3.32it/s] 74%|  | 580/785 [03:51<01:01,  3.36it/s] 74%|  | 581/785 [03:51<01:00,  3.38it/s] 74%|  | 582/785 [03:51<00:59,  3.40it/s] 74%|  | 583/785 [03:52<00:59,  3.41it/s] 74%|  | 584/785 [03:52<00:58,  3.42it/s] 75%|  | 585/785 [03:52<00:58,  3.43it/s] 75%|  | 586/785 [03:52<00:57,  3.43it/s] 75%|  | 587/785 [03:53<00:57,  3.44it/s] 75%|  | 588/785 [03:53<00:57,  3.44it/s] 75%|  | 589/785 [03:53<00:56,  3.44it/s] 75%|  | 590/785 [03:54<00:56,  3.44it/s] 75%|  | 591/785 [03:54<00:56,  3.44it/s] 75%|  | 592/785 [03:54<00:56,  3.44it/s] 76%|  | 593/785 [03:55<00:55,  3.44it/s] 76%|  | 594/785 [03:55<00:55,  3.44it/s] 76%|  | 595/785 [03:55<00:55,  3.44it/s] 76%|  | 596/785 [03:55<00:56,  3.35it/s] 76%|  | 597/785 [03:56<00:55,  3.38it/s] 76%|  | 598/785 [03:56<00:55,  3.40it/s] 76%|  | 599/785 [03:56<00:54,  3.41it/s] 76%|  | 600/785 [03:57<00:54,  3.42it/s] 77%|  | 601/785 [03:57<00:53,  3.42it/s] 77%|  | 602/785 [03:57<00:53,  3.43it/s] 77%|  | 603/785 [03:57<00:53,  3.43it/s] 77%|  | 604/785 [03:58<00:52,  3.43it/s] 77%|  | 605/785 [03:58<00:52,  3.44it/s] 77%|  | 606/785 [03:58<00:52,  3.44it/s] 77%|  | 607/785 [03:59<00:52,  3.39it/s] 77%|  | 608/785 [03:59<00:52,  3.40it/s] 78%|  | 609/785 [03:59<00:51,  3.41it/s] 78%|  | 610/785 [03:59<00:51,  3.42it/s] 78%|  | 611/785 [04:00<00:50,  3.43it/s] 78%|  | 612/785 [04:00<00:50,  3.43it/s] 78%|  | 613/785 [04:00<00:50,  3.44it/s] 78%|  | 614/785 [04:01<00:49,  3.44it/s] 78%|  | 615/785 [04:01<00:49,  3.44it/s] 78%|  | 616/785 [04:01<00:49,  3.44it/s] 79%|  | 617/785 [04:02<00:48,  3.44it/s] 79%|  | 618/785 [04:02<00:49,  3.37it/s] 79%|  | 619/785 [04:02<00:49,  3.39it/s] 79%|  | 620/785 [04:02<00:48,  3.40it/s] 79%|  | 621/785 [04:03<00:48,  3.41it/s] 79%|  | 622/785 [04:03<00:47,  3.42it/s] 79%|  | 623/785 [04:03<00:47,  3.42it/s] 79%|  | 624/785 [04:04<00:47,  3.42it/s] 80%|  | 625/785 [04:04<00:46,  3.43it/s] 80%|  | 626/785 [04:04<00:46,  3.43it/s] 80%|  | 627/785 [04:04<00:46,  3.43it/s] 80%|  | 628/785 [04:05<00:41,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 00:00:45,175 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:00:45,175 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 00:00:45,175 >>   Batch size = 8
{'eval_loss': 0.9308773875236511, 'eval_runtime': 9.7798, 'eval_samples_per_second': 355.528, 'eval_steps_per_second': 44.479, 'epoch': 3.0}
{'loss': 0.8242, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.74it/s][A
  3%|         | 12/435 [00:00<00:08, 48.77it/s][A
  4%|         | 17/435 [00:00<00:08, 47.02it/s][A
  5%|         | 22/435 [00:00<00:08, 46.10it/s][A
  6%|         | 27/435 [00:00<00:08, 45.60it/s][A
  7%|         | 32/435 [00:00<00:08, 45.25it/s][A
  9%|         | 37/435 [00:00<00:08, 45.12it/s][A
 10%|         | 42/435 [00:00<00:08, 44.72it/s][A
 11%|         | 47/435 [00:01<00:08, 44.38it/s][A
 12%|        | 52/435 [00:01<00:08, 44.54it/s][A
 13%|        | 57/435 [00:01<00:08, 44.53it/s][A
 14%|        | 62/435 [00:01<00:08, 44.60it/s][A
 15%|        | 67/435 [00:01<00:08, 44.63it/s][A
 17%|        | 72/435 [00:01<00:08, 44.67it/s][A
 18%|        | 77/435 [00:01<00:07, 44.75it/s][A
 19%|        | 82/435 [00:01<00:07, 44.79it/s][A
 20%|        | 87/435 [00:01<00:07, 44.54it/s][A
 21%|        | 92/435 [00:02<00:07, 44.54it/s][A
 22%|       | 97/435 [00:02<00:07, 44.53it/s][A
 23%|       | 102/435 [00:02<00:07, 44.62it/s][A
 25%|       | 107/435 [00:02<00:07, 44.63it/s][A
 26%|       | 112/435 [00:02<00:07, 44.60it/s][A
 27%|       | 117/435 [00:02<00:07, 44.74it/s][A
 28%|       | 122/435 [00:02<00:06, 44.72it/s][A
 29%|       | 127/435 [00:02<00:06, 44.75it/s][A
 30%|       | 132/435 [00:02<00:06, 44.63it/s][A
 31%|      | 137/435 [00:03<00:06, 44.60it/s][A
 33%|      | 142/435 [00:03<00:06, 42.72it/s][A
 34%|      | 147/435 [00:03<00:06, 43.34it/s][A
 35%|      | 152/435 [00:03<00:06, 43.79it/s][A
 36%|      | 157/435 [00:03<00:06, 44.05it/s][A
 37%|      | 162/435 [00:03<00:06, 44.29it/s][A
 38%|      | 167/435 [00:03<00:06, 44.46it/s][A
 40%|      | 172/435 [00:03<00:05, 44.50it/s][A
 41%|      | 177/435 [00:03<00:05, 44.56it/s][A
 42%|     | 182/435 [00:04<00:05, 44.31it/s][A
 43%|     | 187/435 [00:04<00:05, 44.45it/s][A
 44%|     | 192/435 [00:04<00:05, 44.53it/s][A
 45%|     | 197/435 [00:04<00:05, 44.72it/s][A
 46%|     | 202/435 [00:04<00:05, 44.72it/s][A
 48%|     | 207/435 [00:04<00:05, 44.71it/s][A
 49%|     | 212/435 [00:04<00:04, 44.65it/s][A
 50%|     | 217/435 [00:04<00:04, 44.65it/s][A
 51%|     | 222/435 [00:04<00:04, 44.46it/s][A
 52%|    | 227/435 [00:05<00:04, 44.30it/s][A
 53%|    | 232/435 [00:05<00:04, 44.39it/s][A
 54%|    | 237/435 [00:05<00:04, 44.51it/s][A
 56%|    | 242/435 [00:05<00:04, 44.58it/s][A
 57%|    | 247/435 [00:05<00:04, 44.64it/s][A
 58%|    | 252/435 [00:05<00:04, 44.82it/s][A
 59%|    | 257/435 [00:05<00:03, 44.77it/s][A
 60%|    | 262/435 [00:05<00:03, 44.70it/s][A
 61%|   | 267/435 [00:05<00:03, 44.57it/s][A
 63%|   | 272/435 [00:06<00:03, 44.42it/s][A
 64%|   | 277/435 [00:06<00:03, 44.46it/s][A
 65%|   | 282/435 [00:06<00:03, 44.45it/s][A
 66%|   | 287/435 [00:06<00:03, 44.61it/s][A
 67%|   | 292/435 [00:06<00:03, 44.65it/s][A
 68%|   | 297/435 [00:06<00:03, 44.69it/s][A
 69%|   | 302/435 [00:06<00:02, 44.78it/s][A
 71%|   | 307/435 [00:06<00:02, 44.74it/s][A
 72%|  | 312/435 [00:06<00:02, 44.51it/s][A
 73%|  | 317/435 [00:07<00:02, 44.52it/s][A
 74%|  | 322/435 [00:07<00:02, 44.36it/s][A
 75%|  | 327/435 [00:07<00:02, 44.44it/s][A
 76%|  | 332/435 [00:07<00:02, 44.57it/s][A
 77%|  | 337/435 [00:07<00:02, 44.57it/s][A
 79%|  | 342/435 [00:07<00:02, 44.67it/s][A
 80%|  | 347/435 [00:07<00:01, 44.76it/s][A
 81%|  | 352/435 [00:07<00:01, 44.78it/s][A
 82%| | 357/435 [00:07<00:01, 44.74it/s][A
 83%| | 362/435 [00:08<00:01, 44.60it/s][A
 84%| | 367/435 [00:08<00:01, 44.56it/s][A
 86%| | 372/435 [00:08<00:01, 44.59it/s][A
 87%| | 377/435 [00:08<00:01, 44.66it/s][A
 88%| | 382/435 [00:08<00:01, 44.67it/s][A
 89%| | 387/435 [00:08<00:01, 44.71it/s][A
 90%| | 392/435 [00:08<00:00, 44.72it/s][A
 91%|| 397/435 [00:08<00:00, 44.77it/s][A
 92%|| 402/435 [00:08<00:00, 44.60it/s][A
 94%|| 407/435 [00:09<00:00, 44.49it/s][A
 95%|| 412/435 [00:09<00:00, 42.32it/s][A
 96%|| 417/435 [00:09<00:00, 43.19it/s][A
 97%|| 422/435 [00:09<00:00, 43.70it/s][A
 98%|| 427/435 [00:09<00:00, 44.08it/s][A
 99%|| 432/435 [00:09<00:00, 44.31it/s][A                                                 
                                                 [A 80%|  | 628/785 [04:14<00:41,  3.81it/s]
100%|| 435/435 [00:09<00:00, 44.31it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:00:55,106 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 00:00:55,259 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:00:58,037 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:00:58,150 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:00:58,231 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-628/special_tokens_map.json
 80%|  | 629/785 [04:24<15:43,  6.05s/it] 80%|  | 630/785 [04:25<11:09,  4.32s/it] 80%|  | 631/785 [04:25<07:59,  3.11s/it] 81%|  | 632/785 [04:25<05:47,  2.27s/it] 81%|  | 633/785 [04:25<04:14,  1.68s/it] 81%|  | 634/785 [04:26<03:10,  1.26s/it] 81%|  | 635/785 [04:26<02:27,  1.02it/s] 81%|  | 636/785 [04:26<01:55,  1.29it/s] 81%|  | 637/785 [04:27<01:33,  1.58it/s] 81%| | 638/785 [04:27<01:18,  1.88it/s] 81%| | 639/785 [04:27<01:07,  2.17it/s] 82%| | 640/785 [04:27<00:59,  2.43it/s] 82%| | 641/785 [04:28<00:54,  2.66it/s] 82%| | 642/785 [04:28<00:50,  2.85it/s] 82%| | 643/785 [04:28<00:47,  3.00it/s] 82%| | 644/785 [04:29<00:45,  3.13it/s] 82%| | 645/785 [04:29<00:43,  3.21it/s] 82%| | 646/785 [04:29<00:45,  3.08it/s] 82%| | 647/785 [04:30<00:43,  3.19it/s] 83%| | 648/785 [04:30<00:42,  3.26it/s] 83%| | 649/785 [04:30<00:41,  3.31it/s] 83%| | 650/785 [04:30<00:40,  3.35it/s] 83%| | 651/785 [04:31<00:39,  3.38it/s] 83%| | 652/785 [04:31<00:39,  3.40it/s] 83%| | 653/785 [04:31<00:38,  3.41it/s] 83%| | 654/785 [04:32<00:38,  3.42it/s] 83%| | 655/785 [04:32<00:37,  3.43it/s] 84%| | 656/785 [04:32<00:37,  3.43it/s] 84%| | 657/785 [04:33<00:39,  3.26it/s] 84%| | 658/785 [04:33<00:38,  3.32it/s] 84%| | 659/785 [04:33<00:37,  3.35it/s] 84%| | 660/785 [04:33<00:37,  3.38it/s] 84%| | 661/785 [04:34<00:36,  3.40it/s] 84%| | 662/785 [04:34<00:36,  3.41it/s] 84%| | 663/785 [04:34<00:35,  3.42it/s] 85%| | 664/785 [04:35<00:35,  3.43it/s] 85%| | 665/785 [04:35<00:34,  3.43it/s] 85%| | 666/785 [04:35<00:34,  3.44it/s] 85%| | 667/785 [04:35<00:34,  3.44it/s] 85%| | 668/785 [04:36<00:34,  3.39it/s] 85%| | 669/785 [04:36<00:34,  3.41it/s] 85%| | 670/785 [04:36<00:33,  3.42it/s] 85%| | 671/785 [04:37<00:33,  3.43it/s] 86%| | 672/785 [04:37<00:32,  3.43it/s] 86%| | 673/785 [04:37<00:32,  3.43it/s] 86%| | 674/785 [04:38<00:32,  3.44it/s] 86%| | 675/785 [04:38<00:31,  3.44it/s] 86%| | 676/785 [04:38<00:31,  3.44it/s] 86%| | 677/785 [04:38<00:31,  3.44it/s] 86%| | 678/785 [04:39<00:31,  3.44it/s] 86%| | 679/785 [04:39<00:31,  3.38it/s] 87%| | 680/785 [04:39<00:30,  3.40it/s] 87%| | 681/785 [04:40<00:30,  3.41it/s] 87%| | 682/785 [04:40<00:30,  3.42it/s] 87%| | 683/785 [04:40<00:29,  3.43it/s] 87%| | 684/785 [04:40<00:29,  3.43it/s] 87%| | 685/785 [04:41<00:29,  3.44it/s] 87%| | 686/785 [04:41<00:28,  3.44it/s] 88%| | 687/785 [04:41<00:28,  3.44it/s] 88%| | 688/785 [04:42<00:28,  3.44it/s] 88%| | 689/785 [04:42<00:27,  3.44it/s] 88%| | 690/785 [04:42<00:28,  3.36it/s] 88%| | 691/785 [04:42<00:27,  3.39it/s] 88%| | 692/785 [04:43<00:27,  3.41it/s] 88%| | 693/785 [04:43<00:26,  3.42it/s] 88%| | 694/785 [04:43<00:26,  3.42it/s] 89%| | 695/785 [04:44<00:26,  3.43it/s] 89%| | 696/785 [04:44<00:25,  3.43it/s] 89%| | 697/785 [04:44<00:25,  3.43it/s] 89%| | 698/785 [04:45<00:25,  3.44it/s] 89%| | 699/785 [04:45<00:25,  3.44it/s] 89%| | 700/785 [04:45<00:24,  3.44it/s] 89%| | 701/785 [04:45<00:25,  3.36it/s] 89%| | 702/785 [04:46<00:24,  3.38it/s] 90%| | 703/785 [04:46<00:24,  3.40it/s] 90%| | 704/785 [04:46<00:23,  3.41it/s] 90%| | 705/785 [04:47<00:23,  3.42it/s] 90%| | 706/785 [04:47<00:23,  3.42it/s] 90%| | 707/785 [04:47<00:22,  3.43it/s] 90%| | 708/785 [04:47<00:22,  3.43it/s] 90%| | 709/785 [04:48<00:22,  3.43it/s] 90%| | 710/785 [04:48<00:21,  3.43it/s] 91%| | 711/785 [04:48<00:21,  3.44it/s] 91%| | 712/785 [04:49<00:22,  3.26it/s] 91%| | 713/785 [04:49<00:21,  3.32it/s] 91%| | 714/785 [04:49<00:21,  3.35it/s] 91%| | 715/785 [04:50<00:20,  3.38it/s] 91%| | 716/785 [04:50<00:20,  3.40it/s] 91%|| 717/785 [04:50<00:19,  3.41it/s] 91%|| 718/785 [04:50<00:19,  3.42it/s] 92%|| 719/785 [04:51<00:20,  3.29it/s] 92%|| 720/785 [04:51<00:19,  3.33it/s] 92%|| 721/785 [04:51<00:19,  3.37it/s] 92%|| 722/785 [04:52<00:19,  3.25it/s] 92%|| 723/785 [04:52<00:18,  3.30it/s] 92%|| 724/785 [04:52<00:18,  3.34it/s] 92%|| 725/785 [04:53<00:17,  3.36it/s] 92%|| 726/785 [04:53<00:17,  3.39it/s] 93%|| 727/785 [04:53<00:17,  3.40it/s] 93%|| 728/785 [04:53<00:16,  3.41it/s] 93%|| 729/785 [04:54<00:17,  3.17it/s] 93%|| 730/785 [04:54<00:16,  3.25it/s] 93%|| 731/785 [04:54<00:16,  3.30it/s] 93%|| 732/785 [04:55<00:15,  3.34it/s] 93%|| 733/785 [04:55<00:15,  3.37it/s] 94%|| 734/785 [04:55<00:15,  3.39it/s] 94%|| 735/785 [04:56<00:14,  3.41it/s] 94%|| 736/785 [04:56<00:14,  3.42it/s] 94%|| 737/785 [04:56<00:14,  3.42it/s] 94%|| 738/785 [04:56<00:13,  3.43it/s] 94%|| 739/785 [04:57<00:14,  3.22it/s] 94%|| 740/785 [04:57<00:13,  3.28it/s] 94%|| 741/785 [04:57<00:13,  3.33it/s] 95%|| 742/785 [04:58<00:12,  3.36it/s] 95%|| 743/785 [04:58<00:12,  3.38it/s] 95%|| 744/785 [04:58<00:12,  3.40it/s] 95%|| 745/785 [04:58<00:11,  3.41it/s] 95%|| 746/785 [04:59<00:11,  3.42it/s] 95%|| 747/785 [04:59<00:11,  3.43it/s] 95%|| 748/785 [04:59<00:10,  3.43it/s] 95%|| 749/785 [05:00<00:10,  3.43it/s] 96%|| 750/785 [05:00<00:10,  3.31it/s] 96%|| 751/785 [05:00<00:10,  3.35it/s] 96%|| 752/785 [05:01<00:09,  3.37it/s] 96%|| 753/785 [05:01<00:09,  3.39it/s] 96%|| 754/785 [05:01<00:09,  3.40it/s] 96%|| 755/785 [05:01<00:08,  3.42it/s] 96%|| 756/785 [05:02<00:08,  3.42it/s] 96%|| 757/785 [05:02<00:08,  3.43it/s] 97%|| 758/785 [05:02<00:07,  3.43it/s] 97%|| 759/785 [05:03<00:07,  3.44it/s] 97%|| 760/785 [05:03<00:07,  3.44it/s] 97%|| 761/785 [05:03<00:07,  3.36it/s] 97%|| 762/785 [05:03<00:06,  3.38it/s] 97%|| 763/785 [05:04<00:06,  3.40it/s] 97%|| 764/785 [05:04<00:06,  3.41it/s] 97%|| 765/785 [05:04<00:05,  3.42it/s] 98%|| 766/785 [05:05<00:05,  3.43it/s] 98%|| 767/785 [05:05<00:05,  3.43it/s] 98%|| 768/785 [05:05<00:04,  3.44it/s] 98%|| 769/785 [05:06<00:04,  3.44it/s] 98%|| 770/785 [05:06<00:04,  3.44it/s] 98%|| 771/785 [05:06<00:04,  3.44it/s] 98%|| 772/785 [05:06<00:03,  3.32it/s] 98%|| 773/785 [05:07<00:03,  3.36it/s] 99%|| 774/785 [05:07<00:03,  3.38it/s] 99%|| 775/785 [05:07<00:02,  3.40it/s] 99%|| 776/785 [05:08<00:02,  3.41it/s] 99%|| 777/785 [05:08<00:02,  3.42it/s] 99%|| 778/785 [05:08<00:02,  3.43it/s] 99%|| 779/785 [05:08<00:01,  3.43it/s] 99%|| 780/785 [05:09<00:01,  3.43it/s] 99%|| 781/785 [05:09<00:01,  3.43it/s]100%|| 782/785 [05:09<00:00,  3.44it/s]100%|| 783/785 [05:10<00:00,  3.29it/s]100%|| 784/785 [05:10<00:00,  3.33it/s]100%|| 785/785 [05:10<00:00,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 00:01:50,620 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:01:50,620 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 00:01:50,620 >>   Batch size = 8
{'eval_loss': 0.9392514228820801, 'eval_runtime': 9.7762, 'eval_samples_per_second': 355.658, 'eval_steps_per_second': 44.496, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.33it/s][A
  3%|         | 12/435 [00:00<00:08, 48.68it/s][A
  4%|         | 17/435 [00:00<00:08, 47.13it/s][A
  5%|         | 22/435 [00:00<00:08, 46.33it/s][A
  6%|         | 27/435 [00:00<00:08, 45.68it/s][A
  7%|         | 32/435 [00:00<00:08, 45.33it/s][A
  9%|         | 37/435 [00:00<00:08, 44.90it/s][A
 10%|         | 42/435 [00:00<00:08, 44.67it/s][A
 11%|         | 47/435 [00:01<00:08, 44.71it/s][A
 12%|        | 52/435 [00:01<00:08, 44.70it/s][A
 13%|        | 57/435 [00:01<00:08, 44.74it/s][A
 14%|        | 62/435 [00:01<00:08, 44.71it/s][A
 15%|        | 67/435 [00:01<00:08, 44.80it/s][A
 17%|        | 72/435 [00:01<00:08, 44.69it/s][A
 18%|        | 77/435 [00:01<00:08, 44.64it/s][A
 19%|        | 82/435 [00:01<00:07, 44.50it/s][A
 20%|        | 87/435 [00:01<00:07, 44.29it/s][A
 21%|        | 92/435 [00:02<00:07, 44.33it/s][A
 22%|       | 97/435 [00:02<00:07, 44.34it/s][A
 23%|       | 102/435 [00:02<00:07, 44.33it/s][A
 25%|       | 107/435 [00:02<00:07, 42.63it/s][A
 26%|       | 112/435 [00:02<00:07, 43.18it/s][A
 27%|       | 117/435 [00:02<00:07, 43.75it/s][A
 28%|       | 122/435 [00:02<00:07, 44.07it/s][A
 29%|       | 127/435 [00:02<00:06, 44.25it/s][A
 30%|       | 132/435 [00:02<00:06, 44.42it/s][A
 31%|      | 137/435 [00:03<00:06, 44.44it/s][A
 33%|      | 142/435 [00:03<00:06, 44.49it/s][A
 34%|      | 147/435 [00:03<00:06, 44.26it/s][A
 35%|      | 152/435 [00:03<00:06, 44.47it/s][A
 36%|      | 157/435 [00:03<00:06, 44.60it/s][A
 37%|      | 162/435 [00:03<00:06, 44.61it/s][A
 38%|      | 167/435 [00:03<00:06, 44.61it/s][A
 40%|      | 172/435 [00:03<00:05, 44.58it/s][A
 41%|      | 177/435 [00:03<00:05, 44.57it/s][A
 42%|     | 182/435 [00:04<00:05, 44.55it/s][A
 43%|     | 187/435 [00:04<00:05, 44.48it/s][A
 44%|     | 192/435 [00:04<00:05, 44.51it/s][A
 45%|     | 197/435 [00:04<00:05, 44.55it/s][A
 46%|     | 202/435 [00:04<00:05, 44.58it/s][A
 48%|     | 207/435 [00:04<00:05, 44.59it/s][A
 49%|     | 212/435 [00:04<00:05, 44.58it/s][A
 50%|     | 217/435 [00:04<00:04, 44.69it/s][A
 51%|     | 222/435 [00:04<00:04, 44.68it/s][A
 52%|    | 227/435 [00:05<00:04, 44.68it/s][A
 53%|    | 232/435 [00:05<00:04, 44.58it/s][A
 54%|    | 237/435 [00:05<00:04, 44.47it/s][A
 56%|    | 242/435 [00:05<00:04, 41.65it/s][A
 57%|    | 247/435 [00:05<00:04, 42.69it/s][A
 58%|    | 252/435 [00:05<00:04, 43.37it/s][A
 59%|    | 257/435 [00:05<00:04, 43.85it/s][A
 60%|    | 262/435 [00:05<00:03, 44.21it/s][A
 61%|   | 267/435 [00:05<00:03, 44.39it/s][A
 63%|   | 272/435 [00:06<00:03, 44.32it/s][A
 64%|   | 277/435 [00:06<00:03, 44.44it/s][A
 65%|   | 282/435 [00:06<00:03, 44.13it/s][A
 66%|   | 287/435 [00:06<00:03, 44.19it/s][A
 67%|   | 292/435 [00:06<00:03, 44.36it/s][A
 68%|   | 297/435 [00:06<00:03, 44.53it/s][A
 69%|   | 302/435 [00:06<00:02, 44.70it/s][A
 71%|   | 307/435 [00:06<00:02, 44.80it/s][A
 72%|  | 312/435 [00:07<00:02, 44.83it/s][A
 73%|  | 317/435 [00:07<00:02, 44.82it/s][A
 74%|  | 322/435 [00:07<00:02, 44.66it/s][A
 75%|  | 327/435 [00:07<00:02, 44.52it/s][A
 76%|  | 332/435 [00:07<00:02, 44.41it/s][A
 77%|  | 337/435 [00:07<00:02, 44.37it/s][A
 79%|  | 342/435 [00:07<00:02, 44.50it/s][A
 80%|  | 347/435 [00:07<00:01, 44.75it/s][A
 81%|  | 352/435 [00:07<00:01, 44.82it/s][A
 82%| | 357/435 [00:08<00:01, 44.85it/s][A
 83%| | 362/435 [00:08<00:01, 44.81it/s][A
 84%| | 367/435 [00:08<00:01, 44.70it/s][A
 86%| | 372/435 [00:08<00:01, 42.56it/s][A
 87%| | 377/435 [00:08<00:01, 41.50it/s][A
 88%| | 382/435 [00:08<00:01, 42.53it/s][A
 89%| | 387/435 [00:08<00:01, 43.21it/s][A
 90%| | 392/435 [00:08<00:00, 43.78it/s][A
 91%|| 397/435 [00:08<00:00, 44.03it/s][A
 92%|| 402/435 [00:09<00:00, 44.37it/s][A
 94%|| 407/435 [00:09<00:00, 44.57it/s][A
 95%|| 412/435 [00:09<00:00, 44.46it/s][A
 96%|| 417/435 [00:09<00:00, 44.23it/s][A
 97%|| 422/435 [00:09<00:00, 44.26it/s][A
 98%|| 427/435 [00:09<00:00, 44.40it/s][A
 99%|| 432/435 [00:09<00:00, 44.56it/s][A                                                 
                                                 [A100%|| 785/785 [05:20<00:00,  3.73it/s]
100%|| 435/435 [00:09<00:00, 44.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:02:00,546 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 00:02:00,746 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:02:04,052 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:02:04,437 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:02:04,576 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:02:14,998 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:02:15,082 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-314 (score: 0.9264002442359924).
                                                 100%|| 785/785 [05:48<00:00,  3.73it/s]100%|| 785/785 [05:48<00:00,  2.25it/s]
[INFO|trainer.py:1894] 2023-08-28 00:02:28,382 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:02:28,587 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:02:33,367 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:02:33,589 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:02:33,743 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:02:34,279 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:34,279 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:34,280 >>   train_loss               =     0.8106
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:34,280 >>   train_runtime            = 0:05:48.34
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:34,280 >>   train_samples            =      10022
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:34,280 >>   train_samples_per_second =    143.852
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:34,280 >>   train_steps_per_second   =      2.254
{'eval_loss': 0.9410570859909058, 'eval_runtime': 9.8133, 'eval_samples_per_second': 354.314, 'eval_steps_per_second': 44.327, 'epoch': 5.0}
{'train_runtime': 348.345, 'train_samples_per_second': 143.852, 'train_steps_per_second': 2.254, 'train_loss': 0.810573893747512, 'epoch': 5.0}
08/28/2023 00:02:34 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:02:34,563 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:02:34,563 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 00:02:34,563 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 55.48it/s]  3%|         | 12/435 [00:00<00:08, 48.90it/s]  4%|         | 17/435 [00:00<00:08, 47.55it/s]  5%|         | 22/435 [00:00<00:08, 46.67it/s]  6%|         | 27/435 [00:00<00:08, 46.27it/s]  7%|         | 32/435 [00:00<00:08, 45.89it/s]  9%|         | 37/435 [00:00<00:10, 39.16it/s] 10%|         | 42/435 [00:00<00:09, 40.94it/s] 11%|         | 47/435 [00:01<00:09, 42.25it/s] 12%|        | 52/435 [00:01<00:08, 43.13it/s] 13%|        | 57/435 [00:01<00:08, 43.73it/s] 14%|        | 62/435 [00:01<00:08, 44.26it/s] 15%|        | 67/435 [00:01<00:08, 44.57it/s] 17%|        | 72/435 [00:01<00:08, 44.78it/s] 18%|        | 77/435 [00:01<00:08, 44.40it/s] 19%|        | 82/435 [00:01<00:07, 44.39it/s] 20%|        | 87/435 [00:01<00:07, 44.56it/s] 21%|        | 92/435 [00:02<00:07, 44.72it/s] 22%|       | 97/435 [00:02<00:07, 44.73it/s] 23%|       | 102/435 [00:02<00:07, 44.79it/s] 25%|       | 107/435 [00:02<00:07, 44.81it/s] 26%|       | 112/435 [00:02<00:07, 45.01it/s] 27%|       | 117/435 [00:02<00:07, 44.89it/s] 28%|       | 122/435 [00:02<00:07, 44.71it/s] 29%|       | 127/435 [00:02<00:06, 44.70it/s] 30%|       | 132/435 [00:02<00:06, 44.74it/s] 31%|      | 137/435 [00:03<00:06, 44.98it/s] 33%|      | 142/435 [00:03<00:06, 45.00it/s] 34%|      | 147/435 [00:03<00:06, 45.09it/s] 35%|      | 152/435 [00:03<00:06, 45.11it/s] 36%|      | 157/435 [00:03<00:06, 45.07it/s] 37%|      | 162/435 [00:03<00:06, 44.98it/s] 38%|      | 167/435 [00:03<00:05, 44.80it/s] 40%|      | 172/435 [00:03<00:06, 39.92it/s] 41%|      | 177/435 [00:04<00:06, 41.37it/s] 42%|     | 182/435 [00:04<00:05, 42.48it/s] 43%|     | 187/435 [00:04<00:05, 43.33it/s] 44%|     | 192/435 [00:04<00:05, 43.91it/s] 45%|     | 197/435 [00:04<00:05, 44.32it/s] 46%|     | 202/435 [00:04<00:05, 44.65it/s] 48%|     | 207/435 [00:04<00:05, 44.67it/s] 49%|     | 212/435 [00:04<00:05, 44.48it/s] 50%|     | 217/435 [00:04<00:04, 44.51it/s] 51%|     | 222/435 [00:05<00:04, 44.69it/s] 52%|    | 227/435 [00:05<00:04, 44.85it/s] 53%|    | 232/435 [00:05<00:04, 45.00it/s] 54%|    | 237/435 [00:05<00:04, 45.03it/s] 56%|    | 242/435 [00:05<00:04, 45.15it/s] 57%|    | 247/435 [00:05<00:04, 45.08it/s] 58%|    | 252/435 [00:05<00:04, 45.02it/s] 59%|    | 257/435 [00:05<00:03, 44.76it/s] 60%|    | 262/435 [00:05<00:03, 44.75it/s] 61%|   | 267/435 [00:06<00:03, 44.74it/s] 63%|   | 272/435 [00:06<00:03, 44.87it/s] 64%|   | 277/435 [00:06<00:03, 44.95it/s] 65%|   | 282/435 [00:06<00:03, 44.95it/s] 66%|   | 287/435 [00:06<00:03, 45.00it/s] 67%|   | 292/435 [00:06<00:03, 44.92it/s] 68%|   | 297/435 [00:06<00:03, 44.77it/s] 69%|   | 302/435 [00:06<00:02, 44.55it/s] 71%|   | 307/435 [00:06<00:02, 43.21it/s] 72%|  | 312/435 [00:07<00:02, 43.69it/s] 73%|  | 317/435 [00:07<00:02, 43.84it/s] 74%|  | 322/435 [00:07<00:02, 44.52it/s] 75%|  | 327/435 [00:07<00:02, 44.78it/s] 76%|  | 332/435 [00:07<00:02, 44.96it/s] 77%|  | 337/435 [00:07<00:02, 45.01it/s] 79%|  | 342/435 [00:07<00:02, 44.92it/s] 80%|  | 347/435 [00:07<00:01, 44.67it/s] 81%|  | 352/435 [00:07<00:01, 44.69it/s] 82%| | 357/435 [00:08<00:01, 44.72it/s] 83%| | 362/435 [00:08<00:01, 44.89it/s] 84%| | 367/435 [00:08<00:01, 44.93it/s] 86%| | 372/435 [00:08<00:01, 45.12it/s] 87%| | 377/435 [00:08<00:01, 45.11it/s] 88%| | 382/435 [00:08<00:01, 45.12it/s] 89%| | 387/435 [00:08<00:01, 45.04it/s] 90%| | 392/435 [00:08<00:00, 44.85it/s] 91%|| 397/435 [00:08<00:00, 44.77it/s] 92%|| 402/435 [00:09<00:00, 44.80it/s] 94%|| 407/435 [00:09<00:00, 44.88it/s] 95%|| 412/435 [00:09<00:00, 44.98it/s] 96%|| 417/435 [00:09<00:00, 45.05it/s] 97%|| 422/435 [00:09<00:00, 45.02it/s] 98%|| 427/435 [00:09<00:00, 45.08it/s] 99%|| 432/435 [00:09<00:00, 44.94it/s]100%|| 435/435 [00:09<00:00, 44.54it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:02:44,346 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:44,346 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:44,346 >>   eval_loss               =     0.9264
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:44,346 >>   eval_runtime            = 0:00:09.78
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:44,346 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:44,346 >>   eval_samples_per_second =    355.415
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:44,346 >>   eval_steps_per_second   =     44.465
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:44,346 >>   perplexity              =     2.5254
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-314
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-628
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-471
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-157
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_1', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:22<07:08, 22.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:39<05:43, 19.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:56<05:08, 18.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:12<04:40, 17.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:29<04:18, 17.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:44<03:51, 16.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:01<03:37, 16.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:19<03:25, 17.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:35<03:04, 16.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:52<02:48, 16.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:09<02:32, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:25<02:12, 16.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:43<01:58, 16.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:57<01:37, 16.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:14<01:22, 16.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:31<01:06, 16.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:47<00:48, 16.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:06<00:34, 17.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:21<00:16, 16.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:36<00:00, 16.18s/it]Generating: 100%|| 20/20 [05:36<00:00, 16.84s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 441, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 512, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7379807692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 234, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 625, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7512019230769231, 'errors': {'', "('Hans', 'head of government', '', 'He was succeeded by his brother , Dr . Hans .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : military branch .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('British ships', 'military branch', '', 'The Battle of Bataillon was the battle of the Battle of the Bastille , where British ships sank at least 1,400 French ships .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 181, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 274, 'raw': 384}
{'target': 600, 'success': 295, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 339, 'raw': 480}
{'target': 600, 'success': 361, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 428, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 602, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.6967592592592593, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('European Championships', 'winner', '', 'The previous year , he won the European Championships , and finished runner in 5th place in the category of medals .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : crosses . Context : Later in the year ( 11411231 ) he married Brigadier John B. Stoughton , sister of King James VI , the King of England . Head Entity : Robert Stoughton , Tail Entity : John B . Stoughton .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : crosses .', 'success_rate': 0.77, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('FreeBSD', 'operating system', '', 'The operating system is based on FreeBSD 2.1 .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 519, 'raw': 704}
{'target': 600, 'success': 544, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : participant .', 'success_rate': 0.7415865384615384, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : position played on team / speciality . Context : Later in 2008 , he played in the United States national team squad for the 2002 FIFA World Cup and 2010 FIFA World Cup games . Head Entity : Walter , Tail Entity : United States national team .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 99, 'raw': 160}
{'target': 600, 'success': 119, 'raw': 192}
{'target': 600, 'success': 141, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 183, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 224, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 266, 'raw': 416}
{'target': 600, 'success': 287, 'raw': 448}
{'target': 600, 'success': 312, 'raw': 480}
{'target': 600, 'success': 330, 'raw': 512}
{'target': 600, 'success': 348, 'raw': 544}
{'target': 600, 'success': 367, 'raw': 576}
{'target': 600, 'success': 386, 'raw': 608}
{'target': 600, 'success': 404, 'raw': 640}
{'target': 600, 'success': 421, 'raw': 672}
{'target': 600, 'success': 448, 'raw': 704}
{'target': 600, 'success': 465, 'raw': 736}
{'target': 600, 'success': 482, 'raw': 768}
{'target': 600, 'success': 499, 'raw': 800}
{'target': 600, 'success': 525, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 574, 'raw': 896}
{'target': 600, 'success': 594, 'raw': 928}
{'target': 600, 'success': 611, 'raw': 960}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.6364583333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : publisher . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : season 2 , Tail Entity : the Walking Dead .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8288043478260869, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 17454
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17554, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:30, 30.28s/it]Extractor Estimating: 2it [00:32, 13.51s/it]Extractor Estimating: 3it [00:32,  7.64s/it]Extractor Estimating: 4it [00:33,  4.88s/it]Extractor Estimating: 5it [00:34,  3.36s/it]Extractor Estimating: 6it [00:34,  2.44s/it]Extractor Estimating: 7it [00:35,  1.83s/it]Extractor Estimating: 8it [00:35,  1.46s/it]Extractor Estimating: 9it [00:36,  1.17s/it]Extractor Estimating: 10it [00:37,  1.02s/it]Extractor Estimating: 11it [00:37,  1.13it/s]Extractor Estimating: 12it [00:38,  1.20it/s]Extractor Estimating: 13it [00:39,  1.30it/s]Extractor Estimating: 14it [00:39,  1.40it/s]Extractor Estimating: 15it [00:40,  1.40it/s]Extractor Estimating: 16it [00:40,  1.48it/s]Extractor Estimating: 17it [00:41,  1.51it/s]Extractor Estimating: 18it [00:42,  1.54it/s]Extractor Estimating: 19it [00:42,  1.56it/s]Extractor Estimating: 20it [00:43,  1.62it/s]Extractor Estimating: 21it [00:43,  1.68it/s]Extractor Estimating: 22it [00:45,  1.28it/s]Extractor Estimating: 23it [00:45,  1.38it/s]Extractor Estimating: 24it [00:46,  1.47it/s]Extractor Estimating: 25it [00:46,  1.56it/s]Extractor Estimating: 26it [00:47,  1.53it/s]Extractor Estimating: 27it [00:48,  1.51it/s]Extractor Estimating: 28it [00:49,  1.17it/s]Extractor Estimating: 29it [00:50,  1.00it/s]Extractor Estimating: 30it [00:51,  1.08it/s]Extractor Estimating: 31it [00:52,  1.19it/s]Extractor Estimating: 32it [00:52,  1.26it/s]Extractor Estimating: 33it [00:53,  1.35it/s]Extractor Estimating: 34it [00:54,  1.36it/s]Extractor Estimating: 35it [00:54,  1.40it/s]Extractor Estimating: 36it [00:55,  1.45it/s]Extractor Estimating: 37it [00:56,  1.46it/s]Extractor Estimating: 38it [00:56,  1.51it/s]Extractor Estimating: 39it [00:57,  1.53it/s]Extractor Estimating: 40it [00:58,  1.52it/s]Extractor Estimating: 41it [00:58,  1.53it/s]Extractor Estimating: 42it [00:59,  1.55it/s]Extractor Estimating: 43it [01:00,  1.50it/s]Extractor Estimating: 44it [01:00,  1.54it/s]Extractor Estimating: 45it [01:01,  1.52it/s]Extractor Estimating: 46it [01:02,  1.52it/s]Extractor Estimating: 47it [01:02,  1.53it/s]Extractor Estimating: 48it [01:03,  1.52it/s]Extractor Estimating: 49it [01:03,  1.57it/s]Extractor Estimating: 50it [01:04,  1.56it/s]Extractor Estimating: 51it [01:05,  1.58it/s]Extractor Estimating: 52it [01:05,  1.57it/s]Extractor Estimating: 53it [01:07,  1.23it/s]Extractor Estimating: 54it [01:07,  1.30it/s]Extractor Estimating: 55it [01:08,  1.44it/s]Extractor Estimating: 56it [01:08,  1.55it/s]Extractor Estimating: 57it [01:09,  1.60it/s]Extractor Estimating: 58it [01:10,  1.61it/s]Extractor Estimating: 59it [01:10,  1.59it/s]Extractor Estimating: 60it [01:11,  1.63it/s]Extractor Estimating: 61it [01:11,  1.63it/s]Extractor Estimating: 62it [01:12,  1.66it/s]Extractor Estimating: 63it [01:13,  1.61it/s]Extractor Estimating: 64it [01:13,  1.60it/s]Extractor Estimating: 65it [01:14,  1.58it/s]Extractor Estimating: 66it [01:14,  1.64it/s]Extractor Estimating: 67it [01:15,  1.59it/s]Extractor Estimating: 68it [01:16,  1.62it/s]Extractor Estimating: 69it [01:16,  1.58it/s]Extractor Estimating: 70it [01:17,  1.63it/s]Extractor Estimating: 71it [01:18,  1.62it/s]Extractor Estimating: 72it [01:18,  1.64it/s]Extractor Estimating: 73it [01:19,  1.54it/s]Extractor Estimating: 74it [01:20,  1.55it/s]Extractor Estimating: 75it [01:20,  1.56it/s]Extractor Estimating: 76it [01:21,  1.58it/s]Extractor Estimating: 77it [01:21,  1.54it/s]Extractor Estimating: 78it [01:22,  1.54it/s]Extractor Estimating: 79it [01:23,  1.53it/s]Extractor Estimating: 80it [01:23,  1.51it/s]Extractor Estimating: 81it [01:24,  1.51it/s]Extractor Estimating: 82it [01:25,  1.55it/s]Extractor Estimating: 83it [01:25,  1.50it/s]Extractor Estimating: 84it [01:26,  1.49it/s]Extractor Estimating: 85it [01:27,  1.52it/s]Extractor Estimating: 86it [01:27,  1.56it/s]Extractor Estimating: 87it [01:28,  1.54it/s]Extractor Estimating: 88it [01:29,  1.52it/s]Extractor Estimating: 89it [01:29,  1.59it/s]Extractor Estimating: 90it [01:30,  1.58it/s]Extractor Estimating: 91it [01:31,  1.47it/s]Extractor Estimating: 92it [01:31,  1.50it/s]Extractor Estimating: 93it [01:32,  1.51it/s]Extractor Estimating: 94it [01:33,  1.59it/s]Extractor Estimating: 95it [01:33,  1.45it/s]Extractor Estimating: 96it [01:34,  1.46it/s]Extractor Estimating: 97it [01:35,  1.48it/s]Extractor Estimating: 98it [01:35,  1.50it/s]Extractor Estimating: 99it [01:36,  1.51it/s]Extractor Estimating: 100it [01:37,  1.53it/s]Extractor Estimating: 101it [01:37,  1.56it/s]Extractor Estimating: 102it [01:38,  1.58it/s]Extractor Estimating: 103it [01:39,  1.53it/s]Extractor Estimating: 104it [01:39,  1.56it/s]Extractor Estimating: 105it [01:40,  1.59it/s]Extractor Estimating: 106it [01:40,  1.62it/s]Extractor Estimating: 107it [01:41,  1.63it/s]Extractor Estimating: 108it [01:42,  1.62it/s]Extractor Estimating: 109it [01:42,  1.69it/s]Extractor Estimating: 110it [01:43,  1.64it/s]Extractor Estimating: 111it [01:43,  1.65it/s]Extractor Estimating: 112it [01:44,  1.64it/s]Extractor Estimating: 113it [01:45,  1.63it/s]Extractor Estimating: 114it [01:45,  1.62it/s]Extractor Estimating: 115it [01:46,  1.65it/s]Extractor Estimating: 116it [01:46,  1.66it/s]Extractor Estimating: 117it [01:47,  1.62it/s]Extractor Estimating: 118it [01:48,  1.57it/s]Extractor Estimating: 119it [01:48,  1.59it/s]Extractor Estimating: 120it [01:49,  1.64it/s]Extractor Estimating: 121it [01:50,  1.59it/s]Extractor Estimating: 122it [01:50,  1.59it/s]Extractor Estimating: 123it [01:51,  1.59it/s]Extractor Estimating: 124it [01:51,  1.63it/s]Extractor Estimating: 125it [01:52,  1.66it/s]Extractor Estimating: 126it [01:53,  1.63it/s]Extractor Estimating: 127it [01:53,  1.60it/s]Extractor Estimating: 128it [01:54,  1.60it/s]Extractor Estimating: 129it [01:55,  1.56it/s]Extractor Estimating: 130it [01:56,  1.15it/s]Extractor Estimating: 131it [01:57,  1.24it/s]Extractor Estimating: 132it [01:57,  1.29it/s]Extractor Estimating: 133it [01:58,  1.31it/s]Extractor Estimating: 134it [01:59,  1.36it/s]Extractor Estimating: 135it [01:59,  1.40it/s]Extractor Estimating: 136it [02:00,  1.43it/s]Extractor Estimating: 137it [02:01,  1.46it/s]Extractor Estimating: 138it [02:01,  1.51it/s]Extractor Estimating: 139it [02:02,  1.52it/s]Extractor Estimating: 140it [02:03,  1.48it/s]Extractor Estimating: 141it [02:03,  1.48it/s]Extractor Estimating: 142it [02:04,  1.49it/s]Extractor Estimating: 143it [02:05,  1.46it/s]Extractor Estimating: 144it [02:05,  1.49it/s]Extractor Estimating: 145it [02:06,  1.56it/s]Extractor Estimating: 146it [02:07,  1.50it/s]Extractor Estimating: 147it [02:07,  1.45it/s]Extractor Estimating: 148it [02:08,  1.47it/s]Extractor Estimating: 149it [02:09,  1.50it/s]Extractor Estimating: 150it [02:09,  1.52it/s]Extractor Estimating: 151it [02:10,  1.52it/s]Extractor Estimating: 152it [02:11,  1.53it/s]Extractor Estimating: 153it [02:11,  1.58it/s]Extractor Estimating: 154it [02:12,  1.54it/s]Extractor Estimating: 155it [02:13,  1.60it/s]Extractor Estimating: 156it [02:13,  1.55it/s]Extractor Estimating: 157it [02:14,  1.53it/s]Extractor Estimating: 158it [02:14,  1.61it/s]Extractor Estimating: 159it [02:15,  1.59it/s]Extractor Estimating: 160it [02:16,  1.54it/s]Extractor Estimating: 161it [02:16,  1.58it/s]Extractor Estimating: 162it [02:17,  1.67it/s]Extractor Estimating: 163it [02:17,  1.71it/s]Extractor Estimating: 164it [02:18,  1.70it/s]Extractor Estimating: 165it [02:19,  1.63it/s]Extractor Estimating: 166it [02:19,  1.63it/s]Extractor Estimating: 167it [02:20,  1.62it/s]Extractor Estimating: 168it [02:21,  1.63it/s]Extractor Estimating: 169it [02:21,  1.69it/s]Extractor Estimating: 170it [02:22,  1.59it/s]Extractor Estimating: 171it [02:22,  1.58it/s]Extractor Estimating: 172it [02:23,  1.59it/s]Extractor Estimating: 173it [02:24,  1.62it/s]Extractor Estimating: 174it [02:24,  1.61it/s]Extractor Estimating: 175it [02:25,  1.63it/s]Extractor Estimating: 176it [02:26,  1.47it/s]Extractor Estimating: 177it [02:26,  1.46it/s]Extractor Estimating: 178it [02:27,  1.44it/s]Extractor Estimating: 179it [02:28,  1.48it/s]Extractor Estimating: 180it [02:28,  1.54it/s]Extractor Estimating: 181it [02:29,  1.57it/s]Extractor Estimating: 182it [02:30,  1.57it/s]Extractor Estimating: 183it [02:30,  1.54it/s]Extractor Estimating: 184it [02:31,  1.49it/s]Extractor Estimating: 185it [02:32,  1.46it/s]Extractor Estimating: 186it [02:32,  1.44it/s]Extractor Estimating: 187it [02:33,  1.45it/s]Extractor Estimating: 188it [02:34,  1.51it/s]Extractor Estimating: 189it [02:34,  1.47it/s]Extractor Estimating: 190it [02:35,  1.48it/s]Extractor Estimating: 191it [02:36,  1.54it/s]Extractor Estimating: 192it [02:36,  1.47it/s]Extractor Estimating: 193it [02:37,  1.42it/s]Extractor Estimating: 194it [02:38,  1.50it/s]Extractor Estimating: 195it [02:39,  1.38it/s]Extractor Estimating: 196it [02:39,  1.37it/s]Extractor Estimating: 197it [02:40,  1.42it/s]Extractor Estimating: 198it [02:41,  1.46it/s]Extractor Estimating: 199it [02:41,  1.49it/s]Extractor Estimating: 200it [02:42,  1.54it/s]Extractor Estimating: 201it [02:43,  1.56it/s]Extractor Estimating: 202it [02:43,  1.51it/s]Extractor Estimating: 203it [02:44,  1.47it/s]Extractor Estimating: 204it [02:45,  1.51it/s]Extractor Estimating: 205it [02:45,  1.51it/s]Extractor Estimating: 206it [02:46,  1.54it/s]Extractor Estimating: 207it [02:46,  1.55it/s]Extractor Estimating: 208it [02:47,  1.58it/s]Extractor Estimating: 209it [02:48,  1.52it/s]Extractor Estimating: 210it [02:49,  1.47it/s]Extractor Estimating: 211it [02:49,  1.53it/s]Extractor Estimating: 212it [02:50,  1.54it/s]Extractor Estimating: 213it [02:50,  1.50it/s]Extractor Estimating: 214it [02:51,  1.43it/s]Extractor Estimating: 215it [02:52,  1.47it/s]Extractor Estimating: 216it [02:53,  1.46it/s]Extractor Estimating: 217it [02:53,  1.48it/s]Extractor Estimating: 218it [02:54,  1.47it/s]Extractor Estimating: 219it [02:55,  1.39it/s]Extractor Estimating: 220it [02:55,  1.44it/s]Extractor Estimating: 221it [02:56,  1.45it/s]Extractor Estimating: 222it [02:57,  1.45it/s]Extractor Estimating: 223it [02:57,  1.46it/s]Extractor Estimating: 224it [02:58,  1.47it/s]Extractor Estimating: 225it [02:59,  1.49it/s]Extractor Estimating: 226it [02:59,  1.46it/s]Extractor Estimating: 227it [03:00,  1.46it/s]Extractor Estimating: 228it [03:01,  1.48it/s]Extractor Estimating: 229it [03:01,  1.47it/s]Extractor Estimating: 230it [03:02,  1.46it/s]Extractor Estimating: 231it [03:03,  1.46it/s]Extractor Estimating: 232it [03:03,  1.50it/s]Extractor Estimating: 233it [03:04,  1.54it/s]Extractor Estimating: 234it [03:06,  1.09s/it]Extractor Estimating: 235it [03:07,  1.03it/s]Extractor Estimating: 236it [03:08,  1.14it/s]Extractor Estimating: 237it [03:08,  1.23it/s]Extractor Estimating: 238it [03:09,  1.30it/s]Extractor Estimating: 239it [03:09,  1.39it/s]Extractor Estimating: 240it [03:10,  1.38it/s]Extractor Estimating: 241it [03:11,  1.44it/s]Extractor Estimating: 242it [03:12,  1.46it/s]Extractor Estimating: 243it [03:12,  1.50it/s]Extractor Estimating: 244it [03:13,  1.51it/s]Extractor Estimating: 245it [03:13,  1.52it/s]Extractor Estimating: 246it [03:14,  1.52it/s]Extractor Estimating: 247it [03:15,  1.51it/s]Extractor Estimating: 248it [03:15,  1.53it/s]Extractor Estimating: 249it [03:16,  1.52it/s]Extractor Estimating: 250it [03:17,  1.50it/s]Extractor Estimating: 251it [03:18,  1.38it/s]Extractor Estimating: 252it [03:18,  1.45it/s]Extractor Estimating: 253it [03:19,  1.46it/s]Extractor Estimating: 254it [03:20,  1.45it/s]Extractor Estimating: 255it [03:20,  1.47it/s]Extractor Estimating: 256it [03:21,  1.45it/s]Extractor Estimating: 257it [03:22,  1.41it/s]Extractor Estimating: 258it [03:22,  1.42it/s]Extractor Estimating: 259it [03:23,  1.42it/s]Extractor Estimating: 260it [03:24,  1.41it/s]Extractor Estimating: 261it [03:25,  1.42it/s]Extractor Estimating: 262it [03:25,  1.42it/s]Extractor Estimating: 263it [03:26,  1.40it/s]Extractor Estimating: 264it [03:27,  1.45it/s]Extractor Estimating: 265it [03:27,  1.46it/s]Extractor Estimating: 266it [03:28,  1.51it/s]Extractor Estimating: 267it [03:29,  1.50it/s]Extractor Estimating: 268it [03:29,  1.50it/s]Extractor Estimating: 269it [03:30,  1.51it/s]Extractor Estimating: 270it [03:31,  1.44it/s]Extractor Estimating: 271it [03:31,  1.47it/s]Extractor Estimating: 272it [03:32,  1.43it/s]Extractor Estimating: 273it [03:33,  1.49it/s]Extractor Estimating: 274it [03:33,  1.46it/s]Extractor Estimating: 275it [03:34,  1.47it/s]Extractor Estimating: 276it [03:35,  1.53it/s]Extractor Estimating: 277it [03:35,  1.53it/s]Extractor Estimating: 278it [03:36,  1.55it/s]Extractor Estimating: 279it [03:37,  1.56it/s]Extractor Estimating: 280it [03:37,  1.55it/s]Extractor Estimating: 281it [03:38,  1.58it/s]Extractor Estimating: 282it [03:38,  1.56it/s]Extractor Estimating: 283it [03:39,  1.59it/s]Extractor Estimating: 284it [03:40,  1.64it/s]Extractor Estimating: 285it [03:40,  1.56it/s]Extractor Estimating: 286it [03:41,  1.58it/s]Extractor Estimating: 287it [03:42,  1.53it/s]Extractor Estimating: 288it [03:42,  1.56it/s]Extractor Estimating: 289it [03:43,  1.57it/s]Extractor Estimating: 290it [03:43,  1.60it/s]Extractor Estimating: 291it [03:44,  1.66it/s]Extractor Estimating: 292it [03:45,  1.49it/s]Extractor Estimating: 293it [03:46,  1.51it/s]Extractor Estimating: 294it [03:46,  1.52it/s]Extractor Estimating: 295it [03:47,  1.56it/s]Extractor Estimating: 296it [03:47,  1.61it/s]Extractor Estimating: 297it [03:48,  1.54it/s]Extractor Estimating: 298it [03:49,  1.59it/s]Extractor Estimating: 299it [03:49,  1.60it/s]Extractor Estimating: 300it [03:50,  1.59it/s]Extractor Estimating: 301it [03:50,  1.62it/s]Extractor Estimating: 302it [03:51,  1.54it/s]Extractor Estimating: 303it [03:52,  1.54it/s]Extractor Estimating: 304it [03:52,  1.54it/s]Extractor Estimating: 305it [03:53,  1.56it/s]Extractor Estimating: 306it [03:54,  1.53it/s]Extractor Estimating: 307it [03:54,  1.50it/s]Extractor Estimating: 308it [03:55,  1.50it/s]Extractor Estimating: 309it [03:56,  1.52it/s]Extractor Estimating: 310it [03:56,  1.56it/s]Extractor Estimating: 311it [03:57,  1.60it/s]Extractor Estimating: 312it [03:58,  1.59it/s]Extractor Estimating: 313it [03:58,  1.60it/s]Extractor Estimating: 314it [03:59,  1.58it/s]Extractor Estimating: 315it [04:00,  1.58it/s]Extractor Estimating: 316it [04:00,  1.62it/s]Extractor Estimating: 317it [04:01,  1.61it/s]Extractor Estimating: 318it [04:01,  1.58it/s]Extractor Estimating: 319it [04:02,  1.56it/s]Extractor Estimating: 320it [04:03,  1.63it/s]Extractor Estimating: 321it [04:03,  1.64it/s]Extractor Estimating: 322it [04:04,  1.65it/s]Extractor Estimating: 323it [04:04,  1.66it/s]Extractor Estimating: 324it [04:05,  1.62it/s]Extractor Estimating: 325it [04:06,  1.58it/s]Extractor Estimating: 326it [04:07,  1.46it/s]Extractor Estimating: 327it [04:07,  1.55it/s]Extractor Estimating: 328it [04:08,  1.64it/s]Extractor Estimating: 329it [04:08,  1.57it/s]Extractor Estimating: 330it [04:09,  1.57it/s]Extractor Estimating: 331it [04:10,  1.49it/s]Extractor Estimating: 332it [04:10,  1.52it/s]Extractor Estimating: 333it [04:11,  1.59it/s]Extractor Estimating: 334it [04:11,  1.63it/s]Extractor Estimating: 335it [04:12,  1.62it/s]Extractor Estimating: 336it [04:13,  1.67it/s]Extractor Estimating: 337it [04:13,  1.68it/s]Extractor Estimating: 338it [04:14,  1.67it/s]Extractor Estimating: 339it [04:15,  1.60it/s]Extractor Estimating: 340it [04:15,  1.58it/s]Extractor Estimating: 341it [04:16,  1.60it/s]Extractor Estimating: 342it [04:16,  1.69it/s]Extractor Estimating: 343it [04:17,  1.65it/s]Extractor Estimating: 344it [04:18,  1.61it/s]Extractor Estimating: 345it [04:18,  1.60it/s]Extractor Estimating: 346it [04:19,  1.58it/s]Extractor Estimating: 347it [04:19,  1.62it/s]Extractor Estimating: 348it [04:20,  1.63it/s]Extractor Estimating: 349it [04:21,  1.57it/s]Extractor Estimating: 350it [04:21,  1.63it/s]Extractor Estimating: 351it [04:22,  1.63it/s]Extractor Estimating: 352it [04:23,  1.61it/s]Extractor Estimating: 353it [04:23,  1.59it/s]Extractor Estimating: 354it [04:24,  1.60it/s]Extractor Estimating: 355it [04:24,  1.61it/s]Extractor Estimating: 356it [04:25,  1.63it/s]Extractor Estimating: 357it [04:26,  1.65it/s]Extractor Estimating: 358it [04:26,  1.59it/s]Extractor Estimating: 359it [04:27,  1.61it/s]Extractor Estimating: 360it [04:28,  1.61it/s]Extractor Estimating: 361it [04:28,  1.63it/s]Extractor Estimating: 362it [04:29,  1.70it/s]Extractor Estimating: 363it [04:29,  1.68it/s]Extractor Estimating: 364it [04:30,  1.73it/s]Extractor Estimating: 365it [04:30,  1.72it/s]Extractor Estimating: 366it [04:31,  1.65it/s]Extractor Estimating: 367it [04:32,  1.65it/s]Extractor Estimating: 368it [04:32,  1.55it/s]Extractor Estimating: 369it [04:33,  1.59it/s]Extractor Estimating: 370it [04:34,  1.63it/s]Extractor Estimating: 371it [04:34,  1.67it/s]Extractor Estimating: 372it [04:35,  1.66it/s]Extractor Estimating: 373it [04:35,  1.67it/s]Extractor Estimating: 374it [04:36,  1.68it/s]Extractor Estimating: 375it [04:37,  1.67it/s]Extractor Estimating: 376it [04:37,  1.68it/s]Extractor Estimating: 377it [04:38,  1.68it/s]Extractor Estimating: 378it [04:38,  1.73it/s]Extractor Estimating: 379it [04:39,  1.69it/s]Extractor Estimating: 380it [04:39,  1.70it/s]Extractor Estimating: 381it [04:40,  1.67it/s]Extractor Estimating: 382it [04:41,  1.67it/s]Extractor Estimating: 383it [04:41,  1.65it/s]Extractor Estimating: 384it [04:42,  1.59it/s]Extractor Estimating: 385it [04:43,  1.61it/s]Extractor Estimating: 386it [04:43,  1.61it/s]Extractor Estimating: 387it [04:44,  1.65it/s]Extractor Estimating: 388it [04:44,  1.68it/s]Extractor Estimating: 389it [04:45,  1.69it/s]Extractor Estimating: 390it [04:46,  1.63it/s]Extractor Estimating: 391it [04:46,  1.65it/s]Extractor Estimating: 392it [04:47,  1.65it/s]Extractor Estimating: 393it [04:47,  1.68it/s]Extractor Estimating: 394it [04:48,  1.68it/s]Extractor Estimating: 395it [04:49,  1.68it/s]Extractor Estimating: 396it [04:49,  1.67it/s]Extractor Estimating: 397it [04:50,  1.65it/s]Extractor Estimating: 398it [04:50,  1.64it/s]Extractor Estimating: 399it [04:51,  1.64it/s]Extractor Estimating: 400it [04:52,  1.62it/s]Extractor Estimating: 401it [04:52,  1.60it/s]Extractor Estimating: 402it [04:53,  1.55it/s]Extractor Estimating: 403it [04:54,  1.52it/s]Extractor Estimating: 404it [04:54,  1.53it/s]Extractor Estimating: 405it [04:55,  1.56it/s]Extractor Estimating: 406it [04:56,  1.56it/s]Extractor Estimating: 407it [04:56,  1.56it/s]Extractor Estimating: 408it [04:57,  1.53it/s]Extractor Estimating: 409it [04:57,  1.61it/s]Extractor Estimating: 410it [04:58,  1.60it/s]Extractor Estimating: 411it [04:59,  1.61it/s]Extractor Estimating: 412it [04:59,  1.56it/s]Extractor Estimating: 413it [05:00,  1.65it/s]Extractor Estimating: 414it [05:01,  1.65it/s]Extractor Estimating: 415it [05:01,  1.59it/s]Extractor Estimating: 416it [05:02,  1.57it/s]Extractor Estimating: 417it [05:02,  1.59it/s]Extractor Estimating: 418it [05:03,  1.63it/s]Extractor Estimating: 419it [05:04,  1.48it/s]Extractor Estimating: 420it [05:04,  1.51it/s]Extractor Estimating: 421it [05:05,  1.57it/s]Extractor Estimating: 422it [05:06,  1.58it/s]Extractor Estimating: 423it [05:06,  1.55it/s]Extractor Estimating: 424it [05:07,  1.57it/s]Extractor Estimating: 425it [05:08,  1.57it/s]Extractor Estimating: 426it [05:08,  1.61it/s]Extractor Estimating: 427it [05:09,  1.58it/s]Extractor Estimating: 428it [05:10,  1.55it/s]Extractor Estimating: 429it [05:10,  1.58it/s]Extractor Estimating: 430it [05:11,  1.55it/s]Extractor Estimating: 431it [05:11,  1.58it/s]Extractor Estimating: 432it [05:12,  1.65it/s]Extractor Estimating: 433it [05:13,  1.65it/s]Extractor Estimating: 434it [05:13,  1.62it/s]Extractor Estimating: 435it [05:14,  1.64it/s]Extractor Estimating: 436it [05:14,  1.62it/s]Extractor Estimating: 437it [05:15,  1.64it/s]Extractor Estimating: 438it [05:16,  1.48it/s]Extractor Estimating: 439it [05:16,  1.52it/s]Extractor Estimating: 440it [05:17,  1.55it/s]Extractor Estimating: 441it [05:18,  1.61it/s]Extractor Estimating: 442it [05:18,  1.59it/s]Extractor Estimating: 443it [05:19,  1.58it/s]Extractor Estimating: 444it [05:20,  1.60it/s]Extractor Estimating: 445it [05:20,  1.65it/s]Extractor Estimating: 446it [05:21,  1.67it/s]Extractor Estimating: 447it [05:21,  1.69it/s]Extractor Estimating: 448it [05:22,  1.65it/s]Extractor Estimating: 449it [05:22,  1.72it/s]Extractor Estimating: 450it [05:23,  1.64it/s]Extractor Estimating: 451it [05:24,  1.63it/s]Extractor Estimating: 452it [05:24,  1.52it/s]Extractor Estimating: 453it [05:25,  1.39it/s]Extractor Estimating: 454it [05:26,  1.42it/s]Extractor Estimating: 455it [05:27,  1.48it/s]Extractor Estimating: 456it [05:27,  1.55it/s]Extractor Estimating: 457it [05:28,  1.51it/s]Extractor Estimating: 458it [05:29,  1.50it/s]Extractor Estimating: 459it [05:29,  1.49it/s]Extractor Estimating: 460it [05:30,  1.47it/s]Extractor Estimating: 461it [05:31,  1.50it/s]Extractor Estimating: 462it [05:31,  1.45it/s]Extractor Estimating: 463it [05:32,  1.48it/s]Extractor Estimating: 464it [05:33,  1.48it/s]Extractor Estimating: 465it [05:33,  1.45it/s]Extractor Estimating: 466it [05:34,  1.45it/s]Extractor Estimating: 467it [05:35,  1.42it/s]Extractor Estimating: 468it [05:35,  1.43it/s]Extractor Estimating: 469it [05:36,  1.44it/s]Extractor Estimating: 470it [05:37,  1.49it/s]Extractor Estimating: 471it [05:38,  1.46it/s]Extractor Estimating: 472it [05:38,  1.42it/s]Extractor Estimating: 473it [05:39,  1.43it/s]Extractor Estimating: 474it [05:40,  1.46it/s]Extractor Estimating: 475it [05:40,  1.49it/s]Extractor Estimating: 476it [05:41,  1.51it/s]Extractor Estimating: 477it [05:42,  1.52it/s]Extractor Estimating: 478it [05:42,  1.53it/s]Extractor Estimating: 479it [05:43,  1.60it/s]Extractor Estimating: 480it [05:43,  1.62it/s]Extractor Estimating: 481it [05:44,  1.65it/s]Extractor Estimating: 482it [05:45,  1.61it/s]Extractor Estimating: 483it [05:45,  1.57it/s]Extractor Estimating: 484it [05:46,  1.57it/s]Extractor Estimating: 485it [05:46,  1.60it/s]Extractor Estimating: 486it [05:47,  1.53it/s]Extractor Estimating: 487it [05:48,  1.55it/s]Extractor Estimating: 488it [05:48,  1.61it/s]Extractor Estimating: 489it [05:49,  1.66it/s]Extractor Estimating: 490it [05:50,  1.61it/s]Extractor Estimating: 491it [05:50,  1.66it/s]Extractor Estimating: 492it [05:51,  1.60it/s]Extractor Estimating: 493it [05:51,  1.58it/s]Extractor Estimating: 494it [05:52,  1.52it/s]Extractor Estimating: 495it [05:53,  1.59it/s]Extractor Estimating: 496it [05:53,  1.57it/s]Extractor Estimating: 497it [05:54,  1.60it/s]Extractor Estimating: 498it [05:55,  1.59it/s]Extractor Estimating: 499it [05:56,  1.43it/s]Extractor Estimating: 500it [05:56,  1.59it/s]Extractor Estimating: 500it [05:56,  1.40it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 10252 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 31273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_train_large/unseen_15_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31373, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.341, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.054, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.011, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.003, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 72, avg_time 1.002, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 172, avg_time 2.090, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 272, avg_time 1.022, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 372, avg_time 1.016, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 44, avg_time 0.994, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 144, avg_time 0.994, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 244, avg_time 2.068, loss:nan
g_step 1200, step 344, avg_time 1.030, loss:nan
g_step 1300, step 16, avg_time 1.011, loss:nan
g_step 1400, step 116, avg_time 1.021, loss:nan
g_step 1500, step 216, avg_time 1.021, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 316, avg_time 2.045, loss:nan
g_step 1700, step 416, avg_time 1.015, loss:nan
g_step 1800, step 88, avg_time 1.022, loss:nan
g_step 1900, step 188, avg_time 1.005, loss:nan
g_step 2000, step 288, avg_time 1.007, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 388, avg_time 2.068, loss:nan
g_step 2200, step 60, avg_time 1.004, loss:nan
g_step 2300, step 160, avg_time 1.020, loss:nan
g_step 2400, step 260, avg_time 1.010, loss:nan
g_step 2500, step 360, avg_time 1.019, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 32, avg_time 2.067, loss:nan
g_step 2700, step 132, avg_time 1.011, loss:nan
g_step 2800, step 232, avg_time 1.012, loss:nan
g_step 2900, step 332, avg_time 1.006, loss:nan
g_step 3000, step 4, avg_time 1.007, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 104, avg_time 2.062, loss:nan
g_step 3200, step 204, avg_time 1.017, loss:nan
g_step 3300, step 304, avg_time 1.014, loss:nan
g_step 3400, step 404, avg_time 1.012, loss:nan
g_step 3500, step 76, avg_time 1.008, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 176, avg_time 2.070, loss:nan
g_step 3700, step 276, avg_time 1.010, loss:nan
g_step 3800, step 376, avg_time 1.002, loss:nan
g_step 3900, step 48, avg_time 1.009, loss:nan
g_step 4000, step 148, avg_time 1.024, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 248, avg_time 2.058, loss:nan
g_step 4200, step 348, avg_time 1.012, loss:nan
g_step 4300, step 20, avg_time 1.013, loss:nan
g_step 4400, step 120, avg_time 1.006, loss:nan
g_step 4500, step 220, avg_time 1.015, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 320, avg_time 2.080, loss:nan
g_step 4700, step 420, avg_time 1.010, loss:nan
g_step 4800, step 92, avg_time 1.001, loss:nan
g_step 4900, step 192, avg_time 1.013, loss:nan
g_step 5000, step 292, avg_time 1.031, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 392, avg_time 2.057, loss:nan
g_step 5200, step 64, avg_time 1.018, loss:nan
g_step 5300, step 164, avg_time 1.013, loss:nan
g_step 5400, step 264, avg_time 1.015, loss:nan
g_step 5500, step 364, avg_time 1.020, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 36, avg_time 2.049, loss:nan
g_step 5700, step 136, avg_time 1.019, loss:nan
g_step 5800, step 236, avg_time 1.034, loss:nan
g_step 5900, step 336, avg_time 1.007, loss:nan
g_step 6000, step 8, avg_time 1.003, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 108, avg_time 2.069, loss:nan
g_step 6200, step 208, avg_time 1.017, loss:nan
g_step 6300, step 308, avg_time 1.003, loss:nan
g_step 6400, step 408, avg_time 1.010, loss:nan
g_step 6500, step 80, avg_time 1.019, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 180, avg_time 2.050, loss:nan
g_step 6700, step 280, avg_time 1.013, loss:nan
g_step 6800, step 380, avg_time 1.017, loss:nan
g_step 6900, step 52, avg_time 1.011, loss:nan
g_step 7000, step 152, avg_time 1.006, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 252, avg_time 2.072, loss:nan
g_step 7200, step 352, avg_time 1.007, loss:nan
g_step 7300, step 24, avg_time 1.025, loss:nan
g_step 7400, step 124, avg_time 1.011, loss:nan
g_step 7500, step 224, avg_time 1.010, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 324, avg_time 2.073, loss:nan
g_step 7700, step 424, avg_time 1.009, loss:nan
g_step 7800, step 96, avg_time 1.001, loss:nan
g_step 7900, step 196, avg_time 1.009, loss:nan
g_step 8000, step 296, avg_time 0.997, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 396, avg_time 2.067, loss:nan
g_step 8200, step 68, avg_time 0.999, loss:nan
g_step 8300, step 168, avg_time 1.033, loss:nan
g_step 8400, step 268, avg_time 1.004, loss:nan
g_step 8500, step 368, avg_time 1.009, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 03:19:43 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 03:19:43 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_03-19-43_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 03:19:45 - WARNING - datasets.builder -   Using custom data configuration default-a89027d36bb6cda7
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a89027d36bb6cda7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 03:19:48,731 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:19:48,732 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:19:48,733 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:19:48,734 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:19:48,864 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:19:48,932 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:19:48,932 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:19:48,932 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:19:48,932 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:19:48,932 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:19:48,932 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 03:19:49,330 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:19:52,478 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 03:19:52,519 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a89027d36bb6cda7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 03:19:52 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x146ae1724200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:08,  1.17ba/s] 18%|        | 2/11 [00:01<00:04,  2.08ba/s] 27%|       | 3/11 [00:01<00:03,  2.34ba/s] 36%|      | 4/11 [00:01<00:02,  2.88ba/s] 45%|     | 5/11 [00:01<00:01,  3.30ba/s] 55%|    | 6/11 [00:02<00:01,  3.61ba/s] 64%|   | 7/11 [00:02<00:01,  3.87ba/s] 73%|  | 8/11 [00:02<00:00,  4.03ba/s] 82%| | 9/11 [00:02<00:00,  4.16ba/s] 91%| | 10/11 [00:03<00:00,  4.25ba/s]100%|| 11/11 [00:03<00:00,  5.04ba/s]100%|| 11/11 [00:03<00:00,  3.52ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.00ba/s] 50%|     | 2/4 [00:00<00:00,  3.73ba/s] 75%|  | 3/4 [00:00<00:00,  4.03ba/s]100%|| 4/4 [00:00<00:00,  5.18ba/s]100%|| 4/4 [00:00<00:00,  4.50ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:02,  4.94ba/s] 27%|       | 3/11 [00:00<00:00,  8.02ba/s] 45%|     | 5/11 [00:00<00:00,  8.98ba/s] 55%|    | 6/11 [00:00<00:00,  9.19ba/s] 73%|  | 8/11 [00:00<00:00,  9.62ba/s] 91%| | 10/11 [00:01<00:00,  9.85ba/s]100%|| 11/11 [00:01<00:00,  9.82ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.62ba/s] 75%|  | 3/4 [00:00<00:00,  8.36ba/s]100%|| 4/4 [00:00<00:00,  8.90ba/s]
[INFO|trainer.py:414] 2023-08-28 03:19:59,125 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 03:19:59,207 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 03:19:59,207 >>   Num examples = 10320
[INFO|trainer.py:1149] 2023-08-28 03:19:59,207 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 03:19:59,208 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 03:19:59,208 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 03:19:59,208 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 03:19:59,208 >>   Total optimization steps = 805
  0%|          | 0/805 [00:00<?, ?it/s]  0%|          | 1/805 [00:00<03:55,  3.41it/s]  0%|          | 2/805 [00:00<03:47,  3.53it/s]  0%|          | 3/805 [00:00<03:44,  3.57it/s]  0%|          | 4/805 [00:01<03:44,  3.57it/s]  1%|          | 5/805 [00:01<03:44,  3.56it/s]  1%|          | 6/805 [00:01<03:44,  3.56it/s]  1%|          | 7/805 [00:01<03:44,  3.56it/s]  1%|          | 8/805 [00:02<03:43,  3.56it/s]  1%|          | 9/805 [00:02<03:43,  3.56it/s]  1%|          | 10/805 [00:02<03:49,  3.46it/s]  1%|         | 11/805 [00:03<03:47,  3.49it/s]  1%|         | 12/805 [00:03<03:45,  3.51it/s]  2%|         | 13/805 [00:03<03:44,  3.53it/s]  2%|         | 14/805 [00:03<03:43,  3.54it/s]  2%|         | 15/805 [00:04<03:42,  3.54it/s]  2%|         | 16/805 [00:04<03:42,  3.55it/s]  2%|         | 17/805 [00:04<03:41,  3.55it/s]  2%|         | 18/805 [00:05<03:41,  3.55it/s]  2%|         | 19/805 [00:05<03:41,  3.55it/s]  2%|         | 20/805 [00:05<03:40,  3.55it/s]  3%|         | 21/805 [00:05<03:45,  3.47it/s]  3%|         | 22/805 [00:06<03:43,  3.50it/s]  3%|         | 23/805 [00:06<03:42,  3.52it/s]  3%|         | 24/805 [00:06<03:41,  3.53it/s]  3%|         | 25/805 [00:07<03:40,  3.54it/s]  3%|         | 26/805 [00:07<03:39,  3.55it/s]  3%|         | 27/805 [00:07<03:38,  3.55it/s]  3%|         | 28/805 [00:07<03:38,  3.56it/s]  4%|         | 29/805 [00:08<03:38,  3.56it/s]  4%|         | 30/805 [00:08<03:37,  3.56it/s]  4%|         | 31/805 [00:08<03:37,  3.56it/s]  4%|         | 32/805 [00:09<03:44,  3.44it/s]  4%|         | 33/805 [00:09<03:42,  3.48it/s]  4%|         | 34/805 [00:09<03:40,  3.50it/s]  4%|         | 35/805 [00:09<03:38,  3.52it/s]  4%|         | 36/805 [00:10<03:37,  3.53it/s]  5%|         | 37/805 [00:10<03:36,  3.54it/s]  5%|         | 38/805 [00:10<03:36,  3.55it/s]  5%|         | 39/805 [00:11<03:35,  3.55it/s]  5%|         | 40/805 [00:11<03:35,  3.55it/s]  5%|         | 41/805 [00:11<03:34,  3.55it/s]  5%|         | 42/805 [00:11<03:34,  3.55it/s]  5%|         | 43/805 [00:12<03:38,  3.48it/s]  5%|         | 44/805 [00:12<03:37,  3.50it/s]  6%|         | 45/805 [00:12<03:36,  3.52it/s]  6%|         | 46/805 [00:13<03:35,  3.53it/s]  6%|         | 47/805 [00:13<03:34,  3.53it/s]  6%|         | 48/805 [00:13<03:33,  3.54it/s]  6%|         | 49/805 [00:13<03:33,  3.55it/s]  6%|         | 50/805 [00:14<03:32,  3.55it/s]  6%|         | 51/805 [00:14<03:32,  3.55it/s]  6%|         | 52/805 [00:14<03:31,  3.56it/s]  7%|         | 53/805 [00:14<03:31,  3.56it/s]  7%|         | 54/805 [00:15<03:35,  3.49it/s]  7%|         | 55/805 [00:15<03:33,  3.51it/s]  7%|         | 56/805 [00:15<03:32,  3.53it/s]  7%|         | 57/805 [00:16<03:31,  3.54it/s]  7%|         | 58/805 [00:16<03:30,  3.54it/s]  7%|         | 59/805 [00:16<03:30,  3.55it/s]  7%|         | 60/805 [00:16<03:29,  3.55it/s]  8%|         | 61/805 [00:17<03:29,  3.55it/s]  8%|         | 62/805 [00:17<03:29,  3.55it/s]  8%|         | 63/805 [00:17<03:27,  3.57it/s]  8%|         | 64/805 [00:18<03:26,  3.58it/s]  8%|         | 65/805 [00:18<03:33,  3.47it/s]  8%|         | 66/805 [00:18<03:30,  3.51it/s]  8%|         | 67/805 [00:18<03:28,  3.54it/s]  8%|         | 68/805 [00:19<03:26,  3.56it/s]  9%|         | 69/805 [00:19<03:25,  3.58it/s]  9%|         | 70/805 [00:19<03:24,  3.59it/s]  9%|         | 71/805 [00:20<03:24,  3.59it/s]  9%|         | 72/805 [00:20<03:23,  3.60it/s]  9%|         | 73/805 [00:20<03:23,  3.60it/s]  9%|         | 74/805 [00:20<03:22,  3.60it/s]  9%|         | 75/805 [00:21<03:22,  3.60it/s]  9%|         | 76/805 [00:21<03:22,  3.61it/s] 10%|         | 77/805 [00:21<03:21,  3.61it/s] 10%|         | 78/805 [00:22<03:21,  3.61it/s] 10%|         | 79/805 [00:22<03:21,  3.61it/s] 10%|         | 80/805 [00:22<03:21,  3.61it/s] 10%|         | 81/805 [00:22<03:28,  3.47it/s] 10%|         | 82/805 [00:23<03:25,  3.52it/s] 10%|         | 83/805 [00:23<03:23,  3.54it/s] 10%|         | 84/805 [00:23<03:22,  3.56it/s] 11%|         | 85/805 [00:23<03:21,  3.57it/s] 11%|         | 86/805 [00:24<03:20,  3.58it/s] 11%|         | 87/805 [00:24<03:19,  3.59it/s] 11%|         | 88/805 [00:24<03:19,  3.60it/s] 11%|         | 89/805 [00:25<03:18,  3.60it/s] 11%|         | 90/805 [00:25<03:18,  3.60it/s] 11%|        | 91/805 [00:25<03:17,  3.61it/s] 11%|        | 92/805 [00:25<03:21,  3.54it/s] 12%|        | 93/805 [00:26<03:20,  3.56it/s] 12%|        | 94/805 [00:26<03:18,  3.58it/s] 12%|        | 95/805 [00:26<03:17,  3.59it/s] 12%|        | 96/805 [00:27<03:17,  3.59it/s] 12%|        | 97/805 [00:27<03:16,  3.60it/s] 12%|        | 98/805 [00:27<03:16,  3.60it/s] 12%|        | 99/805 [00:27<03:16,  3.60it/s] 12%|        | 100/805 [00:28<03:15,  3.60it/s] 13%|        | 101/805 [00:28<03:15,  3.61it/s] 13%|        | 102/805 [00:28<03:14,  3.61it/s] 13%|        | 103/805 [00:29<03:19,  3.52it/s] 13%|        | 104/805 [00:29<03:17,  3.55it/s] 13%|        | 105/805 [00:29<03:16,  3.57it/s] 13%|        | 106/805 [00:29<03:15,  3.58it/s] 13%|        | 107/805 [00:30<03:14,  3.59it/s] 13%|        | 108/805 [00:30<03:13,  3.59it/s] 14%|        | 109/805 [00:30<03:13,  3.60it/s] 14%|        | 110/805 [00:30<03:12,  3.60it/s] 14%|        | 111/805 [00:31<03:12,  3.61it/s] 14%|        | 112/805 [00:31<03:12,  3.60it/s] 14%|        | 113/805 [00:31<03:12,  3.60it/s] 14%|        | 114/805 [00:32<03:14,  3.55it/s] 14%|        | 115/805 [00:32<03:13,  3.57it/s] 14%|        | 116/805 [00:32<03:12,  3.58it/s] 15%|        | 117/805 [00:32<03:11,  3.59it/s] 15%|        | 118/805 [00:33<03:11,  3.59it/s] 15%|        | 119/805 [00:33<03:10,  3.60it/s] 15%|        | 120/805 [00:33<03:10,  3.60it/s] 15%|        | 121/805 [00:34<03:09,  3.60it/s] 15%|        | 122/805 [00:34<03:09,  3.60it/s] 15%|        | 123/805 [00:34<03:09,  3.61it/s] 15%|        | 124/805 [00:34<03:09,  3.60it/s] 16%|        | 125/805 [00:35<03:16,  3.45it/s] 16%|        | 126/805 [00:35<03:14,  3.50it/s] 16%|        | 127/805 [00:35<03:12,  3.52it/s] 16%|        | 128/805 [00:36<03:12,  3.52it/s] 16%|        | 129/805 [00:36<03:11,  3.53it/s] 16%|        | 130/805 [00:36<03:10,  3.54it/s] 16%|        | 131/805 [00:36<03:10,  3.54it/s] 16%|        | 132/805 [00:37<03:09,  3.55it/s] 17%|        | 133/805 [00:37<03:08,  3.56it/s] 17%|        | 134/805 [00:37<03:08,  3.57it/s] 17%|        | 135/805 [00:37<03:07,  3.58it/s] 17%|        | 136/805 [00:38<03:09,  3.52it/s] 17%|        | 137/805 [00:38<03:08,  3.54it/s] 17%|        | 138/805 [00:38<03:07,  3.56it/s] 17%|        | 139/805 [00:39<03:06,  3.57it/s] 17%|        | 140/805 [00:39<03:05,  3.58it/s] 18%|        | 141/805 [00:39<03:04,  3.59it/s] 18%|        | 142/805 [00:39<03:04,  3.60it/s] 18%|        | 143/805 [00:40<03:05,  3.56it/s] 18%|        | 144/805 [00:40<03:05,  3.57it/s] 18%|        | 145/805 [00:40<03:04,  3.58it/s] 18%|        | 146/805 [00:41<03:03,  3.59it/s] 18%|        | 147/805 [00:41<03:07,  3.50it/s] 18%|        | 148/805 [00:41<03:05,  3.53it/s] 19%|        | 149/805 [00:41<03:11,  3.43it/s] 19%|        | 150/805 [00:42<03:09,  3.46it/s] 19%|        | 151/805 [00:42<03:07,  3.50it/s] 19%|        | 152/805 [00:42<03:05,  3.53it/s] 19%|        | 153/805 [00:43<03:03,  3.55it/s] 19%|        | 154/805 [00:43<03:02,  3.56it/s] 19%|        | 155/805 [00:43<03:01,  3.57it/s] 19%|        | 156/805 [00:43<03:05,  3.49it/s] 20%|        | 157/805 [00:44<03:55,  2.76it/s] 20%|        | 158/805 [00:44<03:43,  2.89it/s] 20%|        | 159/805 [00:45<03:30,  3.07it/s] 20%|        | 160/805 [00:45<03:20,  3.22it/s] 20%|        | 161/805 [00:45<03:13,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 03:20:44,834 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:20:44,835 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 03:20:44,835 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.05it/s][A
  3%|         | 12/435 [00:00<00:08, 49.15it/s][A
  4%|         | 17/435 [00:00<00:08, 47.26it/s][A
  5%|         | 22/435 [00:00<00:08, 46.43it/s][A
  6%|         | 27/435 [00:00<00:08, 45.81it/s][A
  7%|         | 32/435 [00:00<00:08, 45.29it/s][A
  9%|         | 37/435 [00:00<00:08, 45.06it/s][A
 10%|         | 42/435 [00:00<00:08, 44.76it/s][A
 11%|         | 47/435 [00:01<00:08, 44.66it/s][A
 12%|        | 52/435 [00:01<00:08, 44.91it/s][A
 13%|        | 57/435 [00:01<00:08, 44.93it/s][A
 14%|        | 62/435 [00:01<00:08, 45.06it/s][A
 15%|        | 67/435 [00:01<00:08, 45.06it/s][A
 17%|        | 72/435 [00:01<00:08, 45.03it/s][A
 18%|        | 77/435 [00:01<00:07, 44.92it/s][A
 19%|        | 82/435 [00:01<00:07, 44.75it/s][A
 20%|        | 87/435 [00:01<00:08, 41.66it/s][A
 21%|        | 92/435 [00:02<00:08, 42.68it/s][A
 22%|       | 97/435 [00:02<00:07, 43.39it/s][A
 23%|       | 102/435 [00:02<00:07, 43.98it/s][A
 25%|       | 107/435 [00:02<00:07, 44.24it/s][A
 26%|       | 112/435 [00:02<00:07, 44.43it/s][A
 27%|       | 117/435 [00:02<00:07, 44.56it/s][A
 28%|       | 122/435 [00:02<00:07, 44.59it/s][A
 29%|       | 127/435 [00:02<00:06, 44.26it/s][A
 30%|       | 132/435 [00:02<00:06, 44.34it/s][A
 31%|      | 137/435 [00:03<00:06, 44.55it/s][A
 33%|      | 142/435 [00:03<00:06, 44.65it/s][A
 34%|      | 147/435 [00:03<00:06, 44.78it/s][A
 35%|      | 152/435 [00:03<00:06, 44.82it/s][A
 36%|      | 157/435 [00:03<00:06, 45.00it/s][A
 37%|      | 162/435 [00:03<00:06, 45.00it/s][A
 38%|      | 167/435 [00:03<00:05, 44.80it/s][A
 40%|      | 172/435 [00:03<00:05, 44.65it/s][A
 41%|      | 177/435 [00:03<00:05, 44.65it/s][A
 42%|     | 182/435 [00:04<00:05, 44.74it/s][A
 43%|     | 187/435 [00:04<00:05, 44.88it/s][A
 44%|     | 192/435 [00:04<00:05, 44.76it/s][A
 45%|     | 197/435 [00:04<00:05, 44.92it/s][A
 46%|     | 202/435 [00:04<00:05, 44.99it/s][A
 48%|     | 207/435 [00:04<00:05, 43.11it/s][A
 49%|     | 212/435 [00:04<00:05, 43.65it/s][A
 50%|     | 217/435 [00:04<00:04, 43.85it/s][A
 51%|     | 222/435 [00:04<00:04, 44.21it/s][A
 52%|    | 227/435 [00:05<00:04, 44.47it/s][A
 53%|    | 232/435 [00:05<00:04, 44.50it/s][A
 54%|    | 237/435 [00:05<00:04, 44.60it/s][A
 56%|    | 242/435 [00:05<00:04, 44.76it/s][A
 57%|    | 247/435 [00:05<00:04, 44.67it/s][A
 58%|    | 252/435 [00:05<00:04, 44.77it/s][A
 59%|    | 257/435 [00:05<00:03, 44.77it/s][A
 60%|    | 262/435 [00:05<00:03, 44.72it/s][A
 61%|   | 267/435 [00:05<00:03, 44.76it/s][A
 63%|   | 272/435 [00:06<00:03, 44.80it/s][A
 64%|   | 277/435 [00:06<00:03, 44.90it/s][A
 65%|   | 282/435 [00:06<00:03, 44.87it/s][A
 66%|   | 287/435 [00:06<00:03, 44.86it/s][A
 67%|   | 292/435 [00:06<00:03, 44.81it/s][A
 68%|   | 297/435 [00:06<00:03, 44.90it/s][A
 69%|   | 302/435 [00:06<00:02, 44.92it/s][A
 71%|   | 307/435 [00:06<00:02, 44.79it/s][A
 72%|  | 312/435 [00:06<00:02, 44.90it/s][A
 73%|  | 317/435 [00:07<00:02, 44.88it/s][A
 74%|  | 322/435 [00:07<00:02, 44.97it/s][A
 75%|  | 327/435 [00:07<00:02, 44.82it/s][A
 76%|  | 332/435 [00:07<00:02, 44.79it/s][A
 77%|  | 337/435 [00:07<00:02, 39.66it/s][A
 79%|  | 342/435 [00:07<00:02, 41.21it/s][A
 80%|  | 347/435 [00:07<00:02, 42.33it/s][A
 81%|  | 352/435 [00:07<00:01, 43.05it/s][A
 82%| | 357/435 [00:08<00:01, 43.59it/s][A
 83%| | 362/435 [00:08<00:01, 44.07it/s][A
 84%| | 367/435 [00:08<00:01, 44.37it/s][A
 86%| | 372/435 [00:08<00:01, 44.55it/s][A
 87%| | 377/435 [00:08<00:01, 44.23it/s][A
 88%| | 382/435 [00:08<00:01, 44.28it/s][A
 89%| | 387/435 [00:08<00:01, 44.49it/s][A
 90%| | 392/435 [00:08<00:00, 44.65it/s][A
 91%|| 397/435 [00:08<00:00, 44.74it/s][A
 92%|| 402/435 [00:09<00:00, 44.76it/s][A
 94%|| 407/435 [00:09<00:00, 44.87it/s][A
 95%|| 412/435 [00:09<00:00, 44.95it/s][A
 96%|| 417/435 [00:09<00:00, 44.75it/s][A
 97%|| 422/435 [00:09<00:00, 44.77it/s][A
 98%|| 427/435 [00:09<00:00, 44.64it/s][A
 99%|| 432/435 [00:09<00:00, 44.74it/s][A                                                 
                                                 [A 20%|        | 161/805 [00:55<03:13,  3.32it/s]
100%|| 435/435 [00:09<00:00, 44.74it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:20:54,883 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-161
[INFO|configuration_utils.py:351] 2023-08-28 03:20:55,069 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-161/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:20:57,763 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-161/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:20:57,941 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-161/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:20:57,998 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-161/special_tokens_map.json
 20%|        | 162/805 [00:59<48:19,  4.51s/it] 20%|        | 163/805 [01:00<34:40,  3.24s/it] 20%|        | 164/805 [01:00<25:08,  2.35s/it] 20%|        | 165/805 [01:00<18:27,  1.73s/it] 21%|        | 166/805 [01:01<13:47,  1.29s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 21%|        | 167/805 [01:01<10:50,  1.02s/it] 21%|        | 168/805 [01:01<08:28,  1.25it/s] 21%|        | 169/805 [01:01<06:52,  1.54it/s] 21%|        | 170/805 [01:02<05:41,  1.86it/s] 21%|        | 171/805 [01:02<04:51,  2.18it/s] 21%|       | 172/805 [01:02<04:16,  2.47it/s] 21%|       | 173/805 [01:03<03:52,  2.72it/s] 22%|       | 174/805 [01:03<03:35,  2.93it/s] 22%|       | 175/805 [01:03<03:22,  3.11it/s] 22%|       | 176/805 [01:03<03:14,  3.24it/s] 22%|       | 177/805 [01:04<03:08,  3.33it/s] 22%|       | 178/805 [01:04<03:03,  3.41it/s] 22%|       | 179/805 [01:04<03:00,  3.47it/s] 22%|       | 180/805 [01:05<02:59,  3.48it/s] 22%|       | 181/805 [01:05<02:57,  3.52it/s] 23%|       | 182/805 [01:05<02:55,  3.55it/s] 23%|       | 183/805 [01:05<02:54,  3.56it/s] 23%|       | 184/805 [01:06<02:53,  3.57it/s] 23%|       | 185/805 [01:06<02:53,  3.58it/s] 23%|       | 186/805 [01:06<02:52,  3.58it/s] 23%|       | 187/805 [01:07<02:52,  3.58it/s] 23%|       | 188/805 [01:07<02:52,  3.58it/s] 23%|       | 189/805 [01:07<02:51,  3.59it/s] 24%|       | 190/805 [01:07<02:51,  3.59it/s] 24%|       | 191/805 [01:08<02:54,  3.51it/s] 24%|       | 192/805 [01:08<02:53,  3.54it/s] 24%|       | 193/805 [01:08<02:51,  3.56it/s] 24%|       | 194/805 [01:08<02:51,  3.57it/s] 24%|       | 195/805 [01:09<02:50,  3.58it/s] 24%|       | 196/805 [01:09<02:50,  3.58it/s] 24%|       | 197/805 [01:09<02:49,  3.58it/s] 25%|       | 198/805 [01:10<02:49,  3.59it/s] 25%|       | 199/805 [01:10<02:48,  3.59it/s] 25%|       | 200/805 [01:10<02:48,  3.58it/s] 25%|       | 201/805 [01:10<02:48,  3.58it/s] 25%|       | 202/805 [01:11<02:54,  3.46it/s] 25%|       | 203/805 [01:11<02:51,  3.50it/s] 25%|       | 204/805 [01:11<02:49,  3.54it/s] 25%|       | 205/805 [01:12<02:48,  3.55it/s] 26%|       | 206/805 [01:12<02:48,  3.56it/s] 26%|       | 207/805 [01:12<02:47,  3.57it/s] 26%|       | 208/805 [01:12<02:46,  3.58it/s] 26%|       | 209/805 [01:13<02:46,  3.58it/s] 26%|       | 210/805 [01:13<02:46,  3.58it/s] 26%|       | 211/805 [01:13<02:45,  3.59it/s] 26%|       | 212/805 [01:14<02:45,  3.59it/s] 26%|       | 213/805 [01:14<02:52,  3.44it/s] 27%|       | 214/805 [01:14<02:49,  3.48it/s] 27%|       | 215/805 [01:14<02:47,  3.52it/s] 27%|       | 216/805 [01:15<02:46,  3.55it/s] 27%|       | 217/805 [01:15<02:44,  3.56it/s] 27%|       | 218/805 [01:15<02:44,  3.58it/s] 27%|       | 219/805 [01:16<02:43,  3.58it/s] 27%|       | 220/805 [01:16<02:43,  3.59it/s] 27%|       | 221/805 [01:16<02:42,  3.58it/s] 28%|       | 222/805 [01:16<02:42,  3.59it/s] 28%|       | 223/805 [01:17<02:41,  3.60it/s] 28%|       | 224/805 [01:17<02:47,  3.48it/s] 28%|       | 225/805 [01:17<02:45,  3.51it/s] 28%|       | 226/805 [01:17<02:43,  3.53it/s] 28%|       | 227/805 [01:18<02:42,  3.55it/s] 28%|       | 228/805 [01:18<02:41,  3.57it/s] 28%|       | 229/805 [01:18<02:41,  3.57it/s] 29%|       | 230/805 [01:19<02:40,  3.58it/s] 29%|       | 231/805 [01:19<02:40,  3.59it/s] 29%|       | 232/805 [01:19<02:39,  3.59it/s] 29%|       | 233/805 [01:19<02:39,  3.59it/s] 29%|       | 234/805 [01:20<02:38,  3.59it/s] 29%|       | 235/805 [01:20<02:42,  3.50it/s] 29%|       | 236/805 [01:20<02:41,  3.53it/s] 29%|       | 237/805 [01:21<02:48,  3.37it/s] 30%|       | 238/805 [01:21<02:45,  3.43it/s] 30%|       | 239/805 [01:21<02:42,  3.48it/s] 30%|       | 240/805 [01:21<02:40,  3.52it/s] 30%|       | 241/805 [01:22<02:39,  3.54it/s] 30%|       | 242/805 [01:22<02:38,  3.56it/s] 30%|       | 243/805 [01:22<02:37,  3.57it/s] 30%|       | 244/805 [01:23<02:36,  3.58it/s] 30%|       | 245/805 [01:23<02:36,  3.58it/s] 31%|       | 246/805 [01:23<02:36,  3.58it/s] 31%|       | 247/805 [01:23<02:35,  3.58it/s] 31%|       | 248/805 [01:24<02:37,  3.54it/s] 31%|       | 249/805 [01:24<02:36,  3.56it/s] 31%|       | 250/805 [01:24<02:35,  3.57it/s] 31%|       | 251/805 [01:25<02:34,  3.58it/s] 31%|      | 252/805 [01:25<02:34,  3.59it/s] 31%|      | 253/805 [01:25<02:33,  3.59it/s] 32%|      | 254/805 [01:25<02:33,  3.59it/s] 32%|      | 255/805 [01:26<02:33,  3.59it/s] 32%|      | 256/805 [01:26<02:32,  3.59it/s] 32%|      | 257/805 [01:26<02:32,  3.59it/s] 32%|      | 258/805 [01:26<02:32,  3.59it/s] 32%|      | 259/805 [01:27<02:35,  3.52it/s] 32%|      | 260/805 [01:27<02:33,  3.55it/s] 32%|      | 261/805 [01:27<02:32,  3.56it/s] 33%|      | 262/805 [01:28<02:32,  3.57it/s] 33%|      | 263/805 [01:28<02:31,  3.57it/s] 33%|      | 264/805 [01:28<02:31,  3.58it/s] 33%|      | 265/805 [01:28<02:30,  3.58it/s] 33%|      | 266/805 [01:29<02:30,  3.59it/s] 33%|      | 267/805 [01:29<02:29,  3.59it/s] 33%|      | 268/805 [01:29<02:29,  3.59it/s] 33%|      | 269/805 [01:30<02:28,  3.60it/s] 34%|      | 270/805 [01:30<02:33,  3.49it/s] 34%|      | 271/805 [01:30<02:31,  3.52it/s] 34%|      | 272/805 [01:30<02:30,  3.54it/s] 34%|      | 273/805 [01:31<02:29,  3.56it/s] 34%|      | 274/805 [01:31<02:28,  3.57it/s] 34%|      | 275/805 [01:31<02:27,  3.58it/s] 34%|      | 276/805 [01:32<02:27,  3.59it/s] 34%|      | 277/805 [01:32<02:27,  3.59it/s] 35%|      | 278/805 [01:32<02:26,  3.59it/s] 35%|      | 279/805 [01:32<02:26,  3.59it/s] 35%|      | 280/805 [01:33<02:26,  3.59it/s] 35%|      | 281/805 [01:33<02:29,  3.49it/s] 35%|      | 282/805 [01:33<02:28,  3.52it/s] 35%|      | 283/805 [01:33<02:27,  3.54it/s] 35%|      | 284/805 [01:34<02:26,  3.56it/s] 35%|      | 285/805 [01:34<02:25,  3.57it/s] 36%|      | 286/805 [01:34<02:25,  3.57it/s] 36%|      | 287/805 [01:35<02:24,  3.57it/s] 36%|      | 288/805 [01:35<02:24,  3.58it/s] 36%|      | 289/805 [01:35<02:23,  3.59it/s] 36%|      | 290/805 [01:35<02:23,  3.59it/s] 36%|      | 291/805 [01:36<02:23,  3.59it/s] 36%|      | 292/805 [01:36<02:28,  3.46it/s] 36%|      | 293/805 [01:36<02:26,  3.50it/s] 37%|      | 294/805 [01:37<02:24,  3.52it/s] 37%|      | 295/805 [01:37<02:24,  3.54it/s] 37%|      | 296/805 [01:37<02:23,  3.56it/s] 37%|      | 297/805 [01:37<02:22,  3.56it/s] 37%|      | 298/805 [01:38<02:21,  3.57it/s] 37%|      | 299/805 [01:38<02:21,  3.58it/s] 37%|      | 300/805 [01:38<02:20,  3.58it/s] 37%|      | 301/805 [01:39<02:20,  3.58it/s] 38%|      | 302/805 [01:39<02:19,  3.60it/s] 38%|      | 303/805 [01:39<02:23,  3.50it/s] 38%|      | 304/805 [01:39<02:21,  3.53it/s] 38%|      | 305/805 [01:40<02:20,  3.55it/s] 38%|      | 306/805 [01:40<02:20,  3.56it/s] 38%|      | 307/805 [01:40<02:19,  3.57it/s] 38%|      | 308/805 [01:41<02:18,  3.58it/s] 38%|      | 309/805 [01:41<02:18,  3.58it/s] 39%|      | 310/805 [01:41<02:25,  3.41it/s] 39%|      | 311/805 [01:41<02:25,  3.39it/s] 39%|      | 312/805 [01:42<02:23,  3.43it/s] 39%|      | 313/805 [01:42<02:21,  3.48it/s] 39%|      | 314/805 [01:42<02:19,  3.51it/s] 39%|      | 315/805 [01:43<02:18,  3.54it/s] 39%|      | 316/805 [01:43<02:17,  3.56it/s] 39%|      | 317/805 [01:43<02:16,  3.57it/s] 40%|      | 318/805 [01:43<02:16,  3.58it/s] 40%|      | 319/805 [01:44<02:15,  3.58it/s] 40%|      | 320/805 [01:44<02:24,  3.35it/s] 40%|      | 321/805 [01:44<02:24,  3.34it/s] 40%|      | 322/805 [01:45<02:21,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 03:21:44,325 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:21:44,326 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 03:21:44,326 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.8192, 'eval_samples_per_second': 354.101, 'eval_steps_per_second': 44.301, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.44it/s][A
  3%|         | 12/435 [00:00<00:08, 49.10it/s][A
  4%|         | 17/435 [00:00<00:08, 47.34it/s][A
  5%|         | 22/435 [00:00<00:08, 46.47it/s][A
  6%|         | 27/435 [00:00<00:08, 45.89it/s][A
  7%|         | 32/435 [00:00<00:08, 45.41it/s][A
  9%|         | 37/435 [00:00<00:08, 45.04it/s][A
 10%|         | 42/435 [00:00<00:08, 44.63it/s][A
 11%|         | 47/435 [00:01<00:08, 44.72it/s][A
 12%|        | 52/435 [00:01<00:08, 44.91it/s][A
 13%|        | 57/435 [00:01<00:08, 44.95it/s][A
 14%|        | 62/435 [00:01<00:08, 44.47it/s][A
 15%|        | 67/435 [00:01<00:08, 44.72it/s][A
 17%|        | 72/435 [00:01<00:08, 44.73it/s][A
 18%|        | 77/435 [00:01<00:07, 44.85it/s][A
 19%|        | 82/435 [00:01<00:07, 44.76it/s][A
 20%|        | 87/435 [00:01<00:07, 44.59it/s][A
 21%|        | 92/435 [00:02<00:07, 44.60it/s][A
 22%|       | 97/435 [00:02<00:07, 44.64it/s][A
 23%|       | 102/435 [00:02<00:07, 44.70it/s][A
 25%|       | 107/435 [00:02<00:07, 44.88it/s][A
 26%|       | 112/435 [00:02<00:07, 44.92it/s][A
 27%|       | 117/435 [00:02<00:07, 44.98it/s][A
 28%|       | 122/435 [00:02<00:06, 44.92it/s][A
 29%|       | 127/435 [00:02<00:06, 44.96it/s][A
 30%|       | 132/435 [00:02<00:06, 44.79it/s][A
 31%|      | 137/435 [00:03<00:06, 44.76it/s][A
 33%|      | 142/435 [00:03<00:06, 44.77it/s][A
 34%|      | 147/435 [00:03<00:06, 44.67it/s][A
 35%|      | 152/435 [00:03<00:06, 44.89it/s][A
 36%|      | 157/435 [00:03<00:06, 44.89it/s][A
 37%|      | 162/435 [00:03<00:06, 44.95it/s][A
 38%|      | 167/435 [00:03<00:05, 44.94it/s][A
 40%|      | 172/435 [00:03<00:05, 44.89it/s][A
 41%|      | 177/435 [00:03<00:05, 44.94it/s][A
 42%|     | 182/435 [00:04<00:05, 44.82it/s][A
 43%|     | 187/435 [00:04<00:05, 44.87it/s][A
 44%|     | 192/435 [00:04<00:05, 44.82it/s][A
 45%|     | 197/435 [00:04<00:05, 41.32it/s][A
 46%|     | 202/435 [00:04<00:05, 42.41it/s][A
 48%|     | 207/435 [00:04<00:05, 43.19it/s][A
 49%|     | 212/435 [00:04<00:05, 43.75it/s][A
 50%|     | 217/435 [00:04<00:04, 44.20it/s][A
 51%|     | 222/435 [00:04<00:04, 44.34it/s][A
 52%|    | 227/435 [00:05<00:04, 44.58it/s][A
 53%|    | 232/435 [00:05<00:04, 44.63it/s][A
 54%|    | 237/435 [00:05<00:04, 44.42it/s][A
 56%|    | 242/435 [00:05<00:04, 44.46it/s][A
 57%|    | 247/435 [00:05<00:04, 44.41it/s][A
 58%|    | 252/435 [00:05<00:04, 44.60it/s][A
 59%|    | 257/435 [00:05<00:03, 44.77it/s][A
 60%|    | 262/435 [00:05<00:03, 44.91it/s][A
 61%|   | 267/435 [00:05<00:03, 44.99it/s][A
 63%|   | 272/435 [00:06<00:03, 44.88it/s][A
 64%|   | 277/435 [00:06<00:03, 44.90it/s][A
 65%|   | 282/435 [00:06<00:03, 44.74it/s][A
 66%|   | 287/435 [00:06<00:03, 44.75it/s][A
 67%|   | 292/435 [00:06<00:03, 41.86it/s][A
 68%|   | 297/435 [00:06<00:03, 42.79it/s][A
 69%|   | 302/435 [00:06<00:03, 43.51it/s][A
 71%|   | 307/435 [00:06<00:02, 44.00it/s][A
 72%|  | 312/435 [00:06<00:02, 44.38it/s][A
 73%|  | 317/435 [00:07<00:02, 44.50it/s][A
 74%|  | 322/435 [00:07<00:02, 44.55it/s][A
 75%|  | 327/435 [00:07<00:02, 44.54it/s][A
 76%|  | 332/435 [00:07<00:02, 44.32it/s][A
 77%|  | 337/435 [00:07<00:02, 44.48it/s][A
 79%|  | 342/435 [00:07<00:02, 44.52it/s][A
 80%|  | 347/435 [00:07<00:01, 44.60it/s][A
 81%|  | 352/435 [00:07<00:01, 44.83it/s][A
 82%| | 357/435 [00:07<00:01, 44.87it/s][A
 83%| | 362/435 [00:08<00:01, 45.06it/s][A
 84%| | 367/435 [00:08<00:01, 44.94it/s][A
 86%| | 372/435 [00:08<00:01, 44.79it/s][A
 87%| | 377/435 [00:08<00:01, 44.61it/s][A
 88%| | 382/435 [00:08<00:01, 44.51it/s][A
 89%| | 387/435 [00:08<00:01, 44.62it/s][A
 90%| | 392/435 [00:08<00:00, 44.58it/s][A
 91%|| 397/435 [00:08<00:00, 44.75it/s][A
 92%|| 402/435 [00:09<00:00, 44.82it/s][A
 94%|| 407/435 [00:09<00:00, 44.91it/s][A
 95%|| 412/435 [00:09<00:00, 45.02it/s][A
 96%|| 417/435 [00:09<00:00, 44.89it/s][A
 97%|| 422/435 [00:09<00:00, 44.86it/s][A
 98%|| 427/435 [00:09<00:00, 42.77it/s][A
 99%|| 432/435 [00:09<00:00, 43.48it/s][A                                                 
                                                 [A 40%|      | 322/805 [01:54<02:21,  3.41it/s]
100%|| 435/435 [00:09<00:00, 43.48it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:21:54,422 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-322
[INFO|configuration_utils.py:351] 2023-08-28 03:21:54,757 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-322/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:21:58,605 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-322/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:21:58,870 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-322/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:21:59,040 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-322/special_tokens_map.json
 40%|      | 323/805 [02:01<40:09,  5.00s/it] 40%|      | 324/805 [02:01<28:43,  3.58s/it] 40%|      | 325/805 [02:01<20:44,  2.59s/it] 40%|      | 326/805 [02:01<15:09,  1.90s/it] 41%|      | 327/805 [02:02<11:15,  1.41s/it] 41%|      | 328/805 [02:02<08:32,  1.07s/it] 41%|      | 329/805 [02:02<06:42,  1.18it/s] 41%|      | 330/805 [02:03<05:21,  1.48it/s] 41%|      | 331/805 [02:03<04:24,  1.79it/s] 41%|      | 332/805 [02:03<03:44,  2.11it/s] 41%|     | 333/805 [02:03<03:16,  2.41it/s] 41%|     | 334/805 [02:04<02:56,  2.67it/s] 42%|     | 335/805 [02:04<02:42,  2.89it/s] 42%|     | 336/805 [02:04<02:32,  3.07it/s] 42%|     | 337/805 [02:05<02:25,  3.21it/s] 42%|     | 338/805 [02:05<02:20,  3.32it/s] 42%|     | 339/805 [02:05<02:16,  3.40it/s] 42%|     | 340/805 [02:05<02:18,  3.36it/s] 42%|     | 341/805 [02:06<02:15,  3.43it/s] 42%|     | 342/805 [02:06<02:13,  3.48it/s] 43%|     | 343/805 [02:06<02:11,  3.52it/s] 43%|     | 344/805 [02:06<02:09,  3.55it/s] 43%|     | 345/805 [02:07<02:08,  3.57it/s] 43%|     | 346/805 [02:07<02:08,  3.57it/s] 43%|     | 347/805 [02:07<02:07,  3.58it/s] 43%|     | 348/805 [02:08<02:07,  3.59it/s] 43%|     | 349/805 [02:08<02:07,  3.59it/s] 43%|     | 350/805 [02:08<02:06,  3.58it/s] 44%|     | 351/805 [02:08<02:10,  3.49it/s] 44%|     | 352/805 [02:09<02:08,  3.52it/s] 44%|     | 353/805 [02:09<02:07,  3.54it/s] 44%|     | 354/805 [02:09<02:06,  3.56it/s] 44%|     | 355/805 [02:10<02:05,  3.57it/s] 44%|     | 356/805 [02:10<02:05,  3.58it/s] 44%|     | 357/805 [02:10<02:04,  3.59it/s] 44%|     | 358/805 [02:10<02:04,  3.60it/s] 45%|     | 359/805 [02:11<02:03,  3.60it/s] 45%|     | 360/805 [02:11<02:03,  3.60it/s] 45%|     | 361/805 [02:11<02:03,  3.60it/s] 45%|     | 362/805 [02:12<02:07,  3.48it/s] 45%|     | 363/805 [02:12<02:05,  3.51it/s] 45%|     | 364/805 [02:12<02:04,  3.54it/s] 45%|     | 365/805 [02:12<02:03,  3.56it/s] 45%|     | 366/805 [02:13<02:02,  3.57it/s] 46%|     | 367/805 [02:13<02:02,  3.58it/s] 46%|     | 368/805 [02:13<02:02,  3.58it/s] 46%|     | 369/805 [02:13<02:01,  3.59it/s] 46%|     | 370/805 [02:14<02:01,  3.59it/s] 46%|     | 371/805 [02:14<02:00,  3.60it/s] 46%|     | 372/805 [02:14<02:00,  3.60it/s] 46%|     | 373/805 [02:15<02:11,  3.29it/s] 46%|     | 374/805 [02:15<02:07,  3.38it/s] 47%|     | 375/805 [02:15<02:04,  3.45it/s] 47%|     | 376/805 [02:16<02:02,  3.49it/s] 47%|     | 377/805 [02:16<02:01,  3.53it/s] 47%|     | 378/805 [02:16<02:00,  3.55it/s] 47%|     | 379/805 [02:16<01:59,  3.56it/s] 47%|     | 380/805 [02:17<01:59,  3.57it/s] 47%|     | 381/805 [02:17<01:58,  3.57it/s] 47%|     | 382/805 [02:17<01:57,  3.59it/s] 48%|     | 383/805 [02:17<01:57,  3.60it/s] 48%|     | 384/805 [02:18<01:59,  3.54it/s] 48%|     | 385/805 [02:18<01:58,  3.55it/s] 48%|     | 386/805 [02:18<01:57,  3.56it/s] 48%|     | 387/805 [02:19<01:56,  3.57it/s] 48%|     | 388/805 [02:19<01:56,  3.58it/s] 48%|     | 389/805 [02:19<01:56,  3.57it/s] 48%|     | 390/805 [02:19<01:55,  3.58it/s] 49%|     | 391/805 [02:20<01:55,  3.59it/s] 49%|     | 392/805 [02:20<01:54,  3.59it/s] 49%|     | 393/805 [02:20<01:54,  3.59it/s] 49%|     | 394/805 [02:21<01:54,  3.60it/s] 49%|     | 395/805 [02:21<01:58,  3.46it/s] 49%|     | 396/805 [02:21<01:56,  3.50it/s] 49%|     | 397/805 [02:21<01:55,  3.52it/s] 49%|     | 398/805 [02:22<01:54,  3.54it/s] 50%|     | 399/805 [02:22<01:54,  3.56it/s] 50%|     | 400/805 [02:22<01:58,  3.41it/s] 50%|     | 401/805 [02:23<01:56,  3.47it/s] 50%|     | 402/805 [02:23<01:54,  3.51it/s] 50%|     | 403/805 [02:23<01:53,  3.54it/s] 50%|     | 404/805 [02:23<01:52,  3.56it/s] 50%|     | 405/805 [02:24<01:51,  3.57it/s] 50%|     | 406/805 [02:24<01:51,  3.58it/s] 51%|     | 407/805 [02:24<01:51,  3.58it/s] 51%|     | 408/805 [02:24<01:50,  3.59it/s] 51%|     | 409/805 [02:25<01:50,  3.60it/s] 51%|     | 410/805 [02:25<01:58,  3.33it/s] 51%|     | 411/805 [02:25<01:55,  3.40it/s] 51%|     | 412/805 [02:26<01:53,  3.46it/s] 51%|    | 413/805 [02:26<01:51,  3.50it/s] 51%|    | 414/805 [02:26<01:50,  3.53it/s] 52%|    | 415/805 [02:27<01:49,  3.55it/s] 52%|    | 416/805 [02:27<01:48,  3.57it/s] 52%|    | 417/805 [02:27<01:48,  3.58it/s] 52%|    | 418/805 [02:27<01:47,  3.59it/s] 52%|    | 419/805 [02:28<01:47,  3.59it/s] 52%|    | 420/805 [02:28<01:47,  3.60it/s] 52%|    | 421/805 [02:28<01:52,  3.43it/s] 52%|    | 422/805 [02:29<01:50,  3.47it/s] 53%|    | 423/805 [02:29<01:48,  3.51it/s] 53%|    | 424/805 [02:29<01:47,  3.54it/s] 53%|    | 425/805 [02:29<01:46,  3.56it/s] 53%|    | 426/805 [02:30<01:46,  3.57it/s] 53%|    | 427/805 [02:30<01:45,  3.58it/s] 53%|    | 428/805 [02:30<01:44,  3.59it/s] 53%|    | 429/805 [02:30<01:44,  3.60it/s] 53%|    | 430/805 [02:31<01:44,  3.60it/s] 54%|    | 431/805 [02:31<01:43,  3.60it/s] 54%|    | 432/805 [02:31<01:46,  3.51it/s] 54%|    | 433/805 [02:32<01:45,  3.54it/s] 54%|    | 434/805 [02:32<01:44,  3.55it/s] 54%|    | 435/805 [02:32<01:43,  3.57it/s] 54%|    | 436/805 [02:32<01:43,  3.58it/s] 54%|    | 437/805 [02:33<01:42,  3.59it/s] 54%|    | 438/805 [02:33<01:42,  3.59it/s] 55%|    | 439/805 [02:33<01:41,  3.59it/s] 55%|    | 440/805 [02:34<01:41,  3.60it/s] 55%|    | 441/805 [02:34<01:41,  3.60it/s] 55%|    | 442/805 [02:34<01:40,  3.60it/s] 55%|    | 443/805 [02:34<01:44,  3.47it/s] 55%|    | 444/805 [02:35<01:42,  3.51it/s] 55%|    | 445/805 [02:35<01:41,  3.54it/s] 55%|    | 446/805 [02:35<01:40,  3.56it/s] 56%|    | 447/805 [02:36<01:40,  3.58it/s] 56%|    | 448/805 [02:36<01:39,  3.58it/s] 56%|    | 449/805 [02:36<01:39,  3.59it/s] 56%|    | 450/805 [02:36<01:38,  3.59it/s] 56%|    | 451/805 [02:37<01:38,  3.59it/s] 56%|    | 452/805 [02:37<01:38,  3.60it/s] 56%|    | 453/805 [02:37<01:37,  3.60it/s] 56%|    | 454/805 [02:37<01:39,  3.53it/s] 57%|    | 455/805 [02:38<01:38,  3.55it/s] 57%|    | 456/805 [02:38<01:37,  3.56it/s] 57%|    | 457/805 [02:38<01:37,  3.57it/s] 57%|    | 458/805 [02:39<01:36,  3.58it/s] 57%|    | 459/805 [02:39<01:36,  3.58it/s] 57%|    | 460/805 [02:39<01:36,  3.58it/s] 57%|    | 461/805 [02:39<01:35,  3.59it/s] 57%|    | 462/805 [02:40<01:35,  3.59it/s] 58%|    | 463/805 [02:40<01:35,  3.60it/s] 58%|    | 464/805 [02:40<01:34,  3.60it/s] 58%|    | 465/805 [02:41<01:36,  3.54it/s] 58%|    | 466/805 [02:41<01:35,  3.55it/s] 58%|    | 467/805 [02:41<01:34,  3.57it/s] 58%|    | 468/805 [02:41<01:34,  3.58it/s] 58%|    | 469/805 [02:42<01:33,  3.59it/s] 58%|    | 470/805 [02:42<01:33,  3.59it/s] 59%|    | 471/805 [02:42<01:32,  3.60it/s] 59%|    | 472/805 [02:42<01:32,  3.60it/s] 59%|    | 473/805 [02:43<01:32,  3.60it/s] 59%|    | 474/805 [02:43<01:32,  3.60it/s] 59%|    | 475/805 [02:43<01:31,  3.59it/s] 59%|    | 476/805 [02:44<01:33,  3.53it/s] 59%|    | 477/805 [02:44<01:32,  3.55it/s] 59%|    | 478/805 [02:44<01:41,  3.21it/s] 60%|    | 479/805 [02:45<01:38,  3.30it/s] 60%|    | 480/805 [02:45<01:36,  3.38it/s] 60%|    | 481/805 [02:45<01:34,  3.44it/s] 60%|    | 482/805 [02:45<01:32,  3.49it/s] 60%|    | 483/805 [02:46<01:31,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 03:22:45,411 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:22:45,412 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 03:22:45,412 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.8043, 'eval_samples_per_second': 354.641, 'eval_steps_per_second': 44.368, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.91it/s][A
  3%|         | 12/435 [00:00<00:08, 49.03it/s][A
  4%|         | 17/435 [00:00<00:08, 47.18it/s][A
  5%|         | 22/435 [00:00<00:08, 46.35it/s][A
  6%|         | 27/435 [00:00<00:08, 45.71it/s][A
  7%|         | 32/435 [00:00<00:09, 44.64it/s][A
  9%|         | 37/435 [00:00<00:08, 44.61it/s][A
 10%|         | 42/435 [00:00<00:08, 44.43it/s][A
 11%|         | 47/435 [00:01<00:08, 44.65it/s][A
 12%|        | 52/435 [00:01<00:08, 44.79it/s][A
 13%|        | 57/435 [00:01<00:08, 44.85it/s][A
 14%|        | 62/435 [00:01<00:08, 44.91it/s][A
 15%|        | 67/435 [00:01<00:08, 44.98it/s][A
 17%|        | 72/435 [00:01<00:08, 45.01it/s][A
 18%|        | 77/435 [00:01<00:07, 44.96it/s][A
 19%|        | 82/435 [00:01<00:07, 44.86it/s][A
 20%|        | 87/435 [00:01<00:07, 44.68it/s][A
 21%|        | 92/435 [00:02<00:07, 44.69it/s][A
 22%|       | 97/435 [00:02<00:07, 44.87it/s][A
 23%|       | 102/435 [00:02<00:07, 44.84it/s][A
 25%|       | 107/435 [00:02<00:07, 44.96it/s][A
 26%|       | 112/435 [00:02<00:07, 44.92it/s][A
 27%|       | 117/435 [00:02<00:07, 45.00it/s][A
 28%|       | 122/435 [00:02<00:06, 45.01it/s][A
 29%|       | 127/435 [00:02<00:06, 44.85it/s][A
 30%|       | 132/435 [00:02<00:06, 44.85it/s][A
 31%|      | 137/435 [00:03<00:06, 44.74it/s][A
 33%|      | 142/435 [00:03<00:06, 44.82it/s][A
 34%|      | 147/435 [00:03<00:06, 44.89it/s][A
 35%|      | 152/435 [00:03<00:06, 44.87it/s][A
 36%|      | 157/435 [00:03<00:06, 44.94it/s][A
 37%|      | 162/435 [00:03<00:06, 44.91it/s][A
 38%|      | 167/435 [00:03<00:06, 41.21it/s][A
 40%|      | 172/435 [00:03<00:06, 42.37it/s][A
 41%|      | 177/435 [00:03<00:05, 43.14it/s][A
 42%|     | 182/435 [00:04<00:05, 43.71it/s][A
 43%|     | 187/435 [00:04<00:05, 44.11it/s][A
 44%|     | 192/435 [00:04<00:05, 44.39it/s][A
 45%|     | 197/435 [00:04<00:05, 44.62it/s][A
 46%|     | 202/435 [00:04<00:05, 44.66it/s][A
 48%|     | 207/435 [00:04<00:05, 44.51it/s][A
 49%|     | 212/435 [00:04<00:05, 44.44it/s][A
 50%|     | 217/435 [00:04<00:04, 44.54it/s][A
 51%|     | 222/435 [00:04<00:04, 44.81it/s][A
 52%|    | 227/435 [00:05<00:04, 44.79it/s][A
 53%|    | 232/435 [00:05<00:04, 44.99it/s][A
 54%|    | 237/435 [00:05<00:04, 44.88it/s][A
 56%|    | 242/435 [00:05<00:04, 44.89it/s][A
 57%|    | 247/435 [00:05<00:04, 44.80it/s][A
 58%|    | 252/435 [00:05<00:04, 44.65it/s][A
 59%|    | 257/435 [00:05<00:03, 44.68it/s][A
 60%|    | 262/435 [00:05<00:03, 44.69it/s][A
 61%|   | 267/435 [00:05<00:03, 44.83it/s][A
 63%|   | 272/435 [00:06<00:03, 44.86it/s][A
 64%|   | 277/435 [00:06<00:03, 44.94it/s][A
 65%|   | 282/435 [00:06<00:03, 45.03it/s][A
 66%|   | 287/435 [00:06<00:03, 44.99it/s][A
 67%|   | 292/435 [00:06<00:03, 45.03it/s][A
 68%|   | 297/435 [00:06<00:03, 44.83it/s][A
 69%|   | 302/435 [00:06<00:03, 40.67it/s][A
 71%|   | 307/435 [00:06<00:03, 40.81it/s][A
 72%|  | 312/435 [00:07<00:02, 42.05it/s][A
 73%|  | 317/435 [00:07<00:02, 43.01it/s][A
 74%|  | 322/435 [00:07<00:02, 43.57it/s][A
 75%|  | 327/435 [00:07<00:02, 44.10it/s][A
 76%|  | 332/435 [00:07<00:02, 44.38it/s][A
 77%|  | 337/435 [00:07<00:02, 44.42it/s][A
 79%|  | 342/435 [00:07<00:02, 44.29it/s][A
 80%|  | 347/435 [00:07<00:01, 44.10it/s][A
 81%|  | 352/435 [00:07<00:01, 44.26it/s][A
 82%| | 357/435 [00:08<00:01, 44.53it/s][A
 83%| | 362/435 [00:08<00:01, 44.72it/s][A
 84%| | 367/435 [00:08<00:01, 44.92it/s][A
 86%| | 372/435 [00:08<00:01, 44.88it/s][A
 87%| | 377/435 [00:08<00:01, 45.07it/s][A
 88%| | 382/435 [00:08<00:01, 44.87it/s][A
 89%| | 387/435 [00:08<00:01, 44.61it/s][A
 90%| | 392/435 [00:08<00:00, 44.44it/s][A
 91%|| 397/435 [00:08<00:00, 44.34it/s][A
 92%|| 402/435 [00:09<00:00, 44.60it/s][A
 94%|| 407/435 [00:09<00:00, 44.76it/s][A
 95%|| 412/435 [00:09<00:00, 44.90it/s][A
 96%|| 417/435 [00:09<00:00, 45.00it/s][A
 97%|| 422/435 [00:09<00:00, 45.02it/s][A
 98%|| 427/435 [00:09<00:00, 44.99it/s][A
 99%|| 432/435 [00:09<00:00, 44.73it/s][A                                                 
                                                 [A 60%|    | 483/805 [02:56<01:31,  3.52it/s]
100%|| 435/435 [00:09<00:00, 44.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:22:55,735 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-483
[INFO|configuration_utils.py:351] 2023-08-28 03:22:56,011 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-483/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:22:59,520 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-483/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:22:59,714 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-483/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:22:59,800 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-483/special_tokens_map.json
 60%|    | 484/805 [03:02<26:32,  4.96s/it] 60%|    | 485/805 [03:02<18:58,  3.56s/it] 60%|    | 486/805 [03:02<13:41,  2.58s/it] 60%|    | 487/805 [03:02<10:00,  1.89s/it] 61%|    | 488/805 [03:03<07:25,  1.40s/it] 61%|    | 489/805 [03:03<05:36,  1.07s/it] 61%|    | 490/805 [03:03<04:22,  1.20it/s] 61%|    | 491/805 [03:04<03:29,  1.50it/s] 61%|    | 492/805 [03:04<02:52,  1.82it/s] 61%|    | 493/805 [03:04<02:26,  2.13it/s] 61%|   | 494/805 [03:04<02:08,  2.43it/s] 61%|   | 495/805 [03:05<01:55,  2.69it/s] 62%|   | 496/805 [03:05<01:45,  2.92it/s] 62%|   | 497/805 [03:05<01:39,  3.10it/s] 62%|   | 498/805 [03:05<01:35,  3.23it/s] 62%|   | 499/805 [03:06<01:31,  3.33it/s] 62%|   | 500/805 [03:06<01:29,  3.40it/s]                                                  62%|   | 500/805 [03:06<01:29,  3.40it/s] 62%|   | 501/805 [03:06<01:30,  3.36it/s] 62%|   | 502/805 [03:07<01:28,  3.44it/s] 62%|   | 503/805 [03:07<01:26,  3.49it/s] 63%|   | 504/805 [03:07<01:25,  3.52it/s] 63%|   | 505/805 [03:07<01:24,  3.54it/s] 63%|   | 506/805 [03:08<01:24,  3.56it/s] 63%|   | 507/805 [03:08<01:23,  3.57it/s] 63%|   | 508/805 [03:08<01:22,  3.58it/s] 63%|   | 509/805 [03:09<01:22,  3.59it/s] 63%|   | 510/805 [03:09<01:22,  3.59it/s] 63%|   | 511/805 [03:09<01:21,  3.60it/s] 64%|   | 512/805 [03:09<01:22,  3.56it/s] 64%|   | 513/805 [03:10<01:21,  3.57it/s] 64%|   | 514/805 [03:10<01:21,  3.57it/s] 64%|   | 515/805 [03:10<01:20,  3.58it/s] 64%|   | 516/805 [03:10<01:20,  3.59it/s] 64%|   | 517/805 [03:11<01:20,  3.59it/s] 64%|   | 518/805 [03:11<01:19,  3.59it/s] 64%|   | 519/805 [03:11<01:19,  3.60it/s] 65%|   | 520/805 [03:12<01:19,  3.60it/s] 65%|   | 521/805 [03:12<01:19,  3.59it/s] 65%|   | 522/805 [03:12<01:18,  3.59it/s] 65%|   | 523/805 [03:12<01:20,  3.49it/s] 65%|   | 524/805 [03:13<01:19,  3.52it/s] 65%|   | 525/805 [03:13<01:19,  3.54it/s] 65%|   | 526/805 [03:13<01:18,  3.55it/s] 65%|   | 527/805 [03:14<01:18,  3.56it/s] 66%|   | 528/805 [03:14<01:17,  3.57it/s] 66%|   | 529/805 [03:14<01:17,  3.58it/s] 66%|   | 530/805 [03:14<01:16,  3.58it/s] 66%|   | 531/805 [03:15<01:16,  3.58it/s] 66%|   | 532/805 [03:15<01:16,  3.59it/s] 66%|   | 533/805 [03:15<01:15,  3.60it/s] 66%|   | 534/805 [03:16<01:21,  3.31it/s] 66%|   | 535/805 [03:16<01:19,  3.39it/s] 67%|   | 536/805 [03:16<01:17,  3.45it/s] 67%|   | 537/805 [03:16<01:16,  3.50it/s] 67%|   | 538/805 [03:17<01:15,  3.52it/s] 67%|   | 539/805 [03:17<01:15,  3.54it/s] 67%|   | 540/805 [03:17<01:14,  3.56it/s] 67%|   | 541/805 [03:18<01:13,  3.58it/s] 67%|   | 542/805 [03:18<01:13,  3.59it/s] 67%|   | 543/805 [03:18<01:12,  3.59it/s] 68%|   | 544/805 [03:18<01:12,  3.60it/s] 68%|   | 545/805 [03:19<01:16,  3.42it/s] 68%|   | 546/805 [03:19<01:14,  3.47it/s] 68%|   | 547/805 [03:19<01:13,  3.51it/s] 68%|   | 548/805 [03:20<01:12,  3.53it/s] 68%|   | 549/805 [03:20<01:12,  3.55it/s] 68%|   | 550/805 [03:20<01:11,  3.57it/s] 68%|   | 551/805 [03:20<01:11,  3.57it/s] 69%|   | 552/805 [03:21<01:10,  3.57it/s] 69%|   | 553/805 [03:21<01:10,  3.58it/s] 69%|   | 554/805 [03:21<01:09,  3.59it/s] 69%|   | 555/805 [03:21<01:09,  3.60it/s] 69%|   | 556/805 [03:22<01:10,  3.52it/s] 69%|   | 557/805 [03:22<01:10,  3.54it/s] 69%|   | 558/805 [03:22<01:09,  3.56it/s] 69%|   | 559/805 [03:23<01:08,  3.57it/s] 70%|   | 560/805 [03:23<01:08,  3.57it/s] 70%|   | 561/805 [03:23<01:08,  3.58it/s] 70%|   | 562/805 [03:24<01:11,  3.41it/s] 70%|   | 563/805 [03:24<01:09,  3.46it/s] 70%|   | 564/805 [03:24<01:08,  3.50it/s] 70%|   | 565/805 [03:24<01:08,  3.52it/s] 70%|   | 566/805 [03:25<01:07,  3.54it/s] 70%|   | 567/805 [03:25<01:06,  3.55it/s] 71%|   | 568/805 [03:25<01:06,  3.56it/s] 71%|   | 569/805 [03:25<01:05,  3.58it/s] 71%|   | 570/805 [03:26<01:05,  3.58it/s] 71%|   | 571/805 [03:26<01:05,  3.58it/s] 71%|   | 572/805 [03:26<01:04,  3.59it/s] 71%|   | 573/805 [03:27<01:08,  3.39it/s] 71%|  | 574/805 [03:27<01:06,  3.45it/s] 71%|  | 575/805 [03:27<01:05,  3.49it/s] 72%|  | 576/805 [03:27<01:04,  3.53it/s] 72%|  | 577/805 [03:28<01:04,  3.55it/s] 72%|  | 578/805 [03:28<01:03,  3.56it/s] 72%|  | 579/805 [03:28<01:03,  3.58it/s] 72%|  | 580/805 [03:29<01:02,  3.59it/s] 72%|  | 581/805 [03:29<01:02,  3.59it/s] 72%|  | 582/805 [03:29<01:02,  3.59it/s] 72%|  | 583/805 [03:29<01:01,  3.60it/s] 73%|  | 584/805 [03:30<01:02,  3.52it/s] 73%|  | 585/805 [03:30<01:02,  3.54it/s] 73%|  | 586/805 [03:30<01:01,  3.55it/s] 73%|  | 587/805 [03:31<01:01,  3.57it/s] 73%|  | 588/805 [03:31<01:00,  3.59it/s] 73%|  | 589/805 [03:31<01:00,  3.59it/s] 73%|  | 590/805 [03:31<00:59,  3.60it/s] 73%|  | 591/805 [03:32<00:59,  3.60it/s] 74%|  | 592/805 [03:32<00:59,  3.60it/s] 74%|  | 593/805 [03:32<00:58,  3.60it/s] 74%|  | 594/805 [03:32<00:58,  3.61it/s] 74%|  | 595/805 [03:33<01:00,  3.45it/s] 74%|  | 596/805 [03:33<00:59,  3.50it/s] 74%|  | 597/805 [03:33<00:59,  3.52it/s] 74%|  | 598/805 [03:34<00:58,  3.55it/s] 74%|  | 599/805 [03:34<00:57,  3.57it/s] 75%|  | 600/805 [03:34<00:57,  3.57it/s] 75%|  | 601/805 [03:34<00:57,  3.57it/s] 75%|  | 602/805 [03:35<00:56,  3.58it/s] 75%|  | 603/805 [03:35<00:56,  3.59it/s] 75%|  | 604/805 [03:35<00:56,  3.59it/s] 75%|  | 605/805 [03:36<00:55,  3.59it/s] 75%|  | 606/805 [03:36<00:57,  3.46it/s] 75%|  | 607/805 [03:36<00:56,  3.51it/s] 76%|  | 608/805 [03:36<00:55,  3.54it/s] 76%|  | 609/805 [03:37<00:55,  3.56it/s] 76%|  | 610/805 [03:37<00:54,  3.56it/s] 76%|  | 611/805 [03:37<00:54,  3.58it/s] 76%|  | 612/805 [03:38<00:53,  3.59it/s] 76%|  | 613/805 [03:38<00:53,  3.59it/s] 76%|  | 614/805 [03:38<00:53,  3.60it/s] 76%|  | 615/805 [03:38<00:52,  3.60it/s] 77%|  | 616/805 [03:39<00:52,  3.60it/s] 77%|  | 617/805 [03:39<00:53,  3.51it/s] 77%|  | 618/805 [03:39<00:52,  3.53it/s] 77%|  | 619/805 [03:40<00:52,  3.55it/s] 77%|  | 620/805 [03:40<00:51,  3.57it/s] 77%|  | 621/805 [03:40<00:51,  3.58it/s] 77%|  | 622/805 [03:40<00:51,  3.58it/s] 77%|  | 623/805 [03:41<00:50,  3.58it/s] 78%|  | 624/805 [03:41<00:50,  3.59it/s] 78%|  | 625/805 [03:41<00:50,  3.60it/s] 78%|  | 626/805 [03:41<00:49,  3.60it/s] 78%|  | 627/805 [03:42<00:49,  3.60it/s] 78%|  | 628/805 [03:42<00:50,  3.54it/s] 78%|  | 629/805 [03:42<00:49,  3.56it/s] 78%|  | 630/805 [03:43<00:48,  3.57it/s] 78%|  | 631/805 [03:43<00:48,  3.58it/s] 79%|  | 632/805 [03:43<00:48,  3.59it/s] 79%|  | 633/805 [03:43<00:47,  3.60it/s] 79%|  | 634/805 [03:44<00:47,  3.59it/s] 79%|  | 635/805 [03:44<01:01,  2.75it/s] 79%|  | 636/805 [03:45<00:57,  2.95it/s] 79%|  | 637/805 [03:45<00:53,  3.12it/s] 79%|  | 638/805 [03:45<00:52,  3.18it/s] 79%|  | 639/805 [03:45<00:50,  3.30it/s] 80%|  | 640/805 [03:46<00:48,  3.38it/s] 80%|  | 641/805 [03:46<00:47,  3.44it/s] 80%|  | 642/805 [03:46<00:46,  3.49it/s] 80%|  | 643/805 [03:47<00:45,  3.53it/s] 80%|  | 644/805 [03:47<00:45,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 03:23:46,534 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:23:46,534 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 03:23:46,534 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.8112, 'eval_samples_per_second': 354.389, 'eval_steps_per_second': 44.337, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.1940993788819876e-05, 'epoch': 3.11}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.02it/s][A
  3%|         | 12/435 [00:00<00:08, 49.18it/s][A
  4%|         | 17/435 [00:00<00:08, 47.29it/s][A
  5%|         | 22/435 [00:00<00:08, 46.48it/s][A
  6%|         | 27/435 [00:00<00:08, 45.74it/s][A
  7%|         | 32/435 [00:00<00:08, 45.33it/s][A
  9%|         | 37/435 [00:00<00:08, 45.08it/s][A
 10%|         | 42/435 [00:00<00:08, 44.69it/s][A
 11%|         | 47/435 [00:01<00:08, 44.80it/s][A
 12%|        | 52/435 [00:01<00:08, 42.59it/s][A
 13%|        | 57/435 [00:01<00:08, 43.33it/s][A
 14%|        | 62/435 [00:01<00:08, 43.86it/s][A
 15%|        | 67/435 [00:01<00:08, 44.23it/s][A
 17%|        | 72/435 [00:01<00:08, 44.53it/s][A
 18%|        | 77/435 [00:01<00:08, 44.74it/s][A
 19%|        | 82/435 [00:01<00:07, 44.73it/s][A
 20%|        | 87/435 [00:01<00:07, 44.65it/s][A
 21%|        | 92/435 [00:02<00:07, 44.47it/s][A
 22%|       | 97/435 [00:02<00:07, 44.51it/s][A
 23%|       | 102/435 [00:02<00:07, 44.66it/s][A
 25%|       | 107/435 [00:02<00:07, 44.81it/s][A
 26%|       | 112/435 [00:02<00:07, 44.91it/s][A
 27%|       | 117/435 [00:02<00:07, 44.93it/s][A
 28%|       | 122/435 [00:02<00:06, 44.99it/s][A
 29%|       | 127/435 [00:02<00:06, 45.05it/s][A
 30%|       | 132/435 [00:02<00:06, 44.89it/s][A
 31%|      | 137/435 [00:03<00:06, 44.73it/s][A
 33%|      | 142/435 [00:03<00:06, 44.74it/s][A
 34%|      | 147/435 [00:03<00:06, 44.66it/s][A
 35%|      | 152/435 [00:03<00:06, 44.79it/s][A
 36%|      | 157/435 [00:03<00:06, 44.81it/s][A
 37%|      | 162/435 [00:03<00:06, 44.95it/s][A
 38%|      | 167/435 [00:03<00:05, 44.96it/s][A
 40%|      | 172/435 [00:03<00:05, 44.99it/s][A
 41%|      | 177/435 [00:03<00:05, 44.97it/s][A
 42%|     | 182/435 [00:04<00:05, 44.79it/s][A
 43%|     | 187/435 [00:04<00:06, 40.30it/s][A
 44%|     | 192/435 [00:04<00:05, 41.68it/s][A
 45%|     | 197/435 [00:04<00:05, 42.74it/s][A
 46%|     | 202/435 [00:04<00:05, 43.37it/s][A
 48%|     | 207/435 [00:04<00:05, 43.96it/s][A
 49%|     | 212/435 [00:04<00:05, 44.32it/s][A
 50%|     | 217/435 [00:04<00:04, 44.56it/s][A
 51%|     | 222/435 [00:04<00:04, 44.56it/s][A
 52%|    | 227/435 [00:05<00:04, 44.28it/s][A
 53%|    | 232/435 [00:05<00:04, 44.24it/s][A
 54%|    | 237/435 [00:05<00:04, 44.43it/s][A
 56%|    | 242/435 [00:05<00:04, 44.55it/s][A
 57%|    | 247/435 [00:05<00:04, 44.86it/s][A
 58%|    | 252/435 [00:05<00:04, 44.93it/s][A
 59%|    | 257/435 [00:05<00:03, 45.02it/s][A
 60%|    | 262/435 [00:05<00:03, 45.02it/s][A
 61%|   | 267/435 [00:05<00:03, 44.84it/s][A
 63%|   | 272/435 [00:06<00:03, 44.64it/s][A
 64%|   | 277/435 [00:06<00:03, 44.44it/s][A
 65%|   | 282/435 [00:06<00:03, 44.46it/s][A
 66%|   | 287/435 [00:06<00:03, 44.64it/s][A
 67%|   | 292/435 [00:06<00:03, 44.68it/s][A
 68%|   | 297/435 [00:06<00:03, 44.94it/s][A
 69%|   | 302/435 [00:06<00:02, 44.95it/s][A
 71%|   | 307/435 [00:06<00:02, 45.04it/s][A
 72%|  | 312/435 [00:06<00:02, 44.89it/s][A
 73%|  | 317/435 [00:07<00:02, 44.92it/s][A
 74%|  | 322/435 [00:07<00:02, 43.02it/s][A
 75%|  | 327/435 [00:07<00:02, 43.64it/s][A
 76%|  | 332/435 [00:07<00:02, 44.05it/s][A
 77%|  | 337/435 [00:07<00:02, 44.31it/s][A
 79%|  | 342/435 [00:07<00:02, 44.49it/s][A
 80%|  | 347/435 [00:07<00:01, 44.75it/s][A
 81%|  | 352/435 [00:07<00:01, 44.71it/s][A
 82%| | 357/435 [00:08<00:01, 44.73it/s][A
 83%| | 362/435 [00:08<00:01, 44.60it/s][A
 84%| | 367/435 [00:08<00:01, 44.46it/s][A
 86%| | 372/435 [00:08<00:01, 44.58it/s][A
 87%| | 377/435 [00:08<00:01, 44.63it/s][A
 88%| | 382/435 [00:08<00:01, 44.64it/s][A
 89%| | 387/435 [00:08<00:01, 44.89it/s][A
 90%| | 392/435 [00:08<00:00, 44.89it/s][A
 91%|| 397/435 [00:08<00:00, 45.04it/s][A
 92%|| 402/435 [00:09<00:00, 44.89it/s][A
 94%|| 407/435 [00:09<00:00, 44.91it/s][A
 95%|| 412/435 [00:09<00:00, 44.79it/s][A
 96%|| 417/435 [00:09<00:00, 44.71it/s][A
 97%|| 422/435 [00:09<00:00, 44.79it/s][A
 98%|| 427/435 [00:09<00:00, 44.67it/s][A
 99%|| 432/435 [00:09<00:00, 44.83it/s][A                                                 
                                                 [A 80%|  | 644/805 [03:57<00:45,  3.55it/s]
100%|| 435/435 [00:09<00:00, 44.83it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:23:56,848 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-644
[INFO|configuration_utils.py:351] 2023-08-28 03:23:57,318 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-644/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:24:01,961 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-644/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:24:02,245 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-644/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:24:02,397 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-644/special_tokens_map.json
 80%|  | 645/805 [04:04<14:29,  5.44s/it] 80%|  | 646/805 [04:05<10:18,  3.89s/it] 80%|  | 647/805 [04:05<07:23,  2.81s/it] 80%|  | 648/805 [04:05<05:21,  2.05s/it] 81%|  | 649/805 [04:05<03:56,  1.52s/it] 81%|  | 650/805 [04:06<02:58,  1.15s/it] 81%|  | 651/805 [04:06<02:17,  1.12it/s] 81%|  | 652/805 [04:06<01:48,  1.41it/s] 81%|  | 653/805 [04:07<01:27,  1.73it/s] 81%|  | 654/805 [04:07<01:13,  2.05it/s] 81%| | 655/805 [04:07<01:03,  2.35it/s] 81%| | 656/805 [04:07<00:56,  2.62it/s] 82%| | 657/805 [04:08<00:51,  2.86it/s] 82%| | 658/805 [04:08<00:48,  3.04it/s] 82%| | 659/805 [04:08<00:45,  3.19it/s] 82%| | 660/805 [04:08<00:43,  3.31it/s] 82%| | 661/805 [04:09<00:43,  3.30it/s] 82%| | 662/805 [04:09<00:42,  3.39it/s] 82%| | 663/805 [04:09<00:41,  3.45it/s] 82%| | 664/805 [04:10<00:40,  3.49it/s] 83%| | 665/805 [04:10<00:39,  3.52it/s] 83%| | 666/805 [04:10<00:39,  3.55it/s] 83%| | 667/805 [04:10<00:38,  3.56it/s] 83%| | 668/805 [04:11<00:38,  3.58it/s] 83%| | 669/805 [04:11<00:37,  3.59it/s] 83%| | 670/805 [04:11<00:37,  3.60it/s] 83%| | 671/805 [04:12<00:37,  3.60it/s] 83%| | 672/805 [04:12<00:37,  3.52it/s] 84%| | 673/805 [04:12<00:37,  3.54it/s] 84%| | 674/805 [04:12<00:36,  3.55it/s] 84%| | 675/805 [04:13<00:36,  3.57it/s] 84%| | 676/805 [04:13<00:36,  3.57it/s] 84%| | 677/805 [04:13<00:35,  3.58it/s] 84%| | 678/805 [04:14<00:35,  3.58it/s] 84%| | 679/805 [04:14<00:35,  3.59it/s] 84%| | 680/805 [04:14<00:34,  3.60it/s] 85%| | 681/805 [04:14<00:34,  3.60it/s] 85%| | 682/805 [04:15<00:34,  3.60it/s] 85%| | 683/805 [04:15<00:34,  3.50it/s] 85%| | 684/805 [04:15<00:34,  3.53it/s] 85%| | 685/805 [04:15<00:33,  3.55it/s] 85%| | 686/805 [04:16<00:33,  3.56it/s] 85%| | 687/805 [04:16<00:33,  3.57it/s] 85%| | 688/805 [04:16<00:32,  3.57it/s] 86%| | 689/805 [04:17<00:32,  3.58it/s] 86%| | 690/805 [04:17<00:32,  3.59it/s] 86%| | 691/805 [04:17<00:31,  3.59it/s] 86%| | 692/805 [04:17<00:31,  3.60it/s] 86%| | 693/805 [04:18<00:31,  3.60it/s] 86%| | 694/805 [04:18<00:32,  3.44it/s] 86%| | 695/805 [04:18<00:31,  3.49it/s] 86%| | 696/805 [04:19<00:30,  3.52it/s] 87%| | 697/805 [04:19<00:30,  3.55it/s] 87%| | 698/805 [04:19<00:30,  3.56it/s] 87%| | 699/805 [04:19<00:29,  3.57it/s] 87%| | 700/805 [04:20<00:29,  3.57it/s] 87%| | 701/805 [04:20<00:29,  3.58it/s] 87%| | 702/805 [04:20<00:28,  3.59it/s] 87%| | 703/805 [04:21<00:28,  3.59it/s] 87%| | 704/805 [04:21<00:28,  3.60it/s] 88%| | 705/805 [04:21<00:28,  3.46it/s] 88%| | 706/805 [04:21<00:28,  3.50it/s] 88%| | 707/805 [04:22<00:27,  3.53it/s] 88%| | 708/805 [04:22<00:27,  3.55it/s] 88%| | 709/805 [04:22<00:26,  3.57it/s] 88%| | 710/805 [04:22<00:26,  3.58it/s] 88%| | 711/805 [04:23<00:26,  3.59it/s] 88%| | 712/805 [04:23<00:25,  3.59it/s] 89%| | 713/805 [04:23<00:25,  3.60it/s] 89%| | 714/805 [04:24<00:25,  3.59it/s] 89%| | 715/805 [04:24<00:25,  3.59it/s] 89%| | 716/805 [04:24<00:25,  3.48it/s] 89%| | 717/805 [04:24<00:25,  3.51it/s] 89%| | 718/805 [04:25<00:24,  3.53it/s] 89%| | 719/805 [04:25<00:24,  3.55it/s] 89%| | 720/805 [04:25<00:23,  3.56it/s] 90%| | 721/805 [04:26<00:23,  3.57it/s] 90%| | 722/805 [04:26<00:23,  3.58it/s] 90%| | 723/805 [04:26<00:22,  3.58it/s] 90%| | 724/805 [04:26<00:22,  3.59it/s] 90%| | 725/805 [04:27<00:22,  3.60it/s] 90%| | 726/805 [04:27<00:22,  3.59it/s] 90%| | 727/805 [04:27<00:21,  3.59it/s] 90%| | 728/805 [04:28<00:21,  3.60it/s] 91%| | 729/805 [04:28<00:21,  3.60it/s] 91%| | 730/805 [04:28<00:25,  2.89it/s] 91%| | 731/805 [04:29<00:24,  3.07it/s] 91%| | 732/805 [04:29<00:22,  3.22it/s] 91%| | 733/805 [04:29<00:21,  3.32it/s] 91%| | 734/805 [04:29<00:20,  3.40it/s] 91%|| 735/805 [04:30<00:20,  3.46it/s] 91%|| 736/805 [04:30<00:19,  3.50it/s] 92%|| 737/805 [04:30<00:19,  3.53it/s] 92%|| 738/805 [04:31<00:18,  3.55it/s] 92%|| 739/805 [04:31<00:18,  3.56it/s] 92%|| 740/805 [04:31<00:18,  3.45it/s] 92%|| 741/805 [04:31<00:18,  3.49it/s] 92%|| 742/805 [04:32<00:17,  3.52it/s] 92%|| 743/805 [04:32<00:17,  3.54it/s] 92%|| 744/805 [04:32<00:17,  3.56it/s] 93%|| 745/805 [04:33<00:16,  3.58it/s] 93%|| 746/805 [04:33<00:16,  3.59it/s] 93%|| 747/805 [04:33<00:16,  3.59it/s] 93%|| 748/805 [04:33<00:15,  3.59it/s] 93%|| 749/805 [04:34<00:15,  3.60it/s] 93%|| 750/805 [04:34<00:15,  3.59it/s] 93%|| 751/805 [04:34<00:15,  3.50it/s] 93%|| 752/805 [04:34<00:15,  3.53it/s] 94%|| 753/805 [04:35<00:14,  3.55it/s] 94%|| 754/805 [04:35<00:14,  3.56it/s] 94%|| 755/805 [04:35<00:13,  3.57it/s] 94%|| 756/805 [04:36<00:13,  3.58it/s] 94%|| 757/805 [04:36<00:13,  3.59it/s] 94%|| 758/805 [04:36<00:13,  3.60it/s] 94%|| 759/805 [04:36<00:12,  3.60it/s] 94%|| 760/805 [04:37<00:12,  3.60it/s] 95%|| 761/805 [04:37<00:12,  3.60it/s] 95%|| 762/805 [04:37<00:12,  3.49it/s] 95%|| 763/805 [04:38<00:11,  3.53it/s] 95%|| 764/805 [04:38<00:11,  3.55it/s] 95%|| 765/805 [04:38<00:11,  3.56it/s] 95%|| 766/805 [04:38<00:10,  3.57it/s] 95%|| 767/805 [04:39<00:10,  3.58it/s] 95%|| 768/805 [04:39<00:10,  3.58it/s] 96%|| 769/805 [04:39<00:10,  3.58it/s] 96%|| 770/805 [04:40<00:09,  3.59it/s] 96%|| 771/805 [04:40<00:09,  3.55it/s] 96%|| 772/805 [04:40<00:09,  3.55it/s] 96%|| 773/805 [04:40<00:09,  3.43it/s] 96%|| 774/805 [04:41<00:08,  3.48it/s] 96%|| 775/805 [04:41<00:08,  3.52it/s] 96%|| 776/805 [04:41<00:08,  3.55it/s] 97%|| 777/805 [04:42<00:07,  3.56it/s] 97%|| 778/805 [04:42<00:07,  3.57it/s] 97%|| 779/805 [04:42<00:07,  3.58it/s] 97%|| 780/805 [04:42<00:06,  3.58it/s] 97%|| 781/805 [04:43<00:06,  3.59it/s] 97%|| 782/805 [04:43<00:06,  3.59it/s] 97%|| 783/805 [04:43<00:06,  3.59it/s] 97%|| 784/805 [04:43<00:05,  3.51it/s] 98%|| 785/805 [04:44<00:05,  3.54it/s] 98%|| 786/805 [04:44<00:05,  3.50it/s] 98%|| 787/805 [04:44<00:05,  3.33it/s] 98%|| 788/805 [04:45<00:05,  3.39it/s] 98%|| 789/805 [04:45<00:04,  3.45it/s] 98%|| 790/805 [04:45<00:04,  3.50it/s] 98%|| 791/805 [04:45<00:03,  3.53it/s] 98%|| 792/805 [04:46<00:03,  3.55it/s] 99%|| 793/805 [04:46<00:03,  3.57it/s] 99%|| 794/805 [04:46<00:03,  3.58it/s] 99%|| 795/805 [04:47<00:02,  3.45it/s] 99%|| 796/805 [04:47<00:02,  3.49it/s] 99%|| 797/805 [04:47<00:02,  3.51it/s] 99%|| 798/805 [04:47<00:01,  3.54it/s] 99%|| 799/805 [04:48<00:01,  3.56it/s] 99%|| 800/805 [04:48<00:01,  3.57it/s]100%|| 801/805 [04:48<00:01,  3.58it/s]100%|| 802/805 [04:49<00:00,  3.59it/s]100%|| 803/805 [04:49<00:00,  3.59it/s]100%|| 804/805 [04:49<00:00,  3.59it/s]100%|| 805/805 [04:49<00:00,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 03:24:49,168 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:24:49,169 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 03:24:49,169 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.8005, 'eval_samples_per_second': 354.779, 'eval_steps_per_second': 44.386, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.87it/s][A
  3%|         | 12/435 [00:00<00:08, 49.19it/s][A
  4%|         | 17/435 [00:00<00:08, 47.39it/s][A
  5%|         | 22/435 [00:00<00:08, 46.56it/s][A
  6%|         | 27/435 [00:00<00:08, 46.00it/s][A
  7%|         | 32/435 [00:00<00:08, 45.69it/s][A
  9%|         | 37/435 [00:00<00:08, 45.56it/s][A
 10%|         | 42/435 [00:00<00:08, 45.12it/s][A
 11%|         | 47/435 [00:01<00:08, 44.87it/s][A
 12%|        | 52/435 [00:01<00:08, 44.67it/s][A
 13%|        | 57/435 [00:01<00:08, 44.79it/s][A
 14%|        | 62/435 [00:01<00:08, 44.83it/s][A
 15%|        | 67/435 [00:01<00:08, 44.90it/s][A
 17%|        | 72/435 [00:01<00:08, 45.00it/s][A
 18%|        | 77/435 [00:01<00:07, 44.99it/s][A
 19%|        | 82/435 [00:01<00:07, 45.05it/s][A
 20%|        | 87/435 [00:01<00:07, 44.88it/s][A
 21%|        | 92/435 [00:02<00:07, 44.77it/s][A
 22%|       | 97/435 [00:02<00:07, 44.72it/s][A
 23%|       | 102/435 [00:02<00:07, 44.60it/s][A
 25%|       | 107/435 [00:02<00:07, 44.80it/s][A
 26%|       | 112/435 [00:02<00:07, 44.84it/s][A
 27%|       | 117/435 [00:02<00:07, 44.94it/s][A
 28%|       | 122/435 [00:02<00:06, 44.96it/s][A
 29%|       | 127/435 [00:02<00:06, 44.94it/s][A
 30%|       | 132/435 [00:02<00:06, 45.00it/s][A
 31%|      | 137/435 [00:03<00:06, 44.85it/s][A
 33%|      | 142/435 [00:03<00:07, 41.66it/s][A
 34%|      | 147/435 [00:03<00:06, 42.66it/s][A
 35%|      | 152/435 [00:03<00:06, 43.35it/s][A
 36%|      | 157/435 [00:03<00:06, 43.84it/s][A
 37%|      | 162/435 [00:03<00:06, 44.16it/s][A
 38%|      | 167/435 [00:03<00:06, 44.41it/s][A
 40%|      | 172/435 [00:03<00:05, 44.66it/s][A
 41%|      | 177/435 [00:03<00:05, 44.71it/s][A
 42%|     | 182/435 [00:04<00:05, 44.55it/s][A
 43%|     | 187/435 [00:04<00:05, 44.52it/s][A
 44%|     | 192/435 [00:04<00:05, 44.65it/s][A
 45%|     | 197/435 [00:04<00:05, 44.71it/s][A
 46%|     | 202/435 [00:04<00:05, 44.81it/s][A
 48%|     | 207/435 [00:04<00:05, 44.83it/s][A
 49%|     | 212/435 [00:04<00:04, 44.89it/s][A
 50%|     | 217/435 [00:04<00:04, 44.99it/s][A
 51%|     | 222/435 [00:04<00:04, 44.86it/s][A
 52%|    | 227/435 [00:05<00:04, 44.78it/s][A
 53%|    | 232/435 [00:05<00:04, 44.71it/s][A
 54%|    | 237/435 [00:05<00:04, 44.70it/s][A
 56%|    | 242/435 [00:05<00:04, 44.79it/s][A
 57%|    | 247/435 [00:05<00:04, 44.76it/s][A
 58%|    | 252/435 [00:05<00:04, 44.94it/s][A
 59%|    | 257/435 [00:05<00:03, 44.89it/s][A
 60%|    | 262/435 [00:05<00:03, 44.94it/s][A
 61%|   | 267/435 [00:05<00:03, 44.96it/s][A
 63%|   | 272/435 [00:06<00:03, 44.86it/s][A
 64%|   | 277/435 [00:06<00:03, 44.93it/s][A
 65%|   | 282/435 [00:06<00:03, 44.78it/s][A
 66%|   | 287/435 [00:06<00:03, 44.87it/s][A
 67%|   | 292/435 [00:06<00:03, 44.84it/s][A
 68%|   | 297/435 [00:06<00:03, 44.88it/s][A
 69%|   | 302/435 [00:06<00:02, 44.92it/s][A
 71%|   | 307/435 [00:06<00:02, 44.89it/s][A
 72%|  | 312/435 [00:06<00:02, 44.91it/s][A
 73%|  | 317/435 [00:07<00:02, 44.81it/s][A
 74%|  | 322/435 [00:07<00:02, 44.88it/s][A
 75%|  | 327/435 [00:07<00:02, 44.89it/s][A
 76%|  | 332/435 [00:07<00:02, 44.90it/s][A
 77%|  | 337/435 [00:07<00:02, 44.90it/s][A
 79%|  | 342/435 [00:07<00:02, 44.83it/s][A
 80%|  | 347/435 [00:07<00:01, 44.92it/s][A
 81%|  | 352/435 [00:07<00:01, 44.90it/s][A
 82%| | 357/435 [00:07<00:01, 44.84it/s][A
 83%| | 362/435 [00:08<00:01, 44.90it/s][A
 84%| | 367/435 [00:08<00:01, 44.84it/s][A
 86%| | 372/435 [00:08<00:01, 44.91it/s][A
 87%| | 377/435 [00:08<00:01, 44.87it/s][A
 88%| | 382/435 [00:08<00:01, 44.92it/s][A
 89%| | 387/435 [00:08<00:01, 44.87it/s][A
 90%| | 392/435 [00:08<00:00, 44.82it/s][A
 91%|| 397/435 [00:08<00:00, 44.77it/s][A
 92%|| 402/435 [00:09<00:00, 44.81it/s][A
 94%|| 407/435 [00:09<00:00, 38.91it/s][A
 95%|| 412/435 [00:09<00:00, 40.54it/s][A
 96%|| 417/435 [00:09<00:00, 41.74it/s][A
 97%|| 422/435 [00:09<00:00, 42.65it/s][A
 98%|| 427/435 [00:09<00:00, 43.42it/s][A
 99%|| 432/435 [00:09<00:00, 43.88it/s][A                                                 
                                                 [A100%|| 805/805 [04:59<00:00,  3.60it/s]
100%|| 435/435 [00:09<00:00, 43.88it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:24:59,416 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-805
[INFO|configuration_utils.py:351] 2023-08-28 03:24:59,786 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-805/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:25:04,019 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-805/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:25:04,333 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-805/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:25:04,491 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-805/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 03:25:05,690 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 03:25:05,690 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-161 (score: 0.9974275827407837).
                                                 100%|| 805/805 [05:16<00:00,  3.60it/s]100%|| 805/805 [05:16<00:00,  2.55it/s]
[INFO|trainer.py:1894] 2023-08-28 03:25:15,446 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 03:25:15,699 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:25:21,111 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:25:21,272 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:25:21,351 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:25:21,846 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:21,846 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:21,846 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:21,846 >>   train_runtime            = 0:05:16.17
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:21,846 >>   train_samples            =      10320
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:21,846 >>   train_samples_per_second =    163.202
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:21,846 >>   train_steps_per_second   =      2.546
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7718, 'eval_samples_per_second': 355.819, 'eval_steps_per_second': 44.516, 'epoch': 5.0}
{'train_runtime': 316.1729, 'train_samples_per_second': 163.202, 'train_steps_per_second': 2.546, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 03:25:22 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 03:25:22,091 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:25:22,091 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 03:25:22,091 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 55.67it/s]  3%|         | 12/435 [00:00<00:08, 49.02it/s]  4%|         | 17/435 [00:00<00:08, 47.51it/s]  5%|         | 22/435 [00:00<00:08, 46.63it/s]  6%|         | 27/435 [00:00<00:08, 46.23it/s]  7%|         | 32/435 [00:00<00:08, 45.96it/s]  9%|         | 37/435 [00:00<00:08, 45.71it/s] 10%|         | 42/435 [00:00<00:08, 45.40it/s] 11%|         | 47/435 [00:01<00:08, 44.93it/s] 12%|        | 52/435 [00:01<00:08, 44.80it/s] 13%|        | 57/435 [00:01<00:08, 44.92it/s] 14%|        | 62/435 [00:01<00:08, 44.97it/s] 15%|        | 67/435 [00:01<00:08, 45.17it/s] 17%|        | 72/435 [00:01<00:08, 45.13it/s] 18%|        | 77/435 [00:01<00:07, 45.16it/s] 19%|        | 82/435 [00:01<00:07, 45.20it/s] 20%|        | 87/435 [00:01<00:07, 45.06it/s] 21%|        | 92/435 [00:02<00:07, 44.95it/s] 22%|       | 97/435 [00:02<00:07, 43.05it/s] 23%|       | 102/435 [00:02<00:07, 43.72it/s] 25%|       | 107/435 [00:02<00:07, 44.21it/s] 26%|       | 112/435 [00:02<00:07, 44.46it/s] 27%|       | 117/435 [00:02<00:07, 44.79it/s] 28%|       | 122/435 [00:02<00:06, 44.76it/s] 29%|       | 127/435 [00:02<00:06, 44.76it/s] 30%|       | 132/435 [00:02<00:06, 44.64it/s] 31%|      | 137/435 [00:03<00:06, 44.43it/s] 33%|      | 142/435 [00:03<00:06, 44.38it/s] 34%|      | 147/435 [00:03<00:06, 44.78it/s] 35%|      | 152/435 [00:03<00:06, 44.87it/s] 36%|      | 157/435 [00:03<00:06, 45.06it/s] 37%|      | 162/435 [00:03<00:06, 45.06it/s] 38%|      | 167/435 [00:03<00:05, 45.07it/s] 40%|      | 172/435 [00:03<00:05, 45.03it/s] 41%|      | 177/435 [00:03<00:05, 44.95it/s] 42%|     | 182/435 [00:04<00:05, 44.87it/s] 43%|     | 187/435 [00:04<00:05, 44.81it/s] 44%|     | 192/435 [00:04<00:05, 44.82it/s] 45%|     | 197/435 [00:04<00:05, 44.88it/s] 46%|     | 202/435 [00:04<00:05, 45.06it/s] 48%|     | 207/435 [00:04<00:05, 45.06it/s] 49%|     | 212/435 [00:04<00:04, 45.03it/s] 50%|     | 217/435 [00:04<00:04, 44.94it/s] 51%|     | 222/435 [00:04<00:04, 44.88it/s] 52%|    | 227/435 [00:05<00:04, 44.78it/s] 53%|    | 232/435 [00:05<00:04, 40.92it/s] 54%|    | 237/435 [00:05<00:04, 42.15it/s] 56%|    | 242/435 [00:05<00:04, 43.07it/s] 57%|    | 247/435 [00:05<00:04, 43.76it/s] 58%|    | 252/435 [00:05<00:04, 44.18it/s] 59%|    | 257/435 [00:05<00:03, 44.61it/s] 60%|    | 262/435 [00:05<00:03, 44.73it/s] 61%|   | 267/435 [00:05<00:03, 44.84it/s] 63%|   | 272/435 [00:06<00:03, 44.49it/s] 64%|   | 277/435 [00:06<00:03, 44.46it/s] 65%|   | 282/435 [00:06<00:03, 44.67it/s] 66%|   | 287/435 [00:06<00:03, 44.78it/s] 67%|   | 292/435 [00:06<00:03, 44.91it/s] 68%|   | 297/435 [00:06<00:03, 45.01it/s] 69%|   | 302/435 [00:06<00:02, 45.13it/s] 71%|   | 307/435 [00:06<00:02, 45.10it/s] 72%|  | 312/435 [00:06<00:02, 45.00it/s] 73%|  | 317/435 [00:07<00:02, 44.79it/s] 74%|  | 322/435 [00:07<00:02, 44.75it/s] 75%|  | 327/435 [00:07<00:02, 44.80it/s] 76%|  | 332/435 [00:07<00:02, 44.90it/s] 77%|  | 337/435 [00:07<00:02, 44.99it/s] 79%|  | 342/435 [00:07<00:02, 45.01it/s] 80%|  | 347/435 [00:07<00:01, 45.00it/s] 81%|  | 352/435 [00:07<00:01, 45.00it/s] 82%| | 357/435 [00:07<00:01, 45.05it/s] 83%| | 362/435 [00:08<00:01, 44.92it/s] 84%| | 367/435 [00:08<00:01, 43.40it/s] 86%| | 372/435 [00:08<00:01, 43.79it/s] 87%| | 377/435 [00:08<00:01, 44.31it/s] 88%| | 382/435 [00:08<00:01, 44.43it/s] 89%| | 387/435 [00:08<00:01, 44.52it/s] 90%| | 392/435 [00:08<00:00, 44.71it/s] 91%|| 397/435 [00:08<00:00, 44.80it/s] 92%|| 402/435 [00:08<00:00, 44.86it/s] 94%|| 407/435 [00:09<00:00, 44.61it/s] 95%|| 412/435 [00:09<00:00, 44.77it/s] 96%|| 417/435 [00:09<00:00, 44.77it/s] 97%|| 422/435 [00:09<00:00, 44.96it/s] 98%|| 427/435 [00:09<00:00, 44.96it/s] 99%|| 432/435 [00:09<00:00, 44.90it/s]100%|| 435/435 [00:09<00:00, 44.77it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:25:31,827 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:31,827 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:31,827 >>   eval_loss               =     0.9974
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:31,827 >>   eval_runtime            = 0:00:09.73
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:31,827 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:31,827 >>   eval_samples_per_second =    357.152
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:31,827 >>   eval_steps_per_second   =     44.683
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:25:31,827 >>   perplexity              =     2.7113
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:40,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:40,944 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:40,944 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:40,944 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:40,944 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:25:41,804 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:25:41,805 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:25:42,590 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:25:43,743 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:25:43,743 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:46,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:46,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:46,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:46,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:25:46,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:25:47,597 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:25:47,598 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:25:48,201 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:25:48,434 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:25:48,434 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-483
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-644
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-322
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-805
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/checkpoint-161
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.78it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.75it/s]Extractor Predicting: 6it [00:03,  1.73it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:05,  1.68it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:08,  1.66it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.70it/s]Extractor Predicting: 18it [00:10,  1.72it/s]Extractor Predicting: 19it [00:11,  1.73it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:12,  1.74it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:14,  1.75it/s]Extractor Predicting: 25it [00:14,  1.71it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:15,  1.72it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:17,  1.66it/s]Extractor Predicting: 31it [00:18,  1.66it/s]Extractor Predicting: 32it [00:18,  1.67it/s]Extractor Predicting: 33it [00:19,  1.63it/s]Extractor Predicting: 34it [00:20,  1.64it/s]Extractor Predicting: 35it [00:20,  1.70it/s]Extractor Predicting: 36it [00:21,  1.74it/s]Extractor Predicting: 37it [00:21,  1.73it/s]Extractor Predicting: 38it [00:22,  1.67it/s]Extractor Predicting: 39it [00:23,  1.69it/s]Extractor Predicting: 40it [00:23,  1.66it/s]Extractor Predicting: 41it [00:24,  1.71it/s]Extractor Predicting: 42it [00:24,  1.71it/s]Extractor Predicting: 43it [00:25,  1.69it/s]Extractor Predicting: 44it [00:26,  1.69it/s]Extractor Predicting: 45it [00:26,  1.67it/s]Extractor Predicting: 46it [00:27,  1.67it/s]Extractor Predicting: 47it [00:27,  1.70it/s]Extractor Predicting: 48it [00:28,  1.71it/s]Extractor Predicting: 49it [00:28,  1.74it/s]Extractor Predicting: 50it [00:29,  1.79it/s]Extractor Predicting: 51it [00:30,  1.78it/s]Extractor Predicting: 52it [00:30,  1.70it/s]Extractor Predicting: 53it [00:31,  1.69it/s]Extractor Predicting: 54it [00:31,  1.72it/s]Extractor Predicting: 55it [00:32,  1.70it/s]Extractor Predicting: 56it [00:33,  1.69it/s]Extractor Predicting: 57it [00:33,  1.65it/s]Extractor Predicting: 58it [00:34,  1.69it/s]Extractor Predicting: 59it [00:34,  1.74it/s]Extractor Predicting: 60it [00:35,  1.76it/s]Extractor Predicting: 61it [00:35,  1.77it/s]Extractor Predicting: 62it [00:36,  1.79it/s]Extractor Predicting: 63it [00:37,  1.76it/s]Extractor Predicting: 64it [00:37,  1.76it/s]Extractor Predicting: 65it [00:38,  1.76it/s]Extractor Predicting: 66it [00:38,  1.73it/s]Extractor Predicting: 67it [00:39,  1.74it/s]Extractor Predicting: 68it [00:39,  1.76it/s]Extractor Predicting: 69it [00:40,  1.71it/s]Extractor Predicting: 70it [00:41,  1.71it/s]Extractor Predicting: 71it [00:41,  1.71it/s]Extractor Predicting: 72it [00:42,  1.74it/s]Extractor Predicting: 73it [00:42,  1.74it/s]Extractor Predicting: 74it [00:43,  1.76it/s]Extractor Predicting: 75it [00:43,  1.79it/s]Extractor Predicting: 76it [00:44,  1.74it/s]Extractor Predicting: 77it [00:45,  1.71it/s]Extractor Predicting: 78it [00:45,  1.71it/s]Extractor Predicting: 79it [00:46,  1.68it/s]Extractor Predicting: 80it [00:46,  1.67it/s]Extractor Predicting: 81it [00:47,  1.63it/s]Extractor Predicting: 82it [00:48,  1.68it/s]Extractor Predicting: 83it [00:48,  1.69it/s]Extractor Predicting: 84it [00:49,  1.71it/s]Extractor Predicting: 85it [00:49,  1.71it/s]Extractor Predicting: 86it [00:50,  1.72it/s]Extractor Predicting: 87it [00:51,  1.69it/s]Extractor Predicting: 88it [00:51,  1.68it/s]Extractor Predicting: 89it [00:52,  1.72it/s]Extractor Predicting: 90it [00:52,  1.72it/s]Extractor Predicting: 91it [00:53,  1.77it/s]Extractor Predicting: 92it [00:53,  1.81it/s]Extractor Predicting: 93it [00:54,  1.63it/s]Extractor Predicting: 94it [00:55,  1.65it/s]Extractor Predicting: 95it [00:55,  1.72it/s]Extractor Predicting: 96it [00:56,  1.74it/s]Extractor Predicting: 97it [00:56,  1.75it/s]Extractor Predicting: 98it [00:57,  1.76it/s]Extractor Predicting: 99it [00:58,  1.70it/s]Extractor Predicting: 100it [00:58,  1.68it/s]Extractor Predicting: 101it [00:59,  1.68it/s]Extractor Predicting: 102it [00:59,  1.77it/s]Extractor Predicting: 103it [01:00,  1.81it/s]Extractor Predicting: 104it [01:00,  1.80it/s]Extractor Predicting: 105it [01:01,  1.77it/s]Extractor Predicting: 106it [01:01,  1.77it/s]Extractor Predicting: 107it [01:02,  1.79it/s]Extractor Predicting: 108it [01:03,  1.78it/s]Extractor Predicting: 109it [01:03,  1.78it/s]Extractor Predicting: 110it [01:04,  1.78it/s]Extractor Predicting: 111it [01:04,  1.74it/s]Extractor Predicting: 112it [01:05,  1.75it/s]Extractor Predicting: 113it [01:05,  1.82it/s]Extractor Predicting: 114it [01:06,  1.86it/s]Extractor Predicting: 115it [01:06,  1.84it/s]Extractor Predicting: 116it [01:07,  1.84it/s]Extractor Predicting: 117it [01:08,  1.79it/s]Extractor Predicting: 118it [01:08,  1.78it/s]Extractor Predicting: 119it [01:09,  1.80it/s]Extractor Predicting: 120it [01:09,  1.76it/s]Extractor Predicting: 121it [01:10,  1.74it/s]Extractor Predicting: 122it [01:10,  1.75it/s]Extractor Predicting: 123it [01:11,  1.70it/s]Extractor Predicting: 124it [01:12,  1.69it/s]Extractor Predicting: 125it [01:12,  1.72it/s]Extractor Predicting: 126it [01:13,  1.75it/s]Extractor Predicting: 127it [01:13,  1.73it/s]Extractor Predicting: 128it [01:14,  1.75it/s]Extractor Predicting: 129it [01:14,  1.75it/s]Extractor Predicting: 130it [01:15,  1.69it/s]Extractor Predicting: 131it [01:16,  1.71it/s]Extractor Predicting: 132it [01:16,  1.69it/s]Extractor Predicting: 133it [01:17,  1.73it/s]Extractor Predicting: 134it [01:17,  1.76it/s]Extractor Predicting: 135it [01:18,  1.75it/s]Extractor Predicting: 136it [01:19,  1.68it/s]Extractor Predicting: 137it [01:19,  1.74it/s]Extractor Predicting: 138it [01:20,  1.73it/s]Extractor Predicting: 139it [01:20,  1.71it/s]Extractor Predicting: 140it [01:21,  1.73it/s]Extractor Predicting: 141it [01:21,  1.75it/s]Extractor Predicting: 142it [01:22,  1.70it/s]Extractor Predicting: 143it [01:23,  1.74it/s]Extractor Predicting: 144it [01:23,  1.79it/s]Extractor Predicting: 144it [01:23,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:26,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:26,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:26,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:26,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:26,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:27:26,672 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:27:26,673 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:27:27,254 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:27:28,347 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:27:28,347 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:31,352 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:31,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:31,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:31,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:31,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:27:32,062 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:27:32,063 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:27:32,659 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:27:32,831 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:27:32,832 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.74it/s]Extractor Predicting: 6it [00:03,  1.76it/s]Extractor Predicting: 7it [00:04,  1.80it/s]Extractor Predicting: 8it [00:04,  1.82it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.80it/s]Extractor Predicting: 11it [00:06,  1.81it/s]Extractor Predicting: 12it [00:06,  1.79it/s]Extractor Predicting: 13it [00:07,  1.78it/s]Extractor Predicting: 14it [00:07,  1.76it/s]Extractor Predicting: 15it [00:08,  1.79it/s]Extractor Predicting: 16it [00:09,  1.78it/s]Extractor Predicting: 17it [00:09,  1.76it/s]Extractor Predicting: 18it [00:10,  1.80it/s]Extractor Predicting: 19it [00:10,  1.83it/s]Extractor Predicting: 20it [00:11,  1.76it/s]Extractor Predicting: 21it [00:11,  1.78it/s]Extractor Predicting: 22it [00:12,  1.81it/s]Extractor Predicting: 23it [00:12,  1.81it/s]Extractor Predicting: 24it [00:13,  1.79it/s]Extractor Predicting: 25it [00:14,  1.78it/s]Extractor Predicting: 26it [00:14,  1.78it/s]Extractor Predicting: 27it [00:15,  1.76it/s]Extractor Predicting: 28it [00:15,  1.78it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:16,  1.82it/s]Extractor Predicting: 31it [00:17,  1.79it/s]Extractor Predicting: 32it [00:17,  1.84it/s]Extractor Predicting: 33it [00:18,  1.80it/s]Extractor Predicting: 34it [00:19,  1.77it/s]Extractor Predicting: 35it [00:19,  1.77it/s]Extractor Predicting: 36it [00:20,  1.78it/s]Extractor Predicting: 37it [00:20,  1.81it/s]Extractor Predicting: 38it [00:21,  1.77it/s]Extractor Predicting: 39it [00:21,  1.78it/s]Extractor Predicting: 40it [00:22,  1.77it/s]Extractor Predicting: 41it [00:23,  1.79it/s]Extractor Predicting: 42it [00:23,  1.83it/s]Extractor Predicting: 43it [00:24,  1.76it/s]Extractor Predicting: 44it [00:24,  1.73it/s]Extractor Predicting: 45it [00:25,  1.72it/s]Extractor Predicting: 46it [00:25,  1.71it/s]Extractor Predicting: 47it [00:26,  1.76it/s]Extractor Predicting: 48it [00:27,  1.75it/s]Extractor Predicting: 49it [00:27,  1.78it/s]Extractor Predicting: 50it [00:28,  1.73it/s]Extractor Predicting: 51it [00:28,  1.73it/s]Extractor Predicting: 52it [00:29,  1.72it/s]Extractor Predicting: 53it [00:29,  1.76it/s]Extractor Predicting: 54it [00:30,  1.72it/s]Extractor Predicting: 55it [00:31,  1.78it/s]Extractor Predicting: 56it [00:31,  1.75it/s]Extractor Predicting: 57it [00:32,  1.74it/s]Extractor Predicting: 58it [00:32,  1.71it/s]Extractor Predicting: 59it [00:33,  1.69it/s]Extractor Predicting: 60it [00:34,  1.73it/s]Extractor Predicting: 61it [00:34,  1.72it/s]Extractor Predicting: 62it [00:35,  1.70it/s]Extractor Predicting: 63it [00:35,  1.73it/s]Extractor Predicting: 64it [00:36,  1.72it/s]Extractor Predicting: 65it [00:36,  1.79it/s]Extractor Predicting: 66it [00:37,  1.75it/s]Extractor Predicting: 67it [00:38,  1.73it/s]Extractor Predicting: 68it [00:38,  1.69it/s]Extractor Predicting: 69it [00:39,  1.71it/s]Extractor Predicting: 70it [00:39,  1.68it/s]Extractor Predicting: 71it [00:40,  1.70it/s]Extractor Predicting: 72it [00:41,  1.69it/s]Extractor Predicting: 73it [00:41,  1.67it/s]Extractor Predicting: 74it [00:42,  1.68it/s]Extractor Predicting: 75it [00:42,  1.69it/s]Extractor Predicting: 76it [00:43,  1.72it/s]Extractor Predicting: 77it [00:43,  1.72it/s]Extractor Predicting: 78it [00:44,  1.75it/s]Extractor Predicting: 79it [00:45,  1.75it/s]Extractor Predicting: 80it [00:45,  1.79it/s]Extractor Predicting: 81it [00:46,  1.59it/s]Extractor Predicting: 82it [00:46,  1.65it/s]Extractor Predicting: 83it [00:47,  1.68it/s]Extractor Predicting: 84it [00:48,  1.70it/s]Extractor Predicting: 85it [00:48,  1.67it/s]Extractor Predicting: 86it [00:49,  1.64it/s]Extractor Predicting: 87it [00:49,  1.62it/s]Extractor Predicting: 88it [00:50,  1.67it/s]Extractor Predicting: 89it [00:51,  1.66it/s]Extractor Predicting: 90it [00:51,  1.62it/s]Extractor Predicting: 91it [00:52,  1.62it/s]Extractor Predicting: 92it [00:52,  1.64it/s]Extractor Predicting: 93it [00:53,  1.67it/s]Extractor Predicting: 94it [00:54,  1.69it/s]Extractor Predicting: 95it [00:54,  1.67it/s]Extractor Predicting: 96it [00:55,  1.68it/s]Extractor Predicting: 97it [00:55,  1.66it/s]Extractor Predicting: 98it [00:56,  1.62it/s]Extractor Predicting: 99it [00:57,  1.64it/s]Extractor Predicting: 100it [00:57,  1.66it/s]Extractor Predicting: 101it [00:58,  1.68it/s]Extractor Predicting: 102it [00:58,  1.70it/s]Extractor Predicting: 103it [00:59,  1.68it/s]Extractor Predicting: 104it [01:00,  1.66it/s]Extractor Predicting: 105it [01:00,  1.69it/s]Extractor Predicting: 106it [01:01,  1.69it/s]Extractor Predicting: 107it [01:01,  1.70it/s]Extractor Predicting: 108it [01:02,  1.69it/s]Extractor Predicting: 109it [01:03,  1.67it/s]Extractor Predicting: 110it [01:03,  1.67it/s]Extractor Predicting: 111it [01:04,  1.69it/s]Extractor Predicting: 112it [01:04,  1.68it/s]Extractor Predicting: 113it [01:05,  1.69it/s]Extractor Predicting: 114it [01:06,  1.70it/s]Extractor Predicting: 115it [01:06,  1.70it/s]Extractor Predicting: 116it [01:07,  1.65it/s]Extractor Predicting: 117it [01:07,  1.65it/s]Extractor Predicting: 118it [01:08,  1.68it/s]Extractor Predicting: 119it [01:09,  1.66it/s]Extractor Predicting: 120it [01:09,  1.65it/s]Extractor Predicting: 121it [01:10,  1.67it/s]Extractor Predicting: 122it [01:10,  1.71it/s]Extractor Predicting: 123it [01:11,  1.71it/s]Extractor Predicting: 124it [01:12,  1.72it/s]Extractor Predicting: 125it [01:12,  1.71it/s]Extractor Predicting: 126it [01:13,  1.75it/s]Extractor Predicting: 127it [01:13,  1.76it/s]Extractor Predicting: 128it [01:14,  1.73it/s]Extractor Predicting: 129it [01:14,  1.78it/s]Extractor Predicting: 130it [01:15,  1.77it/s]Extractor Predicting: 131it [01:15,  1.76it/s]Extractor Predicting: 132it [01:16,  1.75it/s]Extractor Predicting: 133it [01:17,  1.81it/s]Extractor Predicting: 134it [01:17,  1.82it/s]Extractor Predicting: 135it [01:18,  1.76it/s]Extractor Predicting: 136it [01:18,  1.77it/s]Extractor Predicting: 137it [01:19,  1.79it/s]Extractor Predicting: 138it [01:19,  1.80it/s]Extractor Predicting: 139it [01:20,  1.79it/s]Extractor Predicting: 140it [01:20,  1.79it/s]Extractor Predicting: 141it [01:21,  1.78it/s]Extractor Predicting: 142it [01:22,  1.80it/s]Extractor Predicting: 143it [01:22,  1.77it/s]Extractor Predicting: 144it [01:23,  1.72it/s]Extractor Predicting: 145it [01:23,  1.72it/s]Extractor Predicting: 146it [01:24,  1.75it/s]Extractor Predicting: 147it [01:24,  1.81it/s]Extractor Predicting: 148it [01:25,  1.82it/s]Extractor Predicting: 149it [01:26,  1.83it/s]Extractor Predicting: 150it [01:26,  1.84it/s]Extractor Predicting: 151it [01:27,  1.84it/s]Extractor Predicting: 152it [01:27,  1.86it/s]Extractor Predicting: 153it [01:28,  1.86it/s]Extractor Predicting: 154it [01:28,  1.90it/s]Extractor Predicting: 155it [01:29,  1.88it/s]Extractor Predicting: 156it [01:29,  1.83it/s]Extractor Predicting: 157it [01:30,  1.89it/s]Extractor Predicting: 158it [01:30,  1.89it/s]Extractor Predicting: 159it [01:31,  1.96it/s]Extractor Predicting: 160it [01:31,  2.03it/s]Extractor Predicting: 161it [01:32,  1.95it/s]Extractor Predicting: 162it [01:32,  1.90it/s]Extractor Predicting: 163it [01:33,  1.86it/s]Extractor Predicting: 164it [01:33,  1.88it/s]Extractor Predicting: 165it [01:34,  1.93it/s]Extractor Predicting: 166it [01:34,  1.94it/s]Extractor Predicting: 167it [01:35,  1.94it/s]Extractor Predicting: 168it [01:35,  1.90it/s]Extractor Predicting: 169it [01:36,  1.90it/s]Extractor Predicting: 170it [01:37,  1.89it/s]Extractor Predicting: 171it [01:37,  1.94it/s]Extractor Predicting: 172it [01:38,  1.94it/s]Extractor Predicting: 173it [01:38,  1.95it/s]Extractor Predicting: 174it [01:39,  1.83it/s]Extractor Predicting: 175it [01:39,  1.80it/s]Extractor Predicting: 176it [01:40,  1.78it/s]Extractor Predicting: 177it [01:40,  1.77it/s]Extractor Predicting: 178it [01:41,  1.72it/s]Extractor Predicting: 179it [01:42,  1.70it/s]Extractor Predicting: 180it [01:42,  1.74it/s]Extractor Predicting: 181it [01:43,  1.69it/s]Extractor Predicting: 182it [01:43,  1.71it/s]Extractor Predicting: 183it [01:44,  1.73it/s]Extractor Predicting: 184it [01:45,  1.72it/s]Extractor Predicting: 185it [01:45,  1.73it/s]Extractor Predicting: 186it [01:46,  1.76it/s]Extractor Predicting: 187it [01:47,  1.47it/s]Extractor Predicting: 188it [01:47,  1.52it/s]Extractor Predicting: 189it [01:48,  1.59it/s]Extractor Predicting: 190it [01:48,  1.62it/s]Extractor Predicting: 191it [01:49,  1.62it/s]Extractor Predicting: 192it [01:50,  1.61it/s]Extractor Predicting: 193it [01:50,  1.61it/s]Extractor Predicting: 194it [01:51,  1.62it/s]Extractor Predicting: 195it [01:51,  1.65it/s]Extractor Predicting: 196it [01:52,  1.69it/s]Extractor Predicting: 197it [01:53,  1.72it/s]Extractor Predicting: 198it [01:53,  1.68it/s]Extractor Predicting: 199it [01:54,  1.71it/s]Extractor Predicting: 200it [01:55,  1.47it/s]Extractor Predicting: 201it [01:55,  1.52it/s]Extractor Predicting: 202it [01:56,  1.54it/s]Extractor Predicting: 203it [01:57,  1.52it/s]Extractor Predicting: 204it [01:57,  1.55it/s]Extractor Predicting: 205it [01:58,  1.57it/s]Extractor Predicting: 206it [01:58,  1.58it/s]Extractor Predicting: 207it [01:59,  1.61it/s]Extractor Predicting: 208it [02:00,  1.58it/s]Extractor Predicting: 209it [02:00,  1.55it/s]Extractor Predicting: 210it [02:01,  1.56it/s]Extractor Predicting: 211it [02:02,  1.56it/s]Extractor Predicting: 212it [02:02,  1.59it/s]Extractor Predicting: 213it [02:03,  1.60it/s]Extractor Predicting: 214it [02:03,  1.60it/s]Extractor Predicting: 215it [02:04,  1.57it/s]Extractor Predicting: 216it [02:05,  1.59it/s]Extractor Predicting: 217it [02:05,  1.60it/s]Extractor Predicting: 218it [02:06,  1.60it/s]Extractor Predicting: 219it [02:07,  1.59it/s]Extractor Predicting: 220it [02:07,  1.55it/s]Extractor Predicting: 221it [02:08,  1.55it/s]Extractor Predicting: 222it [02:09,  1.54it/s]Extractor Predicting: 223it [02:09,  1.59it/s]Extractor Predicting: 224it [02:10,  1.59it/s]Extractor Predicting: 225it [02:10,  1.60it/s]Extractor Predicting: 226it [02:11,  1.58it/s]Extractor Predicting: 227it [02:12,  1.60it/s]Extractor Predicting: 228it [02:12,  1.61it/s]Extractor Predicting: 229it [02:13,  1.63it/s]Extractor Predicting: 230it [02:13,  1.66it/s]Extractor Predicting: 231it [02:14,  1.71it/s]Extractor Predicting: 232it [02:15,  1.75it/s]Extractor Predicting: 233it [02:15,  1.77it/s]Extractor Predicting: 234it [02:16,  1.71it/s]Extractor Predicting: 235it [02:16,  1.71it/s]Extractor Predicting: 236it [02:17,  1.72it/s]Extractor Predicting: 237it [02:17,  1.72it/s]Extractor Predicting: 238it [02:18,  1.73it/s]Extractor Predicting: 239it [02:19,  1.70it/s]Extractor Predicting: 240it [02:19,  1.75it/s]Extractor Predicting: 241it [02:20,  1.72it/s]Extractor Predicting: 242it [02:20,  1.77it/s]Extractor Predicting: 243it [02:21,  1.77it/s]Extractor Predicting: 244it [02:21,  1.84it/s]Extractor Predicting: 245it [02:22,  1.79it/s]Extractor Predicting: 246it [02:23,  1.75it/s]Extractor Predicting: 247it [02:23,  1.67it/s]Extractor Predicting: 248it [02:24,  1.67it/s]Extractor Predicting: 249it [02:24,  1.72it/s]Extractor Predicting: 250it [02:25,  1.72it/s]Extractor Predicting: 251it [02:25,  1.75it/s]Extractor Predicting: 252it [02:26,  1.80it/s]Extractor Predicting: 253it [02:27,  1.74it/s]Extractor Predicting: 254it [02:27,  1.71it/s]Extractor Predicting: 255it [02:28,  1.72it/s]Extractor Predicting: 256it [02:28,  1.72it/s]Extractor Predicting: 257it [02:29,  1.68it/s]Extractor Predicting: 258it [02:30,  1.65it/s]Extractor Predicting: 259it [02:30,  1.67it/s]Extractor Predicting: 260it [02:31,  1.66it/s]Extractor Predicting: 261it [02:31,  1.66it/s]Extractor Predicting: 262it [02:32,  1.68it/s]Extractor Predicting: 263it [02:33,  1.70it/s]Extractor Predicting: 264it [02:33,  1.68it/s]Extractor Predicting: 265it [02:34,  1.68it/s]Extractor Predicting: 266it [02:34,  1.63it/s]Extractor Predicting: 267it [02:35,  1.64it/s]Extractor Predicting: 268it [02:36,  1.63it/s]Extractor Predicting: 269it [02:36,  1.66it/s]Extractor Predicting: 270it [02:37,  1.66it/s]Extractor Predicting: 271it [02:37,  1.66it/s]Extractor Predicting: 272it [02:38,  1.70it/s]Extractor Predicting: 273it [02:39,  1.65it/s]Extractor Predicting: 274it [02:39,  1.63it/s]Extractor Predicting: 275it [02:40,  1.64it/s]Extractor Predicting: 276it [02:41,  1.61it/s]Extractor Predicting: 277it [02:41,  1.63it/s]Extractor Predicting: 278it [02:42,  1.65it/s]Extractor Predicting: 279it [02:42,  1.63it/s]Extractor Predicting: 280it [02:43,  1.65it/s]Extractor Predicting: 281it [02:44,  1.67it/s]Extractor Predicting: 282it [02:44,  1.66it/s]Extractor Predicting: 283it [02:45,  1.63it/s]Extractor Predicting: 284it [02:45,  1.70it/s]Extractor Predicting: 285it [02:46,  1.68it/s]Extractor Predicting: 286it [02:46,  1.72it/s]Extractor Predicting: 287it [02:47,  1.69it/s]Extractor Predicting: 288it [02:48,  1.68it/s]Extractor Predicting: 289it [02:48,  1.68it/s]Extractor Predicting: 290it [02:49,  1.68it/s]Extractor Predicting: 291it [02:50,  1.62it/s]Extractor Predicting: 292it [02:50,  1.59it/s]Extractor Predicting: 293it [02:51,  1.64it/s]Extractor Predicting: 294it [02:51,  1.61it/s]Extractor Predicting: 295it [02:52,  1.65it/s]Extractor Predicting: 296it [02:53,  1.63it/s]Extractor Predicting: 297it [02:53,  1.65it/s]Extractor Predicting: 298it [02:54,  1.63it/s]Extractor Predicting: 299it [02:54,  1.62it/s]Extractor Predicting: 300it [02:55,  1.66it/s]Extractor Predicting: 301it [02:56,  1.64it/s]Extractor Predicting: 302it [02:56,  1.65it/s]Extractor Predicting: 303it [02:57,  1.62it/s]Extractor Predicting: 304it [02:58,  1.61it/s]Extractor Predicting: 305it [02:58,  1.62it/s]Extractor Predicting: 306it [02:59,  1.67it/s]Extractor Predicting: 307it [02:59,  1.63it/s]Extractor Predicting: 308it [03:00,  1.66it/s]Extractor Predicting: 309it [03:00,  1.67it/s]Extractor Predicting: 310it [03:01,  1.70it/s]Extractor Predicting: 311it [03:02,  1.53it/s]Extractor Predicting: 312it [03:02,  1.62it/s]Extractor Predicting: 313it [03:03,  1.69it/s]Extractor Predicting: 314it [03:03,  1.73it/s]Extractor Predicting: 315it [03:04,  1.77it/s]Extractor Predicting: 316it [03:05,  1.71it/s]Extractor Predicting: 317it [03:05,  1.50it/s]Extractor Predicting: 318it [03:06,  1.56it/s]Extractor Predicting: 319it [03:07,  1.60it/s]Extractor Predicting: 320it [03:07,  1.62it/s]Extractor Predicting: 321it [03:08,  1.62it/s]Extractor Predicting: 322it [03:08,  1.65it/s]Extractor Predicting: 323it [03:09,  1.62it/s]Extractor Predicting: 324it [03:10,  1.63it/s]Extractor Predicting: 325it [03:10,  1.64it/s]Extractor Predicting: 326it [03:11,  1.65it/s]Extractor Predicting: 327it [03:11,  1.69it/s]Extractor Predicting: 328it [03:12,  1.68it/s]Extractor Predicting: 329it [03:13,  1.70it/s]Extractor Predicting: 330it [03:13,  1.70it/s]Extractor Predicting: 331it [03:14,  1.68it/s]Extractor Predicting: 332it [03:14,  1.66it/s]Extractor Predicting: 333it [03:15,  1.69it/s]Extractor Predicting: 334it [03:16,  1.68it/s]Extractor Predicting: 335it [03:16,  1.71it/s]Extractor Predicting: 336it [03:17,  1.68it/s]Extractor Predicting: 337it [03:17,  1.69it/s]Extractor Predicting: 338it [03:18,  1.68it/s]Extractor Predicting: 339it [03:19,  1.74it/s]Extractor Predicting: 340it [03:19,  1.75it/s]Extractor Predicting: 341it [03:20,  1.75it/s]Extractor Predicting: 342it [03:20,  1.73it/s]Extractor Predicting: 343it [03:21,  1.72it/s]Extractor Predicting: 344it [03:21,  1.72it/s]Extractor Predicting: 345it [03:22,  1.68it/s]Extractor Predicting: 346it [03:23,  1.69it/s]Extractor Predicting: 347it [03:23,  1.68it/s]Extractor Predicting: 348it [03:24,  1.69it/s]Extractor Predicting: 349it [03:24,  1.73it/s]Extractor Predicting: 350it [03:25,  1.69it/s]Extractor Predicting: 351it [03:26,  1.72it/s]Extractor Predicting: 352it [03:26,  1.69it/s]Extractor Predicting: 353it [03:27,  1.71it/s]Extractor Predicting: 354it [03:27,  1.75it/s]Extractor Predicting: 355it [03:28,  1.78it/s]Extractor Predicting: 356it [03:28,  1.71it/s]Extractor Predicting: 357it [03:29,  1.72it/s]Extractor Predicting: 358it [03:30,  1.74it/s]Extractor Predicting: 359it [03:30,  1.73it/s]Extractor Predicting: 360it [03:31,  1.74it/s]Extractor Predicting: 361it [03:31,  1.75it/s]Extractor Predicting: 362it [03:32,  1.70it/s]Extractor Predicting: 363it [03:32,  1.72it/s]Extractor Predicting: 364it [03:33,  1.75it/s]Extractor Predicting: 365it [03:34,  1.77it/s]Extractor Predicting: 366it [03:34,  1.75it/s]Extractor Predicting: 367it [03:35,  1.70it/s]Extractor Predicting: 368it [03:35,  1.71it/s]Extractor Predicting: 369it [03:36,  1.70it/s]Extractor Predicting: 370it [03:36,  1.76it/s]Extractor Predicting: 371it [03:37,  1.78it/s]Extractor Predicting: 372it [03:38,  1.78it/s]Extractor Predicting: 373it [03:38,  1.75it/s]Extractor Predicting: 374it [03:39,  1.77it/s]Extractor Predicting: 375it [03:39,  1.75it/s]Extractor Predicting: 376it [03:40,  1.77it/s]Extractor Predicting: 377it [03:40,  1.78it/s]Extractor Predicting: 378it [03:41,  1.81it/s]Extractor Predicting: 379it [03:42,  1.76it/s]Extractor Predicting: 380it [03:42,  1.77it/s]Extractor Predicting: 381it [03:43,  1.73it/s]Extractor Predicting: 382it [03:43,  1.73it/s]Extractor Predicting: 383it [03:44,  1.78it/s]Extractor Predicting: 384it [03:44,  1.82it/s]Extractor Predicting: 385it [03:45,  1.79it/s]Extractor Predicting: 386it [03:45,  1.79it/s]Extractor Predicting: 387it [03:46,  1.72it/s]Extractor Predicting: 388it [03:47,  1.72it/s]Extractor Predicting: 389it [03:47,  1.74it/s]Extractor Predicting: 390it [03:48,  1.74it/s]Extractor Predicting: 391it [03:48,  1.73it/s]Extractor Predicting: 392it [03:49,  1.74it/s]Extractor Predicting: 393it [03:50,  1.75it/s]Extractor Predicting: 394it [03:50,  1.68it/s]Extractor Predicting: 395it [03:51,  1.64it/s]Extractor Predicting: 396it [03:51,  1.63it/s]Extractor Predicting: 397it [03:52,  1.63it/s]Extractor Predicting: 398it [03:53,  1.61it/s]Extractor Predicting: 399it [03:53,  1.61it/s]Extractor Predicting: 400it [03:54,  1.66it/s]Extractor Predicting: 401it [03:55,  1.64it/s]Extractor Predicting: 402it [03:55,  1.61it/s]Extractor Predicting: 403it [03:56,  1.56it/s]Extractor Predicting: 404it [03:57,  1.53it/s]Extractor Predicting: 405it [03:57,  1.55it/s]Extractor Predicting: 406it [03:58,  1.57it/s]Extractor Predicting: 407it [03:58,  1.56it/s]Extractor Predicting: 408it [03:59,  1.57it/s]Extractor Predicting: 409it [04:00,  1.60it/s]Extractor Predicting: 410it [04:00,  1.58it/s]Extractor Predicting: 411it [04:01,  1.41it/s]Extractor Predicting: 412it [04:02,  1.48it/s]Extractor Predicting: 413it [04:02,  1.52it/s]Extractor Predicting: 414it [04:03,  1.58it/s]Extractor Predicting: 415it [04:04,  1.64it/s]Extractor Predicting: 416it [04:04,  1.66it/s]Extractor Predicting: 417it [04:05,  1.67it/s]Extractor Predicting: 418it [04:05,  1.67it/s]Extractor Predicting: 419it [04:06,  1.62it/s]Extractor Predicting: 420it [04:07,  1.61it/s]Extractor Predicting: 421it [04:07,  1.74it/s]Extractor Predicting: 421it [04:07,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:51,046 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:51,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:51,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:51,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:51,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:31:51,961 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:31:51,962 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:31:52,580 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:31:53,803 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:31:53,803 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:56,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:56,787 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:56,787 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:56,787 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:56,787 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:31:57,532 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:31:57,533 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:31:58,153 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:31:58,385 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:31:58,386 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.45it/s]Extractor Predicting: 9it [00:05,  1.52it/s]
[INFO|configuration_utils.py:515] 2023-08-28 03:32:06,179 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:32:06,180 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:32:06,271 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:32:06,319 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 03:32:06,357 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:32:17,772 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 03:32:17,810 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 03:32:18,022 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:32:18,023 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:32:18,098 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:32:18,169 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:32:18,169 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:32:18,169 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:32:18,169 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:32:18,169 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:32:18,169 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 03:32:18,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:19,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:19,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:20,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:20,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:21,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:22,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:22,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:23,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:24,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:25,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:25,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:26,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:27,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:27,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:28,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:28,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:29,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:30,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:31,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:32,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:33,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:33,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:34,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:34,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:35,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:17<05:33, 17.57s/it][WARNING|generation_utils.py:914] 2023-08-28 03:32:36,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:36,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:37,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:38,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:38,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:39,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:40,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:40,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:41,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:42,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:42,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:43,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:44,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:44,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:45,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:46,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:46,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:47,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:48,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:48,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:49,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:49,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:50,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:51,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:51,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:52,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:34<05:08, 17.16s/it][WARNING|generation_utils.py:914] 2023-08-28 03:32:52,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:53,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:54,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:55,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:55,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:56,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:57,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:57,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:58,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:59,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:59,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:00,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:00,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:01,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:02,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:02,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:03,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:04,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:04,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:05,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:05,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:06,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:07,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:08,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:08,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:09,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:51<04:51, 17.17s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:10,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:10,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:11,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:12,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:12,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:13,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:14,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:14,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:15,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:16,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:16,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:17,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:18,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:18,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:19,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:20,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:20,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:21,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:22,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:22,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:23,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:24,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:24,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:25,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:26,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:08<04:30, 16.94s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:26,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:27,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:27,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:28,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:29,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:29,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:30,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:30,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:31,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:32,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:32,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:33,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:34,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:34,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:35,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:35,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:36,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:37,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:37,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:38,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:38,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:39,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:40,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:40,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:41,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:42,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:42,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:24<04:12, 16.82s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:43,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:43,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:44,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:45,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:45,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:46,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:47,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:47,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:48,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:49,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:49,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:50,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:51,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:51,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:52,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:53,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:54,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:54,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:55,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:55,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:56,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:57,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:57,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:39<03:47, 16.26s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:58,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:59,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:59,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:00,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:01,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:01,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:02,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:03,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:03,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:04,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:05,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:05,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:06,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:07,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:07,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:08,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:09,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:10,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:10,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:11,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:12,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:13,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:13,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:14,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:57<03:35, 16.54s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:15,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:16,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:17,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:17,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:18,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:19,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:19,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:20,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:21,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:22,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:22,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:23,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:24,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:24,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:25,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:26,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:26,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:27,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:28,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:29,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:29,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:30,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:31,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:32,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:32,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:15<03:24, 17.04s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:33,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:34,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:35,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:36,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:36,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:37,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:38,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:38,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:39,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:40,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:40,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:41,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:42,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:43,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:43,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:44,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:45,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:45,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:46,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:47,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:48,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:48,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:49,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:31<03:05, 16.83s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:50,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:50,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:51,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:52,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:53,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:53,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:54,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:55,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:55,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:56,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:57,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:58,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:59,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:59,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:00,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:01,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:01,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:02,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:03,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:03,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:04,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:04,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:05,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:06,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:48<02:48, 16.86s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:07,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:08,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:09,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:09,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:10,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:11,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:12,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:13,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:13,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:14,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:15,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:16,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:16,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:17,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:18,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:18,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:19,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:20,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:20,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:21,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:22,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:22,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:23,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:05<02:33, 17.03s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:24,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:25,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:25,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:26,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:27,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:27,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:28,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:29,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:29,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:30,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:31,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:32,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:32,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:33,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:34,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:34,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:35,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:36,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:36,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:37,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:38,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:38,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:39,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:21<02:13, 16.67s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:40,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:40,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:41,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:42,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:43,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:44,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:44,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:45,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:46,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:46,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:47,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:48,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:49,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:49,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:50,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:51,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:51,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:52,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:53,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:53,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:54,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:55,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:55,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:56,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:57,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:39<01:58, 16.94s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:57,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:58,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:59,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:59,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:00,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:01,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:01,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:02,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:02,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:03,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:03,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:04,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:05,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:05,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:06,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:07,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:07,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:08,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:09,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:09,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:10,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:11,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:11,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:53<01:37, 16.18s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:12,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:12,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:13,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:14,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:14,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:15,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:16,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:16,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:17,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:18,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:18,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:19,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:20,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:20,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:21,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:21,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:22,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:23,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:23,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:24,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:25,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:25,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:26,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:27,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:27,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:28,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:10<01:21, 16.37s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:29,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:29,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:30,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:31,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:31,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:32,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:33,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:34,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:34,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:35,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:36,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:36,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:37,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:37,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:38,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:39,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:40,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:40,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:41,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:42,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:42,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:43,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:44,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:44,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:45,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:27<01:06, 16.59s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:46,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:46,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:47,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:48,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:48,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:49,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:50,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:51,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:51,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:52,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:53,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:53,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:54,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:55,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:56,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:56,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:57,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:57,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:58,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:59,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:59,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:00,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:01,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:43<00:48, 16.25s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:01,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:02,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:02,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:03,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:04,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:04,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:05,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:06,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:06,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:07,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:08,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:08,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:09,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:10,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:10,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:11,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:12,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:12,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:13,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:14,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:14,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:15,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:16,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:16,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:17,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:17,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:18,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:18,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:19,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:20,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:02<00:34, 17.09s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:20,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:21,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:22,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:22,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:23,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:24,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:24,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:25,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:26,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:27,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:27,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:28,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:29,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:29,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:30,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:31,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:31,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:32,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:33,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:33,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:34,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:35,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:35,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:17<00:16, 16.64s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:36,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:36,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:37,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:38,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:38,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:39,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:40,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:41,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:41,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:42,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:43,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:43,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:44,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:45,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:45,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:46,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:47,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:47,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:48,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:49,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:49,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:50,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:51,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:33<00:00, 16.34s/it]Generating: 100%|| 20/20 [05:33<00:00, 16.67s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:00,626 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:00,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:00,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:00,649 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:00,649 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:38:01,626 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:38:01,627 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:38:02,478 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:38:03,614 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:38:03,614 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:06,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:06,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:06,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:06,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:06,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:38:07,544 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:38:07,545 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:38:08,161 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:38:08,379 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:38:08,379 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 441, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 512, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7379807692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 234, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 625, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7512019230769231, 'errors': {'', "('Hans', 'head of government', '', 'He was succeeded by his brother , Dr . Hans .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : military branch .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('British ships', 'military branch', '', 'The Battle of Bataillon was the battle of the Battle of the Bastille , where British ships sank at least 1,400 French ships .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 181, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 274, 'raw': 384}
{'target': 600, 'success': 295, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 339, 'raw': 480}
{'target': 600, 'success': 361, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 428, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 602, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.6967592592592593, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('European Championships', 'winner', '', 'The previous year , he won the European Championships , and finished runner in 5th place in the category of medals .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : crosses . Context : Later in the year ( 11411231 ) he married Brigadier John B. Stoughton , sister of King James VI , the King of England . Head Entity : Robert Stoughton , Tail Entity : John B . Stoughton .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : crosses .', 'success_rate': 0.77, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('FreeBSD', 'operating system', '', 'The operating system is based on FreeBSD 2.1 .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 519, 'raw': 704}
{'target': 600, 'success': 544, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : participant .', 'success_rate': 0.7415865384615384, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : position played on team / speciality . Context : Later in 2008 , he played in the United States national team squad for the 2002 FIFA World Cup and 2010 FIFA World Cup games . Head Entity : Walter , Tail Entity : United States national team .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 99, 'raw': 160}
{'target': 600, 'success': 119, 'raw': 192}
{'target': 600, 'success': 141, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 183, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 224, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 266, 'raw': 416}
{'target': 600, 'success': 287, 'raw': 448}
{'target': 600, 'success': 312, 'raw': 480}
{'target': 600, 'success': 330, 'raw': 512}
{'target': 600, 'success': 348, 'raw': 544}
{'target': 600, 'success': 367, 'raw': 576}
{'target': 600, 'success': 386, 'raw': 608}
{'target': 600, 'success': 404, 'raw': 640}
{'target': 600, 'success': 421, 'raw': 672}
{'target': 600, 'success': 448, 'raw': 704}
{'target': 600, 'success': 465, 'raw': 736}
{'target': 600, 'success': 482, 'raw': 768}
{'target': 600, 'success': 499, 'raw': 800}
{'target': 600, 'success': 525, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 574, 'raw': 896}
{'target': 600, 'success': 594, 'raw': 928}
{'target': 600, 'success': 611, 'raw': 960}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.6364583333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : publisher . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : season 2 , Tail Entity : the Walking Dead .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8288043478260869, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 17454
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17554, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.53it/s]Extractor Estimating: 2it [00:01,  1.50it/s]Extractor Estimating: 3it [00:01,  1.51it/s]Extractor Estimating: 4it [00:02,  1.52it/s]Extractor Estimating: 5it [00:03,  1.55it/s]Extractor Estimating: 6it [00:03,  1.49it/s]Extractor Estimating: 7it [00:04,  1.56it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:05,  1.62it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:07,  1.63it/s]Extractor Estimating: 12it [00:07,  1.63it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.55it/s]Extractor Estimating: 16it [00:10,  1.51it/s]Extractor Estimating: 17it [00:10,  1.52it/s]Extractor Estimating: 18it [00:11,  1.55it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:12,  1.62it/s]Extractor Estimating: 21it [00:13,  1.68it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:15,  1.66it/s]Extractor Estimating: 25it [00:15,  1.69it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:17,  1.57it/s]Extractor Estimating: 28it [00:17,  1.50it/s]Extractor Estimating: 29it [00:18,  1.41it/s]Extractor Estimating: 30it [00:19,  1.42it/s]Extractor Estimating: 31it [00:20,  1.41it/s]Extractor Estimating: 32it [00:20,  1.43it/s]Extractor Estimating: 33it [00:21,  1.48it/s]Extractor Estimating: 34it [00:22,  1.44it/s]Extractor Estimating: 35it [00:22,  1.46it/s]Extractor Estimating: 36it [00:23,  1.47it/s]Extractor Estimating: 37it [00:24,  1.48it/s]Extractor Estimating: 38it [00:24,  1.52it/s]Extractor Estimating: 39it [00:25,  1.54it/s]Extractor Estimating: 40it [00:25,  1.52it/s]Extractor Estimating: 41it [00:26,  1.51it/s]Extractor Estimating: 42it [00:27,  1.53it/s]Extractor Estimating: 43it [00:27,  1.49it/s]Extractor Estimating: 44it [00:28,  1.52it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:30,  1.44it/s]Extractor Estimating: 47it [00:30,  1.47it/s]Extractor Estimating: 48it [00:31,  1.47it/s]Extractor Estimating: 49it [00:31,  1.53it/s]Extractor Estimating: 50it [00:32,  1.55it/s]Extractor Estimating: 51it [00:33,  1.55it/s]Extractor Estimating: 52it [00:33,  1.55it/s]Extractor Estimating: 53it [00:34,  1.51it/s]Extractor Estimating: 54it [00:35,  1.56it/s]Extractor Estimating: 55it [00:35,  1.63it/s]Extractor Estimating: 56it [00:36,  1.69it/s]Extractor Estimating: 57it [00:36,  1.70it/s]Extractor Estimating: 58it [00:37,  1.67it/s]Extractor Estimating: 59it [00:38,  1.62it/s]Extractor Estimating: 60it [00:38,  1.67it/s]Extractor Estimating: 61it [00:39,  1.66it/s]Extractor Estimating: 62it [00:39,  1.66it/s]Extractor Estimating: 63it [00:40,  1.60it/s]Extractor Estimating: 64it [00:41,  1.58it/s]Extractor Estimating: 65it [00:41,  1.59it/s]Extractor Estimating: 66it [00:42,  1.64it/s]Extractor Estimating: 67it [00:43,  1.57it/s]Extractor Estimating: 68it [00:43,  1.60it/s]Extractor Estimating: 69it [00:44,  1.56it/s]Extractor Estimating: 70it [00:44,  1.62it/s]Extractor Estimating: 71it [00:45,  1.61it/s]Extractor Estimating: 72it [00:46,  1.60it/s]Extractor Estimating: 73it [00:46,  1.53it/s]Extractor Estimating: 74it [00:47,  1.54it/s]Extractor Estimating: 75it [00:48,  1.55it/s]Extractor Estimating: 76it [00:48,  1.56it/s]Extractor Estimating: 77it [00:49,  1.52it/s]Extractor Estimating: 78it [00:50,  1.54it/s]Extractor Estimating: 79it [00:50,  1.52it/s]Extractor Estimating: 80it [00:51,  1.50it/s]Extractor Estimating: 81it [00:52,  1.50it/s]Extractor Estimating: 82it [00:52,  1.53it/s]Extractor Estimating: 83it [00:53,  1.52it/s]Extractor Estimating: 84it [00:54,  1.51it/s]Extractor Estimating: 85it [00:54,  1.53it/s]Extractor Estimating: 86it [00:55,  1.57it/s]Extractor Estimating: 87it [00:56,  1.53it/s]Extractor Estimating: 88it [00:56,  1.54it/s]Extractor Estimating: 89it [00:57,  1.60it/s]Extractor Estimating: 90it [00:57,  1.59it/s]Extractor Estimating: 91it [00:58,  1.47it/s]Extractor Estimating: 92it [00:59,  1.46it/s]Extractor Estimating: 93it [01:00,  1.50it/s]Extractor Estimating: 94it [01:00,  1.57it/s]Extractor Estimating: 95it [01:01,  1.57it/s]Extractor Estimating: 96it [01:01,  1.54it/s]Extractor Estimating: 97it [01:02,  1.51it/s]Extractor Estimating: 98it [01:03,  1.54it/s]Extractor Estimating: 99it [01:04,  1.40it/s]Extractor Estimating: 100it [01:04,  1.45it/s]Extractor Estimating: 101it [01:05,  1.50it/s]Extractor Estimating: 102it [01:06,  1.50it/s]Extractor Estimating: 103it [01:06,  1.49it/s]Extractor Estimating: 104it [01:07,  1.52it/s]Extractor Estimating: 105it [01:07,  1.57it/s]Extractor Estimating: 106it [01:08,  1.62it/s]Extractor Estimating: 107it [01:09,  1.63it/s]Extractor Estimating: 108it [01:09,  1.63it/s]Extractor Estimating: 109it [01:10,  1.70it/s]Extractor Estimating: 110it [01:10,  1.62it/s]Extractor Estimating: 111it [01:11,  1.64it/s]Extractor Estimating: 112it [01:12,  1.62it/s]Extractor Estimating: 113it [01:12,  1.65it/s]Extractor Estimating: 114it [01:13,  1.62it/s]Extractor Estimating: 115it [01:13,  1.64it/s]Extractor Estimating: 116it [01:14,  1.65it/s]Extractor Estimating: 117it [01:15,  1.60it/s]Extractor Estimating: 118it [01:15,  1.56it/s]Extractor Estimating: 119it [01:16,  1.58it/s]Extractor Estimating: 120it [01:17,  1.61it/s]Extractor Estimating: 121it [01:17,  1.60it/s]Extractor Estimating: 122it [01:18,  1.58it/s]Extractor Estimating: 123it [01:19,  1.59it/s]Extractor Estimating: 124it [01:19,  1.62it/s]Extractor Estimating: 125it [01:20,  1.63it/s]Extractor Estimating: 126it [01:20,  1.62it/s]Extractor Estimating: 127it [01:21,  1.59it/s]Extractor Estimating: 128it [01:22,  1.59it/s]Extractor Estimating: 129it [01:22,  1.54it/s]Extractor Estimating: 130it [01:23,  1.38it/s]Extractor Estimating: 131it [01:24,  1.44it/s]Extractor Estimating: 132it [01:25,  1.43it/s]Extractor Estimating: 133it [01:25,  1.40it/s]Extractor Estimating: 134it [01:26,  1.42it/s]Extractor Estimating: 135it [01:27,  1.42it/s]Extractor Estimating: 136it [01:27,  1.46it/s]Extractor Estimating: 137it [01:28,  1.47it/s]Extractor Estimating: 138it [01:29,  1.52it/s]Extractor Estimating: 139it [01:29,  1.52it/s]Extractor Estimating: 140it [01:30,  1.47it/s]Extractor Estimating: 141it [01:31,  1.50it/s]Extractor Estimating: 142it [01:31,  1.50it/s]Extractor Estimating: 143it [01:32,  1.46it/s]Extractor Estimating: 144it [01:33,  1.49it/s]Extractor Estimating: 145it [01:33,  1.51it/s]Extractor Estimating: 146it [01:34,  1.50it/s]Extractor Estimating: 147it [01:35,  1.44it/s]Extractor Estimating: 148it [01:35,  1.46it/s]Extractor Estimating: 149it [01:36,  1.49it/s]Extractor Estimating: 150it [01:37,  1.49it/s]Extractor Estimating: 151it [01:37,  1.54it/s]Extractor Estimating: 152it [01:38,  1.54it/s]Extractor Estimating: 153it [01:39,  1.58it/s]Extractor Estimating: 154it [01:39,  1.54it/s]Extractor Estimating: 155it [01:40,  1.59it/s]Extractor Estimating: 156it [01:40,  1.56it/s]Extractor Estimating: 157it [01:41,  1.53it/s]Extractor Estimating: 158it [01:42,  1.61it/s]Extractor Estimating: 159it [01:42,  1.58it/s]Extractor Estimating: 160it [01:43,  1.53it/s]Extractor Estimating: 161it [01:44,  1.58it/s]Extractor Estimating: 162it [01:44,  1.64it/s]Extractor Estimating: 163it [01:45,  1.69it/s]Extractor Estimating: 164it [01:45,  1.68it/s]Extractor Estimating: 165it [01:46,  1.62it/s]Extractor Estimating: 166it [01:47,  1.62it/s]Extractor Estimating: 167it [01:47,  1.62it/s]Extractor Estimating: 168it [01:48,  1.61it/s]Extractor Estimating: 169it [01:48,  1.67it/s]Extractor Estimating: 170it [01:49,  1.44it/s]Extractor Estimating: 171it [01:50,  1.47it/s]Extractor Estimating: 172it [01:51,  1.52it/s]Extractor Estimating: 173it [01:51,  1.54it/s]Extractor Estimating: 174it [01:52,  1.55it/s]Extractor Estimating: 175it [01:52,  1.58it/s]Extractor Estimating: 176it [01:53,  1.57it/s]Extractor Estimating: 177it [01:54,  1.54it/s]Extractor Estimating: 178it [01:55,  1.46it/s]Extractor Estimating: 179it [01:55,  1.48it/s]Extractor Estimating: 180it [01:56,  1.55it/s]Extractor Estimating: 181it [01:56,  1.57it/s]Extractor Estimating: 182it [01:57,  1.59it/s]Extractor Estimating: 183it [01:58,  1.52it/s]Extractor Estimating: 184it [01:59,  1.47it/s]Extractor Estimating: 185it [01:59,  1.44it/s]Extractor Estimating: 186it [02:00,  1.42it/s]Extractor Estimating: 187it [02:01,  1.45it/s]Extractor Estimating: 188it [02:01,  1.47it/s]Extractor Estimating: 189it [02:02,  1.43it/s]Extractor Estimating: 190it [02:03,  1.46it/s]Extractor Estimating: 191it [02:03,  1.52it/s]Extractor Estimating: 192it [02:04,  1.47it/s]Extractor Estimating: 193it [02:05,  1.36it/s]Extractor Estimating: 194it [02:05,  1.45it/s]Extractor Estimating: 195it [02:06,  1.35it/s]Extractor Estimating: 196it [02:07,  1.34it/s]Extractor Estimating: 197it [02:08,  1.39it/s]Extractor Estimating: 198it [02:08,  1.44it/s]Extractor Estimating: 199it [02:09,  1.47it/s]Extractor Estimating: 200it [02:10,  1.52it/s]Extractor Estimating: 201it [02:10,  1.53it/s]Extractor Estimating: 202it [02:11,  1.51it/s]Extractor Estimating: 203it [02:12,  1.42it/s]Extractor Estimating: 204it [02:12,  1.47it/s]Extractor Estimating: 205it [02:13,  1.48it/s]Extractor Estimating: 206it [02:14,  1.51it/s]Extractor Estimating: 207it [02:14,  1.55it/s]Extractor Estimating: 208it [02:15,  1.55it/s]Extractor Estimating: 209it [02:16,  1.50it/s]Extractor Estimating: 210it [02:16,  1.45it/s]Extractor Estimating: 211it [02:17,  1.51it/s]Extractor Estimating: 212it [02:18,  1.52it/s]Extractor Estimating: 213it [02:18,  1.47it/s]Extractor Estimating: 214it [02:19,  1.42it/s]Extractor Estimating: 215it [02:20,  1.46it/s]Extractor Estimating: 216it [02:20,  1.45it/s]Extractor Estimating: 217it [02:21,  1.46it/s]Extractor Estimating: 218it [02:22,  1.43it/s]Extractor Estimating: 219it [02:23,  1.40it/s]Extractor Estimating: 220it [02:23,  1.44it/s]Extractor Estimating: 221it [02:24,  1.45it/s]Extractor Estimating: 222it [02:25,  1.44it/s]Extractor Estimating: 223it [02:25,  1.42it/s]Extractor Estimating: 224it [02:26,  1.46it/s]Extractor Estimating: 225it [02:27,  1.48it/s]Extractor Estimating: 226it [02:27,  1.45it/s]Extractor Estimating: 227it [02:28,  1.45it/s]Extractor Estimating: 228it [02:29,  1.44it/s]Extractor Estimating: 229it [02:29,  1.46it/s]Extractor Estimating: 230it [02:30,  1.44it/s]Extractor Estimating: 231it [02:31,  1.45it/s]Extractor Estimating: 232it [02:31,  1.48it/s]Extractor Estimating: 233it [02:32,  1.50it/s]Extractor Estimating: 234it [02:33,  1.37it/s]Extractor Estimating: 235it [02:34,  1.39it/s]Extractor Estimating: 236it [02:34,  1.42it/s]Extractor Estimating: 237it [02:35,  1.48it/s]Extractor Estimating: 238it [02:36,  1.47it/s]Extractor Estimating: 239it [02:36,  1.52it/s]Extractor Estimating: 240it [02:37,  1.46it/s]Extractor Estimating: 241it [02:38,  1.50it/s]Extractor Estimating: 242it [02:38,  1.55it/s]Extractor Estimating: 243it [02:39,  1.41it/s]Extractor Estimating: 244it [02:40,  1.44it/s]Extractor Estimating: 245it [02:40,  1.46it/s]Extractor Estimating: 246it [02:41,  1.47it/s]Extractor Estimating: 247it [02:42,  1.50it/s]Extractor Estimating: 248it [02:42,  1.50it/s]Extractor Estimating: 249it [02:43,  1.49it/s]Extractor Estimating: 250it [02:44,  1.48it/s]Extractor Estimating: 251it [02:44,  1.48it/s]Extractor Estimating: 252it [02:45,  1.53it/s]Extractor Estimating: 253it [02:46,  1.49it/s]Extractor Estimating: 254it [02:46,  1.47it/s]Extractor Estimating: 255it [02:47,  1.48it/s]Extractor Estimating: 256it [02:48,  1.46it/s]Extractor Estimating: 257it [02:49,  1.44it/s]Extractor Estimating: 258it [02:49,  1.41it/s]Extractor Estimating: 259it [02:50,  1.42it/s]Extractor Estimating: 260it [02:51,  1.41it/s]Extractor Estimating: 261it [02:51,  1.40it/s]Extractor Estimating: 262it [02:52,  1.42it/s]Extractor Estimating: 263it [02:53,  1.39it/s]Extractor Estimating: 264it [02:53,  1.43it/s]Extractor Estimating: 265it [02:54,  1.45it/s]Extractor Estimating: 266it [02:55,  1.49it/s]Extractor Estimating: 267it [02:55,  1.49it/s]Extractor Estimating: 268it [02:56,  1.48it/s]Extractor Estimating: 269it [02:57,  1.49it/s]Extractor Estimating: 270it [02:58,  1.43it/s]Extractor Estimating: 271it [02:58,  1.46it/s]Extractor Estimating: 272it [02:59,  1.45it/s]Extractor Estimating: 273it [03:00,  1.49it/s]Extractor Estimating: 274it [03:00,  1.46it/s]Extractor Estimating: 275it [03:01,  1.46it/s]Extractor Estimating: 276it [03:02,  1.52it/s]Extractor Estimating: 277it [03:02,  1.55it/s]Extractor Estimating: 278it [03:03,  1.52it/s]Extractor Estimating: 279it [03:03,  1.53it/s]Extractor Estimating: 280it [03:04,  1.53it/s]Extractor Estimating: 281it [03:05,  1.56it/s]Extractor Estimating: 282it [03:05,  1.57it/s]Extractor Estimating: 283it [03:06,  1.57it/s]Extractor Estimating: 284it [03:07,  1.62it/s]Extractor Estimating: 285it [03:07,  1.54it/s]Extractor Estimating: 286it [03:08,  1.56it/s]Extractor Estimating: 287it [03:09,  1.53it/s]Extractor Estimating: 288it [03:09,  1.51it/s]Extractor Estimating: 289it [03:10,  1.48it/s]Extractor Estimating: 290it [03:11,  1.53it/s]Extractor Estimating: 291it [03:11,  1.61it/s]Extractor Estimating: 292it [03:12,  1.61it/s]Extractor Estimating: 293it [03:12,  1.59it/s]Extractor Estimating: 294it [03:13,  1.56it/s]Extractor Estimating: 295it [03:14,  1.58it/s]Extractor Estimating: 296it [03:14,  1.62it/s]Extractor Estimating: 297it [03:15,  1.58it/s]Extractor Estimating: 298it [03:16,  1.61it/s]Extractor Estimating: 299it [03:16,  1.59it/s]Extractor Estimating: 300it [03:17,  1.58it/s]Extractor Estimating: 301it [03:17,  1.60it/s]Extractor Estimating: 302it [03:18,  1.52it/s]Extractor Estimating: 303it [03:19,  1.52it/s]Extractor Estimating: 304it [03:20,  1.52it/s]Extractor Estimating: 305it [03:20,  1.55it/s]Extractor Estimating: 306it [03:21,  1.52it/s]Extractor Estimating: 307it [03:22,  1.48it/s]Extractor Estimating: 308it [03:22,  1.48it/s]Extractor Estimating: 309it [03:23,  1.50it/s]Extractor Estimating: 310it [03:23,  1.54it/s]Extractor Estimating: 311it [03:24,  1.57it/s]Extractor Estimating: 312it [03:25,  1.57it/s]Extractor Estimating: 313it [03:25,  1.58it/s]Extractor Estimating: 314it [03:26,  1.55it/s]Extractor Estimating: 315it [03:27,  1.55it/s]Extractor Estimating: 316it [03:27,  1.59it/s]Extractor Estimating: 317it [03:28,  1.59it/s]Extractor Estimating: 318it [03:29,  1.56it/s]Extractor Estimating: 319it [03:29,  1.54it/s]Extractor Estimating: 320it [03:30,  1.62it/s]Extractor Estimating: 321it [03:31,  1.47it/s]Extractor Estimating: 322it [03:31,  1.53it/s]Extractor Estimating: 323it [03:32,  1.56it/s]Extractor Estimating: 324it [03:32,  1.53it/s]Extractor Estimating: 325it [03:33,  1.53it/s]Extractor Estimating: 326it [03:34,  1.43it/s]Extractor Estimating: 327it [03:34,  1.53it/s]Extractor Estimating: 328it [03:35,  1.62it/s]Extractor Estimating: 329it [03:36,  1.53it/s]Extractor Estimating: 330it [03:36,  1.57it/s]Extractor Estimating: 331it [03:37,  1.64it/s]Extractor Estimating: 332it [03:38,  1.63it/s]Extractor Estimating: 333it [03:38,  1.67it/s]Extractor Estimating: 334it [03:39,  1.66it/s]Extractor Estimating: 335it [03:39,  1.68it/s]Extractor Estimating: 336it [03:40,  1.72it/s]Extractor Estimating: 337it [03:40,  1.71it/s]Extractor Estimating: 338it [03:41,  1.68it/s]Extractor Estimating: 339it [03:42,  1.60it/s]Extractor Estimating: 340it [03:42,  1.62it/s]Extractor Estimating: 341it [03:43,  1.62it/s]Extractor Estimating: 342it [03:43,  1.70it/s]Extractor Estimating: 343it [03:44,  1.64it/s]Extractor Estimating: 344it [03:45,  1.59it/s]Extractor Estimating: 345it [03:45,  1.61it/s]Extractor Estimating: 346it [03:46,  1.58it/s]Extractor Estimating: 347it [03:47,  1.62it/s]Extractor Estimating: 348it [03:47,  1.59it/s]Extractor Estimating: 349it [03:48,  1.55it/s]Extractor Estimating: 350it [03:49,  1.60it/s]Extractor Estimating: 351it [03:49,  1.61it/s]Extractor Estimating: 352it [03:50,  1.59it/s]Extractor Estimating: 353it [03:50,  1.56it/s]Extractor Estimating: 354it [03:51,  1.57it/s]Extractor Estimating: 355it [03:52,  1.58it/s]Extractor Estimating: 356it [03:52,  1.61it/s]Extractor Estimating: 357it [03:53,  1.63it/s]Extractor Estimating: 358it [03:54,  1.59it/s]Extractor Estimating: 359it [03:54,  1.60it/s]Extractor Estimating: 360it [03:55,  1.60it/s]Extractor Estimating: 361it [03:55,  1.61it/s]Extractor Estimating: 362it [03:56,  1.69it/s]Extractor Estimating: 363it [03:57,  1.70it/s]Extractor Estimating: 364it [03:57,  1.75it/s]Extractor Estimating: 365it [03:58,  1.73it/s]Extractor Estimating: 366it [03:58,  1.65it/s]Extractor Estimating: 367it [03:59,  1.65it/s]Extractor Estimating: 368it [04:00,  1.58it/s]Extractor Estimating: 369it [04:00,  1.59it/s]Extractor Estimating: 370it [04:01,  1.62it/s]Extractor Estimating: 371it [04:01,  1.66it/s]Extractor Estimating: 372it [04:02,  1.65it/s]Extractor Estimating: 373it [04:03,  1.67it/s]Extractor Estimating: 374it [04:03,  1.68it/s]Extractor Estimating: 375it [04:04,  1.62it/s]Extractor Estimating: 376it [04:04,  1.63it/s]Extractor Estimating: 377it [04:05,  1.64it/s]Extractor Estimating: 378it [04:06,  1.70it/s]Extractor Estimating: 379it [04:06,  1.70it/s]Extractor Estimating: 380it [04:07,  1.64it/s]Extractor Estimating: 381it [04:07,  1.62it/s]Extractor Estimating: 382it [04:08,  1.64it/s]Extractor Estimating: 383it [04:09,  1.62it/s]Extractor Estimating: 384it [04:09,  1.60it/s]Extractor Estimating: 385it [04:10,  1.56it/s]Extractor Estimating: 386it [04:11,  1.57it/s]Extractor Estimating: 387it [04:11,  1.62it/s]Extractor Estimating: 388it [04:12,  1.65it/s]Extractor Estimating: 389it [04:12,  1.70it/s]Extractor Estimating: 390it [04:13,  1.63it/s]Extractor Estimating: 391it [04:14,  1.64it/s]Extractor Estimating: 392it [04:14,  1.64it/s]Extractor Estimating: 393it [04:15,  1.65it/s]Extractor Estimating: 394it [04:15,  1.68it/s]Extractor Estimating: 395it [04:16,  1.67it/s]Extractor Estimating: 396it [04:17,  1.66it/s]Extractor Estimating: 397it [04:17,  1.63it/s]Extractor Estimating: 398it [04:18,  1.59it/s]Extractor Estimating: 399it [04:19,  1.60it/s]Extractor Estimating: 400it [04:19,  1.59it/s]Extractor Estimating: 401it [04:20,  1.58it/s]Extractor Estimating: 402it [04:21,  1.53it/s]Extractor Estimating: 403it [04:21,  1.53it/s]Extractor Estimating: 404it [04:22,  1.53it/s]Extractor Estimating: 405it [04:22,  1.55it/s]Extractor Estimating: 406it [04:23,  1.55it/s]Extractor Estimating: 407it [04:24,  1.55it/s]Extractor Estimating: 408it [04:24,  1.50it/s]Extractor Estimating: 409it [04:25,  1.58it/s]Extractor Estimating: 410it [04:26,  1.43it/s]Extractor Estimating: 411it [04:27,  1.47it/s]Extractor Estimating: 412it [04:27,  1.46it/s]Extractor Estimating: 413it [04:28,  1.57it/s]Extractor Estimating: 414it [04:28,  1.58it/s]Extractor Estimating: 415it [04:29,  1.55it/s]Extractor Estimating: 416it [04:30,  1.54it/s]Extractor Estimating: 417it [04:30,  1.56it/s]Extractor Estimating: 418it [04:31,  1.60it/s]Extractor Estimating: 419it [04:32,  1.61it/s]Extractor Estimating: 420it [04:32,  1.60it/s]Extractor Estimating: 421it [04:33,  1.64it/s]Extractor Estimating: 422it [04:33,  1.62it/s]Extractor Estimating: 423it [04:34,  1.58it/s]Extractor Estimating: 424it [04:35,  1.59it/s]Extractor Estimating: 425it [04:35,  1.59it/s]Extractor Estimating: 426it [04:36,  1.61it/s]Extractor Estimating: 427it [04:37,  1.58it/s]Extractor Estimating: 428it [04:37,  1.56it/s]Extractor Estimating: 429it [04:38,  1.58it/s]Extractor Estimating: 430it [04:38,  1.55it/s]Extractor Estimating: 431it [04:39,  1.58it/s]Extractor Estimating: 432it [04:40,  1.65it/s]Extractor Estimating: 433it [04:40,  1.64it/s]Extractor Estimating: 434it [04:41,  1.61it/s]Extractor Estimating: 435it [04:41,  1.63it/s]Extractor Estimating: 436it [04:42,  1.59it/s]Extractor Estimating: 437it [04:43,  1.61it/s]Extractor Estimating: 438it [04:43,  1.61it/s]Extractor Estimating: 439it [04:44,  1.61it/s]Extractor Estimating: 440it [04:45,  1.62it/s]Extractor Estimating: 441it [04:45,  1.64it/s]Extractor Estimating: 442it [04:46,  1.61it/s]Extractor Estimating: 443it [04:46,  1.61it/s]Extractor Estimating: 444it [04:47,  1.63it/s]Extractor Estimating: 445it [04:48,  1.66it/s]Extractor Estimating: 446it [04:48,  1.64it/s]Extractor Estimating: 447it [04:49,  1.66it/s]Extractor Estimating: 448it [04:50,  1.63it/s]Extractor Estimating: 449it [04:50,  1.70it/s]Extractor Estimating: 450it [04:51,  1.62it/s]Extractor Estimating: 451it [04:51,  1.59it/s]Extractor Estimating: 452it [04:52,  1.51it/s]Extractor Estimating: 453it [04:53,  1.38it/s]Extractor Estimating: 454it [04:54,  1.41it/s]Extractor Estimating: 455it [04:54,  1.46it/s]Extractor Estimating: 456it [04:55,  1.51it/s]Extractor Estimating: 457it [04:56,  1.49it/s]Extractor Estimating: 458it [04:56,  1.48it/s]Extractor Estimating: 459it [04:57,  1.47it/s]Extractor Estimating: 460it [04:58,  1.45it/s]Extractor Estimating: 461it [04:58,  1.47it/s]Extractor Estimating: 462it [04:59,  1.46it/s]Extractor Estimating: 463it [05:00,  1.48it/s]Extractor Estimating: 464it [05:00,  1.47it/s]Extractor Estimating: 465it [05:01,  1.44it/s]Extractor Estimating: 466it [05:02,  1.41it/s]Extractor Estimating: 467it [05:03,  1.42it/s]Extractor Estimating: 468it [05:03,  1.43it/s]Extractor Estimating: 469it [05:04,  1.43it/s]Extractor Estimating: 470it [05:05,  1.48it/s]Extractor Estimating: 471it [05:05,  1.42it/s]Extractor Estimating: 472it [05:06,  1.42it/s]Extractor Estimating: 473it [05:07,  1.43it/s]Extractor Estimating: 474it [05:07,  1.46it/s]Extractor Estimating: 475it [05:08,  1.49it/s]Extractor Estimating: 476it [05:09,  1.48it/s]Extractor Estimating: 477it [05:09,  1.53it/s]Extractor Estimating: 478it [05:10,  1.53it/s]Extractor Estimating: 479it [05:11,  1.60it/s]Extractor Estimating: 480it [05:11,  1.61it/s]Extractor Estimating: 481it [05:12,  1.59it/s]Extractor Estimating: 482it [05:12,  1.59it/s]Extractor Estimating: 483it [05:13,  1.55it/s]Extractor Estimating: 484it [05:14,  1.55it/s]Extractor Estimating: 485it [05:14,  1.59it/s]Extractor Estimating: 486it [05:15,  1.52it/s]Extractor Estimating: 487it [05:16,  1.56it/s]Extractor Estimating: 488it [05:16,  1.59it/s]Extractor Estimating: 489it [05:17,  1.64it/s]Extractor Estimating: 490it [05:17,  1.59it/s]Extractor Estimating: 491it [05:18,  1.64it/s]Extractor Estimating: 492it [05:19,  1.59it/s]Extractor Estimating: 493it [05:19,  1.56it/s]Extractor Estimating: 494it [05:20,  1.51it/s]Extractor Estimating: 495it [05:21,  1.59it/s]Extractor Estimating: 496it [05:21,  1.56it/s]Extractor Estimating: 497it [05:22,  1.44it/s]Extractor Estimating: 498it [05:23,  1.46it/s]Extractor Estimating: 499it [05:23,  1.49it/s]Extractor Estimating: 500it [05:24,  1.64it/s]Extractor Estimating: 500it [05:24,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:52,526 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:52,553 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:52,553 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:52,554 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:52,554 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:43:53,484 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:43:53,485 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:43:54,211 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:43:55,342 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:43:55,342 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:58,302 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:58,304 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:58,305 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:58,305 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:43:58,305 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:43:59,070 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:43:59,071 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:43:59,705 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:43:59,926 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:43:59,926 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:46:09,975 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:46:10,011 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 10473 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 29489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 29589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=29589, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.046, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.038, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.082, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.032, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 63, avg_time 1.041, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 163, avg_time 2.121, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 263, avg_time 1.051, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 363, avg_time 1.043, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 26, avg_time 1.037, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 126, avg_time 1.042, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 226, avg_time 2.111, loss:nan
g_step 1200, step 326, avg_time 1.037, loss:nan
g_step 1300, step 426, avg_time 1.034, loss:nan
g_step 1400, step 89, avg_time 1.036, loss:nan
g_step 1500, step 189, avg_time 1.044, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 289, avg_time 2.104, loss:nan
g_step 1700, step 389, avg_time 1.056, loss:nan
g_step 1800, step 52, avg_time 1.034, loss:nan
g_step 1900, step 152, avg_time 1.043, loss:nan
g_step 2000, step 252, avg_time 1.045, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 352, avg_time 2.102, loss:nan
g_step 2200, step 15, avg_time 1.030, loss:nan
g_step 2300, step 115, avg_time 1.051, loss:nan
g_step 2400, step 215, avg_time 1.039, loss:nan
g_step 2500, step 315, avg_time 1.025, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 415, avg_time 2.117, loss:nan
g_step 2700, step 78, avg_time 1.040, loss:nan
g_step 2800, step 178, avg_time 1.048, loss:nan
g_step 2900, step 278, avg_time 1.047, loss:nan
g_step 3000, step 378, avg_time 1.038, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 41, avg_time 2.082, loss:nan
g_step 3200, step 141, avg_time 1.069, loss:nan
g_step 3300, step 241, avg_time 1.062, loss:nan
g_step 3400, step 341, avg_time 1.025, loss:nan
g_step 3500, step 4, avg_time 1.035, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 104, avg_time 2.102, loss:nan
g_step 3700, step 204, avg_time 1.033, loss:nan
g_step 3800, step 304, avg_time 1.056, loss:nan
g_step 3900, step 404, avg_time 1.037, loss:nan
g_step 4000, step 67, avg_time 1.031, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 167, avg_time 2.104, loss:nan
g_step 4200, step 267, avg_time 1.024, loss:nan
g_step 4300, step 367, avg_time 1.050, loss:nan
g_step 4400, step 30, avg_time 1.053, loss:nan
g_step 4500, step 130, avg_time 1.050, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 230, avg_time 2.095, loss:nan
g_step 4700, step 330, avg_time 1.049, loss:nan
g_step 4800, step 430, avg_time 1.035, loss:nan
g_step 4900, step 93, avg_time 1.035, loss:nan
g_step 5000, step 193, avg_time 1.060, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 293, avg_time 2.105, loss:nan
g_step 5200, step 393, avg_time 1.032, loss:nan
g_step 5300, step 56, avg_time 1.034, loss:nan
g_step 5400, step 156, avg_time 1.042, loss:nan
g_step 5500, step 256, avg_time 1.047, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 356, avg_time 2.090, loss:nan
g_step 5700, step 19, avg_time 1.075, loss:nan
g_step 5800, step 119, avg_time 1.060, loss:nan
g_step 5900, step 219, avg_time 1.044, loss:nan
g_step 6000, step 319, avg_time 1.049, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 419, avg_time 2.076, loss:nan
g_step 6200, step 82, avg_time 1.043, loss:nan
g_step 6300, step 182, avg_time 1.044, loss:nan
g_step 6400, step 282, avg_time 1.033, loss:nan
g_step 6500, step 382, avg_time 1.052, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 45, avg_time 2.082, loss:nan
g_step 6700, step 145, avg_time 1.050, loss:nan
g_step 6800, step 245, avg_time 1.034, loss:nan
g_step 6900, step 345, avg_time 1.055, loss:nan
g_step 7000, step 8, avg_time 1.060, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 108, avg_time 2.106, loss:nan
g_step 7200, step 208, avg_time 1.043, loss:nan
g_step 7300, step 308, avg_time 1.038, loss:nan
g_step 7400, step 408, avg_time 1.049, loss:nan
g_step 7500, step 71, avg_time 1.042, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 171, avg_time 2.111, loss:nan
g_step 7700, step 271, avg_time 1.053, loss:nan
g_step 7800, step 371, avg_time 1.030, loss:nan
g_step 7900, step 34, avg_time 1.049, loss:nan
g_step 8000, step 134, avg_time 1.026, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 234, avg_time 2.115, loss:nan
g_step 8200, step 334, avg_time 1.051, loss:nan
g_step 8300, step 434, avg_time 1.048, loss:nan
g_step 8400, step 97, avg_time 1.049, loss:nan
g_step 8500, step 197, avg_time 1.024, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 297, avg_time 2.105, loss:nan
g_step 8700, step 397, avg_time 1.060, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:46:10 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:46:10 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-46-09_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:46:11 - WARNING - datasets.builder -   Using custom data configuration default-2369910077658791
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2369910077658791/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:46:12,727 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:46:12,728 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:46:12,729 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:46:12,730 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:46:12,807 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:46:12,843 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:46:12,843 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:46:12,843 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:46:12,843 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:46:12,843 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:46:12,843 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:46:13,140 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:46:16,457 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:46:16,480 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2369910077658791/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.73ba/s] 18%|        | 2/11 [00:00<00:02,  3.62ba/s] 27%|       | 3/11 [00:00<00:02,  3.98ba/s] 36%|      | 4/11 [00:01<00:01,  4.18ba/s] 45%|     | 5/11 [00:01<00:01,  4.30ba/s] 55%|    | 6/11 [00:01<00:01,  4.37ba/s] 64%|   | 7/11 [00:01<00:00,  4.41ba/s] 73%|  | 8/11 [00:01<00:00,  4.45ba/s] 82%| | 9/11 [00:02<00:00,  4.47ba/s] 91%| | 10/11 [00:02<00:00,  4.47ba/s]100%|| 11/11 [00:02<00:00,  5.20ba/s]100%|| 11/11 [00:02<00:00,  4.45ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.45ba/s] 50%|     | 2/4 [00:00<00:00,  4.02ba/s] 75%|  | 3/4 [00:00<00:00,  4.10ba/s]100%|| 4/4 [00:00<00:00,  5.25ba/s]100%|| 4/4 [00:00<00:00,  4.67ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  5.58ba/s] 27%|       | 3/11 [00:00<00:00,  8.48ba/s] 45%|     | 5/11 [00:00<00:00,  9.32ba/s] 64%|   | 7/11 [00:00<00:00,  9.67ba/s] 82%| | 9/11 [00:00<00:00,  9.88ba/s]100%|| 11/11 [00:01<00:00, 10.89ba/s]100%|| 11/11 [00:01<00:00,  9.94ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.93ba/s] 75%|  | 3/4 [00:00<00:00,  8.57ba/s]100%|| 4/4 [00:00<00:00,  9.66ba/s]
[INFO|trainer.py:414] 2023-08-28 06:46:22,343 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:46:22,412 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:46:22,412 >>   Num examples = 10520
[INFO|trainer.py:1149] 2023-08-28 06:46:22,412 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:46:22,412 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:46:22,412 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:46:22,412 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:46:22,412 >>   Total optimization steps = 820
  0%|          | 0/820 [00:00<?, ?it/s]  0%|          | 1/820 [00:00<03:58,  3.44it/s]  0%|          | 2/820 [00:00<04:03,  3.35it/s]  0%|          | 3/820 [00:00<03:55,  3.47it/s]  0%|          | 4/820 [00:01<03:53,  3.50it/s]  1%|          | 5/820 [00:01<03:51,  3.52it/s]  1%|          | 6/820 [00:01<03:50,  3.53it/s]  1%|          | 7/820 [00:01<03:49,  3.54it/s]  1%|          | 8/820 [00:02<03:49,  3.54it/s]  1%|          | 9/820 [00:02<03:48,  3.55it/s]  1%|          | 10/820 [00:02<03:47,  3.56it/s]  1%|         | 11/820 [00:03<03:47,  3.56it/s]  1%|         | 12/820 [00:03<03:47,  3.56it/s]  2%|         | 13/820 [00:03<03:54,  3.44it/s]  2%|         | 14/820 [00:03<03:51,  3.48it/s]  2%|         | 15/820 [00:04<03:50,  3.50it/s]  2%|         | 16/820 [00:04<03:48,  3.52it/s]  2%|         | 17/820 [00:04<03:47,  3.53it/s]  2%|         | 18/820 [00:05<03:46,  3.54it/s]  2%|         | 19/820 [00:05<03:45,  3.55it/s]  2%|         | 20/820 [00:05<03:45,  3.55it/s]  3%|         | 21/820 [00:05<03:44,  3.55it/s]  3%|         | 22/820 [00:06<03:44,  3.56it/s]  3%|         | 23/820 [00:06<03:44,  3.56it/s]  3%|         | 24/820 [00:06<03:47,  3.50it/s]  3%|         | 25/820 [00:07<03:45,  3.52it/s]  3%|         | 26/820 [00:07<03:45,  3.53it/s]  3%|         | 27/820 [00:07<03:44,  3.53it/s]  3%|         | 28/820 [00:07<03:43,  3.54it/s]  4%|         | 29/820 [00:08<03:43,  3.54it/s]  4%|         | 30/820 [00:08<03:43,  3.54it/s]  4%|         | 31/820 [00:08<03:42,  3.54it/s]  4%|         | 32/820 [00:09<03:42,  3.54it/s]  4%|         | 33/820 [00:09<03:42,  3.54it/s]  4%|         | 34/820 [00:09<03:41,  3.55it/s]  4%|         | 35/820 [00:09<03:48,  3.43it/s]  4%|         | 36/820 [00:10<03:46,  3.47it/s]  5%|         | 37/820 [00:10<03:44,  3.49it/s]  5%|         | 38/820 [00:10<03:42,  3.51it/s]  5%|         | 39/820 [00:11<03:41,  3.52it/s]  5%|         | 40/820 [00:11<03:40,  3.53it/s]  5%|         | 41/820 [00:11<03:40,  3.54it/s]  5%|         | 42/820 [00:11<03:39,  3.54it/s]  5%|         | 43/820 [00:12<03:39,  3.54it/s]  5%|         | 44/820 [00:12<03:39,  3.54it/s]  5%|         | 45/820 [00:12<03:38,  3.54it/s]  6%|         | 46/820 [00:13<03:46,  3.42it/s]  6%|         | 47/820 [00:13<03:43,  3.46it/s]  6%|         | 48/820 [00:13<03:41,  3.48it/s]  6%|         | 49/820 [00:13<03:40,  3.50it/s]  6%|         | 50/820 [00:14<03:39,  3.51it/s]  6%|         | 51/820 [00:14<03:38,  3.53it/s]  6%|         | 52/820 [00:14<03:37,  3.53it/s]  6%|         | 53/820 [00:15<03:36,  3.54it/s]  7%|         | 54/820 [00:15<03:36,  3.54it/s]  7%|         | 55/820 [00:15<03:35,  3.55it/s]  7%|         | 56/820 [00:15<03:35,  3.55it/s]  7%|         | 57/820 [00:16<03:44,  3.39it/s]  7%|         | 58/820 [00:16<03:41,  3.44it/s]  7%|         | 59/820 [00:16<03:39,  3.46it/s]  7%|         | 60/820 [00:17<04:58,  2.55it/s]  7%|         | 61/820 [00:17<04:33,  2.78it/s]  8%|         | 62/820 [00:17<04:15,  2.97it/s]  8%|         | 63/820 [00:18<04:02,  3.12it/s]  8%|         | 64/820 [00:18<03:53,  3.24it/s]  8%|         | 65/820 [00:18<03:46,  3.33it/s]  8%|         | 66/820 [00:19<03:42,  3.39it/s]  8%|         | 67/820 [00:19<03:39,  3.43it/s]  8%|         | 68/820 [00:19<03:36,  3.47it/s]  8%|         | 69/820 [00:19<03:35,  3.49it/s]  9%|         | 70/820 [00:20<03:34,  3.50it/s]  9%|         | 71/820 [00:20<03:33,  3.52it/s]  9%|         | 72/820 [00:20<03:32,  3.53it/s]  9%|         | 73/820 [00:21<03:31,  3.53it/s]  9%|         | 74/820 [00:21<03:30,  3.54it/s]  9%|         | 75/820 [00:21<03:38,  3.41it/s]  9%|         | 76/820 [00:21<03:35,  3.45it/s]  9%|         | 77/820 [00:22<03:33,  3.48it/s] 10%|         | 78/820 [00:22<03:32,  3.49it/s] 10%|         | 79/820 [00:22<03:31,  3.51it/s] 10%|         | 80/820 [00:23<03:29,  3.53it/s] 10%|         | 81/820 [00:23<03:28,  3.55it/s] 10%|         | 82/820 [00:23<03:27,  3.56it/s] 10%|         | 83/820 [00:23<03:26,  3.57it/s] 10%|         | 84/820 [00:24<03:25,  3.58it/s] 10%|         | 85/820 [00:24<03:24,  3.59it/s] 10%|         | 86/820 [00:24<03:27,  3.53it/s] 11%|         | 87/820 [00:25<03:26,  3.55it/s] 11%|         | 88/820 [00:25<03:25,  3.57it/s] 11%|         | 89/820 [00:25<03:24,  3.58it/s] 11%|         | 90/820 [00:25<03:23,  3.58it/s] 11%|         | 91/820 [00:26<03:23,  3.59it/s] 11%|         | 92/820 [00:26<03:22,  3.59it/s] 11%|        | 93/820 [00:26<03:22,  3.60it/s] 11%|        | 94/820 [00:27<03:21,  3.60it/s] 12%|        | 95/820 [00:27<03:33,  3.40it/s] 12%|        | 96/820 [00:27<03:30,  3.44it/s] 12%|        | 97/820 [00:27<03:30,  3.44it/s] 12%|        | 98/820 [00:28<03:27,  3.49it/s] 12%|        | 99/820 [00:28<03:24,  3.52it/s] 12%|        | 100/820 [00:28<03:23,  3.54it/s] 12%|        | 101/820 [00:29<03:21,  3.56it/s] 12%|        | 102/820 [00:29<03:21,  3.57it/s] 13%|        | 103/820 [00:29<03:20,  3.58it/s] 13%|        | 104/820 [00:29<03:20,  3.58it/s] 13%|        | 105/820 [00:30<03:19,  3.58it/s] 13%|        | 106/820 [00:30<03:18,  3.59it/s] 13%|        | 107/820 [00:30<03:18,  3.59it/s] 13%|        | 108/820 [00:31<03:24,  3.48it/s] 13%|        | 109/820 [00:31<03:22,  3.51it/s] 13%|        | 110/820 [00:31<03:20,  3.53it/s] 14%|        | 111/820 [00:31<03:19,  3.55it/s] 14%|        | 112/820 [00:32<03:18,  3.57it/s] 14%|        | 113/820 [00:32<03:17,  3.58it/s] 14%|        | 114/820 [00:32<03:16,  3.59it/s] 14%|        | 115/820 [00:32<03:16,  3.58it/s] 14%|        | 116/820 [00:33<03:16,  3.59it/s] 14%|        | 117/820 [00:33<03:15,  3.59it/s] 14%|        | 118/820 [00:33<03:15,  3.59it/s] 15%|        | 119/820 [00:34<03:24,  3.42it/s] 15%|        | 120/820 [00:34<03:21,  3.47it/s] 15%|        | 121/820 [00:34<03:19,  3.51it/s] 15%|        | 122/820 [00:34<03:17,  3.53it/s] 15%|        | 123/820 [00:35<03:16,  3.55it/s] 15%|        | 124/820 [00:35<03:15,  3.56it/s] 15%|        | 125/820 [00:35<03:14,  3.57it/s] 15%|        | 126/820 [00:36<03:13,  3.58it/s] 15%|        | 127/820 [00:36<03:13,  3.59it/s] 16%|        | 128/820 [00:36<03:12,  3.59it/s] 16%|        | 129/820 [00:36<03:12,  3.60it/s] 16%|        | 130/820 [00:37<03:18,  3.48it/s] 16%|        | 131/820 [00:37<03:16,  3.51it/s] 16%|        | 132/820 [00:37<03:14,  3.54it/s] 16%|        | 133/820 [00:38<03:13,  3.56it/s] 16%|        | 134/820 [00:38<03:12,  3.57it/s] 16%|        | 135/820 [00:38<03:11,  3.58it/s] 17%|        | 136/820 [00:38<03:10,  3.59it/s] 17%|        | 137/820 [00:39<03:10,  3.59it/s] 17%|        | 138/820 [00:39<03:09,  3.59it/s] 17%|        | 139/820 [00:39<03:09,  3.59it/s] 17%|        | 140/820 [00:39<03:09,  3.59it/s] 17%|        | 141/820 [00:40<03:17,  3.44it/s] 17%|        | 142/820 [00:40<03:14,  3.49it/s] 17%|        | 143/820 [00:40<03:12,  3.52it/s] 18%|        | 144/820 [00:41<03:10,  3.55it/s] 18%|        | 145/820 [00:41<03:09,  3.56it/s] 18%|        | 146/820 [00:41<03:08,  3.58it/s] 18%|        | 147/820 [00:41<03:07,  3.58it/s] 18%|        | 148/820 [00:42<03:07,  3.59it/s] 18%|        | 149/820 [00:42<03:06,  3.59it/s] 18%|        | 150/820 [00:42<03:06,  3.59it/s] 18%|        | 151/820 [00:43<03:06,  3.59it/s] 19%|        | 152/820 [00:43<03:12,  3.47it/s] 19%|        | 153/820 [00:43<03:10,  3.51it/s] 19%|        | 154/820 [00:43<03:08,  3.53it/s] 19%|        | 155/820 [00:44<03:07,  3.55it/s] 19%|        | 156/820 [00:44<03:06,  3.57it/s] 19%|        | 157/820 [00:44<03:05,  3.58it/s] 19%|        | 158/820 [00:45<03:04,  3.58it/s] 19%|        | 159/820 [00:45<03:04,  3.59it/s] 20%|        | 160/820 [00:45<03:03,  3.59it/s] 20%|        | 161/820 [00:45<03:03,  3.60it/s] 20%|        | 162/820 [00:46<03:02,  3.60it/s] 20%|        | 163/820 [00:46<03:07,  3.51it/s] 20%|        | 164/820 [00:46<03:05,  3.54it/s][INFO|trainer.py:2140] 2023-08-28 06:47:09,199 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:47:09,199 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 06:47:09,199 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.78it/s][A
  3%|         | 12/435 [00:00<00:08, 48.75it/s][A
  4%|         | 17/435 [00:00<00:08, 47.16it/s][A
  5%|         | 22/435 [00:00<00:08, 46.37it/s][A
  6%|         | 27/435 [00:00<00:08, 45.93it/s][A
  7%|         | 32/435 [00:00<00:08, 45.33it/s][A
  9%|         | 37/435 [00:00<00:09, 43.96it/s][A
 10%|         | 42/435 [00:00<00:08, 44.12it/s][A
 11%|         | 47/435 [00:01<00:08, 44.40it/s][A
 12%|        | 52/435 [00:01<00:08, 43.74it/s][A
 13%|        | 57/435 [00:01<00:08, 44.15it/s][A
 14%|        | 62/435 [00:01<00:08, 44.30it/s][A
 15%|        | 67/435 [00:01<00:08, 44.55it/s][A
 17%|        | 72/435 [00:01<00:08, 44.70it/s][A
 18%|        | 77/435 [00:01<00:07, 44.76it/s][A
 19%|        | 82/435 [00:01<00:08, 42.40it/s][A
 20%|        | 87/435 [00:01<00:08, 43.22it/s][A
 21%|        | 92/435 [00:02<00:07, 43.73it/s][A
 22%|       | 97/435 [00:02<00:07, 44.06it/s][A
 23%|       | 102/435 [00:02<00:07, 44.31it/s][A
 25%|       | 107/435 [00:02<00:07, 44.55it/s][A
 26%|       | 112/435 [00:02<00:07, 44.64it/s][A
 27%|       | 117/435 [00:02<00:07, 44.72it/s][A
 28%|       | 122/435 [00:02<00:07, 44.70it/s][A
 29%|       | 127/435 [00:02<00:06, 44.59it/s][A
 30%|       | 132/435 [00:02<00:06, 44.69it/s][A
 31%|      | 137/435 [00:03<00:06, 44.68it/s][A
 33%|      | 142/435 [00:03<00:06, 44.83it/s][A
 34%|      | 147/435 [00:03<00:06, 44.78it/s][A
 35%|      | 152/435 [00:03<00:06, 44.86it/s][A
 36%|      | 157/435 [00:03<00:06, 44.85it/s][A
 37%|      | 162/435 [00:03<00:06, 44.85it/s][A
 38%|      | 167/435 [00:03<00:05, 44.89it/s][A
 40%|      | 172/435 [00:03<00:05, 44.74it/s][A
 41%|      | 177/435 [00:03<00:05, 43.48it/s][A
 42%|     | 182/435 [00:04<00:05, 43.91it/s][A
 43%|     | 187/435 [00:04<00:05, 44.27it/s][A
 44%|     | 192/435 [00:04<00:05, 44.35it/s][A
 45%|     | 197/435 [00:04<00:05, 44.54it/s][A
 46%|     | 202/435 [00:04<00:05, 44.66it/s][A
 48%|     | 207/435 [00:04<00:05, 44.71it/s][A
 49%|     | 212/435 [00:04<00:04, 44.78it/s][A
 50%|     | 217/435 [00:04<00:04, 44.64it/s][A
 51%|     | 222/435 [00:04<00:04, 44.68it/s][A
 52%|    | 227/435 [00:05<00:04, 44.65it/s][A
 53%|    | 232/435 [00:05<00:04, 44.77it/s][A
 54%|    | 237/435 [00:05<00:04, 44.74it/s][A
 56%|    | 242/435 [00:05<00:04, 44.78it/s][A
 57%|    | 247/435 [00:05<00:04, 44.76it/s][A
 58%|    | 252/435 [00:05<00:04, 44.81it/s][A
 59%|    | 257/435 [00:05<00:03, 44.74it/s][A
 60%|    | 262/435 [00:05<00:03, 44.77it/s][A
 61%|   | 267/435 [00:05<00:03, 44.81it/s][A
 63%|   | 272/435 [00:06<00:03, 44.73it/s][A
 64%|   | 277/435 [00:06<00:03, 44.88it/s][A
 65%|   | 282/435 [00:06<00:03, 44.86it/s][A
 66%|   | 287/435 [00:06<00:03, 44.84it/s][A
 67%|   | 292/435 [00:06<00:03, 44.89it/s][A
 68%|   | 297/435 [00:06<00:03, 44.72it/s][A
 69%|   | 302/435 [00:06<00:03, 43.87it/s][A
 71%|   | 307/435 [00:06<00:02, 44.20it/s][A
 72%|  | 312/435 [00:06<00:02, 44.36it/s][A
 73%|  | 317/435 [00:07<00:02, 44.51it/s][A
 74%|  | 322/435 [00:07<00:02, 44.68it/s][A
 75%|  | 327/435 [00:07<00:02, 44.61it/s][A
 76%|  | 332/435 [00:07<00:02, 44.70it/s][A
 77%|  | 337/435 [00:07<00:02, 44.74it/s][A
 79%|  | 342/435 [00:07<00:02, 44.67it/s][A
 80%|  | 347/435 [00:07<00:01, 44.74it/s][A
 81%|  | 352/435 [00:07<00:01, 44.43it/s][A
 82%| | 357/435 [00:08<00:01, 44.88it/s][A
 83%| | 362/435 [00:08<00:01, 43.41it/s][A
 84%| | 367/435 [00:08<00:01, 44.01it/s][A
 86%| | 372/435 [00:08<00:01, 44.28it/s][A
 87%| | 377/435 [00:08<00:01, 44.25it/s][A
 88%| | 382/435 [00:08<00:01, 44.33it/s][A
 89%| | 387/435 [00:08<00:01, 44.51it/s][A
 90%| | 392/435 [00:08<00:00, 44.65it/s][A
 91%|| 397/435 [00:08<00:00, 44.71it/s][A
 92%|| 402/435 [00:09<00:00, 44.54it/s][A
 94%|| 407/435 [00:09<00:00, 44.56it/s][A
 95%|| 412/435 [00:09<00:00, 44.81it/s][A
 96%|| 417/435 [00:09<00:00, 44.89it/s][A
 97%|| 422/435 [00:09<00:00, 44.89it/s][A
 98%|| 427/435 [00:09<00:00, 44.67it/s][A
 99%|| 432/435 [00:09<00:00, 44.62it/s][A                                                 
                                                 [A 20%|        | 164/820 [00:56<03:05,  3.54it/s]
100%|| 435/435 [00:09<00:00, 44.62it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:47:19,236 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 06:47:19,353 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:47:23,039 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:47:23,197 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:47:23,280 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-164/special_tokens_map.json
 20%|        | 165/820 [01:01<52:07,  4.77s/it] 20%|        | 166/820 [01:02<37:21,  3.43s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|        | 167/820 [01:02<27:01,  2.48s/it] 20%|        | 168/820 [01:02<19:48,  1.82s/it] 21%|        | 169/820 [01:03<14:45,  1.36s/it] 21%|        | 170/820 [01:03<11:14,  1.04s/it] 21%|        | 171/820 [01:03<08:46,  1.23it/s] 21%|        | 172/820 [01:04<07:08,  1.51it/s] 21%|        | 173/820 [01:04<05:54,  1.83it/s] 21%|        | 174/820 [01:04<05:02,  2.14it/s] 21%|       | 175/820 [01:04<04:26,  2.42it/s] 21%|       | 176/820 [01:05<04:00,  2.68it/s] 22%|       | 177/820 [01:05<03:42,  2.89it/s] 22%|       | 178/820 [01:05<03:29,  3.06it/s] 22%|       | 179/820 [01:05<03:20,  3.19it/s] 22%|       | 180/820 [01:06<03:14,  3.29it/s] 22%|       | 181/820 [01:06<03:09,  3.36it/s] 22%|       | 182/820 [01:06<03:06,  3.41it/s] 22%|       | 183/820 [01:07<03:08,  3.37it/s] 22%|       | 184/820 [01:07<03:06,  3.42it/s] 23%|       | 185/820 [01:07<03:03,  3.45it/s] 23%|       | 186/820 [01:07<03:02,  3.48it/s] 23%|       | 187/820 [01:08<03:00,  3.50it/s] 23%|       | 188/820 [01:08<02:59,  3.51it/s] 23%|       | 189/820 [01:08<02:59,  3.52it/s] 23%|       | 190/820 [01:09<02:58,  3.52it/s] 23%|       | 191/820 [01:09<02:58,  3.53it/s] 23%|       | 192/820 [01:09<02:57,  3.53it/s] 24%|       | 193/820 [01:09<02:57,  3.53it/s] 24%|       | 194/820 [01:10<03:01,  3.46it/s] 24%|       | 195/820 [01:10<02:59,  3.48it/s] 24%|       | 196/820 [01:10<02:58,  3.50it/s] 24%|       | 197/820 [01:11<02:56,  3.52it/s] 24%|       | 198/820 [01:11<02:55,  3.54it/s] 24%|       | 199/820 [01:11<02:54,  3.56it/s] 24%|       | 200/820 [01:11<02:53,  3.57it/s] 25%|       | 201/820 [01:12<02:52,  3.58it/s] 25%|       | 202/820 [01:12<02:52,  3.59it/s] 25%|       | 203/820 [01:12<02:51,  3.59it/s] 25%|       | 204/820 [01:13<02:51,  3.59it/s] 25%|       | 205/820 [01:13<03:00,  3.41it/s] 25%|       | 206/820 [01:13<02:57,  3.47it/s] 25%|       | 207/820 [01:13<02:54,  3.51it/s] 25%|       | 208/820 [01:14<02:53,  3.53it/s] 25%|       | 209/820 [01:14<02:52,  3.55it/s] 26%|       | 210/820 [01:14<02:51,  3.56it/s] 26%|       | 211/820 [01:15<02:50,  3.57it/s] 26%|       | 212/820 [01:15<02:49,  3.58it/s] 26%|       | 213/820 [01:15<02:49,  3.58it/s] 26%|       | 214/820 [01:15<02:48,  3.59it/s] 26%|       | 215/820 [01:16<02:48,  3.59it/s] 26%|       | 216/820 [01:16<02:56,  3.41it/s] 26%|       | 217/820 [01:16<02:53,  3.47it/s] 27%|       | 218/820 [01:17<02:51,  3.50it/s] 27%|       | 219/820 [01:17<03:29,  2.87it/s] 27%|       | 220/820 [01:17<03:17,  3.04it/s] 27%|       | 221/820 [01:18<03:08,  3.18it/s] 27%|       | 222/820 [01:18<03:02,  3.28it/s] 27%|       | 223/820 [01:18<02:58,  3.34it/s] 27%|       | 224/820 [01:18<02:55,  3.40it/s] 27%|       | 225/820 [01:19<02:53,  3.44it/s] 28%|       | 226/820 [01:19<02:54,  3.40it/s] 28%|       | 227/820 [01:19<02:59,  3.31it/s] 28%|       | 228/820 [01:20<02:54,  3.39it/s] 28%|       | 229/820 [01:20<02:51,  3.44it/s] 28%|       | 230/820 [01:20<02:49,  3.49it/s] 28%|       | 231/820 [01:20<02:47,  3.52it/s] 28%|       | 232/820 [01:21<02:46,  3.54it/s] 28%|       | 233/820 [01:21<02:45,  3.55it/s] 29%|       | 234/820 [01:21<02:44,  3.57it/s] 29%|       | 235/820 [01:22<02:43,  3.57it/s] 29%|       | 236/820 [01:22<02:43,  3.58it/s] 29%|       | 237/820 [01:22<02:42,  3.58it/s] 29%|       | 238/820 [01:22<02:44,  3.54it/s] 29%|       | 239/820 [01:23<02:43,  3.56it/s] 29%|       | 240/820 [01:23<02:42,  3.57it/s] 29%|       | 241/820 [01:23<02:41,  3.57it/s] 30%|       | 242/820 [01:24<02:41,  3.57it/s] 30%|       | 243/820 [01:24<02:41,  3.58it/s] 30%|       | 244/820 [01:24<02:40,  3.58it/s] 30%|       | 245/820 [01:24<02:40,  3.58it/s] 30%|       | 246/820 [01:25<02:40,  3.58it/s] 30%|       | 247/820 [01:25<02:40,  3.58it/s] 30%|       | 248/820 [01:25<02:39,  3.58it/s] 30%|       | 249/820 [01:26<02:42,  3.52it/s] 30%|       | 250/820 [01:26<02:40,  3.54it/s] 31%|       | 251/820 [01:26<02:40,  3.55it/s] 31%|       | 252/820 [01:26<02:39,  3.56it/s] 31%|       | 253/820 [01:27<02:38,  3.57it/s] 31%|       | 254/820 [01:27<02:40,  3.52it/s] 31%|       | 255/820 [01:27<02:40,  3.53it/s] 31%|       | 256/820 [01:27<02:39,  3.54it/s] 31%|      | 257/820 [01:28<02:38,  3.56it/s] 31%|      | 258/820 [01:28<02:37,  3.57it/s] 32%|      | 259/820 [01:28<02:36,  3.58it/s] 32%|      | 260/820 [01:29<02:39,  3.52it/s] 32%|      | 261/820 [01:29<02:37,  3.54it/s] 32%|      | 262/820 [01:29<02:37,  3.55it/s] 32%|      | 263/820 [01:29<02:36,  3.56it/s] 32%|      | 264/820 [01:30<02:35,  3.57it/s] 32%|      | 265/820 [01:30<02:34,  3.58it/s] 32%|      | 266/820 [01:30<02:34,  3.58it/s] 33%|      | 267/820 [01:31<02:34,  3.58it/s] 33%|      | 268/820 [01:31<02:33,  3.58it/s] 33%|      | 269/820 [01:31<02:33,  3.59it/s] 33%|      | 270/820 [01:31<02:33,  3.59it/s] 33%|      | 271/820 [01:32<02:35,  3.54it/s] 33%|      | 272/820 [01:32<02:34,  3.56it/s] 33%|      | 273/820 [01:32<02:33,  3.57it/s] 33%|      | 274/820 [01:33<02:32,  3.58it/s] 34%|      | 275/820 [01:33<02:32,  3.58it/s] 34%|      | 276/820 [01:33<02:31,  3.58it/s] 34%|      | 277/820 [01:33<02:31,  3.58it/s] 34%|      | 278/820 [01:34<02:31,  3.59it/s] 34%|      | 279/820 [01:34<02:31,  3.58it/s] 34%|      | 280/820 [01:34<02:30,  3.58it/s] 34%|      | 281/820 [01:34<02:30,  3.59it/s] 34%|      | 282/820 [01:35<02:33,  3.51it/s] 35%|      | 283/820 [01:35<02:32,  3.53it/s] 35%|      | 284/820 [01:35<02:31,  3.55it/s] 35%|      | 285/820 [01:36<02:30,  3.56it/s] 35%|      | 286/820 [01:36<02:29,  3.57it/s] 35%|      | 287/820 [01:36<02:29,  3.57it/s] 35%|      | 288/820 [01:36<02:29,  3.57it/s] 35%|      | 289/820 [01:37<02:28,  3.58it/s] 35%|      | 290/820 [01:37<02:28,  3.57it/s] 35%|      | 291/820 [01:37<02:27,  3.57it/s] 36%|      | 292/820 [01:38<02:27,  3.58it/s] 36%|      | 293/820 [01:38<02:34,  3.41it/s] 36%|      | 294/820 [01:38<02:32,  3.46it/s] 36%|      | 295/820 [01:38<02:30,  3.50it/s] 36%|      | 296/820 [01:39<02:28,  3.52it/s] 36%|      | 297/820 [01:39<02:27,  3.54it/s] 36%|      | 298/820 [01:39<02:26,  3.55it/s] 36%|      | 299/820 [01:40<02:26,  3.57it/s] 37%|      | 300/820 [01:40<02:25,  3.58it/s] 37%|      | 301/820 [01:40<02:24,  3.58it/s] 37%|      | 302/820 [01:40<02:24,  3.59it/s] 37%|      | 303/820 [01:41<02:24,  3.59it/s] 37%|      | 304/820 [01:41<02:26,  3.52it/s] 37%|      | 305/820 [01:41<02:25,  3.53it/s] 37%|      | 306/820 [01:42<02:24,  3.55it/s] 37%|      | 307/820 [01:42<02:24,  3.56it/s] 38%|      | 308/820 [01:42<02:23,  3.57it/s] 38%|      | 309/820 [01:42<02:22,  3.57it/s] 38%|      | 310/820 [01:43<02:22,  3.58it/s] 38%|      | 311/820 [01:43<02:22,  3.58it/s] 38%|      | 312/820 [01:43<02:21,  3.58it/s] 38%|      | 313/820 [01:43<02:21,  3.59it/s] 38%|      | 314/820 [01:44<02:21,  3.58it/s] 38%|      | 315/820 [01:44<02:23,  3.51it/s] 39%|      | 316/820 [01:44<02:22,  3.53it/s] 39%|      | 317/820 [01:45<02:21,  3.55it/s] 39%|      | 318/820 [01:45<02:20,  3.56it/s] 39%|      | 319/820 [01:45<02:20,  3.56it/s] 39%|      | 320/820 [01:45<02:19,  3.57it/s] 39%|      | 321/820 [01:46<02:19,  3.58it/s] 39%|      | 322/820 [01:46<02:19,  3.58it/s] 39%|      | 323/820 [01:46<02:19,  3.58it/s] 40%|      | 324/820 [01:47<02:18,  3.58it/s] 40%|      | 325/820 [01:47<02:18,  3.58it/s] 40%|      | 326/820 [01:47<02:23,  3.45it/s] 40%|      | 327/820 [01:47<02:21,  3.48it/s] 40%|      | 328/820 [01:48<02:19,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 06:48:10,693 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:48:10,693 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 06:48:10,693 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.8226, 'eval_samples_per_second': 353.98, 'eval_steps_per_second': 44.286, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.02it/s][A
  3%|         | 12/435 [00:00<00:08, 48.73it/s][A
  4%|         | 17/435 [00:00<00:09, 46.43it/s][A
  5%|         | 22/435 [00:00<00:08, 45.93it/s][A
  6%|         | 27/435 [00:00<00:08, 45.43it/s][A
  7%|         | 32/435 [00:00<00:08, 44.93it/s][A
  9%|         | 37/435 [00:00<00:08, 44.65it/s][A
 10%|         | 42/435 [00:00<00:08, 44.57it/s][A
 11%|         | 47/435 [00:01<00:08, 44.71it/s][A
 12%|        | 52/435 [00:01<00:08, 44.79it/s][A
 13%|        | 57/435 [00:01<00:08, 44.89it/s][A
 14%|        | 62/435 [00:01<00:08, 45.00it/s][A
 15%|        | 67/435 [00:01<00:08, 45.03it/s][A
 17%|        | 72/435 [00:01<00:08, 44.79it/s][A
 18%|        | 77/435 [00:01<00:08, 44.51it/s][A
 19%|        | 82/435 [00:01<00:07, 44.37it/s][A
 20%|        | 87/435 [00:01<00:07, 44.39it/s][A
 21%|        | 92/435 [00:02<00:07, 43.69it/s][A
 22%|       | 97/435 [00:02<00:07, 44.12it/s][A
 23%|       | 102/435 [00:02<00:07, 44.38it/s][A
 25%|       | 107/435 [00:02<00:07, 44.48it/s][A
 26%|       | 112/435 [00:02<00:07, 44.72it/s][A
 27%|       | 117/435 [00:02<00:07, 44.77it/s][A
 28%|       | 122/435 [00:02<00:07, 44.60it/s][A
 29%|       | 127/435 [00:02<00:06, 44.48it/s][A
 30%|       | 132/435 [00:02<00:06, 44.38it/s][A
 31%|      | 137/435 [00:03<00:06, 44.38it/s][A
 33%|      | 142/435 [00:03<00:06, 44.59it/s][A
 34%|      | 147/435 [00:03<00:06, 44.56it/s][A
 35%|      | 152/435 [00:03<00:06, 44.74it/s][A
 36%|      | 157/435 [00:03<00:06, 44.84it/s][A
 37%|      | 162/435 [00:03<00:06, 44.82it/s][A
 38%|      | 167/435 [00:03<00:05, 44.88it/s][A
 40%|      | 172/435 [00:03<00:05, 44.76it/s][A
 41%|      | 177/435 [00:03<00:05, 44.78it/s][A
 42%|     | 182/435 [00:04<00:05, 44.69it/s][A
 43%|     | 187/435 [00:04<00:05, 44.54it/s][A
 44%|     | 192/435 [00:04<00:05, 44.71it/s][A
 45%|     | 197/435 [00:04<00:05, 44.63it/s][A
 46%|     | 202/435 [00:04<00:05, 44.82it/s][A
 48%|     | 207/435 [00:04<00:05, 44.85it/s][A
 49%|     | 212/435 [00:04<00:04, 44.84it/s][A
 50%|     | 217/435 [00:04<00:04, 44.85it/s][A
 51%|     | 222/435 [00:04<00:04, 44.80it/s][A
 52%|    | 227/435 [00:05<00:04, 44.65it/s][A
 53%|    | 232/435 [00:05<00:04, 44.59it/s][A
 54%|    | 237/435 [00:05<00:04, 44.73it/s][A
 56%|    | 242/435 [00:05<00:04, 44.62it/s][A
 57%|    | 247/435 [00:05<00:04, 44.72it/s][A
 58%|    | 252/435 [00:05<00:04, 44.78it/s][A
 59%|    | 257/435 [00:05<00:03, 44.81it/s][A
 60%|    | 262/435 [00:05<00:03, 44.85it/s][A
 61%|   | 267/435 [00:05<00:03, 44.86it/s][A
 63%|   | 272/435 [00:06<00:03, 44.79it/s][A
 64%|   | 277/435 [00:06<00:03, 44.73it/s][A
 65%|   | 282/435 [00:06<00:03, 44.80it/s][A
 66%|   | 287/435 [00:06<00:03, 44.85it/s][A
 67%|   | 292/435 [00:06<00:03, 44.80it/s][A
 68%|   | 297/435 [00:06<00:03, 44.74it/s][A
 69%|   | 302/435 [00:06<00:02, 44.72it/s][A
 71%|   | 307/435 [00:06<00:02, 44.77it/s][A
 72%|  | 312/435 [00:06<00:02, 44.77it/s][A
 73%|  | 317/435 [00:07<00:02, 44.74it/s][A
 74%|  | 322/435 [00:07<00:02, 44.88it/s][A
 75%|  | 327/435 [00:07<00:02, 44.71it/s][A
 76%|  | 332/435 [00:07<00:02, 44.76it/s][A
 77%|  | 337/435 [00:07<00:02, 44.73it/s][A
 79%|  | 342/435 [00:07<00:02, 44.77it/s][A
 80%|  | 347/435 [00:07<00:01, 44.21it/s][A
 81%|  | 352/435 [00:07<00:01, 44.45it/s][A
 82%| | 357/435 [00:07<00:01, 44.60it/s][A
 83%| | 362/435 [00:08<00:01, 43.13it/s][A
 84%| | 367/435 [00:08<00:01, 43.71it/s][A
 86%| | 372/435 [00:08<00:01, 43.94it/s][A
 87%| | 377/435 [00:08<00:01, 44.20it/s][A
 88%| | 382/435 [00:08<00:01, 44.31it/s][A
 89%| | 387/435 [00:08<00:01, 44.44it/s][A
 90%| | 392/435 [00:08<00:00, 44.31it/s][A
 91%|| 397/435 [00:08<00:00, 44.56it/s][A
 92%|| 402/435 [00:08<00:00, 44.48it/s][A
 94%|| 407/435 [00:09<00:00, 44.60it/s][A
 95%|| 412/435 [00:09<00:00, 44.77it/s][A
 96%|| 417/435 [00:09<00:00, 44.75it/s][A
 97%|| 422/435 [00:09<00:00, 44.83it/s][A
 98%|| 427/435 [00:09<00:00, 44.86it/s][A
 99%|| 432/435 [00:09<00:00, 44.78it/s][A                                                 
                                                 [A 40%|      | 328/820 [01:58<02:19,  3.52it/s]
100%|| 435/435 [00:09<00:00, 44.78it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:48:20,628 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 06:48:20,731 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:48:23,584 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:48:23,745 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:48:23,815 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-328/special_tokens_map.json
 40%|      | 329/820 [02:02<35:53,  4.39s/it] 40%|      | 330/820 [02:02<25:45,  3.15s/it] 40%|      | 331/820 [02:02<18:41,  2.29s/it] 40%|      | 332/820 [02:03<13:44,  1.69s/it] 41%|      | 333/820 [02:03<10:17,  1.27s/it] 41%|      | 334/820 [02:03<07:52,  1.03it/s] 41%|      | 335/820 [02:03<06:11,  1.31it/s] 41%|      | 336/820 [02:04<05:00,  1.61it/s] 41%|      | 337/820 [02:04<04:10,  1.93it/s] 41%|      | 338/820 [02:04<03:36,  2.23it/s] 41%|     | 339/820 [02:05<03:11,  2.51it/s] 41%|     | 340/820 [02:05<02:53,  2.76it/s] 42%|     | 341/820 [02:05<02:41,  2.97it/s] 42%|     | 342/820 [02:05<02:32,  3.13it/s] 42%|     | 343/820 [02:06<02:26,  3.26it/s] 42%|     | 344/820 [02:06<02:21,  3.35it/s] 42%|     | 345/820 [02:06<02:18,  3.43it/s] 42%|     | 346/820 [02:06<02:16,  3.47it/s] 42%|     | 347/820 [02:07<02:14,  3.50it/s] 42%|     | 348/820 [02:07<02:13,  3.53it/s] 43%|     | 349/820 [02:07<02:15,  3.48it/s] 43%|     | 350/820 [02:08<02:14,  3.51it/s] 43%|     | 351/820 [02:08<02:12,  3.53it/s] 43%|     | 352/820 [02:08<02:11,  3.55it/s] 43%|     | 353/820 [02:08<02:11,  3.56it/s] 43%|     | 354/820 [02:09<02:10,  3.57it/s] 43%|     | 355/820 [02:09<02:10,  3.57it/s] 43%|     | 356/820 [02:09<02:09,  3.57it/s] 44%|     | 357/820 [02:10<02:09,  3.58it/s] 44%|     | 358/820 [02:10<02:08,  3.59it/s] 44%|     | 359/820 [02:10<02:08,  3.58it/s] 44%|     | 360/820 [02:10<02:10,  3.54it/s] 44%|     | 361/820 [02:11<02:09,  3.56it/s] 44%|     | 362/820 [02:11<02:08,  3.57it/s] 44%|     | 363/820 [02:11<02:07,  3.58it/s] 44%|     | 364/820 [02:12<02:07,  3.57it/s] 45%|     | 365/820 [02:12<02:07,  3.57it/s] 45%|     | 366/820 [02:12<02:06,  3.58it/s] 45%|     | 367/820 [02:12<02:06,  3.58it/s] 45%|     | 368/820 [02:13<02:06,  3.58it/s] 45%|     | 369/820 [02:13<02:06,  3.58it/s] 45%|     | 370/820 [02:13<02:05,  3.58it/s] 45%|     | 371/820 [02:13<02:06,  3.56it/s] 45%|     | 372/820 [02:14<02:05,  3.57it/s] 45%|     | 373/820 [02:14<02:05,  3.57it/s] 46%|     | 374/820 [02:14<02:04,  3.58it/s] 46%|     | 375/820 [02:15<02:04,  3.58it/s] 46%|     | 376/820 [02:15<02:04,  3.58it/s] 46%|     | 377/820 [02:15<02:03,  3.58it/s] 46%|     | 378/820 [02:15<02:03,  3.58it/s] 46%|     | 379/820 [02:16<02:03,  3.58it/s] 46%|     | 380/820 [02:16<02:02,  3.59it/s] 46%|     | 381/820 [02:16<02:02,  3.59it/s] 47%|     | 382/820 [02:17<02:50,  2.56it/s] 47%|     | 383/820 [02:17<02:52,  2.54it/s] 47%|     | 384/820 [02:18<02:37,  2.77it/s] 47%|     | 385/820 [02:18<02:26,  2.98it/s] 47%|     | 386/820 [02:18<02:18,  3.13it/s] 47%|     | 387/820 [02:18<02:12,  3.26it/s] 47%|     | 388/820 [02:19<02:08,  3.35it/s] 47%|     | 389/820 [02:19<02:05,  3.42it/s] 48%|     | 390/820 [02:19<02:03,  3.47it/s] 48%|     | 391/820 [02:20<02:06,  3.40it/s] 48%|     | 392/820 [02:20<02:03,  3.46it/s] 48%|     | 393/820 [02:20<02:01,  3.50it/s] 48%|     | 394/820 [02:20<02:00,  3.53it/s] 48%|     | 395/820 [02:21<01:59,  3.54it/s] 48%|     | 396/820 [02:21<01:59,  3.56it/s] 48%|     | 397/820 [02:21<01:58,  3.57it/s] 49%|     | 398/820 [02:22<01:57,  3.58it/s] 49%|     | 399/820 [02:22<01:57,  3.58it/s] 49%|     | 400/820 [02:22<01:57,  3.58it/s] 49%|     | 401/820 [02:22<01:56,  3.59it/s] 49%|     | 402/820 [02:23<01:56,  3.59it/s] 49%|     | 403/820 [02:23<01:56,  3.59it/s] 49%|     | 404/820 [02:23<01:55,  3.59it/s] 49%|     | 405/820 [02:23<01:55,  3.59it/s] 50%|     | 406/820 [02:24<01:55,  3.59it/s] 50%|     | 407/820 [02:24<01:56,  3.54it/s] 50%|     | 408/820 [02:24<01:55,  3.55it/s] 50%|     | 409/820 [02:25<01:55,  3.57it/s] 50%|     | 410/820 [02:25<01:54,  3.58it/s] 50%|     | 411/820 [02:25<01:54,  3.58it/s] 50%|     | 412/820 [02:25<01:53,  3.58it/s] 50%|     | 413/820 [02:26<01:53,  3.59it/s] 50%|     | 414/820 [02:26<01:53,  3.59it/s] 51%|     | 415/820 [02:26<01:52,  3.59it/s] 51%|     | 416/820 [02:27<01:52,  3.58it/s] 51%|     | 417/820 [02:27<01:52,  3.58it/s] 51%|     | 418/820 [02:27<01:56,  3.44it/s] 51%|     | 419/820 [02:27<01:55,  3.48it/s] 51%|     | 420/820 [02:28<01:54,  3.51it/s] 51%|    | 421/820 [02:28<01:52,  3.53it/s] 51%|    | 422/820 [02:28<01:51,  3.55it/s] 52%|    | 423/820 [02:29<01:51,  3.57it/s] 52%|    | 424/820 [02:29<01:50,  3.57it/s] 52%|    | 425/820 [02:29<01:50,  3.58it/s] 52%|    | 426/820 [02:29<01:49,  3.58it/s] 52%|    | 427/820 [02:30<01:49,  3.59it/s] 52%|    | 428/820 [02:30<01:49,  3.59it/s] 52%|    | 429/820 [02:30<01:51,  3.51it/s] 52%|    | 430/820 [02:31<01:50,  3.54it/s] 53%|    | 431/820 [02:31<01:49,  3.55it/s] 53%|    | 432/820 [02:31<01:48,  3.57it/s] 53%|    | 433/820 [02:31<01:48,  3.58it/s] 53%|    | 434/820 [02:32<01:47,  3.58it/s] 53%|    | 435/820 [02:32<01:47,  3.59it/s] 53%|    | 436/820 [02:32<01:47,  3.59it/s] 53%|    | 437/820 [02:32<01:46,  3.59it/s] 53%|    | 438/820 [02:33<01:46,  3.59it/s] 54%|    | 439/820 [02:33<01:45,  3.59it/s] 54%|    | 440/820 [02:33<01:51,  3.42it/s] 54%|    | 441/820 [02:34<01:49,  3.47it/s] 54%|    | 442/820 [02:34<01:47,  3.51it/s] 54%|    | 443/820 [02:34<01:46,  3.53it/s] 54%|    | 444/820 [02:34<01:46,  3.54it/s] 54%|    | 445/820 [02:35<01:45,  3.56it/s] 54%|    | 446/820 [02:35<01:44,  3.57it/s] 55%|    | 447/820 [02:35<01:44,  3.57it/s] 55%|    | 448/820 [02:36<01:44,  3.58it/s] 55%|    | 449/820 [02:36<01:43,  3.58it/s] 55%|    | 450/820 [02:36<01:43,  3.58it/s] 55%|    | 451/820 [02:36<01:45,  3.50it/s] 55%|    | 452/820 [02:37<01:44,  3.52it/s] 55%|    | 453/820 [02:37<01:43,  3.54it/s] 55%|    | 454/820 [02:37<01:42,  3.56it/s] 55%|    | 455/820 [02:38<01:42,  3.57it/s] 56%|    | 456/820 [02:38<01:41,  3.58it/s] 56%|    | 457/820 [02:38<01:41,  3.59it/s] 56%|    | 458/820 [02:38<01:40,  3.59it/s] 56%|    | 459/820 [02:39<01:40,  3.59it/s] 56%|    | 460/820 [02:39<01:40,  3.59it/s] 56%|    | 461/820 [02:39<01:40,  3.58it/s] 56%|    | 462/820 [02:40<01:42,  3.49it/s] 56%|    | 463/820 [02:40<01:41,  3.52it/s] 57%|    | 464/820 [02:40<01:40,  3.54it/s] 57%|    | 465/820 [02:40<01:39,  3.55it/s] 57%|    | 466/820 [02:41<01:39,  3.56it/s] 57%|    | 467/820 [02:41<01:38,  3.57it/s] 57%|    | 468/820 [02:41<01:38,  3.57it/s] 57%|    | 469/820 [02:41<01:38,  3.58it/s] 57%|    | 470/820 [02:42<01:37,  3.58it/s] 57%|    | 471/820 [02:42<01:37,  3.58it/s] 58%|    | 472/820 [02:42<01:37,  3.58it/s] 58%|    | 473/820 [02:43<01:39,  3.49it/s] 58%|    | 474/820 [02:43<01:38,  3.52it/s] 58%|    | 475/820 [02:43<01:37,  3.54it/s] 58%|    | 476/820 [02:43<01:36,  3.55it/s] 58%|    | 477/820 [02:44<01:36,  3.55it/s] 58%|    | 478/820 [02:44<01:35,  3.56it/s] 58%|    | 479/820 [02:44<01:35,  3.57it/s] 59%|    | 480/820 [02:45<01:34,  3.58it/s] 59%|    | 481/820 [02:45<01:34,  3.59it/s] 59%|    | 482/820 [02:45<01:34,  3.59it/s] 59%|    | 483/820 [02:45<01:33,  3.59it/s] 59%|    | 484/820 [02:46<01:35,  3.52it/s] 59%|    | 485/820 [02:46<01:34,  3.54it/s] 59%|    | 486/820 [02:46<01:33,  3.56it/s] 59%|    | 487/820 [02:47<01:33,  3.56it/s] 60%|    | 488/820 [02:47<01:33,  3.56it/s] 60%|    | 489/820 [02:47<01:32,  3.57it/s] 60%|    | 490/820 [02:47<01:34,  3.48it/s] 60%|    | 491/820 [02:48<01:34,  3.50it/s] 60%|    | 492/820 [02:48<01:33,  3.53it/s][INFO|trainer.py:2140] 2023-08-28 06:49:10,906 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:49:10,906 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 06:49:10,906 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.8011, 'eval_samples_per_second': 354.755, 'eval_steps_per_second': 44.383, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|          | 5/435 [00:00<00:09, 45.43it/s][A
  2%|         | 10/435 [00:00<00:08, 47.93it/s][A
  3%|         | 15/435 [00:00<00:09, 46.60it/s][A
  5%|         | 20/435 [00:00<00:09, 45.86it/s][A
  6%|         | 25/435 [00:00<00:09, 43.05it/s][A
  7%|         | 30/435 [00:00<00:09, 43.56it/s][A
  8%|         | 35/435 [00:00<00:09, 43.81it/s][A
  9%|         | 40/435 [00:00<00:08, 43.90it/s][A
 10%|         | 45/435 [00:01<00:08, 44.18it/s][A
 11%|        | 50/435 [00:01<00:08, 44.34it/s][A
 13%|        | 55/435 [00:01<00:08, 44.59it/s][A
 14%|        | 60/435 [00:01<00:08, 44.66it/s][A
 15%|        | 65/435 [00:01<00:08, 44.68it/s][A
 16%|        | 70/435 [00:01<00:08, 44.69it/s][A
 17%|        | 75/435 [00:01<00:08, 44.71it/s][A
 18%|        | 80/435 [00:01<00:07, 44.74it/s][A
 20%|        | 85/435 [00:01<00:07, 44.70it/s][A
 21%|        | 90/435 [00:02<00:07, 44.78it/s][A
 22%|       | 95/435 [00:02<00:07, 44.78it/s][A
 23%|       | 100/435 [00:02<00:07, 44.65it/s][A
 24%|       | 105/435 [00:02<00:07, 44.78it/s][A
 25%|       | 110/435 [00:02<00:07, 44.71it/s][A
 26%|       | 115/435 [00:02<00:07, 44.74it/s][A
 28%|       | 120/435 [00:02<00:07, 44.76it/s][A
 29%|       | 125/435 [00:02<00:06, 44.70it/s][A
 30%|       | 130/435 [00:02<00:06, 44.76it/s][A
 31%|       | 135/435 [00:03<00:06, 44.70it/s][A
 32%|      | 140/435 [00:03<00:06, 44.69it/s][A
 33%|      | 145/435 [00:03<00:07, 41.30it/s][A
 34%|      | 150/435 [00:03<00:06, 42.42it/s][A
 36%|      | 155/435 [00:03<00:06, 43.23it/s][A
 37%|      | 160/435 [00:03<00:06, 43.77it/s][A
 38%|      | 165/435 [00:03<00:06, 44.07it/s][A
 39%|      | 170/435 [00:03<00:05, 44.31it/s][A
 40%|      | 175/435 [00:03<00:05, 44.44it/s][A
 41%|     | 180/435 [00:04<00:05, 44.56it/s][A
 43%|     | 185/435 [00:04<00:05, 44.33it/s][A
 44%|     | 190/435 [00:04<00:05, 44.34it/s][A
 45%|     | 195/435 [00:04<00:05, 44.60it/s][A
 46%|     | 200/435 [00:04<00:05, 44.74it/s][A
 47%|     | 205/435 [00:04<00:05, 44.86it/s][A
 48%|     | 210/435 [00:04<00:05, 44.87it/s][A
 49%|     | 215/435 [00:04<00:04, 44.91it/s][A
 51%|     | 220/435 [00:04<00:04, 44.91it/s][A
 52%|    | 225/435 [00:05<00:04, 44.67it/s][A
 53%|    | 230/435 [00:05<00:04, 44.55it/s][A
 54%|    | 235/435 [00:05<00:04, 44.60it/s][A
 55%|    | 240/435 [00:05<00:04, 44.64it/s][A
 56%|    | 245/435 [00:05<00:04, 44.83it/s][A
 57%|    | 250/435 [00:05<00:04, 44.80it/s][A
 59%|    | 255/435 [00:05<00:04, 44.81it/s][A
 60%|    | 260/435 [00:05<00:03, 44.82it/s][A
 61%|    | 265/435 [00:05<00:03, 44.87it/s][A
 62%|   | 270/435 [00:06<00:03, 44.76it/s][A
 63%|   | 275/435 [00:06<00:03, 44.66it/s][A
 64%|   | 280/435 [00:06<00:03, 43.32it/s][A
 66%|   | 285/435 [00:06<00:03, 43.83it/s][A
 67%|   | 290/435 [00:06<00:03, 44.25it/s][A
 68%|   | 295/435 [00:06<00:03, 44.28it/s][A
 69%|   | 300/435 [00:06<00:03, 44.52it/s][A
 70%|   | 305/435 [00:06<00:02, 44.66it/s][A
 71%|  | 310/435 [00:06<00:02, 44.69it/s][A
 72%|  | 315/435 [00:07<00:02, 44.75it/s][A
 74%|  | 320/435 [00:07<00:02, 44.45it/s][A
 75%|  | 325/435 [00:07<00:02, 44.53it/s][A
 76%|  | 330/435 [00:07<00:02, 44.59it/s][A
 77%|  | 335/435 [00:07<00:02, 44.73it/s][A
 78%|  | 340/435 [00:07<00:02, 44.78it/s][A
 79%|  | 345/435 [00:07<00:02, 44.79it/s][A
 80%|  | 350/435 [00:07<00:01, 44.87it/s][A
 82%| | 355/435 [00:07<00:01, 44.79it/s][A
 83%| | 360/435 [00:08<00:01, 44.72it/s][A
 84%| | 365/435 [00:08<00:01, 44.60it/s][A
 85%| | 370/435 [00:08<00:01, 44.64it/s][A
 86%| | 375/435 [00:08<00:01, 44.71it/s][A
 87%| | 380/435 [00:08<00:01, 44.75it/s][A
 89%| | 385/435 [00:08<00:01, 44.81it/s][A
 90%| | 390/435 [00:08<00:01, 44.78it/s][A
 91%| | 395/435 [00:08<00:00, 44.88it/s][A
 92%|| 400/435 [00:08<00:00, 44.85it/s][A
 93%|| 405/435 [00:09<00:00, 44.82it/s][A
 94%|| 410/435 [00:09<00:00, 44.76it/s][A
 95%|| 415/435 [00:09<00:00, 42.95it/s][A
 97%|| 420/435 [00:09<00:00, 43.57it/s][A
 98%|| 425/435 [00:09<00:00, 43.85it/s][A
 99%|| 430/435 [00:09<00:00, 44.22it/s][A
100%|| 435/435 [00:09<00:00, 44.35it/s][A                                                 
                                                 [A 60%|    | 492/820 [02:58<01:33,  3.53it/s]
100%|| 435/435 [00:09<00:00, 44.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:49:20,890 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 06:49:20,997 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:49:23,930 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:49:24,004 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:49:24,044 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-492/special_tokens_map.json
 60%|    | 493/820 [03:02<24:24,  4.48s/it] 60%|    | 494/820 [03:02<17:29,  3.22s/it] 60%|    | 495/820 [03:03<12:40,  2.34s/it] 60%|    | 496/820 [03:03<09:17,  1.72s/it] 61%|    | 497/820 [03:03<06:56,  1.29s/it] 61%|    | 498/820 [03:04<05:17,  1.01it/s] 61%|    | 499/820 [03:04<04:09,  1.29it/s] 61%|    | 500/820 [03:04<03:20,  1.59it/s]                                                  61%|    | 500/820 [03:04<03:20,  1.59it/s] 61%|    | 501/820 [03:04<02:49,  1.88it/s] 61%|    | 502/820 [03:05<02:25,  2.19it/s] 61%|   | 503/820 [03:05<02:08,  2.47it/s] 61%|   | 504/820 [03:05<01:56,  2.72it/s] 62%|   | 505/820 [03:06<01:47,  2.92it/s] 62%|   | 506/820 [03:06<01:41,  3.09it/s] 62%|   | 507/820 [03:06<01:37,  3.21it/s] 62%|   | 508/820 [03:06<01:34,  3.30it/s] 62%|   | 509/820 [03:07<01:32,  3.37it/s] 62%|   | 510/820 [03:07<01:30,  3.42it/s] 62%|   | 511/820 [03:07<01:29,  3.45it/s] 62%|   | 512/820 [03:08<01:29,  3.43it/s] 63%|   | 513/820 [03:08<01:28,  3.46it/s] 63%|   | 514/820 [03:08<01:27,  3.49it/s] 63%|   | 515/820 [03:08<01:26,  3.51it/s] 63%|   | 516/820 [03:09<01:26,  3.52it/s] 63%|   | 517/820 [03:09<01:25,  3.53it/s] 63%|   | 518/820 [03:09<01:25,  3.53it/s] 63%|   | 519/820 [03:10<01:25,  3.53it/s] 63%|   | 520/820 [03:10<01:24,  3.54it/s] 64%|   | 521/820 [03:10<01:24,  3.54it/s] 64%|   | 522/820 [03:10<01:24,  3.54it/s] 64%|   | 523/820 [03:11<01:25,  3.46it/s] 64%|   | 524/820 [03:11<01:24,  3.48it/s] 64%|   | 525/820 [03:11<01:24,  3.50it/s] 64%|   | 526/820 [03:12<01:23,  3.52it/s] 64%|   | 527/820 [03:12<01:23,  3.52it/s] 64%|   | 528/820 [03:12<01:22,  3.53it/s] 65%|   | 529/820 [03:12<01:22,  3.53it/s] 65%|   | 530/820 [03:13<01:22,  3.53it/s] 65%|   | 531/820 [03:13<01:21,  3.54it/s] 65%|   | 532/820 [03:13<01:21,  3.53it/s] 65%|   | 533/820 [03:14<01:21,  3.54it/s] 65%|   | 534/820 [03:14<01:21,  3.51it/s] 65%|   | 535/820 [03:14<01:20,  3.52it/s] 65%|   | 536/820 [03:14<01:20,  3.53it/s] 65%|   | 537/820 [03:15<01:20,  3.53it/s] 66%|   | 538/820 [03:15<01:19,  3.54it/s] 66%|   | 539/820 [03:15<01:19,  3.54it/s] 66%|   | 540/820 [03:16<01:19,  3.54it/s] 66%|   | 541/820 [03:16<01:18,  3.54it/s] 66%|   | 542/820 [03:16<01:18,  3.54it/s] 66%|   | 543/820 [03:16<01:18,  3.54it/s] 66%|   | 544/820 [03:17<01:17,  3.54it/s] 66%|   | 545/820 [03:17<01:30,  3.02it/s] 67%|   | 546/820 [03:18<01:39,  2.76it/s] 67%|   | 547/820 [03:18<01:32,  2.95it/s] 67%|   | 548/820 [03:18<01:27,  3.10it/s] 67%|   | 549/820 [03:18<01:24,  3.22it/s] 67%|   | 550/820 [03:19<01:21,  3.31it/s] 67%|   | 551/820 [03:19<01:19,  3.38it/s] 67%|   | 552/820 [03:19<01:18,  3.43it/s] 67%|   | 553/820 [03:20<01:17,  3.46it/s] 68%|   | 554/820 [03:20<01:16,  3.49it/s] 68%|   | 555/820 [03:20<01:19,  3.32it/s] 68%|   | 556/820 [03:20<01:18,  3.38it/s] 68%|   | 557/820 [03:21<01:16,  3.43it/s] 68%|   | 558/820 [03:21<01:15,  3.47it/s] 68%|   | 559/820 [03:21<01:14,  3.49it/s] 68%|   | 560/820 [03:22<01:14,  3.51it/s] 68%|   | 561/820 [03:22<01:13,  3.52it/s] 69%|   | 562/820 [03:22<01:13,  3.52it/s] 69%|   | 563/820 [03:22<01:12,  3.53it/s] 69%|   | 564/820 [03:23<01:12,  3.53it/s] 69%|   | 565/820 [03:23<01:11,  3.55it/s] 69%|   | 566/820 [03:23<01:11,  3.56it/s] 69%|   | 567/820 [03:24<01:10,  3.57it/s] 69%|   | 568/820 [03:24<01:10,  3.58it/s] 69%|   | 569/820 [03:24<01:10,  3.58it/s] 70%|   | 570/820 [03:24<01:14,  3.35it/s] 70%|   | 571/820 [03:25<01:13,  3.41it/s] 70%|   | 572/820 [03:25<01:11,  3.46it/s] 70%|   | 573/820 [03:25<01:13,  3.35it/s] 70%|   | 574/820 [03:26<01:11,  3.42it/s] 70%|   | 575/820 [03:26<01:10,  3.47it/s] 70%|   | 576/820 [03:26<01:09,  3.51it/s] 70%|   | 577/820 [03:26<01:08,  3.54it/s] 70%|   | 578/820 [03:27<01:08,  3.56it/s] 71%|   | 579/820 [03:27<01:07,  3.57it/s] 71%|   | 580/820 [03:27<01:07,  3.58it/s] 71%|   | 581/820 [03:28<01:06,  3.59it/s] 71%|   | 582/820 [03:28<01:06,  3.59it/s] 71%|   | 583/820 [03:28<01:05,  3.59it/s] 71%|   | 584/820 [03:28<01:08,  3.47it/s] 71%|  | 585/820 [03:29<01:06,  3.51it/s] 71%|  | 586/820 [03:29<01:06,  3.54it/s] 72%|  | 587/820 [03:29<01:05,  3.55it/s] 72%|  | 588/820 [03:30<01:05,  3.57it/s] 72%|  | 589/820 [03:30<01:04,  3.57it/s] 72%|  | 590/820 [03:30<01:04,  3.58it/s] 72%|  | 591/820 [03:30<01:03,  3.58it/s] 72%|  | 592/820 [03:31<01:03,  3.59it/s] 72%|  | 593/820 [03:31<01:03,  3.59it/s] 72%|  | 594/820 [03:31<01:02,  3.59it/s] 73%|  | 595/820 [03:31<01:03,  3.54it/s] 73%|  | 596/820 [03:32<01:03,  3.55it/s] 73%|  | 597/820 [03:32<01:02,  3.56it/s] 73%|  | 598/820 [03:32<01:02,  3.57it/s] 73%|  | 599/820 [03:33<01:01,  3.58it/s] 73%|  | 600/820 [03:33<01:01,  3.58it/s] 73%|  | 601/820 [03:33<01:01,  3.59it/s] 73%|  | 602/820 [03:33<01:00,  3.59it/s] 74%|  | 603/820 [03:34<01:00,  3.59it/s] 74%|  | 604/820 [03:34<01:00,  3.59it/s] 74%|  | 605/820 [03:34<00:59,  3.60it/s] 74%|  | 606/820 [03:35<01:01,  3.49it/s] 74%|  | 607/820 [03:35<01:00,  3.52it/s] 74%|  | 608/820 [03:35<00:59,  3.54it/s] 74%|  | 609/820 [03:35<00:59,  3.56it/s] 74%|  | 610/820 [03:36<00:58,  3.57it/s] 75%|  | 611/820 [03:36<00:58,  3.58it/s] 75%|  | 612/820 [03:36<00:57,  3.59it/s] 75%|  | 613/820 [03:37<00:57,  3.59it/s] 75%|  | 614/820 [03:37<00:57,  3.59it/s] 75%|  | 615/820 [03:37<00:57,  3.59it/s] 75%|  | 616/820 [03:37<00:56,  3.59it/s] 75%|  | 617/820 [03:38<00:59,  3.40it/s] 75%|  | 618/820 [03:38<00:58,  3.46it/s] 75%|  | 619/820 [03:38<00:57,  3.50it/s] 76%|  | 620/820 [03:39<00:56,  3.52it/s] 76%|  | 621/820 [03:39<00:56,  3.55it/s] 76%|  | 622/820 [03:39<00:55,  3.56it/s] 76%|  | 623/820 [03:39<00:55,  3.57it/s] 76%|  | 624/820 [03:40<00:54,  3.58it/s] 76%|  | 625/820 [03:40<00:54,  3.58it/s] 76%|  | 626/820 [03:40<00:54,  3.59it/s] 76%|  | 627/820 [03:40<00:53,  3.59it/s] 77%|  | 628/820 [03:41<00:54,  3.52it/s] 77%|  | 629/820 [03:41<00:53,  3.54it/s] 77%|  | 630/820 [03:41<00:53,  3.56it/s] 77%|  | 631/820 [03:42<00:52,  3.57it/s] 77%|  | 632/820 [03:42<00:52,  3.58it/s] 77%|  | 633/820 [03:42<00:52,  3.58it/s] 77%|  | 634/820 [03:42<00:51,  3.58it/s] 77%|  | 635/820 [03:43<00:51,  3.59it/s] 78%|  | 636/820 [03:43<00:51,  3.59it/s] 78%|  | 637/820 [03:43<00:50,  3.59it/s] 78%|  | 638/820 [03:44<00:50,  3.60it/s] 78%|  | 639/820 [03:44<00:51,  3.54it/s] 78%|  | 640/820 [03:44<00:50,  3.56it/s] 78%|  | 641/820 [03:44<00:50,  3.57it/s] 78%|  | 642/820 [03:45<00:49,  3.58it/s] 78%|  | 643/820 [03:45<00:49,  3.58it/s] 79%|  | 644/820 [03:45<00:49,  3.59it/s] 79%|  | 645/820 [03:45<00:48,  3.59it/s] 79%|  | 646/820 [03:46<00:48,  3.60it/s] 79%|  | 647/820 [03:46<00:48,  3.59it/s] 79%|  | 648/820 [03:46<00:47,  3.60it/s] 79%|  | 649/820 [03:47<00:47,  3.60it/s] 79%|  | 650/820 [03:47<00:48,  3.52it/s] 79%|  | 651/820 [03:47<00:47,  3.54it/s] 80%|  | 652/820 [03:47<00:47,  3.56it/s] 80%|  | 653/820 [03:48<00:46,  3.57it/s] 80%|  | 654/820 [03:48<00:46,  3.58it/s] 80%|  | 655/820 [03:48<00:46,  3.59it/s] 80%|  | 656/820 [03:49<00:45,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 06:50:11,527 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:50:11,527 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 06:50:11,527 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.853, 'eval_samples_per_second': 352.888, 'eval_steps_per_second': 44.149, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.222560975609756e-05, 'epoch': 3.05}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.41it/s][A
  3%|         | 12/435 [00:00<00:08, 48.63it/s][A
  4%|         | 17/435 [00:00<00:08, 47.07it/s][A
  5%|         | 22/435 [00:00<00:08, 46.22it/s][A
  6%|         | 27/435 [00:00<00:08, 45.52it/s][A
  7%|         | 32/435 [00:00<00:08, 45.23it/s][A
  9%|         | 37/435 [00:00<00:08, 44.80it/s][A
 10%|         | 42/435 [00:00<00:08, 44.66it/s][A
 11%|         | 47/435 [00:01<00:08, 44.63it/s][A
 12%|        | 52/435 [00:01<00:09, 42.26it/s][A
 13%|        | 57/435 [00:01<00:08, 43.09it/s][A
 14%|        | 62/435 [00:01<00:08, 43.67it/s][A
 15%|        | 67/435 [00:01<00:08, 44.12it/s][A
 17%|        | 72/435 [00:01<00:08, 44.33it/s][A
 18%|        | 77/435 [00:01<00:08, 44.55it/s][A
 19%|        | 82/435 [00:01<00:07, 44.65it/s][A
 20%|        | 87/435 [00:01<00:07, 44.65it/s][A
 21%|        | 92/435 [00:02<00:07, 44.55it/s][A
 22%|       | 97/435 [00:02<00:07, 44.39it/s][A
 23%|       | 102/435 [00:02<00:07, 44.56it/s][A
 25%|       | 107/435 [00:02<00:07, 44.72it/s][A
 26%|       | 112/435 [00:02<00:07, 44.78it/s][A
 27%|       | 117/435 [00:02<00:07, 44.89it/s][A
 28%|       | 122/435 [00:02<00:06, 44.86it/s][A
 29%|       | 127/435 [00:02<00:06, 44.82it/s][A
 30%|       | 132/435 [00:02<00:06, 44.78it/s][A
 31%|      | 137/435 [00:03<00:06, 44.76it/s][A
 33%|      | 142/435 [00:03<00:06, 44.77it/s][A
 34%|      | 147/435 [00:03<00:06, 44.72it/s][A
 35%|      | 152/435 [00:03<00:06, 44.74it/s][A
 36%|      | 157/435 [00:03<00:06, 44.72it/s][A
 37%|      | 162/435 [00:03<00:06, 44.78it/s][A
 38%|      | 167/435 [00:03<00:05, 44.90it/s][A
 40%|      | 172/435 [00:03<00:05, 44.85it/s][A
 41%|      | 177/435 [00:03<00:05, 44.88it/s][A
 42%|     | 182/435 [00:04<00:05, 44.85it/s][A
 43%|     | 187/435 [00:04<00:05, 44.82it/s][A
 44%|     | 192/435 [00:04<00:05, 44.85it/s][A
 45%|     | 197/435 [00:04<00:05, 44.82it/s][A
 46%|     | 202/435 [00:04<00:05, 44.80it/s][A
 48%|     | 207/435 [00:04<00:05, 44.74it/s][A
 49%|     | 212/435 [00:04<00:04, 44.83it/s][A
 50%|     | 217/435 [00:04<00:04, 44.83it/s][A
 51%|     | 222/435 [00:04<00:04, 44.85it/s][A
 52%|    | 227/435 [00:05<00:04, 44.89it/s][A
 53%|    | 232/435 [00:05<00:04, 44.86it/s][A
 54%|    | 237/435 [00:05<00:04, 44.86it/s][A
 56%|    | 242/435 [00:05<00:04, 44.79it/s][A
 57%|    | 247/435 [00:05<00:04, 44.86it/s][A
 58%|    | 252/435 [00:05<00:04, 44.78it/s][A
 59%|    | 257/435 [00:05<00:03, 44.86it/s][A
 60%|    | 262/435 [00:05<00:03, 44.75it/s][A
 61%|   | 267/435 [00:05<00:03, 44.62it/s][A
 63%|   | 272/435 [00:06<00:03, 44.78it/s][A
 64%|   | 277/435 [00:06<00:03, 44.76it/s][A
 65%|   | 282/435 [00:06<00:03, 44.84it/s][A
 66%|   | 287/435 [00:06<00:03, 44.89it/s][A
 67%|   | 292/435 [00:06<00:03, 44.77it/s][A
 68%|   | 297/435 [00:06<00:03, 44.88it/s][A
 69%|   | 302/435 [00:06<00:02, 44.88it/s][A
 71%|   | 307/435 [00:06<00:02, 44.88it/s][A
 72%|  | 312/435 [00:06<00:02, 44.85it/s][A
 73%|  | 317/435 [00:07<00:02, 43.42it/s][A
 74%|  | 322/435 [00:07<00:02, 43.62it/s][A
 75%|  | 327/435 [00:07<00:02, 44.20it/s][A
 76%|  | 332/435 [00:07<00:02, 44.43it/s][A
 77%|  | 337/435 [00:07<00:02, 44.55it/s][A
 79%|  | 342/435 [00:07<00:02, 44.60it/s][A
 80%|  | 347/435 [00:07<00:01, 44.67it/s][A
 81%|  | 352/435 [00:07<00:01, 44.74it/s][A
 82%| | 357/435 [00:07<00:01, 44.57it/s][A
 83%| | 362/435 [00:08<00:01, 44.64it/s][A
 84%| | 367/435 [00:08<00:01, 44.70it/s][A
 86%| | 372/435 [00:08<00:01, 44.78it/s][A
 87%| | 377/435 [00:08<00:01, 44.85it/s][A
 88%| | 382/435 [00:08<00:01, 44.90it/s][A
 89%| | 387/435 [00:08<00:01, 44.83it/s][A
 90%| | 392/435 [00:08<00:00, 44.72it/s][A
 91%|| 397/435 [00:08<00:00, 44.75it/s][A
 92%|| 402/435 [00:08<00:00, 44.68it/s][A
 94%|| 407/435 [00:09<00:00, 44.73it/s][A
 95%|| 412/435 [00:09<00:00, 44.83it/s][A
 96%|| 417/435 [00:09<00:00, 44.78it/s][A
 97%|| 422/435 [00:09<00:00, 44.83it/s][A
 98%|| 427/435 [00:09<00:00, 44.74it/s][A
 99%|| 432/435 [00:09<00:00, 44.88it/s][A                                                 
                                                 [A 80%|  | 656/820 [03:58<00:45,  3.59it/s]
100%|| 435/435 [00:09<00:00, 44.88it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:50:21,523 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-656
[INFO|configuration_utils.py:351] 2023-08-28 06:50:21,686 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-656/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:50:24,472 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-656/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:50:24,546 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-656/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:50:24,604 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-656/special_tokens_map.json
 80%|  | 657/820 [04:03<12:22,  4.56s/it] 80%|  | 658/820 [04:03<08:52,  3.29s/it] 80%|  | 659/820 [04:04<06:24,  2.39s/it] 80%|  | 660/820 [04:04<04:40,  1.76s/it] 81%|  | 661/820 [04:04<03:28,  1.31s/it] 81%|  | 662/820 [04:05<02:38,  1.00s/it] 81%|  | 663/820 [04:05<02:03,  1.27it/s] 81%|  | 664/820 [04:05<01:39,  1.57it/s] 81%|  | 665/820 [04:05<01:22,  1.89it/s] 81%|  | 666/820 [04:06<01:10,  2.19it/s] 81%| | 667/820 [04:06<01:01,  2.48it/s] 81%| | 668/820 [04:06<00:55,  2.72it/s] 82%| | 669/820 [04:07<00:53,  2.84it/s] 82%| | 670/820 [04:07<00:49,  3.02it/s] 82%| | 671/820 [04:07<00:47,  3.16it/s] 82%| | 672/820 [04:07<00:45,  3.27it/s] 82%| | 673/820 [04:08<00:43,  3.35it/s] 82%| | 674/820 [04:08<00:42,  3.41it/s] 82%| | 675/820 [04:08<00:42,  3.45it/s] 82%| | 676/820 [04:09<00:41,  3.48it/s] 83%| | 677/820 [04:09<00:40,  3.49it/s] 83%| | 678/820 [04:09<00:40,  3.51it/s] 83%| | 679/820 [04:09<00:40,  3.52it/s] 83%| | 680/820 [04:10<00:40,  3.43it/s] 83%| | 681/820 [04:10<00:40,  3.45it/s] 83%| | 682/820 [04:10<00:39,  3.48it/s] 83%| | 683/820 [04:11<00:39,  3.50it/s] 83%| | 684/820 [04:11<00:38,  3.51it/s] 84%| | 685/820 [04:11<00:38,  3.52it/s] 84%| | 686/820 [04:11<00:37,  3.53it/s] 84%| | 687/820 [04:12<00:37,  3.54it/s] 84%| | 688/820 [04:12<00:37,  3.54it/s] 84%| | 689/820 [04:12<00:37,  3.54it/s] 84%| | 690/820 [04:13<00:36,  3.54it/s] 84%| | 691/820 [04:13<00:37,  3.44it/s] 84%| | 692/820 [04:13<00:36,  3.47it/s] 85%| | 693/820 [04:13<00:36,  3.49it/s] 85%| | 694/820 [04:14<00:35,  3.50it/s] 85%| | 695/820 [04:14<00:35,  3.51it/s] 85%| | 696/820 [04:14<00:35,  3.52it/s] 85%| | 697/820 [04:15<00:34,  3.53it/s] 85%| | 698/820 [04:15<00:34,  3.53it/s] 85%| | 699/820 [04:15<00:34,  3.54it/s] 85%| | 700/820 [04:15<00:33,  3.54it/s] 85%| | 701/820 [04:16<00:33,  3.54it/s] 86%| | 702/820 [04:16<00:33,  3.48it/s] 86%| | 703/820 [04:16<00:33,  3.50it/s] 86%| | 704/820 [04:17<00:33,  3.50it/s] 86%| | 705/820 [04:17<00:32,  3.51it/s] 86%| | 706/820 [04:17<00:37,  3.06it/s] 86%| | 707/820 [04:18<00:42,  2.67it/s] 86%| | 708/820 [04:18<00:38,  2.87it/s] 86%| | 709/820 [04:18<00:36,  3.04it/s] 87%| | 710/820 [04:19<00:34,  3.18it/s] 87%| | 711/820 [04:19<00:33,  3.28it/s] 87%| | 712/820 [04:19<00:33,  3.20it/s] 87%| | 713/820 [04:19<00:32,  3.30it/s] 87%| | 714/820 [04:20<00:31,  3.37it/s] 87%| | 715/820 [04:20<00:30,  3.42it/s] 87%| | 716/820 [04:20<00:30,  3.45it/s] 87%| | 717/820 [04:21<00:29,  3.48it/s] 88%| | 718/820 [04:21<00:29,  3.50it/s] 88%| | 719/820 [04:21<00:28,  3.51it/s] 88%| | 720/820 [04:21<00:28,  3.52it/s] 88%| | 721/820 [04:22<00:28,  3.53it/s] 88%| | 722/820 [04:22<00:27,  3.53it/s] 88%| | 723/820 [04:22<00:28,  3.35it/s] 88%| | 724/820 [04:23<00:28,  3.41it/s] 88%| | 725/820 [04:23<00:27,  3.44it/s] 89%| | 726/820 [04:23<00:27,  3.47it/s] 89%| | 727/820 [04:23<00:26,  3.49it/s] 89%| | 728/820 [04:24<00:26,  3.51it/s] 89%| | 729/820 [04:24<00:25,  3.52it/s] 89%| | 730/820 [04:24<00:25,  3.53it/s] 89%| | 731/820 [04:25<00:25,  3.53it/s] 89%| | 732/820 [04:25<00:24,  3.54it/s] 89%| | 733/820 [04:25<00:24,  3.54it/s] 90%| | 734/820 [04:25<00:24,  3.54it/s] 90%| | 735/820 [04:26<00:24,  3.54it/s] 90%| | 736/820 [04:26<00:23,  3.54it/s] 90%| | 737/820 [04:26<00:23,  3.54it/s] 90%| | 738/820 [04:27<00:23,  3.55it/s] 90%| | 739/820 [04:27<00:24,  3.37it/s] 90%| | 740/820 [04:27<00:24,  3.31it/s] 90%| | 741/820 [04:28<00:23,  3.36it/s] 90%| | 742/820 [04:28<00:22,  3.41it/s] 91%| | 743/820 [04:28<00:22,  3.45it/s] 91%| | 744/820 [04:28<00:21,  3.48it/s] 91%| | 745/820 [04:29<00:21,  3.50it/s] 91%| | 746/820 [04:29<00:21,  3.51it/s] 91%| | 747/820 [04:29<00:20,  3.52it/s] 91%| | 748/820 [04:29<00:20,  3.53it/s] 91%|| 749/820 [04:30<00:20,  3.53it/s] 91%|| 750/820 [04:30<00:20,  3.40it/s] 92%|| 751/820 [04:30<00:20,  3.44it/s] 92%|| 752/820 [04:31<00:19,  3.47it/s] 92%|| 753/820 [04:31<00:19,  3.49it/s] 92%|| 754/820 [04:31<00:18,  3.51it/s] 92%|| 755/820 [04:32<00:18,  3.52it/s] 92%|| 756/820 [04:32<00:18,  3.53it/s] 92%|| 757/820 [04:32<00:17,  3.53it/s] 92%|| 758/820 [04:32<00:17,  3.53it/s] 93%|| 759/820 [04:33<00:17,  3.53it/s] 93%|| 760/820 [04:33<00:16,  3.54it/s] 93%|| 761/820 [04:33<00:17,  3.41it/s] 93%|| 762/820 [04:34<00:16,  3.45it/s] 93%|| 763/820 [04:34<00:16,  3.48it/s] 93%|| 764/820 [04:34<00:16,  3.50it/s] 93%|| 765/820 [04:34<00:15,  3.51it/s] 93%|| 766/820 [04:35<00:15,  3.52it/s] 94%|| 767/820 [04:35<00:15,  3.52it/s] 94%|| 768/820 [04:35<00:14,  3.53it/s] 94%|| 769/820 [04:35<00:14,  3.53it/s] 94%|| 770/820 [04:36<00:14,  3.53it/s] 94%|| 771/820 [04:36<00:13,  3.54it/s] 94%|| 772/820 [04:36<00:14,  3.32it/s] 94%|| 773/820 [04:37<00:13,  3.38it/s] 94%|| 774/820 [04:37<00:13,  3.43it/s] 95%|| 775/820 [04:37<00:13,  3.46it/s] 95%|| 776/820 [04:38<00:12,  3.48it/s] 95%|| 777/820 [04:38<00:12,  3.50it/s] 95%|| 778/820 [04:38<00:11,  3.52it/s] 95%|| 779/820 [04:38<00:11,  3.53it/s] 95%|| 780/820 [04:39<00:11,  3.53it/s] 95%|| 781/820 [04:39<00:11,  3.54it/s] 95%|| 782/820 [04:39<00:10,  3.53it/s] 95%|| 783/820 [04:40<00:10,  3.46it/s] 96%|| 784/820 [04:40<00:10,  3.48it/s] 96%|| 785/820 [04:40<00:09,  3.50it/s] 96%|| 786/820 [04:40<00:09,  3.52it/s] 96%|| 787/820 [04:41<00:09,  3.52it/s] 96%|| 788/820 [04:41<00:09,  3.53it/s] 96%|| 789/820 [04:41<00:08,  3.54it/s] 96%|| 790/820 [04:42<00:08,  3.54it/s] 96%|| 791/820 [04:42<00:08,  3.54it/s] 97%|| 792/820 [04:42<00:07,  3.54it/s] 97%|| 793/820 [04:42<00:07,  3.54it/s] 97%|| 794/820 [04:43<00:07,  3.42it/s] 97%|| 795/820 [04:43<00:07,  3.46it/s] 97%|| 796/820 [04:43<00:06,  3.48it/s] 97%|| 797/820 [04:44<00:06,  3.50it/s] 97%|| 798/820 [04:44<00:06,  3.51it/s] 97%|| 799/820 [04:44<00:05,  3.52it/s] 98%|| 800/820 [04:44<00:05,  3.53it/s] 98%|| 801/820 [04:45<00:05,  3.53it/s] 98%|| 802/820 [04:45<00:05,  3.54it/s] 98%|| 803/820 [04:45<00:04,  3.54it/s] 98%|| 804/820 [04:45<00:04,  3.54it/s] 98%|| 805/820 [04:46<00:04,  3.44it/s] 98%|| 806/820 [04:46<00:04,  3.47it/s] 98%|| 807/820 [04:46<00:03,  3.49it/s] 99%|| 808/820 [04:47<00:03,  3.50it/s] 99%|| 809/820 [04:47<00:03,  3.51it/s] 99%|| 810/820 [04:47<00:02,  3.52it/s] 99%|| 811/820 [04:48<00:02,  3.48it/s] 99%|| 812/820 [04:48<00:02,  3.49it/s] 99%|| 813/820 [04:48<00:01,  3.50it/s] 99%|| 814/820 [04:48<00:01,  3.46it/s] 99%|| 815/820 [04:49<00:01,  3.48it/s]100%|| 816/820 [04:49<00:01,  3.43it/s]100%|| 817/820 [04:49<00:00,  3.45it/s]100%|| 818/820 [04:50<00:00,  3.48it/s]100%|| 819/820 [04:50<00:00,  3.50it/s]100%|| 820/820 [04:50<00:00,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 06:51:13,003 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:51:13,003 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 06:51:13,003 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7927, 'eval_samples_per_second': 355.062, 'eval_steps_per_second': 44.421, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.79it/s][A
  3%|         | 12/435 [00:00<00:08, 48.88it/s][A
  4%|         | 17/435 [00:00<00:08, 47.45it/s][A
  5%|         | 22/435 [00:00<00:08, 46.41it/s][A
  6%|         | 27/435 [00:00<00:08, 45.91it/s][A
  7%|         | 32/435 [00:00<00:08, 45.51it/s][A
  9%|         | 37/435 [00:00<00:08, 45.08it/s][A
 10%|         | 42/435 [00:00<00:08, 44.83it/s][A
 11%|         | 47/435 [00:01<00:08, 44.76it/s][A
 12%|        | 52/435 [00:01<00:08, 44.75it/s][A
 13%|        | 57/435 [00:01<00:08, 44.82it/s][A
 14%|        | 62/435 [00:01<00:08, 44.89it/s][A
 15%|        | 67/435 [00:01<00:08, 45.04it/s][A
 17%|        | 72/435 [00:01<00:08, 44.98it/s][A
 18%|        | 77/435 [00:01<00:08, 43.52it/s][A
 19%|        | 82/435 [00:01<00:08, 43.86it/s][A
 20%|        | 87/435 [00:01<00:07, 44.05it/s][A
 21%|        | 92/435 [00:02<00:07, 44.23it/s][A
 22%|       | 97/435 [00:02<00:07, 44.37it/s][A
 23%|       | 102/435 [00:02<00:07, 44.50it/s][A
 25%|       | 107/435 [00:02<00:07, 44.68it/s][A
 26%|       | 112/435 [00:02<00:07, 44.86it/s][A
 27%|       | 117/435 [00:02<00:07, 44.73it/s][A
 28%|       | 122/435 [00:02<00:06, 44.79it/s][A
 29%|       | 127/435 [00:02<00:06, 44.61it/s][A
 30%|       | 132/435 [00:02<00:06, 44.65it/s][A
 31%|      | 137/435 [00:03<00:06, 44.62it/s][A
 33%|      | 142/435 [00:03<00:06, 44.72it/s][A
 34%|      | 147/435 [00:03<00:06, 44.80it/s][A
 35%|      | 152/435 [00:03<00:06, 44.86it/s][A
 36%|      | 157/435 [00:03<00:06, 44.92it/s][A
 37%|      | 162/435 [00:03<00:06, 44.88it/s][A
 38%|      | 167/435 [00:03<00:05, 44.98it/s][A
 40%|      | 172/435 [00:03<00:05, 44.80it/s][A
 41%|      | 177/435 [00:03<00:05, 44.77it/s][A
 42%|     | 182/435 [00:04<00:05, 44.76it/s][A
 43%|     | 187/435 [00:04<00:05, 44.62it/s][A
 44%|     | 192/435 [00:04<00:05, 44.82it/s][A
 45%|     | 197/435 [00:04<00:05, 44.89it/s][A
 46%|     | 202/435 [00:04<00:05, 44.85it/s][A
 48%|     | 207/435 [00:04<00:05, 44.91it/s][A
 49%|     | 212/435 [00:04<00:04, 44.83it/s][A
 50%|     | 217/435 [00:04<00:04, 44.87it/s][A
 51%|     | 222/435 [00:04<00:04, 44.75it/s][A
 52%|    | 227/435 [00:05<00:04, 44.64it/s][A
 53%|    | 232/435 [00:05<00:04, 44.72it/s][A
 54%|    | 237/435 [00:05<00:04, 44.73it/s][A
 56%|    | 242/435 [00:05<00:04, 44.82it/s][A
 57%|    | 247/435 [00:05<00:04, 44.88it/s][A
 58%|    | 252/435 [00:05<00:04, 44.85it/s][A
 59%|    | 257/435 [00:05<00:03, 44.90it/s][A
 60%|    | 262/435 [00:05<00:03, 44.89it/s][A
 61%|   | 267/435 [00:05<00:03, 44.86it/s][A
 63%|   | 272/435 [00:06<00:03, 44.85it/s][A
 64%|   | 277/435 [00:06<00:03, 44.80it/s][A
 65%|   | 282/435 [00:06<00:03, 44.81it/s][A
 66%|   | 287/435 [00:06<00:03, 44.83it/s][A
 67%|   | 292/435 [00:06<00:03, 44.84it/s][A
 68%|   | 297/435 [00:06<00:03, 44.84it/s][A
 69%|   | 302/435 [00:06<00:02, 44.80it/s][A
 71%|   | 307/435 [00:06<00:02, 44.80it/s][A
 72%|  | 312/435 [00:06<00:02, 44.87it/s][A
 73%|  | 317/435 [00:07<00:02, 44.81it/s][A
 74%|  | 322/435 [00:07<00:02, 43.22it/s][A
 75%|  | 327/435 [00:07<00:02, 43.74it/s][A
 76%|  | 332/435 [00:07<00:02, 44.15it/s][A
 77%|  | 337/435 [00:07<00:02, 44.32it/s][A
 79%|  | 342/435 [00:07<00:02, 44.41it/s][A
 80%|  | 347/435 [00:07<00:01, 44.53it/s][A
 81%|  | 352/435 [00:07<00:01, 44.56it/s][A
 82%| | 357/435 [00:07<00:01, 44.71it/s][A
 83%| | 362/435 [00:08<00:01, 44.47it/s][A
 84%| | 367/435 [00:08<00:01, 44.63it/s][A
 86%| | 372/435 [00:08<00:01, 44.76it/s][A
 87%| | 377/435 [00:08<00:01, 44.69it/s][A
 88%| | 382/435 [00:08<00:01, 44.79it/s][A
 89%| | 387/435 [00:08<00:01, 44.85it/s][A
 90%| | 392/435 [00:08<00:00, 44.89it/s][A
 91%|| 397/435 [00:08<00:00, 44.78it/s][A
 92%|| 402/435 [00:08<00:00, 44.85it/s][A
 94%|| 407/435 [00:09<00:00, 44.83it/s][A
 95%|| 412/435 [00:09<00:00, 44.80it/s][A
 96%|| 417/435 [00:09<00:00, 44.82it/s][A
 97%|| 422/435 [00:09<00:00, 44.87it/s][A
 98%|| 427/435 [00:09<00:00, 44.81it/s][A
 99%|| 432/435 [00:09<00:00, 44.86it/s][A                                                 
                                                 [A100%|| 820/820 [05:00<00:00,  3.51it/s]
100%|| 435/435 [00:09<00:00, 44.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:51:22,912 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-820
[INFO|configuration_utils.py:351] 2023-08-28 06:51:23,042 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-820/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:51:25,992 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-820/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:51:26,160 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:51:26,268 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-820/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:51:27,442 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:51:27,442 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-164 (score: 0.9974275827407837).
                                                 100%|| 820/820 [05:13<00:00,  3.51it/s]100%|| 820/820 [05:13<00:00,  2.62it/s]
[INFO|trainer.py:1894] 2023-08-28 06:51:35,529 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 06:51:35,655 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:51:38,647 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:51:38,784 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:51:38,858 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:51:39,373 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:39,373 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:39,373 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:39,373 >>   train_runtime            = 0:05:13.05
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:39,373 >>   train_samples            =      10520
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:39,373 >>   train_samples_per_second =    168.019
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:39,373 >>   train_steps_per_second   =      2.619
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7257, 'eval_samples_per_second': 357.506, 'eval_steps_per_second': 44.727, 'epoch': 5.0}
{'train_runtime': 313.0596, 'train_samples_per_second': 168.019, 'train_steps_per_second': 2.619, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:51:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:51:39,598 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:51:39,599 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 06:51:39,599 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 55.65it/s]  3%|         | 12/435 [00:00<00:08, 49.01it/s]  4%|         | 17/435 [00:00<00:08, 47.53it/s]  5%|         | 22/435 [00:00<00:10, 41.01it/s]  6%|         | 27/435 [00:00<00:09, 42.27it/s]  7%|         | 32/435 [00:00<00:09, 43.23it/s]  9%|         | 37/435 [00:00<00:09, 43.90it/s] 10%|         | 42/435 [00:01<00:11, 33.16it/s] 11%|         | 47/435 [00:01<00:10, 36.30it/s] 12%|        | 52/435 [00:01<00:09, 38.70it/s] 13%|        | 57/435 [00:01<00:09, 40.51it/s] 14%|        | 62/435 [00:01<00:08, 41.82it/s] 15%|        | 67/435 [00:01<00:08, 42.84it/s] 17%|        | 72/435 [00:01<00:08, 43.53it/s] 18%|        | 77/435 [00:01<00:08, 44.04it/s] 19%|        | 82/435 [00:01<00:08, 44.04it/s] 20%|        | 87/435 [00:02<00:07, 44.07it/s] 21%|        | 92/435 [00:02<00:08, 38.20it/s] 22%|       | 97/435 [00:02<00:08, 40.15it/s] 23%|       | 102/435 [00:02<00:07, 41.64it/s] 25%|       | 107/435 [00:02<00:07, 42.71it/s] 26%|       | 112/435 [00:02<00:07, 43.47it/s] 27%|       | 117/435 [00:02<00:07, 44.01it/s] 28%|       | 122/435 [00:02<00:07, 44.39it/s] 29%|       | 127/435 [00:03<00:06, 44.56it/s] 30%|       | 132/435 [00:03<00:06, 44.36it/s] 31%|      | 137/435 [00:03<00:06, 44.19it/s] 33%|      | 142/435 [00:03<00:06, 44.25it/s] 34%|      | 147/435 [00:03<00:06, 44.49it/s] 35%|      | 152/435 [00:03<00:06, 44.72it/s] 36%|      | 157/435 [00:03<00:06, 44.91it/s] 37%|      | 162/435 [00:03<00:06, 45.01it/s] 38%|      | 167/435 [00:03<00:05, 45.13it/s] 40%|      | 172/435 [00:04<00:05, 45.06it/s] 41%|      | 177/435 [00:04<00:05, 44.85it/s] 42%|     | 182/435 [00:04<00:05, 44.58it/s] 43%|     | 187/435 [00:04<00:05, 44.58it/s] 44%|     | 192/435 [00:04<00:05, 44.72it/s] 45%|     | 197/435 [00:04<00:05, 44.78it/s] 46%|     | 202/435 [00:04<00:05, 44.99it/s] 48%|     | 207/435 [00:04<00:05, 44.94it/s] 49%|     | 212/435 [00:04<00:04, 45.03it/s] 50%|     | 217/435 [00:05<00:04, 45.04it/s] 51%|     | 222/435 [00:05<00:04, 44.98it/s] 52%|    | 227/435 [00:05<00:04, 43.80it/s] 53%|    | 232/435 [00:05<00:04, 44.14it/s] 54%|    | 237/435 [00:05<00:04, 44.27it/s] 56%|    | 242/435 [00:05<00:04, 44.60it/s] 57%|    | 247/435 [00:05<00:04, 44.67it/s] 58%|    | 252/435 [00:05<00:04, 44.81it/s] 59%|    | 257/435 [00:05<00:03, 44.83it/s] 60%|    | 262/435 [00:06<00:03, 44.79it/s] 61%|   | 267/435 [00:06<00:03, 44.67it/s] 63%|   | 272/435 [00:06<00:03, 44.61it/s] 64%|   | 277/435 [00:06<00:03, 44.67it/s] 65%|   | 282/435 [00:06<00:03, 44.79it/s] 66%|   | 287/435 [00:06<00:03, 44.86it/s] 67%|   | 292/435 [00:06<00:03, 44.87it/s] 68%|   | 297/435 [00:06<00:03, 44.92it/s] 69%|   | 302/435 [00:06<00:02, 44.88it/s] 71%|   | 307/435 [00:07<00:02, 44.80it/s] 72%|  | 312/435 [00:07<00:02, 44.78it/s] 73%|  | 317/435 [00:07<00:02, 44.70it/s] 74%|  | 322/435 [00:07<00:02, 44.74it/s] 75%|  | 327/435 [00:07<00:02, 44.85it/s] 76%|  | 332/435 [00:07<00:02, 44.88it/s] 77%|  | 337/435 [00:07<00:02, 44.95it/s] 79%|  | 342/435 [00:07<00:02, 42.68it/s] 80%|  | 347/435 [00:07<00:02, 43.56it/s] 81%|  | 352/435 [00:08<00:01, 43.98it/s] 82%| | 357/435 [00:08<00:01, 44.29it/s] 83%| | 362/435 [00:08<00:01, 43.95it/s] 84%| | 367/435 [00:08<00:01, 44.22it/s] 86%| | 372/435 [00:08<00:01, 44.46it/s] 87%| | 377/435 [00:08<00:01, 44.51it/s] 88%| | 382/435 [00:08<00:01, 44.53it/s] 89%| | 387/435 [00:08<00:01, 44.57it/s] 90%| | 392/435 [00:08<00:00, 44.58it/s] 91%|| 397/435 [00:09<00:00, 44.81it/s] 92%|| 402/435 [00:09<00:00, 44.67it/s] 94%|| 407/435 [00:09<00:00, 44.76it/s] 95%|| 412/435 [00:09<00:00, 44.81it/s] 96%|| 417/435 [00:09<00:00, 44.79it/s] 97%|| 422/435 [00:09<00:00, 44.93it/s] 98%|| 427/435 [00:09<00:00, 44.75it/s] 99%|| 432/435 [00:09<00:00, 44.69it/s]100%|| 435/435 [00:09<00:00, 43.88it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:51:49,531 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:49,531 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:49,531 >>   eval_loss               =     0.9974
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:49,531 >>   eval_runtime            = 0:00:09.93
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:49,531 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:49,531 >>   eval_samples_per_second =    350.078
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:49,531 >>   eval_steps_per_second   =     43.798
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:51:49,531 >>   perplexity              =     2.7113
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:00,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:00,386 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:00,386 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:00,386 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:00,386 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:52:01,376 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:52:01,377 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:52:02,011 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:52:03,122 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:52:03,122 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:06,086 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:06,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:06,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:06,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:06,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:52:06,833 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:52:06,834 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:52:07,435 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:52:07,642 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:52:07,642 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-328
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-656
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/checkpoint-820
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.71it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.67it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.72it/s]Extractor Predicting: 18it [00:10,  1.72it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.70it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.72it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:14,  1.76it/s]Extractor Predicting: 25it [00:14,  1.67it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.66it/s]Extractor Predicting: 29it [00:17,  1.64it/s]Extractor Predicting: 30it [00:17,  1.60it/s]Extractor Predicting: 31it [00:18,  1.62it/s]Extractor Predicting: 32it [00:19,  1.64it/s]Extractor Predicting: 33it [00:19,  1.61it/s]Extractor Predicting: 34it [00:20,  1.64it/s]Extractor Predicting: 35it [00:20,  1.66it/s]Extractor Predicting: 36it [00:21,  1.71it/s]Extractor Predicting: 37it [00:22,  1.71it/s]Extractor Predicting: 38it [00:22,  1.64it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:23,  1.68it/s]Extractor Predicting: 41it [00:24,  1.70it/s]Extractor Predicting: 42it [00:25,  1.71it/s]Extractor Predicting: 43it [00:25,  1.69it/s]Extractor Predicting: 44it [00:26,  1.69it/s]Extractor Predicting: 45it [00:26,  1.67it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.69it/s]Extractor Predicting: 48it [00:28,  1.70it/s]Extractor Predicting: 49it [00:29,  1.73it/s]Extractor Predicting: 50it [00:29,  1.78it/s]Extractor Predicting: 51it [00:30,  1.77it/s]Extractor Predicting: 52it [00:30,  1.71it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:32,  1.70it/s]Extractor Predicting: 55it [00:32,  1.68it/s]Extractor Predicting: 56it [00:33,  1.68it/s]Extractor Predicting: 57it [00:33,  1.67it/s]Extractor Predicting: 58it [00:34,  1.67it/s]Extractor Predicting: 59it [00:35,  1.73it/s]Extractor Predicting: 60it [00:35,  1.75it/s]Extractor Predicting: 61it [00:36,  1.76it/s]Extractor Predicting: 62it [00:36,  1.78it/s]Extractor Predicting: 63it [00:37,  1.77it/s]Extractor Predicting: 64it [00:37,  1.73it/s]Extractor Predicting: 65it [00:38,  1.74it/s]Extractor Predicting: 66it [00:39,  1.72it/s]Extractor Predicting: 67it [00:39,  1.73it/s]Extractor Predicting: 68it [00:40,  1.75it/s]Extractor Predicting: 69it [00:40,  1.75it/s]Extractor Predicting: 70it [00:41,  1.74it/s]Extractor Predicting: 71it [00:41,  1.70it/s]Extractor Predicting: 72it [00:42,  1.73it/s]Extractor Predicting: 73it [00:43,  1.73it/s]Extractor Predicting: 74it [00:43,  1.76it/s]Extractor Predicting: 75it [00:44,  1.79it/s]Extractor Predicting: 76it [00:44,  1.60it/s]Extractor Predicting: 77it [00:45,  1.59it/s]Extractor Predicting: 78it [00:46,  1.62it/s]Extractor Predicting: 79it [00:46,  1.62it/s]Extractor Predicting: 80it [00:47,  1.63it/s]Extractor Predicting: 81it [00:48,  1.62it/s]Extractor Predicting: 82it [00:48,  1.64it/s]Extractor Predicting: 83it [00:49,  1.66it/s]Extractor Predicting: 84it [00:49,  1.68it/s]Extractor Predicting: 85it [00:50,  1.68it/s]Extractor Predicting: 86it [00:50,  1.69it/s]Extractor Predicting: 87it [00:51,  1.69it/s]Extractor Predicting: 88it [00:52,  1.66it/s]Extractor Predicting: 89it [00:52,  1.70it/s]Extractor Predicting: 90it [00:53,  1.71it/s]Extractor Predicting: 91it [00:53,  1.75it/s]Extractor Predicting: 92it [00:54,  1.80it/s]Extractor Predicting: 93it [00:54,  1.81it/s]Extractor Predicting: 94it [00:55,  1.74it/s]Extractor Predicting: 95it [00:56,  1.79it/s]Extractor Predicting: 96it [00:56,  1.78it/s]Extractor Predicting: 97it [00:57,  1.78it/s]Extractor Predicting: 98it [00:57,  1.77it/s]Extractor Predicting: 99it [00:58,  1.72it/s]Extractor Predicting: 100it [00:59,  1.67it/s]Extractor Predicting: 101it [00:59,  1.68it/s]Extractor Predicting: 102it [01:00,  1.76it/s]Extractor Predicting: 103it [01:00,  1.79it/s]Extractor Predicting: 104it [01:01,  1.78it/s]Extractor Predicting: 105it [01:01,  1.78it/s]Extractor Predicting: 106it [01:02,  1.75it/s]Extractor Predicting: 107it [01:02,  1.77it/s]Extractor Predicting: 108it [01:03,  1.76it/s]Extractor Predicting: 109it [01:04,  1.76it/s]Extractor Predicting: 110it [01:04,  1.77it/s]Extractor Predicting: 111it [01:05,  1.78it/s]Extractor Predicting: 112it [01:05,  1.75it/s]Extractor Predicting: 113it [01:06,  1.81it/s]Extractor Predicting: 114it [01:06,  1.85it/s]Extractor Predicting: 115it [01:07,  1.82it/s]Extractor Predicting: 116it [01:07,  1.83it/s]Extractor Predicting: 117it [01:08,  1.83it/s]Extractor Predicting: 118it [01:09,  1.78it/s]Extractor Predicting: 119it [01:09,  1.80it/s]Extractor Predicting: 120it [01:10,  1.74it/s]Extractor Predicting: 121it [01:10,  1.72it/s]Extractor Predicting: 122it [01:11,  1.73it/s]Extractor Predicting: 123it [01:11,  1.74it/s]Extractor Predicting: 124it [01:12,  1.70it/s]Extractor Predicting: 125it [01:13,  1.72it/s]Extractor Predicting: 126it [01:13,  1.74it/s]Extractor Predicting: 127it [01:14,  1.73it/s]Extractor Predicting: 128it [01:14,  1.74it/s]Extractor Predicting: 129it [01:15,  1.75it/s]Extractor Predicting: 130it [01:16,  1.70it/s]Extractor Predicting: 131it [01:16,  1.71it/s]Extractor Predicting: 132it [01:17,  1.69it/s]Extractor Predicting: 133it [01:17,  1.72it/s]Extractor Predicting: 134it [01:18,  1.76it/s]Extractor Predicting: 135it [01:18,  1.75it/s]Extractor Predicting: 136it [01:19,  1.67it/s]Extractor Predicting: 137it [01:20,  1.73it/s]Extractor Predicting: 138it [01:20,  1.72it/s]Extractor Predicting: 139it [01:21,  1.71it/s]Extractor Predicting: 140it [01:21,  1.73it/s]Extractor Predicting: 141it [01:22,  1.76it/s]Extractor Predicting: 142it [01:22,  1.72it/s]Extractor Predicting: 143it [01:23,  1.75it/s]Extractor Predicting: 144it [01:24,  1.80it/s]Extractor Predicting: 144it [01:24,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:42,957 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:43,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:43,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:43,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:43,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:53:43,889 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:53:43,890 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:53:44,182 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:53:45,240 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:53:45,241 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:48,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:48,428 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:48,428 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:48,428 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:48,428 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:53:49,086 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:53:49,088 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:53:49,679 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:53:49,843 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:53:49,843 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:04,  1.77it/s]Extractor Predicting: 8it [00:04,  1.83it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.80it/s]Extractor Predicting: 11it [00:06,  1.81it/s]Extractor Predicting: 12it [00:06,  1.78it/s]Extractor Predicting: 13it [00:07,  1.77it/s]Extractor Predicting: 14it [00:07,  1.78it/s]Extractor Predicting: 15it [00:08,  1.79it/s]Extractor Predicting: 16it [00:09,  1.79it/s]Extractor Predicting: 17it [00:09,  1.75it/s]Extractor Predicting: 18it [00:10,  1.78it/s]Extractor Predicting: 19it [00:10,  1.82it/s]Extractor Predicting: 20it [00:11,  1.77it/s]Extractor Predicting: 21it [00:11,  1.80it/s]Extractor Predicting: 22it [00:12,  1.82it/s]Extractor Predicting: 23it [00:13,  1.79it/s]Extractor Predicting: 24it [00:13,  1.78it/s]Extractor Predicting: 25it [00:14,  1.76it/s]Extractor Predicting: 26it [00:14,  1.78it/s]Extractor Predicting: 27it [00:15,  1.76it/s]Extractor Predicting: 28it [00:15,  1.77it/s]Extractor Predicting: 29it [00:16,  1.76it/s]Extractor Predicting: 30it [00:16,  1.80it/s]Extractor Predicting: 31it [00:17,  1.77it/s]Extractor Predicting: 32it [00:18,  1.85it/s]Extractor Predicting: 33it [00:18,  1.80it/s]Extractor Predicting: 34it [00:19,  1.77it/s]Extractor Predicting: 35it [00:19,  1.76it/s]Extractor Predicting: 36it [00:20,  1.77it/s]Extractor Predicting: 37it [00:20,  1.80it/s]Extractor Predicting: 38it [00:21,  1.82it/s]Extractor Predicting: 39it [00:21,  1.82it/s]Extractor Predicting: 40it [00:22,  1.80it/s]Extractor Predicting: 41it [00:23,  1.79it/s]Extractor Predicting: 42it [00:23,  1.83it/s]Extractor Predicting: 43it [00:24,  1.76it/s]Extractor Predicting: 44it [00:24,  1.78it/s]Extractor Predicting: 45it [00:25,  1.74it/s]Extractor Predicting: 46it [00:25,  1.73it/s]Extractor Predicting: 47it [00:26,  1.76it/s]Extractor Predicting: 48it [00:27,  1.74it/s]Extractor Predicting: 49it [00:27,  1.77it/s]Extractor Predicting: 50it [00:28,  1.77it/s]Extractor Predicting: 51it [00:28,  1.75it/s]Extractor Predicting: 52it [00:29,  1.72it/s]Extractor Predicting: 53it [00:29,  1.74it/s]Extractor Predicting: 54it [00:30,  1.71it/s]Extractor Predicting: 55it [00:31,  1.77it/s]Extractor Predicting: 56it [00:31,  1.76it/s]Extractor Predicting: 57it [00:32,  1.74it/s]Extractor Predicting: 58it [00:32,  1.72it/s]Extractor Predicting: 59it [00:33,  1.67it/s]Extractor Predicting: 60it [00:34,  1.71it/s]Extractor Predicting: 61it [00:34,  1.70it/s]Extractor Predicting: 62it [00:35,  1.71it/s]Extractor Predicting: 63it [00:35,  1.74it/s]Extractor Predicting: 64it [00:36,  1.73it/s]Extractor Predicting: 65it [00:36,  1.79it/s]Extractor Predicting: 66it [00:37,  1.75it/s]Extractor Predicting: 67it [00:38,  1.73it/s]Extractor Predicting: 68it [00:38,  1.71it/s]Extractor Predicting: 69it [00:39,  1.72it/s]Extractor Predicting: 70it [00:39,  1.68it/s]Extractor Predicting: 71it [00:40,  1.68it/s]Extractor Predicting: 72it [00:41,  1.68it/s]Extractor Predicting: 73it [00:41,  1.66it/s]Extractor Predicting: 74it [00:42,  1.69it/s]Extractor Predicting: 75it [00:42,  1.70it/s]Extractor Predicting: 76it [00:43,  1.73it/s]Extractor Predicting: 77it [00:43,  1.69it/s]Extractor Predicting: 78it [00:44,  1.73it/s]Extractor Predicting: 79it [00:45,  1.73it/s]Extractor Predicting: 80it [00:45,  1.80it/s]Extractor Predicting: 81it [00:46,  1.60it/s]Extractor Predicting: 82it [00:46,  1.63it/s]Extractor Predicting: 83it [00:47,  1.66it/s]Extractor Predicting: 84it [00:48,  1.68it/s]Extractor Predicting: 85it [00:48,  1.67it/s]Extractor Predicting: 86it [00:49,  1.63it/s]Extractor Predicting: 87it [00:50,  1.59it/s]Extractor Predicting: 88it [00:50,  1.65it/s]Extractor Predicting: 89it [00:51,  1.64it/s]Extractor Predicting: 90it [00:51,  1.62it/s]Extractor Predicting: 91it [00:52,  1.61it/s]Extractor Predicting: 92it [00:53,  1.62it/s]Extractor Predicting: 93it [00:53,  1.64it/s]Extractor Predicting: 94it [00:54,  1.68it/s]Extractor Predicting: 95it [00:54,  1.68it/s]Extractor Predicting: 96it [00:55,  1.68it/s]Extractor Predicting: 97it [00:56,  1.65it/s]Extractor Predicting: 98it [00:56,  1.59it/s]Extractor Predicting: 99it [00:57,  1.61it/s]Extractor Predicting: 100it [00:57,  1.65it/s]Extractor Predicting: 101it [00:58,  1.67it/s]Extractor Predicting: 102it [00:59,  1.69it/s]Extractor Predicting: 103it [00:59,  1.64it/s]Extractor Predicting: 104it [01:00,  1.63it/s]Extractor Predicting: 105it [01:00,  1.67it/s]Extractor Predicting: 106it [01:01,  1.67it/s]Extractor Predicting: 107it [01:02,  1.68it/s]Extractor Predicting: 108it [01:02,  1.69it/s]Extractor Predicting: 109it [01:03,  1.67it/s]Extractor Predicting: 110it [01:03,  1.66it/s]Extractor Predicting: 111it [01:04,  1.68it/s]Extractor Predicting: 112it [01:05,  1.68it/s]Extractor Predicting: 113it [01:05,  1.69it/s]Extractor Predicting: 114it [01:06,  1.71it/s]Extractor Predicting: 115it [01:06,  1.69it/s]Extractor Predicting: 116it [01:07,  1.65it/s]Extractor Predicting: 117it [01:08,  1.65it/s]Extractor Predicting: 118it [01:08,  1.68it/s]Extractor Predicting: 119it [01:09,  1.66it/s]Extractor Predicting: 120it [01:09,  1.66it/s]Extractor Predicting: 121it [01:10,  1.68it/s]Extractor Predicting: 122it [01:11,  1.68it/s]Extractor Predicting: 123it [01:11,  1.69it/s]Extractor Predicting: 124it [01:12,  1.70it/s]Extractor Predicting: 125it [01:12,  1.69it/s]Extractor Predicting: 126it [01:13,  1.76it/s]Extractor Predicting: 127it [01:13,  1.76it/s]Extractor Predicting: 128it [01:14,  1.71it/s]Extractor Predicting: 129it [01:15,  1.76it/s]Extractor Predicting: 130it [01:15,  1.76it/s]Extractor Predicting: 131it [01:16,  1.75it/s]Extractor Predicting: 132it [01:16,  1.75it/s]Extractor Predicting: 133it [01:17,  1.81it/s]Extractor Predicting: 134it [01:17,  1.80it/s]Extractor Predicting: 135it [01:18,  1.74it/s]Extractor Predicting: 136it [01:19,  1.75it/s]Extractor Predicting: 137it [01:19,  1.78it/s]Extractor Predicting: 138it [01:20,  1.83it/s]Extractor Predicting: 139it [01:20,  1.81it/s]Extractor Predicting: 140it [01:21,  1.77it/s]Extractor Predicting: 141it [01:21,  1.75it/s]Extractor Predicting: 142it [01:22,  1.78it/s]Extractor Predicting: 143it [01:22,  1.75it/s]Extractor Predicting: 144it [01:23,  1.73it/s]Extractor Predicting: 145it [01:24,  1.72it/s]Extractor Predicting: 146it [01:24,  1.72it/s]Extractor Predicting: 147it [01:25,  1.79it/s]Extractor Predicting: 148it [01:25,  1.81it/s]Extractor Predicting: 149it [01:26,  1.83it/s]Extractor Predicting: 150it [01:26,  1.88it/s]Extractor Predicting: 151it [01:27,  1.87it/s]Extractor Predicting: 152it [01:27,  1.85it/s]Extractor Predicting: 153it [01:28,  1.85it/s]Extractor Predicting: 154it [01:28,  1.89it/s]Extractor Predicting: 155it [01:29,  1.86it/s]Extractor Predicting: 156it [01:30,  1.88it/s]Extractor Predicting: 157it [01:30,  1.92it/s]Extractor Predicting: 158it [01:31,  1.88it/s]Extractor Predicting: 159it [01:31,  1.96it/s]Extractor Predicting: 160it [01:31,  2.02it/s]Extractor Predicting: 161it [01:32,  1.95it/s]Extractor Predicting: 162it [01:33,  1.90it/s]Extractor Predicting: 163it [01:33,  1.89it/s]Extractor Predicting: 164it [01:34,  1.88it/s]Extractor Predicting: 165it [01:34,  1.92it/s]Extractor Predicting: 166it [01:35,  1.94it/s]Extractor Predicting: 167it [01:35,  1.94it/s]Extractor Predicting: 168it [01:36,  1.90it/s]Extractor Predicting: 169it [01:36,  1.94it/s]Extractor Predicting: 170it [01:37,  1.90it/s]Extractor Predicting: 171it [01:37,  1.95it/s]Extractor Predicting: 172it [01:38,  1.92it/s]Extractor Predicting: 173it [01:38,  1.93it/s]Extractor Predicting: 174it [01:39,  1.83it/s]Extractor Predicting: 175it [01:39,  1.83it/s]Extractor Predicting: 176it [01:40,  1.80it/s]Extractor Predicting: 177it [01:41,  1.78it/s]Extractor Predicting: 178it [01:41,  1.70it/s]Extractor Predicting: 179it [01:42,  1.70it/s]Extractor Predicting: 180it [01:42,  1.73it/s]Extractor Predicting: 181it [01:43,  1.72it/s]Extractor Predicting: 182it [01:44,  1.73it/s]Extractor Predicting: 183it [01:44,  1.74it/s]Extractor Predicting: 184it [01:45,  1.72it/s]Extractor Predicting: 185it [01:45,  1.73it/s]Extractor Predicting: 186it [01:46,  1.76it/s]Extractor Predicting: 187it [01:46,  1.75it/s]Extractor Predicting: 188it [01:47,  1.72it/s]Extractor Predicting: 189it [01:48,  1.74it/s]Extractor Predicting: 190it [01:48,  1.69it/s]Extractor Predicting: 191it [01:49,  1.67it/s]Extractor Predicting: 192it [01:49,  1.67it/s]Extractor Predicting: 193it [01:50,  1.66it/s]Extractor Predicting: 194it [01:51,  1.66it/s]Extractor Predicting: 195it [01:51,  1.66it/s]Extractor Predicting: 196it [01:52,  1.71it/s]Extractor Predicting: 197it [01:52,  1.73it/s]Extractor Predicting: 198it [01:53,  1.72it/s]Extractor Predicting: 199it [01:54,  1.74it/s]Extractor Predicting: 200it [01:54,  1.48it/s]Extractor Predicting: 201it [01:55,  1.48it/s]Extractor Predicting: 202it [01:56,  1.50it/s]Extractor Predicting: 203it [01:56,  1.51it/s]Extractor Predicting: 204it [01:57,  1.53it/s]Extractor Predicting: 205it [01:58,  1.55it/s]Extractor Predicting: 206it [01:58,  1.54it/s]Extractor Predicting: 207it [01:59,  1.57it/s]Extractor Predicting: 208it [02:00,  1.56it/s]Extractor Predicting: 209it [02:00,  1.53it/s]Extractor Predicting: 210it [02:01,  1.54it/s]Extractor Predicting: 211it [02:02,  1.49it/s]Extractor Predicting: 212it [02:02,  1.54it/s]Extractor Predicting: 213it [02:03,  1.55it/s]Extractor Predicting: 214it [02:03,  1.59it/s]Extractor Predicting: 215it [02:04,  1.56it/s]Extractor Predicting: 216it [02:05,  1.55it/s]Extractor Predicting: 217it [02:05,  1.57it/s]Extractor Predicting: 218it [02:06,  1.57it/s]Extractor Predicting: 219it [02:07,  1.57it/s]Extractor Predicting: 220it [02:07,  1.54it/s]Extractor Predicting: 221it [02:08,  1.52it/s]Extractor Predicting: 222it [02:09,  1.52it/s]Extractor Predicting: 223it [02:09,  1.56it/s]Extractor Predicting: 224it [02:10,  1.59it/s]Extractor Predicting: 225it [02:11,  1.59it/s]Extractor Predicting: 226it [02:11,  1.57it/s]Extractor Predicting: 227it [02:12,  1.58it/s]Extractor Predicting: 228it [02:12,  1.59it/s]Extractor Predicting: 229it [02:13,  1.62it/s]Extractor Predicting: 230it [02:14,  1.64it/s]Extractor Predicting: 231it [02:14,  1.69it/s]Extractor Predicting: 232it [02:15,  1.74it/s]Extractor Predicting: 233it [02:15,  1.74it/s]Extractor Predicting: 234it [02:16,  1.69it/s]Extractor Predicting: 235it [02:16,  1.72it/s]Extractor Predicting: 236it [02:17,  1.72it/s]Extractor Predicting: 237it [02:18,  1.72it/s]Extractor Predicting: 238it [02:18,  1.73it/s]Extractor Predicting: 239it [02:19,  1.66it/s]Extractor Predicting: 240it [02:19,  1.72it/s]Extractor Predicting: 241it [02:20,  1.74it/s]Extractor Predicting: 242it [02:20,  1.78it/s]Extractor Predicting: 243it [02:21,  1.78it/s]Extractor Predicting: 244it [02:22,  1.84it/s]Extractor Predicting: 245it [02:22,  1.77it/s]Extractor Predicting: 246it [02:23,  1.74it/s]Extractor Predicting: 247it [02:23,  1.72it/s]Extractor Predicting: 248it [02:24,  1.70it/s]Extractor Predicting: 249it [02:24,  1.73it/s]Extractor Predicting: 250it [02:25,  1.73it/s]Extractor Predicting: 251it [02:26,  1.73it/s]Extractor Predicting: 252it [02:26,  1.78it/s]Extractor Predicting: 253it [02:27,  1.76it/s]Extractor Predicting: 254it [02:27,  1.72it/s]Extractor Predicting: 255it [02:28,  1.73it/s]Extractor Predicting: 256it [02:29,  1.72it/s]Extractor Predicting: 257it [02:29,  1.60it/s]Extractor Predicting: 258it [02:30,  1.62it/s]Extractor Predicting: 259it [02:30,  1.65it/s]Extractor Predicting: 260it [02:31,  1.65it/s]Extractor Predicting: 261it [02:32,  1.66it/s]Extractor Predicting: 262it [02:32,  1.66it/s]Extractor Predicting: 263it [02:33,  1.70it/s]Extractor Predicting: 264it [02:33,  1.68it/s]Extractor Predicting: 265it [02:34,  1.69it/s]Extractor Predicting: 266it [02:35,  1.67it/s]Extractor Predicting: 267it [02:35,  1.67it/s]Extractor Predicting: 268it [02:36,  1.63it/s]Extractor Predicting: 269it [02:36,  1.65it/s]Extractor Predicting: 270it [02:37,  1.66it/s]Extractor Predicting: 271it [02:38,  1.67it/s]Extractor Predicting: 272it [02:38,  1.71it/s]Extractor Predicting: 273it [02:39,  1.66it/s]Extractor Predicting: 274it [02:39,  1.63it/s]Extractor Predicting: 275it [02:40,  1.65it/s]Extractor Predicting: 276it [02:41,  1.65it/s]Extractor Predicting: 277it [02:41,  1.66it/s]Extractor Predicting: 278it [02:42,  1.67it/s]Extractor Predicting: 279it [02:42,  1.63it/s]Extractor Predicting: 280it [02:43,  1.65it/s]Extractor Predicting: 281it [02:44,  1.68it/s]Extractor Predicting: 282it [02:44,  1.67it/s]Extractor Predicting: 283it [02:45,  1.63it/s]Extractor Predicting: 284it [02:45,  1.68it/s]Extractor Predicting: 285it [02:46,  1.67it/s]Extractor Predicting: 286it [02:47,  1.71it/s]Extractor Predicting: 287it [02:47,  1.71it/s]Extractor Predicting: 288it [02:48,  1.70it/s]Extractor Predicting: 289it [02:48,  1.69it/s]Extractor Predicting: 290it [02:49,  1.67it/s]Extractor Predicting: 291it [02:50,  1.61it/s]Extractor Predicting: 292it [02:50,  1.61it/s]Extractor Predicting: 293it [02:51,  1.65it/s]Extractor Predicting: 294it [02:52,  1.62it/s]Extractor Predicting: 295it [02:52,  1.63it/s]Extractor Predicting: 296it [02:53,  1.62it/s]Extractor Predicting: 297it [02:53,  1.65it/s]Extractor Predicting: 298it [02:54,  1.64it/s]Extractor Predicting: 299it [02:55,  1.62it/s]Extractor Predicting: 300it [02:55,  1.62it/s]Extractor Predicting: 301it [02:56,  1.62it/s]Extractor Predicting: 302it [02:56,  1.67it/s]Extractor Predicting: 303it [02:57,  1.63it/s]Extractor Predicting: 304it [02:58,  1.62it/s]Extractor Predicting: 305it [02:58,  1.59it/s]Extractor Predicting: 306it [02:59,  1.64it/s]Extractor Predicting: 307it [02:59,  1.64it/s]Extractor Predicting: 308it [03:00,  1.66it/s]Extractor Predicting: 309it [03:01,  1.66it/s]Extractor Predicting: 310it [03:01,  1.65it/s]Extractor Predicting: 311it [03:02,  1.64it/s]Extractor Predicting: 312it [03:02,  1.70it/s]Extractor Predicting: 313it [03:03,  1.75it/s]Extractor Predicting: 314it [03:04,  1.76it/s]Extractor Predicting: 315it [03:04,  1.79it/s]Extractor Predicting: 316it [03:05,  1.71it/s]Extractor Predicting: 317it [03:06,  1.46it/s]Extractor Predicting: 318it [03:06,  1.52it/s]Extractor Predicting: 319it [03:07,  1.56it/s]Extractor Predicting: 320it [03:07,  1.58it/s]Extractor Predicting: 321it [03:08,  1.60it/s]Extractor Predicting: 322it [03:09,  1.62it/s]Extractor Predicting: 323it [03:09,  1.59it/s]Extractor Predicting: 324it [03:10,  1.60it/s]Extractor Predicting: 325it [03:11,  1.61it/s]Extractor Predicting: 326it [03:11,  1.64it/s]Extractor Predicting: 327it [03:12,  1.68it/s]Extractor Predicting: 328it [03:12,  1.67it/s]Extractor Predicting: 329it [03:13,  1.69it/s]Extractor Predicting: 330it [03:14,  1.63it/s]Extractor Predicting: 331it [03:14,  1.63it/s]Extractor Predicting: 332it [03:15,  1.64it/s]Extractor Predicting: 333it [03:15,  1.67it/s]Extractor Predicting: 334it [03:16,  1.67it/s]Extractor Predicting: 335it [03:16,  1.69it/s]Extractor Predicting: 336it [03:17,  1.66it/s]Extractor Predicting: 337it [03:18,  1.67it/s]Extractor Predicting: 338it [03:18,  1.69it/s]Extractor Predicting: 339it [03:19,  1.74it/s]Extractor Predicting: 340it [03:19,  1.75it/s]Extractor Predicting: 341it [03:20,  1.71it/s]Extractor Predicting: 342it [03:21,  1.69it/s]Extractor Predicting: 343it [03:21,  1.69it/s]Extractor Predicting: 344it [03:22,  1.73it/s]Extractor Predicting: 345it [03:22,  1.69it/s]Extractor Predicting: 346it [03:23,  1.70it/s]Extractor Predicting: 347it [03:24,  1.67it/s]Extractor Predicting: 348it [03:24,  1.69it/s]Extractor Predicting: 349it [03:25,  1.73it/s]Extractor Predicting: 350it [03:25,  1.73it/s]Extractor Predicting: 351it [03:26,  1.76it/s]Extractor Predicting: 352it [03:26,  1.71it/s]Extractor Predicting: 353it [03:27,  1.62it/s]Extractor Predicting: 354it [03:28,  1.69it/s]Extractor Predicting: 355it [03:28,  1.74it/s]Extractor Predicting: 356it [03:29,  1.72it/s]Extractor Predicting: 357it [03:29,  1.72it/s]Extractor Predicting: 358it [03:30,  1.74it/s]Extractor Predicting: 359it [03:31,  1.73it/s]Extractor Predicting: 360it [03:31,  1.74it/s]Extractor Predicting: 361it [03:32,  1.75it/s]Extractor Predicting: 362it [03:32,  1.76it/s]Extractor Predicting: 363it [03:33,  1.77it/s]Extractor Predicting: 364it [03:33,  1.78it/s]Extractor Predicting: 365it [03:34,  1.79it/s]Extractor Predicting: 366it [03:34,  1.76it/s]Extractor Predicting: 367it [03:35,  1.71it/s]Extractor Predicting: 368it [03:36,  1.71it/s]Extractor Predicting: 369it [03:36,  1.72it/s]Extractor Predicting: 370it [03:37,  1.78it/s]Extractor Predicting: 371it [03:37,  1.79it/s]Extractor Predicting: 372it [03:38,  1.79it/s]Extractor Predicting: 373it [03:38,  1.75it/s]Extractor Predicting: 374it [03:39,  1.74it/s]Extractor Predicting: 375it [03:40,  1.75it/s]Extractor Predicting: 376it [03:40,  1.77it/s]Extractor Predicting: 377it [03:41,  1.74it/s]Extractor Predicting: 378it [03:41,  1.78it/s]Extractor Predicting: 379it [03:42,  1.74it/s]Extractor Predicting: 380it [03:42,  1.75it/s]Extractor Predicting: 381it [03:43,  1.77it/s]Extractor Predicting: 382it [03:44,  1.76it/s]Extractor Predicting: 383it [03:44,  1.78it/s]Extractor Predicting: 384it [03:45,  1.82it/s]Extractor Predicting: 385it [03:45,  1.78it/s]Extractor Predicting: 386it [03:46,  1.76it/s]Extractor Predicting: 387it [03:46,  1.76it/s]Extractor Predicting: 388it [03:47,  1.75it/s]Extractor Predicting: 389it [03:48,  1.77it/s]Extractor Predicting: 390it [03:48,  1.75it/s]Extractor Predicting: 391it [03:49,  1.73it/s]Extractor Predicting: 392it [03:49,  1.74it/s]Extractor Predicting: 393it [03:50,  1.78it/s]Extractor Predicting: 394it [03:50,  1.69it/s]Extractor Predicting: 395it [03:51,  1.64it/s]Extractor Predicting: 396it [03:52,  1.63it/s]Extractor Predicting: 397it [03:52,  1.63it/s]Extractor Predicting: 398it [03:53,  1.63it/s]Extractor Predicting: 399it [03:54,  1.62it/s]Extractor Predicting: 400it [03:54,  1.66it/s]Extractor Predicting: 401it [03:55,  1.63it/s]Extractor Predicting: 402it [03:55,  1.61it/s]Extractor Predicting: 403it [03:56,  1.62it/s]Extractor Predicting: 404it [03:57,  1.57it/s]Extractor Predicting: 405it [03:57,  1.58it/s]Extractor Predicting: 406it [03:58,  1.59it/s]Extractor Predicting: 407it [03:59,  1.57it/s]Extractor Predicting: 408it [03:59,  1.57it/s]Extractor Predicting: 409it [04:00,  1.60it/s]Extractor Predicting: 410it [04:01,  1.58it/s]Extractor Predicting: 411it [04:01,  1.39it/s]Extractor Predicting: 412it [04:02,  1.46it/s]Extractor Predicting: 413it [04:03,  1.52it/s]Extractor Predicting: 414it [04:03,  1.57it/s]Extractor Predicting: 415it [04:04,  1.63it/s]Extractor Predicting: 416it [04:04,  1.60it/s]Extractor Predicting: 417it [04:05,  1.62it/s]Extractor Predicting: 418it [04:06,  1.62it/s]Extractor Predicting: 419it [04:06,  1.58it/s]Extractor Predicting: 420it [04:07,  1.59it/s]Extractor Predicting: 421it [04:07,  1.69it/s]Extractor Predicting: 421it [04:07,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:10,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:10,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:10,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:10,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:10,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:58:11,200 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:58:11,201 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:58:11,798 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:58:12,883 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:58:12,883 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:15,838 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:15,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:15,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:15,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:15,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:58:16,527 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:58:16,528 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:58:17,116 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:58:17,287 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:58:17,287 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  2.09it/s]Extractor Predicting: 9it [00:05,  1.70it/s]
[INFO|configuration_utils.py:515] 2023-08-28 06:58:23,880 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:58:23,881 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:58:23,934 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:58:23,935 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 06:58:23,952 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:58:30,907 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 06:58:30,915 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 06:58:30,974 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:58:30,974 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:58:30,996 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:58:31,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:58:31,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:58:31,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:58:31,020 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:58:31,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:58:31,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 06:58:31,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:31,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:32,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:33,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:33,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:34,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:35,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:35,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:36,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:37,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:38,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:38,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:39,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:39,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:40,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:41,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:41,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:42,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:43,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:43,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:44,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:45,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:46,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:47,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:47,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:48,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:17<05:33, 17.54s/it][WARNING|generation_utils.py:914] 2023-08-28 06:58:48,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:49,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:50,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:50,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:51,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:52,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:52,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:53,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:54,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:54,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:55,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:56,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:56,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:57,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:58,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:58,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:59,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:00,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:00,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:01,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:02,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:02,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:03,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:03,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:04,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:05,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:34<05:08, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-28 06:59:05,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:06,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:07,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:07,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:08,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:09,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:09,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:10,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:11,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:11,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:12,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:13,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:13,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:14,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:14,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:15,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:16,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:16,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:17,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:18,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:18,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:19,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:20,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:20,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:21,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:22,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:51<04:51, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-28 06:59:22,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:23,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:24,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:24,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:25,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:26,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:27,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:27,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:28,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:29,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:29,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:30,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:30,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:31,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:32,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:32,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:33,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:34,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:34,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:35,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:36,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:36,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:37,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:38,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:38,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:08<04:31, 16.98s/it][WARNING|generation_utils.py:914] 2023-08-28 06:59:39,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:40,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:40,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:41,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:42,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:42,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:43,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:43,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:44,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:45,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:45,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:46,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:46,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:47,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:48,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:48,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:49,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:50,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:50,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:51,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:51,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:52,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:53,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:53,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:54,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:55,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:55,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:24<04:13, 16.88s/it][WARNING|generation_utils.py:914] 2023-08-28 06:59:56,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:56,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:57,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:58,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:58,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:59:59,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:00,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:00,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:01,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:02,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:03,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:03,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:04,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:05,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:05,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:06,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:07,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:07,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:08,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:09,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:09,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:10,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:11,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:40<03:49, 16.41s/it][WARNING|generation_utils.py:914] 2023-08-28 07:00:11,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:12,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:13,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:13,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:14,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:15,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:15,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:16,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:17,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:17,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:18,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:19,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:19,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:20,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:21,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:22,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:22,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:23,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:24,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:24,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:25,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:26,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:27,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:27,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:57<03:36, 16.69s/it][WARNING|generation_utils.py:914] 2023-08-28 07:00:29,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:29,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:30,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:31,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:31,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:32,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:33,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:34,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:34,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:35,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:36,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:36,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:37,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:38,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:39,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:39,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:40,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:41,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:41,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:42,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:43,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:44,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:45,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:45,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:46,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:15<03:26, 17.18s/it][WARNING|generation_utils.py:914] 2023-08-28 07:00:47,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:47,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:48,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:49,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:49,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:50,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:51,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:51,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:52,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:53,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:53,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:54,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:55,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:56,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:57,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:57,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:58,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:59,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:00:59,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:00,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:01,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:02,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:02,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:32<03:05, 16.84s/it][WARNING|generation_utils.py:914] 2023-08-28 07:01:03,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:04,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:05,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:05,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:06,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:07,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:07,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:08,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:09,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:10,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:10,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:11,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:12,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:12,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:13,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:14,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:15,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:15,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:16,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:17,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:17,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:18,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:19,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:19,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:49<02:49, 16.92s/it][WARNING|generation_utils.py:914] 2023-08-28 07:01:20,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:21,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:22,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:22,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:23,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:24,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:25,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:26,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:27,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:27,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:28,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:29,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:29,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:30,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:31,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:32,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:32,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:33,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:34,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:34,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:35,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:36,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:36,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:06<02:32, 17.00s/it][WARNING|generation_utils.py:914] 2023-08-28 07:01:37,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:38,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:38,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:39,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:40,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:41,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:41,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:42,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:43,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:43,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:44,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:45,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:45,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:46,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:47,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:47,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:48,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:49,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:49,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:50,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:51,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:51,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:52,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:22<02:12, 16.61s/it][WARNING|generation_utils.py:914] 2023-08-28 07:01:53,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:53,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:54,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:55,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:56,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:57,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:57,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:58,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:59,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:00,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:00,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:01,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:02,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:03,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:03,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:04,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:04,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:05,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:06,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:07,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:07,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:08,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:08,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:09,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:10,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:39<01:58, 16.92s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:11,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:11,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:12,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:12,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:13,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:14,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:14,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:15,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:16,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:16,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:17,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:17,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:18,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:18,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:19,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:20,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:20,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:21,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:22,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:22,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:23,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:24,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:24,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:54<01:36, 16.14s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:25,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:25,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:26,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:27,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:27,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:28,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:29,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:29,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:30,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:31,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:31,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:32,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:33,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:33,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:34,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:34,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:35,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:36,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:36,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:37,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:38,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:38,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:39,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:40,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:40,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:41,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:10<01:21, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:42,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:42,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:43,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:43,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:44,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:45,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:45,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:46,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:47,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:48,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:48,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:49,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:50,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:50,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:51,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:51,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:52,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:53,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:54,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:54,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:55,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:56,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:56,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:57,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:58,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:27<01:05, 16.46s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:58,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:59,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:00,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:00,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:01,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:01,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:03,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:03,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:04,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:05,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:05,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:06,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:07,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:08,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:08,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:09,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:09,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:10,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:11,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:11,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:12,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:13,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:13,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:43<00:48, 16.17s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:14,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:14,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:15,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:16,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:16,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:17,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:18,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:18,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:19,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:20,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:20,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:21,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:22,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:22,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:23,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:24,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:24,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:25,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:26,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:26,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:27,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:28,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:28,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:29,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:29,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:30,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:30,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:31,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:32,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:32,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:01<00:33, 17.00s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:33,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:34,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:34,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:35,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:36,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:36,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:37,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:38,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:38,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:39,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:40,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:40,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:41,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:42,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:42,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:43,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:44,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:44,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:45,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:46,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:46,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:47,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:48,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:17<00:16, 16.53s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:48,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:49,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:49,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:50,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:51,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:51,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:52,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:53,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:54,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:54,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:55,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:56,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:56,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:57,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:58,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:58,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:59,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:00,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:00,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:01,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:02,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:02,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:03,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:32<00:00, 16.23s/it]Generating: 100%|| 20/20 [05:32<00:00, 16.65s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:13,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:13,768 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:13,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:13,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:13,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:04:14,185 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:04:14,186 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:04:14,475 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:04:15,704 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:04:15,704 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:18,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:18,430 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:18,430 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:18,430 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:18,430 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:04:18,960 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:04:18,961 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:04:19,251 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:04:19,494 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:04:19,494 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 441, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 512, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7379807692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 234, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 625, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7512019230769231, 'errors': {'', "('Hans', 'head of government', '', 'He was succeeded by his brother , Dr . Hans .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : military branch .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('British ships', 'military branch', '', 'The Battle of Bataillon was the battle of the Battle of the Bastille , where British ships sank at least 1,400 French ships .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 181, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 274, 'raw': 384}
{'target': 600, 'success': 295, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 339, 'raw': 480}
{'target': 600, 'success': 361, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 428, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 602, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.6967592592592593, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('European Championships', 'winner', '', 'The previous year , he won the European Championships , and finished runner in 5th place in the category of medals .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : crosses . Context : Later in the year ( 11411231 ) he married Brigadier John B. Stoughton , sister of King James VI , the King of England . Head Entity : Robert Stoughton , Tail Entity : John B . Stoughton .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : crosses .', 'success_rate': 0.77, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('FreeBSD', 'operating system', '', 'The operating system is based on FreeBSD 2.1 .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 519, 'raw': 704}
{'target': 600, 'success': 544, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : participant .', 'success_rate': 0.7415865384615384, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : position played on team / speciality . Context : Later in 2008 , he played in the United States national team squad for the 2002 FIFA World Cup and 2010 FIFA World Cup games . Head Entity : Walter , Tail Entity : United States national team .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 99, 'raw': 160}
{'target': 600, 'success': 119, 'raw': 192}
{'target': 600, 'success': 141, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 183, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 224, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 266, 'raw': 416}
{'target': 600, 'success': 287, 'raw': 448}
{'target': 600, 'success': 312, 'raw': 480}
{'target': 600, 'success': 330, 'raw': 512}
{'target': 600, 'success': 348, 'raw': 544}
{'target': 600, 'success': 367, 'raw': 576}
{'target': 600, 'success': 386, 'raw': 608}
{'target': 600, 'success': 404, 'raw': 640}
{'target': 600, 'success': 421, 'raw': 672}
{'target': 600, 'success': 448, 'raw': 704}
{'target': 600, 'success': 465, 'raw': 736}
{'target': 600, 'success': 482, 'raw': 768}
{'target': 600, 'success': 499, 'raw': 800}
{'target': 600, 'success': 525, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 574, 'raw': 896}
{'target': 600, 'success': 594, 'raw': 928}
{'target': 600, 'success': 611, 'raw': 960}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.6364583333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : publisher . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : season 2 , Tail Entity : the Walking Dead .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8288043478260869, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 17454
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17554, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.45it/s]Extractor Estimating: 2it [00:01,  1.45it/s]Extractor Estimating: 3it [00:02,  1.48it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:04,  1.51it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:07,  1.61it/s]Extractor Estimating: 12it [00:07,  1.61it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.61it/s]Extractor Estimating: 15it [00:09,  1.55it/s]Extractor Estimating: 16it [00:10,  1.50it/s]Extractor Estimating: 17it [00:11,  1.49it/s]Extractor Estimating: 18it [00:11,  1.52it/s]Extractor Estimating: 19it [00:12,  1.53it/s]Extractor Estimating: 20it [00:12,  1.62it/s]Extractor Estimating: 21it [00:13,  1.67it/s]Extractor Estimating: 22it [00:14,  1.61it/s]Extractor Estimating: 23it [00:14,  1.62it/s]Extractor Estimating: 24it [00:15,  1.65it/s]Extractor Estimating: 25it [00:15,  1.65it/s]Extractor Estimating: 26it [00:16,  1.58it/s]Extractor Estimating: 27it [00:17,  1.56it/s]Extractor Estimating: 28it [00:17,  1.48it/s]Extractor Estimating: 29it [00:18,  1.40it/s]Extractor Estimating: 30it [00:19,  1.40it/s]Extractor Estimating: 31it [00:20,  1.43it/s]Extractor Estimating: 32it [00:20,  1.42it/s]Extractor Estimating: 33it [00:21,  1.47it/s]Extractor Estimating: 34it [00:22,  1.43it/s]Extractor Estimating: 35it [00:22,  1.44it/s]Extractor Estimating: 36it [00:23,  1.47it/s]Extractor Estimating: 37it [00:24,  1.48it/s]Extractor Estimating: 38it [00:24,  1.52it/s]Extractor Estimating: 39it [00:25,  1.53it/s]Extractor Estimating: 40it [00:26,  1.50it/s]Extractor Estimating: 41it [00:26,  1.52it/s]Extractor Estimating: 42it [00:27,  1.53it/s]Extractor Estimating: 43it [00:28,  1.49it/s]Extractor Estimating: 44it [00:28,  1.52it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:30,  1.51it/s]Extractor Estimating: 47it [00:30,  1.52it/s]Extractor Estimating: 48it [00:31,  1.49it/s]Extractor Estimating: 49it [00:32,  1.53it/s]Extractor Estimating: 50it [00:32,  1.56it/s]Extractor Estimating: 51it [00:33,  1.57it/s]Extractor Estimating: 52it [00:34,  1.57it/s]Extractor Estimating: 53it [00:34,  1.53it/s]Extractor Estimating: 54it [00:35,  1.57it/s]Extractor Estimating: 55it [00:35,  1.65it/s]Extractor Estimating: 56it [00:36,  1.70it/s]Extractor Estimating: 57it [00:36,  1.70it/s]Extractor Estimating: 58it [00:37,  1.67it/s]Extractor Estimating: 59it [00:38,  1.63it/s]Extractor Estimating: 60it [00:38,  1.67it/s]Extractor Estimating: 61it [00:39,  1.66it/s]Extractor Estimating: 62it [00:40,  1.68it/s]Extractor Estimating: 63it [00:40,  1.62it/s]Extractor Estimating: 64it [00:41,  1.57it/s]Extractor Estimating: 65it [00:41,  1.58it/s]Extractor Estimating: 66it [00:42,  1.64it/s]Extractor Estimating: 67it [00:43,  1.58it/s]Extractor Estimating: 68it [00:43,  1.61it/s]Extractor Estimating: 69it [00:44,  1.56it/s]Extractor Estimating: 70it [00:45,  1.61it/s]Extractor Estimating: 71it [00:45,  1.60it/s]Extractor Estimating: 72it [00:46,  1.62it/s]Extractor Estimating: 73it [00:47,  1.45it/s]Extractor Estimating: 74it [00:47,  1.49it/s]Extractor Estimating: 75it [00:48,  1.51it/s]Extractor Estimating: 76it [00:49,  1.54it/s]Extractor Estimating: 77it [00:49,  1.51it/s]Extractor Estimating: 78it [00:50,  1.54it/s]Extractor Estimating: 79it [00:51,  1.52it/s]Extractor Estimating: 80it [00:51,  1.49it/s]Extractor Estimating: 81it [00:52,  1.50it/s]Extractor Estimating: 82it [00:53,  1.53it/s]Extractor Estimating: 83it [00:53,  1.53it/s]Extractor Estimating: 84it [00:54,  1.51it/s]Extractor Estimating: 85it [00:54,  1.53it/s]Extractor Estimating: 86it [00:55,  1.56it/s]Extractor Estimating: 87it [00:56,  1.53it/s]Extractor Estimating: 88it [00:56,  1.53it/s]Extractor Estimating: 89it [00:57,  1.59it/s]Extractor Estimating: 90it [00:58,  1.58it/s]Extractor Estimating: 91it [00:58,  1.44it/s]Extractor Estimating: 92it [00:59,  1.48it/s]Extractor Estimating: 93it [01:00,  1.51it/s]Extractor Estimating: 94it [01:00,  1.58it/s]Extractor Estimating: 95it [01:01,  1.58it/s]Extractor Estimating: 96it [01:02,  1.52it/s]Extractor Estimating: 97it [01:02,  1.51it/s]Extractor Estimating: 98it [01:03,  1.54it/s]Extractor Estimating: 99it [01:04,  1.40it/s]Extractor Estimating: 100it [01:04,  1.44it/s]Extractor Estimating: 101it [01:05,  1.48it/s]Extractor Estimating: 102it [01:06,  1.51it/s]Extractor Estimating: 103it [01:06,  1.47it/s]Extractor Estimating: 104it [01:07,  1.51it/s]Extractor Estimating: 105it [01:08,  1.55it/s]Extractor Estimating: 106it [01:08,  1.60it/s]Extractor Estimating: 107it [01:09,  1.61it/s]Extractor Estimating: 108it [01:09,  1.62it/s]Extractor Estimating: 109it [01:10,  1.69it/s]Extractor Estimating: 110it [01:11,  1.63it/s]Extractor Estimating: 111it [01:11,  1.64it/s]Extractor Estimating: 112it [01:12,  1.61it/s]Extractor Estimating: 113it [01:13,  1.63it/s]Extractor Estimating: 114it [01:13,  1.61it/s]Extractor Estimating: 115it [01:14,  1.64it/s]Extractor Estimating: 116it [01:14,  1.64it/s]Extractor Estimating: 117it [01:15,  1.59it/s]Extractor Estimating: 118it [01:16,  1.55it/s]Extractor Estimating: 119it [01:16,  1.58it/s]Extractor Estimating: 120it [01:17,  1.60it/s]Extractor Estimating: 121it [01:18,  1.59it/s]Extractor Estimating: 122it [01:18,  1.58it/s]Extractor Estimating: 123it [01:19,  1.59it/s]Extractor Estimating: 124it [01:19,  1.62it/s]Extractor Estimating: 125it [01:20,  1.65it/s]Extractor Estimating: 126it [01:21,  1.64it/s]Extractor Estimating: 127it [01:21,  1.61it/s]Extractor Estimating: 128it [01:22,  1.57it/s]Extractor Estimating: 129it [01:23,  1.53it/s]Extractor Estimating: 130it [01:23,  1.41it/s]Extractor Estimating: 131it [01:24,  1.46it/s]Extractor Estimating: 132it [01:25,  1.44it/s]Extractor Estimating: 133it [01:26,  1.41it/s]Extractor Estimating: 134it [01:26,  1.44it/s]Extractor Estimating: 135it [01:27,  1.44it/s]Extractor Estimating: 136it [01:28,  1.47it/s]Extractor Estimating: 137it [01:28,  1.49it/s]Extractor Estimating: 138it [01:29,  1.52it/s]Extractor Estimating: 139it [01:29,  1.53it/s]Extractor Estimating: 140it [01:30,  1.48it/s]Extractor Estimating: 141it [01:31,  1.51it/s]Extractor Estimating: 142it [01:31,  1.51it/s]Extractor Estimating: 143it [01:32,  1.45it/s]Extractor Estimating: 144it [01:33,  1.48it/s]Extractor Estimating: 145it [01:33,  1.55it/s]Extractor Estimating: 146it [01:34,  1.51it/s]Extractor Estimating: 147it [01:35,  1.45it/s]Extractor Estimating: 148it [01:36,  1.47it/s]Extractor Estimating: 149it [01:36,  1.49it/s]Extractor Estimating: 150it [01:37,  1.51it/s]Extractor Estimating: 151it [01:37,  1.53it/s]Extractor Estimating: 152it [01:38,  1.54it/s]Extractor Estimating: 153it [01:39,  1.58it/s]Extractor Estimating: 154it [01:39,  1.54it/s]Extractor Estimating: 155it [01:40,  1.59it/s]Extractor Estimating: 156it [01:41,  1.57it/s]Extractor Estimating: 157it [01:41,  1.55it/s]Extractor Estimating: 158it [01:42,  1.62it/s]Extractor Estimating: 159it [01:43,  1.57it/s]Extractor Estimating: 160it [01:43,  1.52it/s]Extractor Estimating: 161it [01:44,  1.57it/s]Extractor Estimating: 162it [01:44,  1.66it/s]Extractor Estimating: 163it [01:45,  1.70it/s]Extractor Estimating: 164it [01:46,  1.68it/s]Extractor Estimating: 165it [01:46,  1.62it/s]Extractor Estimating: 166it [01:47,  1.62it/s]Extractor Estimating: 167it [01:47,  1.59it/s]Extractor Estimating: 168it [01:48,  1.61it/s]Extractor Estimating: 169it [01:49,  1.67it/s]Extractor Estimating: 170it [01:50,  1.43it/s]Extractor Estimating: 171it [01:50,  1.46it/s]Extractor Estimating: 172it [01:51,  1.51it/s]Extractor Estimating: 173it [01:51,  1.55it/s]Extractor Estimating: 174it [01:52,  1.55it/s]Extractor Estimating: 175it [01:53,  1.55it/s]Extractor Estimating: 176it [01:53,  1.55it/s]Extractor Estimating: 177it [01:54,  1.53it/s]Extractor Estimating: 178it [01:55,  1.47it/s]Extractor Estimating: 179it [01:55,  1.49it/s]Extractor Estimating: 180it [01:56,  1.54it/s]Extractor Estimating: 181it [01:57,  1.57it/s]Extractor Estimating: 182it [01:57,  1.58it/s]Extractor Estimating: 183it [01:58,  1.53it/s]Extractor Estimating: 184it [01:59,  1.48it/s]Extractor Estimating: 185it [01:59,  1.44it/s]Extractor Estimating: 186it [02:00,  1.42it/s]Extractor Estimating: 187it [02:01,  1.44it/s]Extractor Estimating: 188it [02:01,  1.51it/s]Extractor Estimating: 189it [02:02,  1.46it/s]Extractor Estimating: 190it [02:03,  1.46it/s]Extractor Estimating: 191it [02:03,  1.52it/s]Extractor Estimating: 192it [02:04,  1.47it/s]Extractor Estimating: 193it [02:05,  1.41it/s]Extractor Estimating: 194it [02:06,  1.49it/s]Extractor Estimating: 195it [02:06,  1.37it/s]Extractor Estimating: 196it [02:07,  1.36it/s]Extractor Estimating: 197it [02:08,  1.41it/s]Extractor Estimating: 198it [02:08,  1.45it/s]Extractor Estimating: 199it [02:09,  1.49it/s]Extractor Estimating: 200it [02:10,  1.53it/s]Extractor Estimating: 201it [02:10,  1.55it/s]Extractor Estimating: 202it [02:11,  1.52it/s]Extractor Estimating: 203it [02:12,  1.47it/s]Extractor Estimating: 204it [02:12,  1.51it/s]Extractor Estimating: 205it [02:14,  1.02s/it]Extractor Estimating: 206it [02:15,  1.11it/s]Extractor Estimating: 207it [02:15,  1.23it/s]Extractor Estimating: 208it [02:16,  1.33it/s]Extractor Estimating: 209it [02:17,  1.35it/s]Extractor Estimating: 210it [02:17,  1.35it/s]Extractor Estimating: 211it [02:18,  1.43it/s]Extractor Estimating: 212it [02:19,  1.46it/s]Extractor Estimating: 213it [02:19,  1.44it/s]Extractor Estimating: 214it [02:20,  1.41it/s]Extractor Estimating: 215it [02:21,  1.45it/s]Extractor Estimating: 216it [02:22,  1.44it/s]Extractor Estimating: 217it [02:22,  1.46it/s]Extractor Estimating: 218it [02:23,  1.45it/s]Extractor Estimating: 219it [02:24,  1.41it/s]Extractor Estimating: 220it [02:24,  1.45it/s]Extractor Estimating: 221it [02:25,  1.45it/s]Extractor Estimating: 222it [02:26,  1.45it/s]Extractor Estimating: 223it [02:26,  1.45it/s]Extractor Estimating: 224it [02:27,  1.49it/s]Extractor Estimating: 225it [02:28,  1.49it/s]Extractor Estimating: 226it [02:28,  1.45it/s]Extractor Estimating: 227it [02:29,  1.45it/s]Extractor Estimating: 228it [02:30,  1.47it/s]Extractor Estimating: 229it [02:30,  1.45it/s]Extractor Estimating: 230it [02:31,  1.44it/s]Extractor Estimating: 231it [02:32,  1.44it/s]Extractor Estimating: 232it [02:33,  1.48it/s]Extractor Estimating: 233it [02:33,  1.52it/s]Extractor Estimating: 234it [02:34,  1.37it/s]Extractor Estimating: 235it [02:35,  1.39it/s]Extractor Estimating: 236it [02:35,  1.42it/s]Extractor Estimating: 237it [02:36,  1.48it/s]Extractor Estimating: 238it [02:37,  1.48it/s]Extractor Estimating: 239it [02:37,  1.52it/s]Extractor Estimating: 240it [02:38,  1.46it/s]Extractor Estimating: 241it [02:39,  1.49it/s]Extractor Estimating: 242it [02:39,  1.54it/s]Extractor Estimating: 243it [02:40,  1.41it/s]Extractor Estimating: 244it [02:41,  1.44it/s]Extractor Estimating: 245it [02:41,  1.46it/s]Extractor Estimating: 246it [02:42,  1.46it/s]Extractor Estimating: 247it [02:43,  1.50it/s]Extractor Estimating: 248it [02:43,  1.51it/s]Extractor Estimating: 249it [02:44,  1.46it/s]Extractor Estimating: 250it [02:45,  1.45it/s]Extractor Estimating: 251it [02:46,  1.46it/s]Extractor Estimating: 252it [02:46,  1.51it/s]Extractor Estimating: 253it [02:47,  1.49it/s]Extractor Estimating: 254it [02:48,  1.46it/s]Extractor Estimating: 255it [02:48,  1.47it/s]Extractor Estimating: 256it [02:49,  1.32it/s]Extractor Estimating: 257it [02:50,  1.34it/s]Extractor Estimating: 258it [02:51,  1.35it/s]Extractor Estimating: 259it [02:51,  1.37it/s]Extractor Estimating: 260it [02:52,  1.37it/s]Extractor Estimating: 261it [02:53,  1.38it/s]Extractor Estimating: 262it [02:53,  1.41it/s]Extractor Estimating: 263it [02:54,  1.37it/s]Extractor Estimating: 264it [02:55,  1.42it/s]Extractor Estimating: 265it [02:56,  1.44it/s]Extractor Estimating: 266it [02:56,  1.48it/s]Extractor Estimating: 267it [02:57,  1.50it/s]Extractor Estimating: 268it [02:57,  1.49it/s]Extractor Estimating: 269it [02:58,  1.50it/s]Extractor Estimating: 270it [02:59,  1.44it/s]Extractor Estimating: 271it [03:00,  1.45it/s]Extractor Estimating: 272it [03:00,  1.44it/s]Extractor Estimating: 273it [03:01,  1.50it/s]Extractor Estimating: 274it [03:02,  1.47it/s]Extractor Estimating: 275it [03:02,  1.47it/s]Extractor Estimating: 276it [03:03,  1.50it/s]Extractor Estimating: 277it [03:04,  1.54it/s]Extractor Estimating: 278it [03:04,  1.55it/s]Extractor Estimating: 279it [03:05,  1.56it/s]Extractor Estimating: 280it [03:05,  1.55it/s]Extractor Estimating: 281it [03:06,  1.56it/s]Extractor Estimating: 282it [03:07,  1.57it/s]Extractor Estimating: 283it [03:07,  1.60it/s]Extractor Estimating: 284it [03:08,  1.64it/s]Extractor Estimating: 285it [03:09,  1.55it/s]Extractor Estimating: 286it [03:09,  1.56it/s]Extractor Estimating: 287it [03:10,  1.53it/s]Extractor Estimating: 288it [03:11,  1.55it/s]Extractor Estimating: 289it [03:11,  1.56it/s]Extractor Estimating: 290it [03:12,  1.60it/s]Extractor Estimating: 291it [03:12,  1.63it/s]Extractor Estimating: 292it [03:13,  1.63it/s]Extractor Estimating: 293it [03:14,  1.60it/s]Extractor Estimating: 294it [03:14,  1.58it/s]Extractor Estimating: 295it [03:15,  1.59it/s]Extractor Estimating: 296it [03:15,  1.60it/s]Extractor Estimating: 297it [03:16,  1.57it/s]Extractor Estimating: 298it [03:17,  1.60it/s]Extractor Estimating: 299it [03:17,  1.60it/s]Extractor Estimating: 300it [03:18,  1.59it/s]Extractor Estimating: 301it [03:19,  1.60it/s]Extractor Estimating: 302it [03:19,  1.52it/s]Extractor Estimating: 303it [03:20,  1.52it/s]Extractor Estimating: 304it [03:21,  1.54it/s]Extractor Estimating: 305it [03:21,  1.56it/s]Extractor Estimating: 306it [03:22,  1.53it/s]Extractor Estimating: 307it [03:23,  1.49it/s]Extractor Estimating: 308it [03:23,  1.49it/s]Extractor Estimating: 309it [03:24,  1.53it/s]Extractor Estimating: 310it [03:25,  1.55it/s]Extractor Estimating: 311it [03:25,  1.58it/s]Extractor Estimating: 312it [03:26,  1.58it/s]Extractor Estimating: 313it [03:26,  1.59it/s]Extractor Estimating: 314it [03:27,  1.60it/s]Extractor Estimating: 315it [03:28,  1.59it/s]Extractor Estimating: 316it [03:28,  1.62it/s]Extractor Estimating: 317it [03:29,  1.61it/s]Extractor Estimating: 318it [03:30,  1.57it/s]Extractor Estimating: 319it [03:30,  1.55it/s]Extractor Estimating: 320it [03:31,  1.62it/s]Extractor Estimating: 321it [03:32,  1.46it/s]Extractor Estimating: 322it [03:32,  1.52it/s]Extractor Estimating: 323it [03:33,  1.55it/s]Extractor Estimating: 324it [03:34,  1.53it/s]Extractor Estimating: 325it [03:34,  1.53it/s]Extractor Estimating: 326it [03:35,  1.42it/s]Extractor Estimating: 327it [03:36,  1.51it/s]Extractor Estimating: 328it [03:36,  1.60it/s]Extractor Estimating: 329it [03:37,  1.53it/s]Extractor Estimating: 330it [03:37,  1.57it/s]Extractor Estimating: 331it [03:38,  1.63it/s]Extractor Estimating: 332it [03:39,  1.61it/s]Extractor Estimating: 333it [03:39,  1.66it/s]Extractor Estimating: 334it [03:40,  1.67it/s]Extractor Estimating: 335it [03:40,  1.67it/s]Extractor Estimating: 336it [03:41,  1.70it/s]Extractor Estimating: 337it [03:42,  1.69it/s]Extractor Estimating: 338it [03:42,  1.67it/s]Extractor Estimating: 339it [03:43,  1.59it/s]Extractor Estimating: 340it [03:43,  1.61it/s]Extractor Estimating: 341it [03:44,  1.61it/s]Extractor Estimating: 342it [03:45,  1.69it/s]Extractor Estimating: 343it [03:45,  1.61it/s]Extractor Estimating: 344it [03:46,  1.58it/s]Extractor Estimating: 345it [03:47,  1.60it/s]Extractor Estimating: 346it [03:47,  1.57it/s]Extractor Estimating: 347it [03:48,  1.61it/s]Extractor Estimating: 348it [03:48,  1.57it/s]Extractor Estimating: 349it [03:49,  1.53it/s]Extractor Estimating: 350it [03:50,  1.59it/s]Extractor Estimating: 351it [03:50,  1.60it/s]Extractor Estimating: 352it [03:51,  1.58it/s]Extractor Estimating: 353it [03:52,  1.54it/s]Extractor Estimating: 354it [03:52,  1.56it/s]Extractor Estimating: 355it [03:53,  1.57it/s]Extractor Estimating: 356it [03:54,  1.61it/s]Extractor Estimating: 357it [03:54,  1.63it/s]Extractor Estimating: 358it [03:55,  1.58it/s]Extractor Estimating: 359it [03:55,  1.60it/s]Extractor Estimating: 360it [03:56,  1.60it/s]Extractor Estimating: 361it [03:57,  1.62it/s]Extractor Estimating: 362it [03:57,  1.69it/s]Extractor Estimating: 363it [03:58,  1.68it/s]Extractor Estimating: 364it [03:58,  1.72it/s]Extractor Estimating: 365it [03:59,  1.71it/s]Extractor Estimating: 366it [04:00,  1.64it/s]Extractor Estimating: 367it [04:00,  1.64it/s]Extractor Estimating: 368it [04:01,  1.55it/s]Extractor Estimating: 369it [04:02,  1.58it/s]Extractor Estimating: 370it [04:02,  1.61it/s]Extractor Estimating: 371it [04:03,  1.65it/s]Extractor Estimating: 372it [04:03,  1.64it/s]Extractor Estimating: 373it [04:04,  1.66it/s]Extractor Estimating: 374it [04:04,  1.68it/s]Extractor Estimating: 375it [04:05,  1.66it/s]Extractor Estimating: 376it [04:06,  1.65it/s]Extractor Estimating: 377it [04:06,  1.66it/s]Extractor Estimating: 378it [04:07,  1.71it/s]Extractor Estimating: 379it [04:07,  1.71it/s]Extractor Estimating: 380it [04:08,  1.71it/s]Extractor Estimating: 381it [04:09,  1.67it/s]Extractor Estimating: 382it [04:09,  1.67it/s]Extractor Estimating: 383it [04:10,  1.65it/s]Extractor Estimating: 384it [04:10,  1.62it/s]Extractor Estimating: 385it [04:11,  1.61it/s]Extractor Estimating: 386it [04:12,  1.61it/s]Extractor Estimating: 387it [04:12,  1.65it/s]Extractor Estimating: 388it [04:13,  1.68it/s]Extractor Estimating: 389it [04:13,  1.71it/s]Extractor Estimating: 390it [04:14,  1.64it/s]Extractor Estimating: 391it [04:15,  1.65it/s]Extractor Estimating: 392it [04:15,  1.64it/s]Extractor Estimating: 393it [04:16,  1.64it/s]Extractor Estimating: 394it [04:17,  1.67it/s]Extractor Estimating: 395it [04:17,  1.66it/s]Extractor Estimating: 396it [04:18,  1.66it/s]Extractor Estimating: 397it [04:18,  1.63it/s]Extractor Estimating: 398it [04:19,  1.62it/s]Extractor Estimating: 399it [04:20,  1.63it/s]Extractor Estimating: 400it [04:20,  1.61it/s]Extractor Estimating: 401it [04:21,  1.59it/s]Extractor Estimating: 402it [04:22,  1.53it/s]Extractor Estimating: 403it [04:22,  1.55it/s]Extractor Estimating: 404it [04:23,  1.55it/s]Extractor Estimating: 405it [04:23,  1.57it/s]Extractor Estimating: 406it [04:24,  1.56it/s]Extractor Estimating: 407it [04:25,  1.56it/s]Extractor Estimating: 408it [04:25,  1.54it/s]Extractor Estimating: 409it [04:26,  1.61it/s]Extractor Estimating: 410it [04:27,  1.44it/s]Extractor Estimating: 411it [04:28,  1.48it/s]Extractor Estimating: 412it [04:28,  1.44it/s]Extractor Estimating: 413it [04:29,  1.57it/s]Extractor Estimating: 414it [04:29,  1.58it/s]Extractor Estimating: 415it [04:30,  1.53it/s]Extractor Estimating: 416it [04:31,  1.51it/s]Extractor Estimating: 417it [04:31,  1.52it/s]Extractor Estimating: 418it [04:32,  1.58it/s]Extractor Estimating: 419it [04:33,  1.59it/s]Extractor Estimating: 420it [04:33,  1.58it/s]Extractor Estimating: 421it [04:34,  1.61it/s]Extractor Estimating: 422it [04:34,  1.60it/s]Extractor Estimating: 423it [04:35,  1.57it/s]Extractor Estimating: 424it [04:36,  1.58it/s]Extractor Estimating: 425it [04:36,  1.56it/s]Extractor Estimating: 426it [04:37,  1.59it/s]Extractor Estimating: 427it [04:38,  1.56it/s]Extractor Estimating: 428it [04:38,  1.57it/s]Extractor Estimating: 429it [04:39,  1.58it/s]Extractor Estimating: 430it [04:40,  1.55it/s]Extractor Estimating: 431it [04:40,  1.57it/s]Extractor Estimating: 432it [04:41,  1.64it/s]Extractor Estimating: 433it [04:41,  1.64it/s]Extractor Estimating: 434it [04:42,  1.61it/s]Extractor Estimating: 435it [04:43,  1.62it/s]Extractor Estimating: 436it [04:43,  1.59it/s]Extractor Estimating: 437it [04:44,  1.61it/s]Extractor Estimating: 438it [04:45,  1.61it/s]Extractor Estimating: 439it [04:45,  1.60it/s]Extractor Estimating: 440it [04:46,  1.60it/s]Extractor Estimating: 441it [04:46,  1.65it/s]Extractor Estimating: 442it [04:47,  1.58it/s]Extractor Estimating: 443it [04:48,  1.59it/s]Extractor Estimating: 444it [04:48,  1.61it/s]Extractor Estimating: 445it [04:49,  1.65it/s]Extractor Estimating: 446it [04:49,  1.66it/s]Extractor Estimating: 447it [04:50,  1.65it/s]Extractor Estimating: 448it [04:51,  1.62it/s]Extractor Estimating: 449it [04:51,  1.69it/s]Extractor Estimating: 450it [04:52,  1.61it/s]Extractor Estimating: 451it [04:53,  1.61it/s]Extractor Estimating: 452it [04:53,  1.49it/s]Extractor Estimating: 453it [04:54,  1.36it/s]Extractor Estimating: 454it [04:55,  1.40it/s]Extractor Estimating: 455it [04:56,  1.45it/s]Extractor Estimating: 456it [04:56,  1.52it/s]Extractor Estimating: 457it [04:57,  1.49it/s]Extractor Estimating: 458it [04:57,  1.48it/s]Extractor Estimating: 459it [04:58,  1.46it/s]Extractor Estimating: 460it [04:59,  1.45it/s]Extractor Estimating: 461it [05:00,  1.48it/s]Extractor Estimating: 462it [05:00,  1.46it/s]Extractor Estimating: 463it [05:01,  1.49it/s]Extractor Estimating: 464it [05:02,  1.48it/s]Extractor Estimating: 465it [05:02,  1.43it/s]Extractor Estimating: 466it [05:03,  1.43it/s]Extractor Estimating: 467it [05:04,  1.42it/s]Extractor Estimating: 468it [05:04,  1.43it/s]Extractor Estimating: 469it [05:05,  1.44it/s]Extractor Estimating: 470it [05:06,  1.49it/s]Extractor Estimating: 471it [05:06,  1.45it/s]Extractor Estimating: 472it [05:07,  1.42it/s]Extractor Estimating: 473it [05:08,  1.43it/s]Extractor Estimating: 474it [05:09,  1.46it/s]Extractor Estimating: 475it [05:09,  1.49it/s]Extractor Estimating: 476it [05:10,  1.51it/s]Extractor Estimating: 477it [05:10,  1.55it/s]Extractor Estimating: 478it [05:11,  1.54it/s]Extractor Estimating: 479it [05:12,  1.61it/s]Extractor Estimating: 480it [05:12,  1.62it/s]Extractor Estimating: 481it [05:13,  1.62it/s]Extractor Estimating: 482it [05:13,  1.61it/s]Extractor Estimating: 483it [05:14,  1.57it/s]Extractor Estimating: 484it [05:15,  1.56it/s]Extractor Estimating: 485it [05:15,  1.59it/s]Extractor Estimating: 486it [05:16,  1.52it/s]Extractor Estimating: 487it [05:17,  1.56it/s]Extractor Estimating: 488it [05:17,  1.62it/s]Extractor Estimating: 489it [05:18,  1.64it/s]Extractor Estimating: 490it [05:19,  1.59it/s]Extractor Estimating: 491it [05:19,  1.64it/s]Extractor Estimating: 492it [05:20,  1.59it/s]Extractor Estimating: 493it [05:20,  1.58it/s]Extractor Estimating: 494it [05:21,  1.53it/s]Extractor Estimating: 495it [05:22,  1.60it/s]Extractor Estimating: 496it [05:22,  1.57it/s]Extractor Estimating: 497it [05:23,  1.43it/s]Extractor Estimating: 498it [05:24,  1.45it/s]Extractor Estimating: 499it [05:25,  1.49it/s]Extractor Estimating: 500it [05:25,  1.63it/s]Extractor Estimating: 500it [05:25,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:02,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:02,794 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:02,794 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:02,794 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:02,795 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:10:03,498 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:10:03,499 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:10:04,091 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:10:05,170 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:10:05,170 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:08,263 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:08,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:08,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:08,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:08,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:10:09,108 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:10:09,109 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:10:09,747 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:10:10,280 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:10:10,280 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 10:18:50,480 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 10:18:50,753 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 10573 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 27507
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27607, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27607, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.081, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.074, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.064, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.085, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 59, avg_time 1.071, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 159, avg_time 2.157, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 259, avg_time 1.078, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 359, avg_time 1.077, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 18, avg_time 1.077, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 118, avg_time 1.105, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 218, avg_time 2.135, loss:nan
g_step 1200, step 318, avg_time 1.085, loss:nan
g_step 1300, step 418, avg_time 1.063, loss:nan
g_step 1400, step 77, avg_time 1.060, loss:nan
g_step 1500, step 177, avg_time 1.069, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 277, avg_time 2.163, loss:nan
g_step 1700, step 377, avg_time 1.076, loss:nan
g_step 1800, step 36, avg_time 1.066, loss:nan
g_step 1900, step 136, avg_time 1.080, loss:nan
g_step 2000, step 236, avg_time 1.068, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 336, avg_time 2.134, loss:nan
g_step 2200, step 436, avg_time 1.100, loss:nan
g_step 2300, step 95, avg_time 1.089, loss:nan
g_step 2400, step 195, avg_time 1.071, loss:nan
g_step 2500, step 295, avg_time 1.071, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 395, avg_time 2.161, loss:nan
g_step 2700, step 54, avg_time 1.084, loss:nan
g_step 2800, step 154, avg_time 1.099, loss:nan
g_step 2900, step 254, avg_time 1.088, loss:nan
g_step 3000, step 354, avg_time 1.079, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 13, avg_time 2.126, loss:nan
g_step 3200, step 113, avg_time 1.085, loss:nan
g_step 3300, step 213, avg_time 1.087, loss:nan
g_step 3400, step 313, avg_time 1.083, loss:nan
g_step 3500, step 413, avg_time 1.078, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 72, avg_time 2.138, loss:nan
g_step 3700, step 172, avg_time 1.063, loss:nan
g_step 3800, step 272, avg_time 1.079, loss:nan
g_step 3900, step 372, avg_time 1.090, loss:nan
g_step 4000, step 31, avg_time 1.085, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 131, avg_time 2.118, loss:nan
g_step 4200, step 231, avg_time 1.056, loss:nan
g_step 4300, step 331, avg_time 1.087, loss:nan
g_step 4400, step 431, avg_time 1.091, loss:nan
g_step 4500, step 90, avg_time 1.064, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 190, avg_time 2.137, loss:nan
g_step 4700, step 290, avg_time 1.090, loss:nan
g_step 4800, step 390, avg_time 1.067, loss:nan
g_step 4900, step 49, avg_time 1.105, loss:nan
g_step 5000, step 149, avg_time 1.068, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 249, avg_time 2.130, loss:nan
g_step 5200, step 349, avg_time 1.076, loss:nan
g_step 5300, step 8, avg_time 1.075, loss:nan
g_step 5400, step 108, avg_time 1.061, loss:nan
g_step 5500, step 208, avg_time 1.097, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 308, avg_time 2.133, loss:nan
g_step 5700, step 408, avg_time 1.088, loss:nan
g_step 5800, step 67, avg_time 1.065, loss:nan
g_step 5900, step 167, avg_time 1.091, loss:nan
g_step 6000, step 267, avg_time 1.073, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 367, avg_time 2.119, loss:nan
g_step 6200, step 26, avg_time 1.095, loss:nan
g_step 6300, step 126, avg_time 1.082, loss:nan
g_step 6400, step 226, avg_time 1.081, loss:nan
g_step 6500, step 326, avg_time 1.082, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 426, avg_time 2.142, loss:nan
g_step 6700, step 85, avg_time 1.051, loss:nan
g_step 6800, step 185, avg_time 1.089, loss:nan
g_step 6900, step 285, avg_time 1.101, loss:nan
g_step 7000, step 385, avg_time 1.071, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 44, avg_time 2.116, loss:nan
g_step 7200, step 144, avg_time 1.061, loss:nan
g_step 7300, step 244, avg_time 1.100, loss:nan
g_step 7400, step 344, avg_time 1.087, loss:nan
g_step 7500, step 3, avg_time 1.063, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 103, avg_time 2.141, loss:nan
g_step 7700, step 203, avg_time 1.097, loss:nan
g_step 7800, step 303, avg_time 1.063, loss:nan
g_step 7900, step 403, avg_time 1.053, loss:nan
g_step 8000, step 62, avg_time 1.084, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 162, avg_time 2.115, loss:nan
g_step 8200, step 262, avg_time 1.083, loss:nan
g_step 8300, step 362, avg_time 1.090, loss:nan
g_step 8400, step 21, avg_time 1.086, loss:nan
g_step 8500, step 121, avg_time 1.075, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 221, avg_time 2.150, loss:nan
g_step 8700, step 321, avg_time 1.052, loss:nan
g_step 8800, step 421, avg_time 1.076, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:18:50 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:18:50 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-18-50_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:18:52 - WARNING - datasets.builder -   Using custom data configuration default-dae123c9b13b1b04
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-dae123c9b13b1b04/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:18:54,401 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:18:54,402 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:18:54,402 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:18:54,403 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:18:54,538 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:18:54,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:18:54,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:18:54,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:18:54,601 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:18:54,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:18:54,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:18:55,084 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:18:58,178 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:18:58,200 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-dae123c9b13b1b04/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.80ba/s] 18%|        | 2/11 [00:00<00:02,  3.69ba/s] 27%|       | 3/11 [00:00<00:01,  4.04ba/s] 36%|      | 4/11 [00:01<00:01,  4.22ba/s] 45%|     | 5/11 [00:01<00:01,  4.31ba/s] 55%|    | 6/11 [00:01<00:01,  4.37ba/s] 64%|   | 7/11 [00:01<00:00,  4.42ba/s] 73%|  | 8/11 [00:01<00:00,  4.46ba/s] 82%| | 9/11 [00:02<00:00,  4.47ba/s] 91%| | 10/11 [00:02<00:00,  3.75ba/s]100%|| 11/11 [00:02<00:00,  4.40ba/s]100%|| 11/11 [00:02<00:00,  4.20ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.41ba/s] 50%|     | 2/4 [00:00<00:00,  3.98ba/s] 75%|  | 3/4 [00:00<00:00,  4.17ba/s]100%|| 4/4 [00:00<00:00,  5.29ba/s]100%|| 4/4 [00:00<00:00,  4.69ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  5.14ba/s] 27%|       | 3/11 [00:00<00:00,  8.17ba/s] 45%|     | 5/11 [00:00<00:00,  9.14ba/s] 64%|   | 7/11 [00:00<00:00,  9.55ba/s] 82%| | 9/11 [00:00<00:00,  9.84ba/s]100%|| 11/11 [00:01<00:00, 10.72ba/s]100%|| 11/11 [00:01<00:00,  9.75ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.43ba/s] 75%|  | 3/4 [00:00<00:00,  8.39ba/s]100%|| 4/4 [00:00<00:00,  9.42ba/s]
[INFO|trainer.py:414] 2023-08-28 10:19:04,547 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:19:04,645 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:19:04,645 >>   Num examples = 10600
[INFO|trainer.py:1149] 2023-08-28 10:19:04,645 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:19:04,645 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:19:04,645 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:19:04,645 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:19:04,645 >>   Total optimization steps = 830
  0%|          | 0/830 [00:00<?, ?it/s]  0%|          | 1/830 [00:00<04:02,  3.42it/s]  0%|          | 2/830 [00:00<03:55,  3.52it/s]  0%|          | 3/830 [00:00<03:53,  3.55it/s]  0%|          | 4/830 [00:01<03:51,  3.57it/s]  1%|          | 5/830 [00:01<03:52,  3.55it/s]  1%|          | 6/830 [00:01<03:51,  3.55it/s]  1%|          | 7/830 [00:01<03:51,  3.55it/s]  1%|          | 8/830 [00:02<03:50,  3.56it/s]  1%|          | 9/830 [00:02<03:49,  3.58it/s]  1%|          | 10/830 [00:02<03:48,  3.59it/s]  1%|         | 11/830 [00:03<03:48,  3.59it/s]  1%|         | 12/830 [00:03<03:47,  3.60it/s]  2%|         | 13/830 [00:03<03:46,  3.60it/s]  2%|         | 14/830 [00:03<03:46,  3.60it/s]  2%|         | 15/830 [00:04<03:46,  3.60it/s]  2%|         | 16/830 [00:04<03:54,  3.46it/s]  2%|         | 17/830 [00:04<03:52,  3.50it/s]  2%|         | 18/830 [00:05<03:49,  3.53it/s]  2%|         | 19/830 [00:05<03:48,  3.55it/s]  2%|         | 20/830 [00:05<03:47,  3.56it/s]  3%|         | 21/830 [00:05<03:46,  3.58it/s]  3%|         | 22/830 [00:06<03:45,  3.59it/s]  3%|         | 23/830 [00:06<03:44,  3.59it/s]  3%|         | 24/830 [00:06<03:44,  3.60it/s]  3%|         | 25/830 [00:07<03:43,  3.60it/s]  3%|         | 26/830 [00:07<03:43,  3.60it/s]  3%|         | 27/830 [00:07<03:42,  3.60it/s]  3%|         | 28/830 [00:07<03:42,  3.60it/s]  3%|         | 29/830 [00:08<03:42,  3.60it/s]  4%|         | 30/830 [00:08<03:41,  3.61it/s]  4%|         | 31/830 [00:08<03:41,  3.61it/s]  4%|         | 32/830 [00:08<03:41,  3.60it/s]  4%|         | 33/830 [00:09<03:41,  3.60it/s]  4%|         | 34/830 [00:09<03:48,  3.48it/s]  4%|         | 35/830 [00:09<03:46,  3.51it/s]  4%|         | 36/830 [00:10<03:44,  3.54it/s]  4%|         | 37/830 [00:10<03:42,  3.56it/s]  5%|         | 38/830 [00:10<03:41,  3.57it/s]  5%|         | 39/830 [00:10<03:41,  3.58it/s]  5%|         | 40/830 [00:11<03:40,  3.58it/s]  5%|         | 41/830 [00:11<03:40,  3.58it/s]  5%|         | 42/830 [00:11<03:39,  3.59it/s]  5%|         | 43/830 [00:12<03:39,  3.59it/s]  5%|         | 44/830 [00:12<03:38,  3.60it/s]  5%|         | 45/830 [00:12<03:38,  3.60it/s]  6%|         | 46/830 [00:12<03:37,  3.60it/s]  6%|         | 47/830 [00:13<03:37,  3.60it/s]  6%|         | 48/830 [00:13<03:37,  3.60it/s]  6%|         | 49/830 [00:13<03:37,  3.59it/s]  6%|         | 50/830 [00:13<03:36,  3.60it/s]  6%|         | 51/830 [00:14<03:36,  3.60it/s]  6%|         | 52/830 [00:14<03:36,  3.59it/s]  6%|         | 53/830 [00:14<03:45,  3.44it/s]  7%|         | 54/830 [00:15<03:42,  3.49it/s]  7%|         | 55/830 [00:15<03:40,  3.52it/s]  7%|         | 56/830 [00:15<03:38,  3.54it/s]  7%|         | 57/830 [00:15<03:37,  3.56it/s]  7%|         | 58/830 [00:16<03:36,  3.57it/s]  7%|         | 59/830 [00:16<03:35,  3.58it/s]  7%|         | 60/830 [00:16<03:34,  3.59it/s]  7%|         | 61/830 [00:17<03:34,  3.58it/s]  7%|         | 62/830 [00:17<03:34,  3.58it/s]  8%|         | 63/830 [00:17<03:33,  3.59it/s]  8%|         | 64/830 [00:17<03:33,  3.58it/s]  8%|         | 65/830 [00:18<03:33,  3.58it/s]  8%|         | 66/830 [00:18<03:33,  3.59it/s]  8%|         | 67/830 [00:18<03:32,  3.59it/s]  8%|         | 68/830 [00:19<03:32,  3.58it/s]  8%|         | 69/830 [00:19<03:32,  3.58it/s]  8%|         | 70/830 [00:19<03:31,  3.59it/s]  9%|         | 71/830 [00:19<03:36,  3.51it/s]  9%|         | 72/830 [00:20<03:34,  3.54it/s]  9%|         | 73/830 [00:20<03:32,  3.56it/s]  9%|         | 74/830 [00:20<03:31,  3.57it/s]  9%|         | 75/830 [00:21<03:30,  3.58it/s]  9%|         | 76/830 [00:21<03:30,  3.58it/s]  9%|         | 77/830 [00:21<03:30,  3.58it/s]  9%|         | 78/830 [00:21<03:40,  3.42it/s] 10%|         | 79/830 [00:22<03:36,  3.47it/s] 10%|         | 80/830 [00:22<03:33,  3.51it/s] 10%|         | 81/830 [00:22<03:32,  3.53it/s] 10%|         | 82/830 [00:23<03:30,  3.55it/s] 10%|         | 83/830 [00:23<03:29,  3.56it/s] 10%|         | 84/830 [00:23<03:28,  3.57it/s] 10%|         | 85/830 [00:23<03:28,  3.58it/s] 10%|         | 86/830 [00:24<03:27,  3.58it/s] 10%|         | 87/830 [00:24<03:27,  3.59it/s] 11%|         | 88/830 [00:24<03:26,  3.59it/s] 11%|         | 89/830 [00:24<03:35,  3.45it/s] 11%|         | 90/830 [00:25<03:32,  3.49it/s] 11%|         | 91/830 [00:25<03:29,  3.52it/s] 11%|         | 92/830 [00:25<03:28,  3.54it/s] 11%|         | 93/830 [00:26<03:27,  3.56it/s] 11%|        | 94/830 [00:26<03:26,  3.57it/s] 11%|        | 95/830 [00:26<03:25,  3.57it/s] 12%|        | 96/830 [00:26<03:25,  3.58it/s] 12%|        | 97/830 [00:27<03:24,  3.59it/s] 12%|        | 98/830 [00:27<03:23,  3.59it/s] 12%|        | 99/830 [00:27<03:23,  3.60it/s] 12%|        | 100/830 [00:28<03:22,  3.60it/s] 12%|        | 101/830 [00:28<03:22,  3.59it/s] 12%|        | 102/830 [00:28<03:22,  3.59it/s] 12%|        | 103/830 [00:28<03:22,  3.59it/s] 13%|        | 104/830 [00:29<03:22,  3.59it/s] 13%|        | 105/830 [00:29<03:21,  3.59it/s] 13%|        | 106/830 [00:29<03:21,  3.59it/s] 13%|        | 107/830 [00:30<03:31,  3.42it/s] 13%|        | 108/830 [00:30<03:27,  3.47it/s] 13%|        | 109/830 [00:30<03:25,  3.51it/s] 13%|        | 110/830 [00:30<03:24,  3.53it/s] 13%|        | 111/830 [00:31<03:22,  3.54it/s] 13%|        | 112/830 [00:31<03:21,  3.56it/s] 14%|        | 113/830 [00:31<03:20,  3.57it/s] 14%|        | 114/830 [00:31<03:20,  3.58it/s] 14%|        | 115/830 [00:32<03:19,  3.59it/s] 14%|        | 116/830 [00:32<03:18,  3.59it/s] 14%|        | 117/830 [00:32<03:18,  3.59it/s] 14%|        | 118/830 [00:33<03:18,  3.59it/s] 14%|        | 119/830 [00:33<03:18,  3.59it/s] 14%|        | 120/830 [00:33<03:17,  3.59it/s] 15%|        | 121/830 [00:33<03:17,  3.59it/s] 15%|        | 122/830 [00:34<03:17,  3.59it/s] 15%|        | 123/830 [00:34<03:16,  3.59it/s] 15%|        | 124/830 [00:34<03:16,  3.60it/s] 15%|        | 125/830 [00:35<03:15,  3.60it/s] 15%|        | 126/830 [00:35<03:25,  3.42it/s] 15%|        | 127/830 [00:35<03:22,  3.47it/s] 15%|        | 128/830 [00:35<03:20,  3.50it/s] 16%|        | 129/830 [00:36<03:18,  3.52it/s] 16%|        | 130/830 [00:36<03:17,  3.55it/s] 16%|        | 131/830 [00:36<03:16,  3.56it/s] 16%|        | 132/830 [00:37<03:15,  3.57it/s] 16%|        | 133/830 [00:37<03:15,  3.57it/s] 16%|        | 134/830 [00:37<03:14,  3.58it/s] 16%|        | 135/830 [00:37<03:13,  3.59it/s] 16%|        | 136/830 [00:38<03:13,  3.59it/s] 17%|        | 137/830 [00:38<03:13,  3.58it/s] 17%|        | 138/830 [00:38<03:13,  3.58it/s] 17%|        | 139/830 [00:38<03:12,  3.59it/s] 17%|        | 140/830 [00:39<03:12,  3.59it/s] 17%|        | 141/830 [00:39<03:12,  3.59it/s] 17%|        | 142/830 [00:39<03:11,  3.58it/s] 17%|        | 143/830 [00:40<03:11,  3.59it/s] 17%|        | 144/830 [00:40<03:25,  3.34it/s] 17%|        | 145/830 [00:40<03:20,  3.41it/s] 18%|        | 146/830 [00:41<03:17,  3.46it/s] 18%|        | 147/830 [00:41<03:15,  3.50it/s] 18%|        | 148/830 [00:41<03:13,  3.52it/s] 18%|        | 149/830 [00:41<03:12,  3.53it/s] 18%|        | 150/830 [00:42<03:11,  3.55it/s] 18%|        | 151/830 [00:42<03:10,  3.56it/s] 18%|        | 152/830 [00:42<03:10,  3.56it/s] 18%|        | 153/830 [00:42<03:09,  3.57it/s] 19%|        | 154/830 [00:43<03:09,  3.57it/s] 19%|        | 155/830 [00:43<03:08,  3.58it/s] 19%|        | 156/830 [00:43<03:10,  3.55it/s] 19%|        | 157/830 [00:44<03:09,  3.54it/s] 19%|        | 158/830 [00:44<03:09,  3.55it/s] 19%|        | 159/830 [00:44<03:08,  3.57it/s] 19%|        | 160/830 [00:44<03:07,  3.57it/s] 19%|        | 161/830 [00:45<03:07,  3.57it/s] 20%|        | 162/830 [00:45<03:10,  3.50it/s] 20%|        | 163/830 [00:45<03:09,  3.53it/s] 20%|        | 164/830 [00:46<03:49,  2.90it/s] 20%|        | 165/830 [00:46<03:36,  3.07it/s] 20%|        | 166/830 [00:46<03:09,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 10:19:51,407 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:19:51,407 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 10:19:51,407 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.62it/s][A
  3%|         | 12/435 [00:00<00:08, 48.97it/s][A
  4%|         | 17/435 [00:00<00:08, 47.10it/s][A
  5%|         | 22/435 [00:00<00:09, 45.60it/s][A
  6%|         | 27/435 [00:00<00:08, 45.48it/s][A
  7%|         | 32/435 [00:00<00:08, 45.18it/s][A
  9%|         | 37/435 [00:00<00:08, 44.80it/s][A
 10%|         | 42/435 [00:00<00:08, 44.54it/s][A
 11%|         | 47/435 [00:01<00:08, 44.59it/s][A
 12%|        | 52/435 [00:01<00:08, 44.81it/s][A
 13%|        | 57/435 [00:01<00:08, 44.83it/s][A
 14%|        | 62/435 [00:01<00:08, 44.84it/s][A
 15%|        | 67/435 [00:01<00:08, 44.80it/s][A
 17%|        | 72/435 [00:01<00:08, 44.96it/s][A
 18%|        | 77/435 [00:01<00:07, 44.94it/s][A
 19%|        | 82/435 [00:01<00:07, 44.73it/s][A
 20%|        | 87/435 [00:01<00:07, 44.58it/s][A
 21%|        | 92/435 [00:02<00:07, 44.54it/s][A
 22%|       | 97/435 [00:02<00:07, 44.65it/s][A
 23%|       | 102/435 [00:02<00:07, 44.77it/s][A
 25%|       | 107/435 [00:02<00:07, 44.80it/s][A
 26%|       | 112/435 [00:02<00:07, 44.71it/s][A
 27%|       | 117/435 [00:02<00:07, 44.90it/s][A
 28%|       | 122/435 [00:02<00:06, 44.81it/s][A
 29%|       | 127/435 [00:02<00:06, 44.73it/s][A
 30%|       | 132/435 [00:02<00:06, 44.65it/s][A
 31%|      | 137/435 [00:03<00:06, 44.64it/s][A
 33%|      | 142/435 [00:03<00:06, 44.69it/s][A
 34%|      | 147/435 [00:03<00:06, 44.78it/s][A
 35%|      | 152/435 [00:03<00:06, 44.84it/s][A
 36%|      | 157/435 [00:03<00:06, 44.73it/s][A
 37%|      | 162/435 [00:03<00:06, 42.12it/s][A
 38%|      | 167/435 [00:03<00:06, 42.97it/s][A
 40%|      | 172/435 [00:03<00:06, 43.54it/s][A
 41%|      | 177/435 [00:03<00:05, 43.90it/s][A
 42%|     | 182/435 [00:04<00:05, 44.15it/s][A
 43%|     | 187/435 [00:04<00:05, 44.44it/s][A
 44%|     | 192/435 [00:04<00:05, 44.53it/s][A
 45%|     | 197/435 [00:04<00:05, 44.52it/s][A
 46%|     | 202/435 [00:04<00:05, 44.42it/s][A
 48%|     | 207/435 [00:04<00:05, 44.35it/s][A
 49%|     | 212/435 [00:04<00:05, 44.58it/s][A
 50%|     | 217/435 [00:04<00:04, 44.69it/s][A
 51%|     | 222/435 [00:04<00:04, 44.78it/s][A
 52%|    | 227/435 [00:05<00:04, 44.71it/s][A
 53%|    | 232/435 [00:05<00:04, 44.77it/s][A
 54%|    | 237/435 [00:05<00:04, 44.89it/s][A
 56%|    | 242/435 [00:05<00:04, 44.83it/s][A
 57%|    | 247/435 [00:05<00:04, 44.49it/s][A
 58%|    | 252/435 [00:05<00:04, 43.26it/s][A
 59%|    | 257/435 [00:05<00:04, 43.80it/s][A
 60%|    | 262/435 [00:05<00:03, 44.14it/s][A
 61%|   | 267/435 [00:05<00:03, 44.37it/s][A
 63%|   | 272/435 [00:06<00:03, 44.53it/s][A
 64%|   | 277/435 [00:06<00:03, 44.60it/s][A
 65%|   | 282/435 [00:06<00:03, 44.70it/s][A
 66%|   | 287/435 [00:06<00:03, 44.75it/s][A
 67%|   | 292/435 [00:06<00:03, 44.57it/s][A
 68%|   | 297/435 [00:06<00:03, 44.63it/s][A
 69%|   | 302/435 [00:06<00:02, 44.63it/s][A
 71%|   | 307/435 [00:06<00:02, 44.70it/s][A
 72%|  | 312/435 [00:06<00:02, 44.74it/s][A
 73%|  | 317/435 [00:07<00:02, 44.82it/s][A
 74%|  | 322/435 [00:07<00:02, 44.84it/s][A
 75%|  | 327/435 [00:07<00:02, 44.81it/s][A
 76%|  | 332/435 [00:07<00:02, 44.80it/s][A
 77%|  | 337/435 [00:07<00:02, 44.80it/s][A
 79%|  | 342/435 [00:07<00:02, 44.75it/s][A
 80%|  | 347/435 [00:07<00:01, 44.67it/s][A
 81%|  | 352/435 [00:07<00:01, 44.65it/s][A
 82%| | 357/435 [00:07<00:01, 44.73it/s][A
 83%| | 362/435 [00:08<00:01, 44.82it/s][A
 84%| | 367/435 [00:08<00:01, 44.85it/s][A
 86%| | 372/435 [00:08<00:01, 44.73it/s][A
 87%| | 377/435 [00:08<00:01, 44.76it/s][A
 88%| | 382/435 [00:08<00:01, 44.80it/s][A
 89%| | 387/435 [00:08<00:01, 43.89it/s][A
 90%| | 392/435 [00:08<00:00, 44.15it/s][A
 91%|| 397/435 [00:08<00:00, 44.26it/s][A
 92%|| 402/435 [00:09<00:00, 44.37it/s][A
 94%|| 407/435 [00:09<00:00, 44.53it/s][A
 95%|| 412/435 [00:09<00:00, 44.60it/s][A
 96%|| 417/435 [00:09<00:00, 44.59it/s][A
 97%|| 422/435 [00:09<00:00, 44.70it/s][A
 98%|| 427/435 [00:09<00:00, 44.57it/s][A
 99%|| 432/435 [00:09<00:00, 44.57it/s][A                                                 
                                                 [A 20%|        | 166/830 [00:56<03:09,  3.51it/s]
100%|| 435/435 [00:09<00:00, 44.57it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:20:01,520 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 10:20:01,761 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:20:05,145 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:20:05,292 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:20:05,358 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|        | 167/830 [01:01<52:43,  4.77s/it] 20%|        | 168/830 [01:02<37:47,  3.42s/it] 20%|        | 169/830 [01:02<27:20,  2.48s/it] 20%|        | 170/830 [01:02<20:02,  1.82s/it] 21%|        | 171/830 [01:03<14:56,  1.36s/it] 21%|        | 172/830 [01:03<11:22,  1.04s/it] 21%|        | 173/830 [01:03<08:52,  1.23it/s] 21%|        | 174/830 [01:04<07:15,  1.50it/s] 21%|        | 175/830 [01:04<06:00,  1.82it/s] 21%|        | 176/830 [01:04<05:06,  2.13it/s] 21%|       | 177/830 [01:04<04:29,  2.42it/s] 21%|       | 178/830 [01:05<04:03,  2.68it/s] 22%|       | 179/830 [01:05<03:45,  2.89it/s] 22%|       | 180/830 [01:05<03:32,  3.06it/s] 22%|       | 181/830 [01:05<03:23,  3.19it/s] 22%|       | 182/830 [01:06<03:16,  3.29it/s] 22%|       | 183/830 [01:06<03:11,  3.37it/s] 22%|       | 184/830 [01:06<03:08,  3.43it/s] 22%|       | 185/830 [01:07<03:09,  3.40it/s] 22%|       | 186/830 [01:07<03:06,  3.45it/s] 23%|       | 187/830 [01:07<03:04,  3.49it/s] 23%|       | 188/830 [01:07<03:02,  3.52it/s] 23%|       | 189/830 [01:08<03:00,  3.55it/s] 23%|       | 190/830 [01:08<02:59,  3.56it/s] 23%|       | 191/830 [01:08<02:58,  3.58it/s] 23%|       | 192/830 [01:09<02:58,  3.58it/s] 23%|       | 193/830 [01:09<02:57,  3.59it/s] 23%|       | 194/830 [01:09<02:57,  3.59it/s] 23%|       | 195/830 [01:09<02:56,  3.59it/s] 24%|       | 196/830 [01:10<03:04,  3.43it/s] 24%|       | 197/830 [01:10<03:01,  3.48it/s] 24%|       | 198/830 [01:10<02:59,  3.51it/s] 24%|       | 199/830 [01:11<02:58,  3.54it/s] 24%|       | 200/830 [01:11<02:57,  3.55it/s] 24%|       | 201/830 [01:11<02:56,  3.56it/s] 24%|       | 202/830 [01:11<02:55,  3.57it/s] 24%|       | 203/830 [01:12<02:55,  3.58it/s] 25%|       | 204/830 [01:12<02:54,  3.58it/s] 25%|       | 205/830 [01:12<02:54,  3.59it/s] 25%|       | 206/830 [01:13<02:54,  3.58it/s] 25%|       | 207/830 [01:13<02:57,  3.51it/s] 25%|       | 208/830 [01:13<02:55,  3.54it/s] 25%|       | 209/830 [01:13<02:54,  3.56it/s] 25%|       | 210/830 [01:14<02:53,  3.57it/s] 25%|       | 211/830 [01:14<02:53,  3.56it/s] 26%|       | 212/830 [01:14<02:52,  3.57it/s] 26%|       | 213/830 [01:14<02:52,  3.58it/s] 26%|       | 214/830 [01:15<02:51,  3.59it/s] 26%|       | 215/830 [01:15<02:51,  3.59it/s] 26%|       | 216/830 [01:15<02:51,  3.58it/s] 26%|       | 217/830 [01:16<02:50,  3.59it/s] 26%|       | 218/830 [01:16<02:53,  3.54it/s] 26%|       | 219/830 [01:16<02:51,  3.55it/s] 27%|       | 220/830 [01:16<02:51,  3.56it/s] 27%|       | 221/830 [01:17<02:50,  3.57it/s] 27%|       | 222/830 [01:17<02:49,  3.58it/s] 27%|       | 223/830 [01:17<02:49,  3.58it/s] 27%|       | 224/830 [01:18<02:48,  3.59it/s] 27%|       | 225/830 [01:18<02:48,  3.59it/s] 27%|       | 226/830 [01:18<02:48,  3.59it/s] 27%|       | 227/830 [01:18<02:47,  3.59it/s] 27%|       | 228/830 [01:19<02:47,  3.59it/s] 28%|       | 229/830 [01:19<02:52,  3.49it/s] 28%|       | 230/830 [01:19<02:50,  3.52it/s] 28%|       | 231/830 [01:20<02:48,  3.54it/s] 28%|       | 232/830 [01:20<02:48,  3.55it/s] 28%|       | 233/830 [01:20<02:47,  3.56it/s] 28%|       | 234/830 [01:20<02:46,  3.57it/s] 28%|       | 235/830 [01:21<02:46,  3.58it/s] 28%|       | 236/830 [01:21<02:45,  3.59it/s] 29%|       | 237/830 [01:21<02:45,  3.58it/s] 29%|       | 238/830 [01:21<02:45,  3.59it/s] 29%|       | 239/830 [01:22<02:44,  3.59it/s] 29%|       | 240/830 [01:22<02:49,  3.47it/s] 29%|       | 241/830 [01:22<02:48,  3.51it/s] 29%|       | 242/830 [01:23<02:46,  3.54it/s] 29%|       | 243/830 [01:23<02:45,  3.55it/s] 29%|       | 244/830 [01:23<02:44,  3.57it/s] 30%|       | 245/830 [01:23<02:43,  3.57it/s] 30%|       | 246/830 [01:24<02:43,  3.58it/s] 30%|       | 247/830 [01:24<02:42,  3.58it/s] 30%|       | 248/830 [01:24<02:42,  3.59it/s] 30%|       | 249/830 [01:25<02:41,  3.59it/s] 30%|       | 250/830 [01:25<02:41,  3.59it/s] 30%|       | 251/830 [01:25<02:45,  3.49it/s] 30%|       | 252/830 [01:25<02:44,  3.52it/s] 30%|       | 253/830 [01:26<02:42,  3.54it/s] 31%|       | 254/830 [01:26<02:41,  3.56it/s] 31%|       | 255/830 [01:26<02:41,  3.57it/s] 31%|       | 256/830 [01:27<02:40,  3.57it/s] 31%|       | 257/830 [01:27<02:39,  3.58it/s] 31%|       | 258/830 [01:27<02:39,  3.59it/s] 31%|       | 259/830 [01:27<02:39,  3.59it/s] 31%|      | 260/830 [01:28<02:39,  3.58it/s] 31%|      | 261/830 [01:28<02:39,  3.58it/s] 32%|      | 262/830 [01:28<02:38,  3.58it/s] 32%|      | 263/830 [01:29<02:38,  3.59it/s] 32%|      | 264/830 [01:29<02:37,  3.59it/s] 32%|      | 265/830 [01:29<02:37,  3.59it/s] 32%|      | 266/830 [01:29<02:37,  3.59it/s] 32%|      | 267/830 [01:30<02:36,  3.59it/s] 32%|      | 268/830 [01:30<02:36,  3.59it/s] 32%|      | 269/830 [01:30<02:36,  3.59it/s] 33%|      | 270/830 [01:30<02:35,  3.59it/s] 33%|      | 271/830 [01:31<02:35,  3.59it/s] 33%|      | 272/830 [01:31<02:44,  3.39it/s] 33%|      | 273/830 [01:31<02:41,  3.45it/s] 33%|      | 274/830 [01:32<02:39,  3.49it/s] 33%|      | 275/830 [01:32<02:37,  3.52it/s] 33%|      | 276/830 [01:32<02:36,  3.54it/s] 33%|      | 277/830 [01:32<02:35,  3.55it/s] 33%|      | 278/830 [01:33<02:34,  3.57it/s] 34%|      | 279/830 [01:33<02:34,  3.58it/s] 34%|      | 280/830 [01:33<02:33,  3.58it/s] 34%|      | 281/830 [01:34<02:33,  3.58it/s] 34%|      | 282/830 [01:34<02:32,  3.59it/s] 34%|      | 283/830 [01:34<02:39,  3.42it/s] 34%|      | 284/830 [01:34<02:37,  3.46it/s] 34%|      | 285/830 [01:35<02:35,  3.50it/s] 34%|      | 286/830 [01:35<02:34,  3.53it/s] 35%|      | 287/830 [01:35<02:32,  3.55it/s] 35%|      | 288/830 [01:36<02:32,  3.56it/s] 35%|      | 289/830 [01:36<02:31,  3.58it/s] 35%|      | 290/830 [01:36<02:30,  3.58it/s] 35%|      | 291/830 [01:36<02:30,  3.59it/s] 35%|      | 292/830 [01:37<02:29,  3.59it/s] 35%|      | 293/830 [01:37<02:29,  3.59it/s] 35%|      | 294/830 [01:37<02:37,  3.40it/s] 36%|      | 295/830 [01:38<02:34,  3.46it/s] 36%|      | 296/830 [01:38<02:32,  3.50it/s] 36%|      | 297/830 [01:38<02:30,  3.53it/s] 36%|      | 298/830 [01:38<02:29,  3.55it/s] 36%|      | 299/830 [01:39<02:29,  3.56it/s] 36%|      | 300/830 [01:39<02:28,  3.56it/s] 36%|      | 301/830 [01:39<02:28,  3.57it/s] 36%|      | 302/830 [01:40<02:27,  3.58it/s] 37%|      | 303/830 [01:40<02:27,  3.58it/s] 37%|      | 304/830 [01:40<02:26,  3.58it/s] 37%|      | 305/830 [01:40<02:34,  3.40it/s] 37%|      | 306/830 [01:41<02:31,  3.45it/s] 37%|      | 307/830 [01:41<02:29,  3.49it/s] 37%|      | 308/830 [01:41<02:28,  3.52it/s] 37%|      | 309/830 [01:42<02:27,  3.54it/s] 37%|      | 310/830 [01:42<02:26,  3.56it/s] 37%|      | 311/830 [01:42<02:25,  3.57it/s] 38%|      | 312/830 [01:42<02:25,  3.57it/s] 38%|      | 313/830 [01:43<02:24,  3.58it/s] 38%|      | 314/830 [01:43<02:24,  3.58it/s] 38%|      | 315/830 [01:43<02:23,  3.59it/s] 38%|      | 316/830 [01:44<02:30,  3.41it/s] 38%|      | 317/830 [01:44<02:28,  3.46it/s] 38%|      | 318/830 [01:44<02:26,  3.50it/s] 38%|      | 319/830 [01:44<02:24,  3.52it/s] 39%|      | 320/830 [01:45<02:23,  3.55it/s] 39%|      | 321/830 [01:45<02:22,  3.56it/s] 39%|      | 322/830 [01:45<02:22,  3.57it/s] 39%|      | 323/830 [01:45<02:21,  3.58it/s] 39%|      | 324/830 [01:46<03:00,  2.80it/s] 39%|      | 325/830 [01:46<02:48,  2.99it/s] 39%|      | 326/830 [01:47<02:43,  3.07it/s] 39%|      | 327/830 [01:47<02:36,  3.21it/s] 40%|      | 328/830 [01:47<02:31,  3.32it/s] 40%|      | 329/830 [01:47<02:27,  3.40it/s] 40%|      | 330/830 [01:48<02:24,  3.45it/s] 40%|      | 331/830 [01:48<02:22,  3.49it/s] 40%|      | 332/830 [01:48<02:07,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 10:20:53,317 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:20:53,317 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 10:20:53,318 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7606, 'eval_samples_per_second': 356.227, 'eval_steps_per_second': 44.567, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.86it/s][A
  3%|         | 12/435 [00:00<00:08, 49.06it/s][A
  4%|         | 17/435 [00:00<00:08, 47.26it/s][A
  5%|         | 22/435 [00:00<00:08, 46.28it/s][A
  6%|         | 27/435 [00:00<00:08, 45.76it/s][A
  7%|         | 32/435 [00:00<00:08, 45.32it/s][A
  9%|         | 37/435 [00:00<00:08, 44.92it/s][A
 10%|         | 42/435 [00:00<00:08, 44.46it/s][A
 11%|         | 47/435 [00:01<00:08, 44.61it/s][A
 12%|        | 52/435 [00:01<00:08, 44.65it/s][A
 13%|        | 57/435 [00:01<00:08, 42.95it/s][A
 14%|        | 62/435 [00:01<00:08, 43.64it/s][A
 15%|        | 67/435 [00:01<00:08, 44.04it/s][A
 17%|        | 72/435 [00:01<00:08, 44.25it/s][A
 18%|        | 77/435 [00:01<00:08, 44.37it/s][A
 19%|        | 82/435 [00:01<00:07, 44.43it/s][A
 20%|        | 87/435 [00:01<00:07, 44.40it/s][A
 21%|        | 92/435 [00:02<00:07, 44.54it/s][A
 22%|       | 97/435 [00:02<00:07, 44.41it/s][A
 23%|       | 102/435 [00:02<00:07, 44.56it/s][A
 25%|       | 107/435 [00:02<00:07, 44.64it/s][A
 26%|       | 112/435 [00:02<00:07, 44.69it/s][A
 27%|       | 117/435 [00:02<00:07, 44.69it/s][A
 28%|       | 122/435 [00:02<00:06, 44.81it/s][A
 29%|       | 127/435 [00:02<00:06, 44.75it/s][A
 30%|       | 132/435 [00:02<00:06, 44.61it/s][A
 31%|      | 137/435 [00:03<00:06, 44.53it/s][A
 33%|      | 142/435 [00:03<00:06, 44.49it/s][A
 34%|      | 147/435 [00:03<00:06, 44.54it/s][A
 35%|      | 152/435 [00:03<00:06, 44.62it/s][A
 36%|      | 157/435 [00:03<00:06, 44.71it/s][A
 37%|      | 162/435 [00:03<00:06, 44.82it/s][A
 38%|      | 167/435 [00:03<00:05, 44.80it/s][A
 40%|      | 172/435 [00:03<00:05, 44.76it/s][A
 41%|      | 177/435 [00:03<00:05, 44.73it/s][A
 42%|     | 182/435 [00:04<00:05, 44.53it/s][A
 43%|     | 187/435 [00:04<00:05, 44.57it/s][A
 44%|     | 192/435 [00:04<00:05, 43.04it/s][A
 45%|     | 197/435 [00:04<00:05, 43.62it/s][A
 46%|     | 202/435 [00:04<00:05, 43.97it/s][A
 48%|     | 207/435 [00:04<00:05, 44.34it/s][A
 49%|     | 212/435 [00:04<00:05, 44.44it/s][A
 50%|     | 217/435 [00:04<00:04, 44.51it/s][A
 51%|     | 222/435 [00:04<00:04, 44.56it/s][A
 52%|    | 227/435 [00:05<00:04, 44.58it/s][A
 53%|    | 232/435 [00:05<00:04, 44.33it/s][A
 54%|    | 237/435 [00:05<00:04, 44.38it/s][A
 56%|    | 242/435 [00:05<00:04, 44.55it/s][A
 57%|    | 247/435 [00:05<00:04, 44.77it/s][A
 58%|    | 252/435 [00:05<00:04, 44.84it/s][A
 59%|    | 257/435 [00:05<00:03, 44.82it/s][A
 60%|    | 262/435 [00:05<00:03, 44.71it/s][A
 61%|   | 267/435 [00:05<00:03, 44.69it/s][A
 63%|   | 272/435 [00:06<00:03, 44.63it/s][A
 64%|   | 277/435 [00:06<00:03, 44.36it/s][A
 65%|   | 282/435 [00:06<00:03, 44.41it/s][A
 66%|   | 287/435 [00:06<00:03, 44.64it/s][A
 67%|   | 292/435 [00:06<00:03, 44.71it/s][A
 68%|   | 297/435 [00:06<00:03, 44.81it/s][A
 69%|   | 302/435 [00:06<00:02, 44.87it/s][A
 71%|   | 307/435 [00:06<00:02, 44.73it/s][A
 72%|  | 312/435 [00:06<00:02, 44.68it/s][A
 73%|  | 317/435 [00:07<00:02, 44.69it/s][A
 74%|  | 322/435 [00:07<00:02, 44.68it/s][A
 75%|  | 327/435 [00:07<00:02, 41.76it/s][A
 76%|  | 332/435 [00:07<00:02, 42.66it/s][A
 77%|  | 337/435 [00:07<00:02, 43.37it/s][A
 79%|  | 342/435 [00:07<00:02, 43.93it/s][A
 80%|  | 347/435 [00:07<00:01, 44.17it/s][A
 81%|  | 352/435 [00:07<00:01, 44.32it/s][A
 82%| | 357/435 [00:08<00:01, 44.26it/s][A
 83%| | 362/435 [00:08<00:01, 44.44it/s][A
 84%| | 367/435 [00:08<00:01, 44.21it/s][A
 86%| | 372/435 [00:08<00:01, 44.35it/s][A
 87%| | 377/435 [00:08<00:01, 44.44it/s][A
 88%| | 382/435 [00:08<00:01, 44.53it/s][A
 89%| | 387/435 [00:08<00:01, 44.74it/s][A
 90%| | 392/435 [00:08<00:00, 44.78it/s][A
 91%|| 397/435 [00:08<00:00, 44.77it/s][A
 92%|| 402/435 [00:09<00:00, 44.84it/s][A
 94%|| 407/435 [00:09<00:00, 44.60it/s][A
 95%|| 412/435 [00:09<00:00, 44.55it/s][A
 96%|| 417/435 [00:09<00:00, 44.61it/s][A
 97%|| 422/435 [00:09<00:00, 44.66it/s][A
 98%|| 427/435 [00:09<00:00, 44.62it/s][A
 99%|| 432/435 [00:09<00:00, 44.67it/s][A                                                 
                                                 [A 40%|      | 332/830 [01:58<02:07,  3.90it/s]
100%|| 435/435 [00:09<00:00, 44.67it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:21:03,639 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 10:21:04,098 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:21:07,483 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:21:07,585 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:21:07,637 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-332/special_tokens_map.json
 40%|      | 333/830 [02:04<40:25,  4.88s/it] 40%|      | 334/830 [02:04<28:56,  3.50s/it] 40%|      | 335/830 [02:04<20:54,  2.54s/it] 40%|      | 336/830 [02:05<15:18,  1.86s/it] 41%|      | 337/830 [02:05<11:22,  1.39s/it] 41%|      | 338/830 [02:05<08:37,  1.05s/it] 41%|      | 339/830 [02:06<06:42,  1.22it/s] 41%|      | 340/830 [02:06<05:25,  1.51it/s] 41%|      | 341/830 [02:06<04:28,  1.82it/s] 41%|      | 342/830 [02:06<03:48,  2.14it/s] 41%|     | 343/830 [02:07<03:20,  2.43it/s] 41%|     | 344/830 [02:07<03:00,  2.70it/s] 42%|     | 345/830 [02:07<02:46,  2.91it/s] 42%|     | 346/830 [02:07<02:36,  3.09it/s] 42%|     | 347/830 [02:08<02:29,  3.23it/s] 42%|     | 348/830 [02:08<02:24,  3.33it/s] 42%|     | 349/830 [02:08<02:21,  3.40it/s] 42%|     | 350/830 [02:09<02:18,  3.45it/s] 42%|     | 351/830 [02:09<02:21,  3.38it/s] 42%|     | 352/830 [02:09<02:19,  3.44it/s] 43%|     | 353/830 [02:09<02:16,  3.48it/s] 43%|     | 354/830 [02:10<02:15,  3.51it/s] 43%|     | 355/830 [02:10<02:14,  3.53it/s] 43%|     | 356/830 [02:10<02:13,  3.55it/s] 43%|     | 357/830 [02:11<02:12,  3.57it/s] 43%|     | 358/830 [02:11<02:12,  3.57it/s] 43%|     | 359/830 [02:11<02:11,  3.58it/s] 43%|     | 360/830 [02:11<02:11,  3.59it/s] 43%|     | 361/830 [02:12<02:10,  3.59it/s] 44%|     | 362/830 [02:12<02:14,  3.49it/s] 44%|     | 363/830 [02:12<02:12,  3.52it/s] 44%|     | 364/830 [02:13<02:11,  3.55it/s] 44%|     | 365/830 [02:13<02:10,  3.56it/s] 44%|     | 366/830 [02:13<02:09,  3.57it/s] 44%|     | 367/830 [02:13<02:09,  3.58it/s] 44%|     | 368/830 [02:14<02:08,  3.59it/s] 44%|     | 369/830 [02:14<02:08,  3.59it/s] 45%|     | 370/830 [02:14<02:08,  3.59it/s] 45%|     | 371/830 [02:15<02:08,  3.58it/s] 45%|     | 372/830 [02:15<02:07,  3.58it/s] 45%|     | 373/830 [02:15<02:11,  3.47it/s] 45%|     | 374/830 [02:15<02:09,  3.51it/s] 45%|     | 375/830 [02:16<02:08,  3.53it/s] 45%|     | 376/830 [02:16<02:07,  3.55it/s] 45%|     | 377/830 [02:16<02:07,  3.57it/s] 46%|     | 378/830 [02:16<02:06,  3.57it/s] 46%|     | 379/830 [02:17<02:05,  3.58it/s] 46%|     | 380/830 [02:17<02:05,  3.58it/s] 46%|     | 381/830 [02:17<02:05,  3.59it/s] 46%|     | 382/830 [02:18<02:04,  3.59it/s] 46%|     | 383/830 [02:18<02:04,  3.59it/s] 46%|     | 384/830 [02:18<02:09,  3.46it/s] 46%|     | 385/830 [02:18<02:07,  3.49it/s] 47%|     | 386/830 [02:19<02:06,  3.52it/s] 47%|     | 387/830 [02:19<02:05,  3.53it/s] 47%|     | 388/830 [02:19<02:04,  3.55it/s] 47%|     | 389/830 [02:20<02:03,  3.56it/s] 47%|     | 390/830 [02:20<02:03,  3.56it/s] 47%|     | 391/830 [02:20<02:02,  3.57it/s] 47%|     | 392/830 [02:20<02:02,  3.58it/s] 47%|     | 393/830 [02:21<02:02,  3.58it/s] 47%|     | 394/830 [02:21<02:01,  3.58it/s] 48%|     | 395/830 [02:21<02:02,  3.55it/s] 48%|     | 396/830 [02:22<02:01,  3.56it/s] 48%|     | 397/830 [02:22<02:01,  3.57it/s] 48%|     | 398/830 [02:22<02:00,  3.58it/s] 48%|     | 399/830 [02:22<02:00,  3.59it/s] 48%|     | 400/830 [02:23<02:00,  3.58it/s] 48%|     | 401/830 [02:23<02:00,  3.57it/s] 48%|     | 402/830 [02:23<01:59,  3.58it/s] 49%|     | 403/830 [02:24<01:59,  3.58it/s] 49%|     | 404/830 [02:24<01:58,  3.59it/s] 49%|     | 405/830 [02:24<01:58,  3.59it/s] 49%|     | 406/830 [02:24<02:04,  3.42it/s] 49%|     | 407/830 [02:25<02:01,  3.47it/s] 49%|     | 408/830 [02:25<02:00,  3.51it/s] 49%|     | 409/830 [02:25<01:59,  3.54it/s] 49%|     | 410/830 [02:25<01:58,  3.56it/s] 50%|     | 411/830 [02:26<01:57,  3.57it/s] 50%|     | 412/830 [02:26<01:56,  3.58it/s] 50%|     | 413/830 [02:26<01:56,  3.58it/s] 50%|     | 414/830 [02:27<01:56,  3.58it/s] 50%|     | 415/830 [02:27<01:56,  3.57it/s] 50%|     | 416/830 [02:27<01:55,  3.58it/s] 50%|     | 417/830 [02:27<01:58,  3.47it/s] 50%|     | 418/830 [02:28<01:57,  3.50it/s] 50%|     | 419/830 [02:28<01:56,  3.52it/s] 51%|     | 420/830 [02:28<01:55,  3.54it/s] 51%|     | 421/830 [02:29<01:54,  3.56it/s] 51%|     | 422/830 [02:29<01:54,  3.57it/s] 51%|     | 423/830 [02:29<01:53,  3.57it/s] 51%|     | 424/830 [02:29<01:53,  3.57it/s] 51%|     | 425/830 [02:30<01:53,  3.57it/s] 51%|    | 426/830 [02:30<01:52,  3.58it/s] 51%|    | 427/830 [02:30<01:52,  3.58it/s] 52%|    | 428/830 [02:31<01:52,  3.58it/s] 52%|    | 429/830 [02:31<01:51,  3.58it/s] 52%|    | 430/830 [02:31<01:51,  3.59it/s] 52%|    | 431/830 [02:31<01:51,  3.59it/s] 52%|    | 432/830 [02:32<01:50,  3.59it/s] 52%|    | 433/830 [02:32<01:50,  3.60it/s] 52%|    | 434/830 [02:32<01:50,  3.59it/s] 52%|    | 435/830 [02:33<01:56,  3.38it/s] 53%|    | 436/830 [02:33<01:54,  3.44it/s] 53%|    | 437/830 [02:33<01:52,  3.48it/s] 53%|    | 438/830 [02:33<01:51,  3.52it/s] 53%|    | 439/830 [02:34<01:50,  3.54it/s] 53%|    | 440/830 [02:34<01:49,  3.55it/s] 53%|    | 441/830 [02:34<01:49,  3.56it/s] 53%|    | 442/830 [02:34<01:48,  3.57it/s] 53%|    | 443/830 [02:35<01:48,  3.58it/s] 53%|    | 444/830 [02:35<01:47,  3.58it/s] 54%|    | 445/830 [02:35<01:47,  3.58it/s] 54%|    | 446/830 [02:36<01:52,  3.43it/s] 54%|    | 447/830 [02:36<01:50,  3.48it/s] 54%|    | 448/830 [02:36<01:48,  3.51it/s] 54%|    | 449/830 [02:36<01:47,  3.53it/s] 54%|    | 450/830 [02:37<01:46,  3.55it/s] 54%|    | 451/830 [02:37<01:46,  3.57it/s] 54%|    | 452/830 [02:37<01:45,  3.57it/s] 55%|    | 453/830 [02:38<01:45,  3.57it/s] 55%|    | 454/830 [02:38<01:45,  3.58it/s] 55%|    | 455/830 [02:38<01:44,  3.58it/s] 55%|    | 456/830 [02:38<01:44,  3.59it/s] 55%|    | 457/830 [02:39<01:51,  3.34it/s] 55%|    | 458/830 [02:39<01:49,  3.40it/s] 55%|    | 459/830 [02:39<01:47,  3.46it/s] 55%|    | 460/830 [02:40<01:45,  3.50it/s] 56%|    | 461/830 [02:40<01:44,  3.53it/s] 56%|    | 462/830 [02:40<01:43,  3.54it/s] 56%|    | 463/830 [02:40<01:43,  3.56it/s] 56%|    | 464/830 [02:41<01:42,  3.57it/s] 56%|    | 465/830 [02:41<01:42,  3.58it/s] 56%|    | 466/830 [02:41<01:41,  3.58it/s] 56%|    | 467/830 [02:42<01:41,  3.58it/s] 56%|    | 468/830 [02:42<01:45,  3.42it/s] 57%|    | 469/830 [02:42<01:44,  3.47it/s] 57%|    | 470/830 [02:42<01:42,  3.51it/s] 57%|    | 471/830 [02:43<01:41,  3.53it/s] 57%|    | 472/830 [02:43<01:40,  3.55it/s] 57%|    | 473/830 [02:43<01:40,  3.56it/s] 57%|    | 474/830 [02:44<01:41,  3.51it/s] 57%|    | 475/830 [02:44<01:40,  3.53it/s] 57%|    | 476/830 [02:44<01:39,  3.55it/s] 57%|    | 477/830 [02:44<01:39,  3.56it/s] 58%|    | 478/830 [02:45<01:38,  3.57it/s] 58%|    | 479/830 [02:45<01:40,  3.50it/s] 58%|    | 480/830 [02:45<01:39,  3.52it/s] 58%|    | 481/830 [02:46<01:38,  3.53it/s] 58%|    | 482/830 [02:46<01:56,  2.99it/s] 58%|    | 483/830 [02:46<01:50,  3.14it/s] 58%|    | 484/830 [02:47<01:45,  3.27it/s] 58%|    | 485/830 [02:47<01:42,  3.35it/s] 59%|    | 486/830 [02:47<01:40,  3.42it/s] 59%|    | 487/830 [02:47<01:38,  3.47it/s] 59%|    | 488/830 [02:48<01:37,  3.51it/s] 59%|    | 489/830 [02:48<01:36,  3.53it/s] 59%|    | 490/830 [02:48<01:38,  3.46it/s] 59%|    | 491/830 [02:49<01:36,  3.50it/s] 59%|    | 492/830 [02:49<01:35,  3.53it/s] 59%|    | 493/830 [02:49<01:34,  3.55it/s] 60%|    | 494/830 [02:49<01:34,  3.56it/s] 60%|    | 495/830 [02:50<01:33,  3.57it/s] 60%|    | 496/830 [02:50<01:33,  3.57it/s] 60%|    | 497/830 [02:50<01:33,  3.58it/s] 60%|    | 498/830 [02:50<01:23,  3.97it/s][INFO|trainer.py:2140] 2023-08-28 10:21:55,556 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:21:55,556 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 10:21:55,556 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7802, 'eval_samples_per_second': 355.513, 'eval_steps_per_second': 44.478, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.91it/s][A
  3%|         | 12/435 [00:00<00:08, 48.92it/s][A
  4%|         | 17/435 [00:00<00:08, 47.24it/s][A
  5%|         | 22/435 [00:00<00:08, 46.44it/s][A
  6%|         | 27/435 [00:00<00:08, 45.79it/s][A
  7%|         | 32/435 [00:00<00:09, 42.55it/s][A
  9%|         | 37/435 [00:00<00:09, 43.16it/s][A
 10%|         | 42/435 [00:00<00:09, 43.40it/s][A
 11%|         | 47/435 [00:01<00:08, 43.84it/s][A
 12%|        | 52/435 [00:01<00:08, 44.19it/s][A
 13%|        | 57/435 [00:01<00:08, 44.39it/s][A
 14%|        | 62/435 [00:01<00:08, 44.52it/s][A
 15%|        | 67/435 [00:01<00:08, 44.63it/s][A
 17%|        | 72/435 [00:01<00:08, 44.48it/s][A
 18%|        | 77/435 [00:01<00:08, 44.45it/s][A
 19%|        | 82/435 [00:01<00:07, 44.55it/s][A
 20%|        | 87/435 [00:01<00:07, 44.50it/s][A
 21%|        | 92/435 [00:02<00:07, 44.49it/s][A
 22%|       | 97/435 [00:02<00:07, 44.57it/s][A
 23%|       | 102/435 [00:02<00:07, 44.64it/s][A
 25%|       | 107/435 [00:02<00:07, 44.73it/s][A
 26%|       | 112/435 [00:02<00:07, 44.69it/s][A
 27%|       | 117/435 [00:02<00:07, 44.81it/s][A
 28%|       | 122/435 [00:02<00:06, 44.77it/s][A
 29%|       | 127/435 [00:02<00:06, 44.81it/s][A
 30%|       | 132/435 [00:02<00:06, 44.77it/s][A
 31%|      | 137/435 [00:03<00:06, 44.65it/s][A
 33%|      | 142/435 [00:03<00:06, 44.65it/s][A
 34%|      | 147/435 [00:03<00:06, 44.64it/s][A
 35%|      | 152/435 [00:03<00:06, 44.77it/s][A
 36%|      | 157/435 [00:03<00:06, 44.74it/s][A
 37%|      | 162/435 [00:03<00:06, 44.66it/s][A
 38%|      | 167/435 [00:03<00:06, 42.85it/s][A
 40%|      | 172/435 [00:03<00:06, 43.52it/s][A
 41%|      | 177/435 [00:03<00:05, 43.93it/s][A
 42%|     | 182/435 [00:04<00:05, 44.06it/s][A
 43%|     | 187/435 [00:04<00:05, 44.26it/s][A
 44%|     | 192/435 [00:04<00:05, 44.34it/s][A
 45%|     | 197/435 [00:04<00:05, 44.50it/s][A
 46%|     | 202/435 [00:04<00:05, 44.64it/s][A
 48%|     | 207/435 [00:04<00:05, 44.40it/s][A
 49%|     | 212/435 [00:04<00:05, 44.51it/s][A
 50%|     | 217/435 [00:04<00:04, 44.63it/s][A
 51%|     | 222/435 [00:04<00:04, 44.74it/s][A
 52%|    | 227/435 [00:05<00:04, 44.79it/s][A
 53%|    | 232/435 [00:05<00:04, 44.63it/s][A
 54%|    | 237/435 [00:05<00:04, 44.62it/s][A
 56%|    | 242/435 [00:05<00:04, 44.60it/s][A
 57%|    | 247/435 [00:05<00:04, 44.54it/s][A
 58%|    | 252/435 [00:05<00:04, 44.65it/s][A
 59%|    | 257/435 [00:05<00:03, 44.76it/s][A
 60%|    | 262/435 [00:05<00:03, 44.75it/s][A
 61%|   | 267/435 [00:05<00:03, 44.80it/s][A
 63%|   | 272/435 [00:06<00:03, 44.73it/s][A
 64%|   | 277/435 [00:06<00:03, 44.66it/s][A
 65%|   | 282/435 [00:06<00:03, 44.51it/s][A
 66%|   | 287/435 [00:06<00:03, 44.54it/s][A
 67%|   | 292/435 [00:06<00:03, 44.54it/s][A
 68%|   | 297/435 [00:06<00:03, 44.68it/s][A
 69%|   | 302/435 [00:06<00:03, 43.90it/s][A
 71%|   | 307/435 [00:06<00:02, 44.24it/s][A
 72%|  | 312/435 [00:06<00:02, 44.37it/s][A
 73%|  | 317/435 [00:07<00:02, 44.55it/s][A
 74%|  | 322/435 [00:07<00:02, 44.56it/s][A
 75%|  | 327/435 [00:07<00:02, 44.65it/s][A
 76%|  | 332/435 [00:07<00:02, 44.57it/s][A
 77%|  | 337/435 [00:07<00:02, 44.50it/s][A
 79%|  | 342/435 [00:07<00:02, 44.52it/s][A
 80%|  | 347/435 [00:07<00:01, 44.53it/s][A
 81%|  | 352/435 [00:07<00:01, 44.68it/s][A
 82%| | 357/435 [00:08<00:01, 44.74it/s][A
 83%| | 362/435 [00:08<00:01, 44.77it/s][A
 84%| | 367/435 [00:08<00:01, 44.65it/s][A
 86%| | 372/435 [00:08<00:01, 44.64it/s][A
 87%| | 377/435 [00:08<00:01, 44.62it/s][A
 88%| | 382/435 [00:08<00:01, 44.51it/s][A
 89%| | 387/435 [00:08<00:01, 44.52it/s][A
 90%| | 392/435 [00:08<00:00, 44.65it/s][A
 91%|| 397/435 [00:08<00:00, 44.59it/s][A
 92%|| 402/435 [00:09<00:00, 44.74it/s][A
 94%|| 407/435 [00:09<00:00, 44.77it/s][A
 95%|| 412/435 [00:09<00:00, 44.71it/s][A
 96%|| 417/435 [00:09<00:00, 44.68it/s][A
 97%|| 422/435 [00:09<00:00, 44.59it/s][A
 98%|| 427/435 [00:09<00:00, 44.56it/s][A
 99%|| 432/435 [00:09<00:00, 44.56it/s][A                                                 
                                                 [A 60%|    | 498/830 [03:00<01:23,  3.97it/s]
100%|| 435/435 [00:09<00:00, 44.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:22:05,549 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-498
[INFO|configuration_utils.py:351] 2023-08-28 10:22:05,723 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-498/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:22:08,919 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-498/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:22:09,006 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-498/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:22:09,055 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-498/special_tokens_map.json
 60%|    | 499/830 [03:05<25:24,  4.61s/it] 60%|    | 500/830 [03:05<18:11,  3.31s/it]                                                  60%|    | 500/830 [03:05<18:11,  3.31s/it] 60%|    | 501/830 [03:06<13:09,  2.40s/it] 60%|    | 502/830 [03:06<09:39,  1.77s/it] 61%|    | 503/830 [03:06<07:11,  1.32s/it] 61%|    | 504/830 [03:07<05:28,  1.01s/it] 61%|    | 505/830 [03:07<04:16,  1.27it/s] 61%|    | 506/830 [03:07<03:30,  1.54it/s] 61%|    | 507/830 [03:07<02:53,  1.86it/s] 61%|    | 508/830 [03:08<02:28,  2.17it/s] 61%|   | 509/830 [03:08<02:10,  2.47it/s] 61%|   | 510/830 [03:08<01:57,  2.72it/s] 62%|   | 511/830 [03:09<01:48,  2.94it/s] 62%|   | 512/830 [03:09<01:42,  3.10it/s] 62%|   | 513/830 [03:09<01:37,  3.24it/s] 62%|   | 514/830 [03:09<01:34,  3.34it/s] 62%|   | 515/830 [03:10<01:32,  3.41it/s] 62%|   | 516/830 [03:10<01:30,  3.46it/s] 62%|   | 517/830 [03:10<01:33,  3.35it/s] 62%|   | 518/830 [03:11<01:31,  3.42it/s] 63%|   | 519/830 [03:11<01:29,  3.47it/s] 63%|   | 520/830 [03:11<01:28,  3.51it/s] 63%|   | 521/830 [03:11<01:27,  3.53it/s] 63%|   | 522/830 [03:12<01:26,  3.55it/s] 63%|   | 523/830 [03:12<01:26,  3.56it/s] 63%|   | 524/830 [03:12<01:25,  3.57it/s] 63%|   | 525/830 [03:13<01:25,  3.57it/s] 63%|   | 526/830 [03:13<01:24,  3.58it/s] 63%|   | 527/830 [03:13<01:24,  3.58it/s] 64%|   | 528/830 [03:13<01:26,  3.50it/s] 64%|   | 529/830 [03:14<01:25,  3.53it/s] 64%|   | 530/830 [03:14<01:24,  3.55it/s] 64%|   | 531/830 [03:14<01:24,  3.55it/s] 64%|   | 532/830 [03:14<01:23,  3.55it/s] 64%|   | 533/830 [03:15<01:23,  3.57it/s] 64%|   | 534/830 [03:15<01:22,  3.58it/s] 64%|   | 535/830 [03:15<01:22,  3.58it/s] 65%|   | 536/830 [03:16<01:22,  3.58it/s] 65%|   | 537/830 [03:16<01:21,  3.59it/s] 65%|   | 538/830 [03:16<01:21,  3.59it/s] 65%|   | 539/830 [03:16<01:22,  3.52it/s] 65%|   | 540/830 [03:17<01:22,  3.54it/s] 65%|   | 541/830 [03:17<01:21,  3.55it/s] 65%|   | 542/830 [03:17<01:20,  3.57it/s] 65%|   | 543/830 [03:18<01:20,  3.57it/s] 66%|   | 544/830 [03:18<01:19,  3.58it/s] 66%|   | 545/830 [03:18<01:19,  3.58it/s] 66%|   | 546/830 [03:18<01:19,  3.59it/s] 66%|   | 547/830 [03:19<01:18,  3.59it/s] 66%|   | 548/830 [03:19<01:18,  3.59it/s] 66%|   | 549/830 [03:19<01:18,  3.59it/s] 66%|   | 550/830 [03:20<01:19,  3.53it/s] 66%|   | 551/830 [03:20<01:18,  3.55it/s] 67%|   | 552/830 [03:20<01:18,  3.56it/s] 67%|   | 553/830 [03:20<01:17,  3.57it/s] 67%|   | 554/830 [03:21<01:17,  3.57it/s] 67%|   | 555/830 [03:21<01:16,  3.58it/s] 67%|   | 556/830 [03:21<01:16,  3.58it/s] 67%|   | 557/830 [03:21<01:16,  3.59it/s] 67%|   | 558/830 [03:22<01:15,  3.58it/s] 67%|   | 559/830 [03:22<01:15,  3.59it/s] 67%|   | 560/830 [03:22<01:15,  3.59it/s] 68%|   | 561/830 [03:23<01:16,  3.50it/s] 68%|   | 562/830 [03:23<01:15,  3.53it/s] 68%|   | 563/830 [03:23<01:15,  3.54it/s] 68%|   | 564/830 [03:23<01:14,  3.56it/s] 68%|   | 565/830 [03:24<01:14,  3.57it/s] 68%|   | 566/830 [03:24<01:13,  3.58it/s] 68%|   | 567/830 [03:24<01:13,  3.59it/s] 68%|   | 568/830 [03:25<01:13,  3.58it/s] 69%|   | 569/830 [03:25<01:12,  3.58it/s] 69%|   | 570/830 [03:25<01:12,  3.58it/s] 69%|   | 571/830 [03:25<01:12,  3.59it/s] 69%|   | 572/830 [03:26<01:15,  3.42it/s] 69%|   | 573/830 [03:26<01:14,  3.47it/s] 69%|   | 574/830 [03:26<01:13,  3.50it/s] 69%|   | 575/830 [03:27<01:12,  3.53it/s] 69%|   | 576/830 [03:27<01:11,  3.55it/s] 70%|   | 577/830 [03:27<01:11,  3.55it/s] 70%|   | 578/830 [03:27<01:10,  3.56it/s] 70%|   | 579/830 [03:28<01:10,  3.57it/s] 70%|   | 580/830 [03:28<01:09,  3.58it/s] 70%|   | 581/830 [03:28<01:09,  3.58it/s] 70%|   | 582/830 [03:29<01:09,  3.58it/s] 70%|   | 583/830 [03:29<01:10,  3.52it/s] 70%|   | 584/830 [03:29<01:09,  3.54it/s] 70%|   | 585/830 [03:29<01:08,  3.56it/s] 71%|   | 586/830 [03:30<01:08,  3.57it/s] 71%|   | 587/830 [03:30<01:07,  3.57it/s] 71%|   | 588/830 [03:30<01:07,  3.58it/s] 71%|   | 589/830 [03:30<01:07,  3.57it/s] 71%|   | 590/830 [03:31<01:07,  3.58it/s] 71%|   | 591/830 [03:31<01:06,  3.58it/s] 71%|  | 592/830 [03:31<01:06,  3.58it/s] 71%|  | 593/830 [03:32<01:06,  3.58it/s] 72%|  | 594/830 [03:32<01:05,  3.58it/s] 72%|  | 595/830 [03:32<01:05,  3.58it/s] 72%|  | 596/830 [03:32<01:05,  3.58it/s] 72%|  | 597/830 [03:33<01:04,  3.59it/s] 72%|  | 598/830 [03:33<01:04,  3.59it/s] 72%|  | 599/830 [03:33<01:04,  3.59it/s] 72%|  | 600/830 [03:34<01:04,  3.59it/s] 72%|  | 601/830 [03:34<01:06,  3.42it/s] 73%|  | 602/830 [03:34<01:05,  3.48it/s] 73%|  | 603/830 [03:34<01:04,  3.51it/s] 73%|  | 604/830 [03:35<01:03,  3.53it/s] 73%|  | 605/830 [03:35<01:03,  3.55it/s] 73%|  | 606/830 [03:35<01:02,  3.57it/s] 73%|  | 607/830 [03:36<01:02,  3.58it/s] 73%|  | 608/830 [03:36<01:01,  3.58it/s] 73%|  | 609/830 [03:36<01:01,  3.58it/s] 73%|  | 610/830 [03:36<01:01,  3.58it/s] 74%|  | 611/830 [03:37<01:01,  3.59it/s] 74%|  | 612/830 [03:37<01:02,  3.46it/s] 74%|  | 613/830 [03:37<01:02,  3.50it/s] 74%|  | 614/830 [03:38<01:01,  3.53it/s] 74%|  | 615/830 [03:38<01:00,  3.55it/s] 74%|  | 616/830 [03:38<01:00,  3.56it/s] 74%|  | 617/830 [03:38<00:59,  3.57it/s] 74%|  | 618/830 [03:39<00:59,  3.57it/s] 75%|  | 619/830 [03:39<00:58,  3.58it/s] 75%|  | 620/830 [03:39<00:58,  3.58it/s] 75%|  | 621/830 [03:39<00:58,  3.59it/s] 75%|  | 622/830 [03:40<00:58,  3.58it/s] 75%|  | 623/830 [03:40<01:01,  3.36it/s] 75%|  | 624/830 [03:40<01:00,  3.42it/s] 75%|  | 625/830 [03:41<00:59,  3.47it/s] 75%|  | 626/830 [03:41<00:58,  3.51it/s] 76%|  | 627/830 [03:41<00:57,  3.54it/s] 76%|  | 628/830 [03:41<00:56,  3.55it/s] 76%|  | 629/830 [03:42<00:56,  3.57it/s] 76%|  | 630/830 [03:42<00:55,  3.57it/s] 76%|  | 631/830 [03:42<00:55,  3.58it/s] 76%|  | 632/830 [03:43<00:55,  3.58it/s] 76%|  | 633/830 [03:43<00:54,  3.59it/s] 76%|  | 634/830 [03:43<00:57,  3.43it/s] 77%|  | 635/830 [03:43<00:56,  3.47it/s] 77%|  | 636/830 [03:44<00:56,  3.45it/s] 77%|  | 637/830 [03:44<00:55,  3.49it/s] 77%|  | 638/830 [03:44<00:54,  3.52it/s] 77%|  | 639/830 [03:45<00:53,  3.54it/s] 77%|  | 640/830 [03:45<00:53,  3.56it/s] 77%|  | 641/830 [03:45<00:52,  3.57it/s] 77%|  | 642/830 [03:45<00:52,  3.58it/s] 77%|  | 643/830 [03:46<00:52,  3.58it/s] 78%|  | 644/830 [03:46<01:02,  2.98it/s] 78%|  | 645/830 [03:46<01:00,  3.07it/s] 78%|  | 646/830 [03:47<00:57,  3.21it/s] 78%|  | 647/830 [03:47<00:55,  3.32it/s] 78%|  | 648/830 [03:47<00:53,  3.39it/s] 78%|  | 649/830 [03:48<00:52,  3.44it/s] 78%|  | 650/830 [03:48<00:51,  3.49it/s] 78%|  | 651/830 [03:48<00:50,  3.52it/s] 79%|  | 652/830 [03:48<00:50,  3.54it/s] 79%|  | 653/830 [03:49<00:49,  3.55it/s] 79%|  | 654/830 [03:49<00:49,  3.56it/s] 79%|  | 655/830 [03:49<00:49,  3.56it/s] 79%|  | 656/830 [03:50<00:49,  3.50it/s] 79%|  | 657/830 [03:50<00:49,  3.53it/s] 79%|  | 658/830 [03:50<00:48,  3.55it/s] 79%|  | 659/830 [03:50<00:48,  3.56it/s] 80%|  | 660/830 [03:51<00:47,  3.57it/s] 80%|  | 661/830 [03:51<00:47,  3.57it/s] 80%|  | 662/830 [03:51<00:47,  3.57it/s] 80%|  | 663/830 [03:52<00:46,  3.58it/s] 80%|  | 664/830 [03:52<00:41,  3.97it/s][INFO|trainer.py:2140] 2023-08-28 10:22:56,872 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:22:56,872 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 10:22:56,872 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7726, 'eval_samples_per_second': 355.791, 'eval_steps_per_second': 44.512, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.2409638554216866e-05, 'epoch': 3.01}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.78it/s][A
  3%|         | 12/435 [00:00<00:08, 48.66it/s][A
  4%|         | 17/435 [00:00<00:08, 47.11it/s][A
  5%|         | 22/435 [00:00<00:08, 46.19it/s][A
  6%|         | 27/435 [00:00<00:08, 45.68it/s][A
  7%|         | 32/435 [00:00<00:09, 43.45it/s][A
  9%|         | 37/435 [00:00<00:09, 43.83it/s][A
 10%|         | 42/435 [00:00<00:08, 43.94it/s][A
 11%|         | 47/435 [00:01<00:08, 44.19it/s][A
 12%|        | 52/435 [00:01<00:08, 44.43it/s][A
 13%|        | 57/435 [00:01<00:08, 44.60it/s][A
 14%|        | 62/435 [00:01<00:08, 44.66it/s][A
 15%|        | 67/435 [00:01<00:08, 44.64it/s][A
 17%|        | 72/435 [00:01<00:08, 44.53it/s][A
 18%|        | 77/435 [00:01<00:08, 44.44it/s][A
 19%|        | 82/435 [00:01<00:07, 44.43it/s][A
 20%|        | 87/435 [00:01<00:07, 44.49it/s][A
 21%|        | 92/435 [00:02<00:07, 44.54it/s][A
 22%|       | 97/435 [00:02<00:07, 44.69it/s][A
 23%|       | 102/435 [00:02<00:07, 44.78it/s][A
 25%|       | 107/435 [00:02<00:07, 44.82it/s][A
 26%|       | 112/435 [00:02<00:07, 44.77it/s][A
 27%|       | 117/435 [00:02<00:07, 44.74it/s][A
 28%|       | 122/435 [00:02<00:06, 44.76it/s][A
 29%|       | 127/435 [00:02<00:06, 44.65it/s][A
 30%|       | 132/435 [00:02<00:06, 44.60it/s][A
 31%|      | 137/435 [00:03<00:06, 44.66it/s][A
 33%|      | 142/435 [00:03<00:06, 44.63it/s][A
 34%|      | 147/435 [00:03<00:06, 44.73it/s][A
 35%|      | 152/435 [00:03<00:06, 44.78it/s][A
 36%|      | 157/435 [00:03<00:06, 44.76it/s][A
 37%|      | 162/435 [00:03<00:06, 44.83it/s][A
 38%|      | 167/435 [00:03<00:06, 43.60it/s][A
 40%|      | 172/435 [00:03<00:05, 43.94it/s][A
 41%|      | 177/435 [00:03<00:05, 44.14it/s][A
 42%|     | 182/435 [00:04<00:05, 44.29it/s][A
 43%|     | 187/435 [00:04<00:05, 44.48it/s][A
 44%|     | 192/435 [00:04<00:05, 44.48it/s][A
 45%|     | 197/435 [00:04<00:05, 44.51it/s][A
 46%|     | 202/435 [00:04<00:05, 44.61it/s][A
 48%|     | 207/435 [00:04<00:05, 44.51it/s][A
 49%|     | 212/435 [00:04<00:05, 44.56it/s][A
 50%|     | 217/435 [00:04<00:04, 44.54it/s][A
 51%|     | 222/435 [00:04<00:04, 44.60it/s][A
 52%|    | 227/435 [00:05<00:04, 44.71it/s][A
 53%|    | 232/435 [00:05<00:04, 44.68it/s][A
 54%|    | 237/435 [00:05<00:04, 44.78it/s][A
 56%|    | 242/435 [00:05<00:04, 44.78it/s][A
 57%|    | 247/435 [00:05<00:04, 44.68it/s][A
 58%|    | 252/435 [00:05<00:04, 44.77it/s][A
 59%|    | 257/435 [00:05<00:03, 44.75it/s][A
 60%|    | 262/435 [00:05<00:03, 44.62it/s][A
 61%|   | 267/435 [00:05<00:03, 44.67it/s][A
 63%|   | 272/435 [00:06<00:03, 44.66it/s][A
 64%|   | 277/435 [00:06<00:03, 44.78it/s][A
 65%|   | 282/435 [00:06<00:03, 44.75it/s][A
 66%|   | 287/435 [00:06<00:03, 44.56it/s][A
 67%|   | 292/435 [00:06<00:03, 44.63it/s][A
 68%|   | 297/435 [00:06<00:03, 44.62it/s][A
 69%|   | 302/435 [00:06<00:03, 42.75it/s][A
 71%|   | 307/435 [00:06<00:02, 43.34it/s][A
 72%|  | 312/435 [00:06<00:02, 43.83it/s][A
 73%|  | 317/435 [00:07<00:02, 44.19it/s][A
 74%|  | 322/435 [00:07<00:02, 44.34it/s][A
 75%|  | 327/435 [00:07<00:02, 44.39it/s][A
 76%|  | 332/435 [00:07<00:02, 44.53it/s][A
 77%|  | 337/435 [00:07<00:02, 44.58it/s][A
 79%|  | 342/435 [00:07<00:02, 44.36it/s][A
 80%|  | 347/435 [00:07<00:01, 44.46it/s][A
 81%|  | 352/435 [00:07<00:01, 44.55it/s][A
 82%| | 357/435 [00:08<00:01, 44.60it/s][A
 83%| | 362/435 [00:08<00:01, 44.74it/s][A
 84%| | 367/435 [00:08<00:01, 44.81it/s][A
 86%| | 372/435 [00:08<00:01, 44.74it/s][A
 87%| | 377/435 [00:08<00:01, 44.74it/s][A
 88%| | 382/435 [00:08<00:01, 44.62it/s][A
 89%| | 387/435 [00:08<00:01, 44.72it/s][A
 90%| | 392/435 [00:08<00:00, 44.72it/s][A
 91%|| 397/435 [00:08<00:00, 44.80it/s][A
 92%|| 402/435 [00:09<00:00, 44.75it/s][A
 94%|| 407/435 [00:09<00:00, 44.80it/s][A
 95%|| 412/435 [00:09<00:00, 44.71it/s][A
 96%|| 417/435 [00:09<00:00, 44.74it/s][A
 97%|| 422/435 [00:09<00:00, 44.80it/s][A
 98%|| 427/435 [00:09<00:00, 44.73it/s][A
 99%|| 432/435 [00:09<00:00, 44.66it/s][A                                                 
                                                 [A 80%|  | 664/830 [04:01<00:41,  3.97it/s]
100%|| 435/435 [00:09<00:00, 44.66it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:23:06,864 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-664
[INFO|configuration_utils.py:351] 2023-08-28 10:23:07,047 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-664/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:23:10,373 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-664/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:23:10,520 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-664/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:23:10,597 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-664/special_tokens_map.json
 80%|  | 665/830 [04:07<12:47,  4.65s/it] 80%|  | 666/830 [04:07<09:07,  3.34s/it] 80%|  | 667/830 [04:07<06:34,  2.42s/it] 80%|  | 668/830 [04:07<04:48,  1.78s/it] 81%|  | 669/830 [04:08<03:34,  1.33s/it] 81%|  | 670/830 [04:08<02:42,  1.02s/it] 81%|  | 671/830 [04:08<02:06,  1.26it/s] 81%|  | 672/830 [04:09<01:42,  1.53it/s] 81%|  | 673/830 [04:09<01:24,  1.85it/s] 81%|  | 674/830 [04:09<01:12,  2.16it/s] 81%| | 675/830 [04:09<01:03,  2.45it/s] 81%| | 676/830 [04:10<00:57,  2.70it/s] 82%| | 677/830 [04:10<00:52,  2.90it/s] 82%| | 678/830 [04:10<00:49,  3.07it/s] 82%| | 679/830 [04:11<00:47,  3.20it/s] 82%| | 680/830 [04:11<00:45,  3.29it/s] 82%| | 681/830 [04:11<00:44,  3.36it/s] 82%| | 682/830 [04:11<00:43,  3.41it/s] 82%| | 683/830 [04:12<00:43,  3.34it/s] 82%| | 684/830 [04:12<00:42,  3.40it/s] 83%| | 685/830 [04:12<00:42,  3.44it/s] 83%| | 686/830 [04:13<00:41,  3.47it/s] 83%| | 687/830 [04:13<00:41,  3.49it/s] 83%| | 688/830 [04:13<00:40,  3.50it/s] 83%| | 689/830 [04:13<00:40,  3.52it/s] 83%| | 690/830 [04:14<00:39,  3.53it/s] 83%| | 691/830 [04:14<00:39,  3.53it/s] 83%| | 692/830 [04:14<00:39,  3.53it/s] 83%| | 693/830 [04:15<00:38,  3.53it/s] 84%| | 694/830 [04:15<00:39,  3.44it/s] 84%| | 695/830 [04:15<00:38,  3.47it/s] 84%| | 696/830 [04:15<00:38,  3.49it/s] 84%| | 697/830 [04:16<00:37,  3.51it/s] 84%| | 698/830 [04:16<00:37,  3.52it/s] 84%| | 699/830 [04:16<00:37,  3.52it/s] 84%| | 700/830 [04:17<00:36,  3.53it/s] 84%| | 701/830 [04:17<00:36,  3.53it/s] 85%| | 702/830 [04:17<00:36,  3.54it/s] 85%| | 703/830 [04:17<00:35,  3.54it/s] 85%| | 704/830 [04:18<00:35,  3.55it/s] 85%| | 705/830 [04:18<00:36,  3.45it/s] 85%| | 706/830 [04:18<00:35,  3.50it/s] 85%| | 707/830 [04:19<00:34,  3.53it/s] 85%| | 708/830 [04:19<00:34,  3.55it/s] 85%| | 709/830 [04:19<00:33,  3.56it/s] 86%| | 710/830 [04:19<00:33,  3.57it/s] 86%| | 711/830 [04:20<00:33,  3.58it/s] 86%| | 712/830 [04:20<00:32,  3.59it/s] 86%| | 713/830 [04:20<00:32,  3.59it/s] 86%| | 714/830 [04:21<00:32,  3.59it/s] 86%| | 715/830 [04:21<00:32,  3.59it/s] 86%| | 716/830 [04:21<00:32,  3.50it/s] 86%| | 717/830 [04:21<00:32,  3.52it/s] 87%| | 718/830 [04:22<00:31,  3.55it/s] 87%| | 719/830 [04:22<00:31,  3.56it/s] 87%| | 720/830 [04:22<00:30,  3.57it/s] 87%| | 721/830 [04:23<00:30,  3.58it/s] 87%| | 722/830 [04:23<00:30,  3.58it/s] 87%| | 723/830 [04:23<00:29,  3.58it/s] 87%| | 724/830 [04:23<00:29,  3.59it/s] 87%| | 725/830 [04:24<00:29,  3.59it/s] 87%| | 726/830 [04:24<00:28,  3.60it/s] 88%| | 727/830 [04:24<00:29,  3.50it/s] 88%| | 728/830 [04:24<00:28,  3.53it/s] 88%| | 729/830 [04:25<00:28,  3.55it/s] 88%| | 730/830 [04:25<00:28,  3.56it/s] 88%| | 731/830 [04:25<00:27,  3.57it/s] 88%| | 732/830 [04:26<00:27,  3.58it/s] 88%| | 733/830 [04:26<00:27,  3.58it/s] 88%| | 734/830 [04:26<00:26,  3.59it/s] 89%| | 735/830 [04:26<00:26,  3.59it/s] 89%| | 736/830 [04:27<00:26,  3.60it/s] 89%| | 737/830 [04:27<00:25,  3.60it/s] 89%| | 738/830 [04:27<00:26,  3.43it/s] 89%| | 739/830 [04:28<00:26,  3.48it/s] 89%| | 740/830 [04:28<00:25,  3.51it/s] 89%| | 741/830 [04:28<00:25,  3.54it/s] 89%| | 742/830 [04:28<00:24,  3.56it/s] 90%| | 743/830 [04:29<00:24,  3.57it/s] 90%| | 744/830 [04:29<00:24,  3.58it/s] 90%| | 745/830 [04:29<00:23,  3.58it/s] 90%| | 746/830 [04:30<00:23,  3.59it/s] 90%| | 747/830 [04:30<00:23,  3.59it/s] 90%| | 748/830 [04:30<00:22,  3.60it/s] 90%| | 749/830 [04:30<00:23,  3.48it/s] 90%| | 750/830 [04:31<00:22,  3.52it/s] 90%| | 751/830 [04:31<00:22,  3.54it/s] 91%| | 752/830 [04:31<00:21,  3.56it/s] 91%| | 753/830 [04:32<00:21,  3.57it/s] 91%| | 754/830 [04:32<00:21,  3.58it/s] 91%| | 755/830 [04:32<00:20,  3.59it/s] 91%| | 756/830 [04:32<00:20,  3.59it/s] 91%| | 757/830 [04:33<00:20,  3.59it/s] 91%|| 758/830 [04:33<00:20,  3.59it/s] 91%|| 759/830 [04:33<00:19,  3.59it/s] 92%|| 760/830 [04:33<00:19,  3.59it/s] 92%|| 761/830 [04:34<00:19,  3.60it/s] 92%|| 762/830 [04:34<00:18,  3.60it/s] 92%|| 763/830 [04:34<00:18,  3.60it/s] 92%|| 764/830 [04:35<00:18,  3.60it/s] 92%|| 765/830 [04:35<00:18,  3.60it/s] 92%|| 766/830 [04:35<00:17,  3.60it/s] 92%|| 767/830 [04:35<00:18,  3.47it/s] 93%|| 768/830 [04:36<00:17,  3.51it/s] 93%|| 769/830 [04:36<00:17,  3.54it/s] 93%|| 770/830 [04:36<00:16,  3.56it/s] 93%|| 771/830 [04:37<00:16,  3.57it/s] 93%|| 772/830 [04:37<00:16,  3.58it/s] 93%|| 773/830 [04:37<00:15,  3.58it/s] 93%|| 774/830 [04:37<00:15,  3.59it/s] 93%|| 775/830 [04:38<00:15,  3.59it/s] 93%|| 776/830 [04:38<00:15,  3.59it/s] 94%|| 777/830 [04:38<00:14,  3.59it/s] 94%|| 778/830 [04:39<00:15,  3.42it/s] 94%|| 779/830 [04:39<00:14,  3.47it/s] 94%|| 780/830 [04:39<00:14,  3.51it/s] 94%|| 781/830 [04:39<00:13,  3.53it/s] 94%|| 782/830 [04:40<00:13,  3.55it/s] 94%|| 783/830 [04:40<00:13,  3.57it/s] 94%|| 784/830 [04:40<00:12,  3.58it/s] 95%|| 785/830 [04:40<00:12,  3.58it/s] 95%|| 786/830 [04:41<00:12,  3.59it/s] 95%|| 787/830 [04:41<00:11,  3.59it/s] 95%|| 788/830 [04:41<00:11,  3.59it/s] 95%|| 789/830 [04:42<00:12,  3.40it/s] 95%|| 790/830 [04:42<00:11,  3.46it/s] 95%|| 791/830 [04:42<00:11,  3.50it/s] 95%|| 792/830 [04:42<00:10,  3.53it/s] 96%|| 793/830 [04:43<00:10,  3.55it/s] 96%|| 794/830 [04:43<00:10,  3.56it/s] 96%|| 795/830 [04:43<00:09,  3.57it/s] 96%|| 796/830 [04:44<00:09,  3.58it/s] 96%|| 797/830 [04:44<00:09,  3.43it/s] 96%|| 798/830 [04:44<00:09,  3.45it/s] 96%|| 799/830 [04:44<00:08,  3.50it/s] 96%|| 800/830 [04:45<00:08,  3.34it/s] 97%|| 801/830 [04:45<00:08,  3.42it/s] 97%|| 802/830 [04:45<00:08,  3.47it/s] 97%|| 803/830 [04:46<00:07,  3.51it/s] 97%|| 804/830 [04:46<00:07,  3.53it/s] 97%|| 805/830 [04:47<00:10,  2.39it/s] 97%|| 806/830 [04:47<00:09,  2.65it/s] 97%|| 807/830 [04:47<00:08,  2.87it/s] 97%|| 808/830 [04:47<00:07,  3.06it/s] 97%|| 809/830 [04:48<00:06,  3.12it/s] 98%|| 810/830 [04:48<00:06,  3.24it/s] 98%|| 811/830 [04:48<00:05,  3.34it/s] 98%|| 812/830 [04:49<00:05,  3.41it/s] 98%|| 813/830 [04:49<00:04,  3.46it/s] 98%|| 814/830 [04:49<00:04,  3.50it/s] 98%|| 815/830 [04:49<00:04,  3.53it/s] 98%|| 816/830 [04:50<00:03,  3.55it/s] 98%|| 817/830 [04:50<00:03,  3.57it/s] 99%|| 818/830 [04:50<00:03,  3.58it/s] 99%|| 819/830 [04:51<00:03,  3.58it/s] 99%|| 820/830 [04:51<00:02,  3.50it/s] 99%|| 821/830 [04:51<00:02,  3.53it/s] 99%|| 822/830 [04:51<00:02,  3.54it/s] 99%|| 823/830 [04:52<00:01,  3.56it/s] 99%|| 824/830 [04:52<00:01,  3.57it/s] 99%|| 825/830 [04:52<00:01,  3.58it/s]100%|| 826/830 [04:53<00:01,  3.58it/s]100%|| 827/830 [04:53<00:00,  3.59it/s]100%|| 828/830 [04:53<00:00,  3.59it/s]100%|| 829/830 [04:53<00:00,  3.59it/s]100%|| 830/830 [04:54<00:00,  3.99it/s][INFO|trainer.py:2140] 2023-08-28 10:23:58,727 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:23:58,727 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 10:23:58,727 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7653, 'eval_samples_per_second': 356.057, 'eval_steps_per_second': 44.545, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  2%|         | 7/435 [00:00<00:07, 58.69it/s][A
  3%|         | 13/435 [00:00<00:08, 50.13it/s][A
  4%|         | 19/435 [00:00<00:08, 47.73it/s][A
  6%|         | 24/435 [00:00<00:08, 46.83it/s][A
  7%|         | 29/435 [00:00<00:08, 46.13it/s][A
  8%|         | 34/435 [00:00<00:08, 45.57it/s][A
  9%|         | 39/435 [00:00<00:08, 45.14it/s][A
 10%|         | 44/435 [00:00<00:08, 44.79it/s][A
 11%|        | 49/435 [00:01<00:08, 44.75it/s][A
 12%|        | 54/435 [00:01<00:08, 44.77it/s][A
 14%|        | 59/435 [00:01<00:08, 44.81it/s][A
 15%|        | 64/435 [00:01<00:08, 44.94it/s][A
 16%|        | 69/435 [00:01<00:08, 44.89it/s][A
 17%|        | 74/435 [00:01<00:08, 44.87it/s][A
 18%|        | 79/435 [00:01<00:07, 44.84it/s][A
 19%|        | 84/435 [00:01<00:07, 44.79it/s][A
 20%|        | 89/435 [00:01<00:07, 44.55it/s][A
 22%|       | 94/435 [00:02<00:07, 44.50it/s][A
 23%|       | 99/435 [00:02<00:07, 44.63it/s][A
 24%|       | 104/435 [00:02<00:07, 44.65it/s][A
 25%|       | 109/435 [00:02<00:07, 44.81it/s][A
 26%|       | 114/435 [00:02<00:07, 44.82it/s][A
 27%|       | 119/435 [00:02<00:07, 44.78it/s][A
 29%|       | 124/435 [00:02<00:06, 44.88it/s][A
 30%|       | 129/435 [00:02<00:06, 44.78it/s][A
 31%|       | 134/435 [00:02<00:06, 44.69it/s][A
 32%|      | 139/435 [00:03<00:06, 44.72it/s][A
 33%|      | 144/435 [00:03<00:06, 42.39it/s][A
 34%|      | 149/435 [00:03<00:06, 43.17it/s][A
 35%|      | 154/435 [00:03<00:06, 43.73it/s][A
 37%|      | 159/435 [00:03<00:06, 43.98it/s][A
 38%|      | 164/435 [00:03<00:06, 44.28it/s][A
 39%|      | 169/435 [00:03<00:05, 44.51it/s][A
 40%|      | 174/435 [00:03<00:05, 44.58it/s][A
 41%|      | 179/435 [00:03<00:05, 44.67it/s][A
 42%|     | 184/435 [00:04<00:05, 44.43it/s][A
 43%|     | 189/435 [00:04<00:05, 44.50it/s][A
 45%|     | 194/435 [00:04<00:05, 44.63it/s][A
 46%|     | 199/435 [00:04<00:05, 44.69it/s][A
 47%|     | 204/435 [00:04<00:05, 44.71it/s][A
 48%|     | 209/435 [00:04<00:05, 44.76it/s][A
 49%|     | 214/435 [00:04<00:04, 44.74it/s][A
 50%|     | 219/435 [00:04<00:04, 44.82it/s][A
 51%|    | 224/435 [00:04<00:04, 44.64it/s][A
 53%|    | 229/435 [00:05<00:04, 44.54it/s][A
 54%|    | 234/435 [00:05<00:04, 44.50it/s][A
 55%|    | 239/435 [00:05<00:04, 44.54it/s][A
 56%|    | 244/435 [00:05<00:04, 44.67it/s][A
 57%|    | 249/435 [00:05<00:04, 44.77it/s][A
 58%|    | 254/435 [00:05<00:04, 44.76it/s][A
 60%|    | 259/435 [00:05<00:03, 44.77it/s][A
 61%|    | 264/435 [00:05<00:03, 44.73it/s][A
 62%|   | 269/435 [00:05<00:03, 44.78it/s][A
 63%|   | 274/435 [00:06<00:03, 44.59it/s][A
 64%|   | 279/435 [00:06<00:03, 43.99it/s][A
 65%|   | 284/435 [00:06<00:03, 44.17it/s][A
 66%|   | 289/435 [00:06<00:03, 44.31it/s][A
 68%|   | 294/435 [00:06<00:03, 44.49it/s][A
 69%|   | 299/435 [00:06<00:03, 44.63it/s][A
 70%|   | 304/435 [00:06<00:02, 44.71it/s][A
 71%|   | 309/435 [00:06<00:02, 44.72it/s][A
 72%|  | 314/435 [00:07<00:02, 44.65it/s][A
 73%|  | 319/435 [00:07<00:02, 44.49it/s][A
 74%|  | 324/435 [00:07<00:02, 44.52it/s][A
 76%|  | 329/435 [00:07<00:02, 44.53it/s][A
 77%|  | 334/435 [00:07<00:02, 44.66it/s][A
 78%|  | 339/435 [00:07<00:02, 44.65it/s][A
 79%|  | 344/435 [00:07<00:02, 44.76it/s][A
 80%|  | 349/435 [00:07<00:01, 44.83it/s][A
 81%| | 354/435 [00:07<00:01, 44.82it/s][A
 83%| | 359/435 [00:08<00:01, 44.79it/s][A
 84%| | 364/435 [00:08<00:01, 44.64it/s][A
 85%| | 369/435 [00:08<00:01, 44.53it/s][A
 86%| | 374/435 [00:08<00:01, 44.67it/s][A
 87%| | 379/435 [00:08<00:01, 44.66it/s][A
 88%| | 384/435 [00:08<00:01, 44.79it/s][A
 89%| | 389/435 [00:08<00:01, 44.82it/s][A
 91%| | 394/435 [00:08<00:00, 44.85it/s][A
 92%|| 399/435 [00:08<00:00, 44.86it/s][A
 93%|| 404/435 [00:09<00:00, 44.82it/s][A
 94%|| 409/435 [00:09<00:00, 44.73it/s][A
 95%|| 414/435 [00:09<00:00, 44.63it/s][A
 96%|| 419/435 [00:09<00:00, 44.59it/s][A
 97%|| 424/435 [00:09<00:00, 44.65it/s][A
 99%|| 429/435 [00:09<00:00, 44.73it/s][A
100%|| 434/435 [00:09<00:00, 44.79it/s][A                                                 
                                                 [A100%|| 830/830 [05:03<00:00,  3.99it/s]
100%|| 435/435 [00:09<00:00, 44.79it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:24:08,728 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-830
[INFO|configuration_utils.py:351] 2023-08-28 10:24:08,889 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-830/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:24:11,826 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-830/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:24:11,970 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-830/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:24:12,031 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-830/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:24:13,053 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:24:13,054 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-166 (score: 0.9974275827407837).
                                                 100%|| 830/830 [05:16<00:00,  3.99it/s]100%|| 830/830 [05:16<00:00,  2.63it/s]
[INFO|trainer.py:1894] 2023-08-28 10:24:20,712 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 10:24:20,821 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:24:23,886 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:24:23,991 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:24:24,023 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:24:24,448 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:24,449 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:24,449 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:24,449 >>   train_runtime            = 0:05:16.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:24,449 >>   train_samples            =      10600
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:24,449 >>   train_samples_per_second =    167.705
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:24,449 >>   train_steps_per_second   =      2.626
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7609, 'eval_samples_per_second': 356.217, 'eval_steps_per_second': 44.566, 'epoch': 5.0}
{'train_runtime': 316.0309, 'train_samples_per_second': 167.705, 'train_steps_per_second': 2.626, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 10:24:24 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:24:24,648 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:24:24,648 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 10:24:24,648 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 56.06it/s]  3%|         | 12/435 [00:00<00:08, 49.09it/s]  4%|         | 17/435 [00:00<00:08, 47.50it/s]  5%|         | 22/435 [00:00<00:08, 46.47it/s]  6%|         | 27/435 [00:00<00:08, 46.02it/s]  7%|         | 32/435 [00:00<00:08, 45.73it/s]  9%|         | 37/435 [00:00<00:08, 45.56it/s] 10%|         | 42/435 [00:00<00:08, 45.19it/s] 11%|         | 47/435 [00:01<00:08, 44.78it/s] 12%|        | 52/435 [00:01<00:08, 44.80it/s] 13%|        | 57/435 [00:01<00:08, 44.87it/s] 14%|        | 62/435 [00:01<00:08, 44.96it/s] 15%|        | 67/435 [00:01<00:08, 44.89it/s] 17%|        | 72/435 [00:01<00:08, 44.92it/s] 18%|        | 77/435 [00:01<00:07, 44.95it/s] 19%|        | 82/435 [00:01<00:07, 44.85it/s] 20%|        | 87/435 [00:01<00:07, 44.73it/s] 21%|        | 92/435 [00:02<00:07, 44.59it/s] 22%|       | 97/435 [00:02<00:07, 44.63it/s] 23%|       | 102/435 [00:02<00:07, 44.65it/s] 25%|       | 107/435 [00:02<00:07, 42.68it/s] 26%|       | 112/435 [00:02<00:07, 43.33it/s] 27%|       | 117/435 [00:02<00:07, 43.86it/s] 28%|       | 122/435 [00:02<00:07, 44.18it/s] 29%|       | 127/435 [00:02<00:06, 44.51it/s] 30%|       | 132/435 [00:02<00:06, 44.64it/s] 31%|      | 137/435 [00:03<00:06, 44.67it/s] 33%|      | 142/435 [00:03<00:06, 44.73it/s] 34%|      | 147/435 [00:03<00:06, 44.58it/s] 35%|      | 152/435 [00:03<00:06, 44.53it/s] 36%|      | 157/435 [00:03<00:06, 44.63it/s] 37%|      | 162/435 [00:03<00:06, 44.63it/s] 38%|      | 167/435 [00:03<00:05, 44.76it/s] 40%|      | 172/435 [00:03<00:05, 44.81it/s] 41%|      | 177/435 [00:03<00:05, 44.80it/s] 42%|     | 182/435 [00:04<00:05, 44.81it/s] 43%|     | 187/435 [00:04<00:05, 44.73it/s] 44%|     | 192/435 [00:04<00:05, 44.56it/s] 45%|     | 197/435 [00:04<00:05, 44.63it/s] 46%|     | 202/435 [00:04<00:05, 44.60it/s] 48%|     | 207/435 [00:04<00:05, 44.66it/s] 49%|     | 212/435 [00:04<00:04, 44.73it/s] 50%|     | 217/435 [00:04<00:04, 44.80it/s] 51%|     | 222/435 [00:04<00:04, 44.88it/s] 52%|    | 227/435 [00:05<00:04, 44.84it/s] 53%|    | 232/435 [00:05<00:04, 44.86it/s] 54%|    | 237/435 [00:05<00:04, 44.69it/s] 56%|    | 242/435 [00:05<00:04, 42.99it/s] 57%|    | 247/435 [00:05<00:04, 43.59it/s] 58%|    | 252/435 [00:05<00:04, 43.97it/s] 59%|    | 257/435 [00:05<00:04, 44.24it/s] 60%|    | 262/435 [00:05<00:03, 44.44it/s] 61%|   | 267/435 [00:05<00:03, 44.60it/s] 63%|   | 272/435 [00:06<00:03, 44.57it/s] 64%|   | 277/435 [00:06<00:03, 44.53it/s] 65%|   | 282/435 [00:06<00:03, 44.34it/s] 66%|   | 287/435 [00:06<00:03, 44.40it/s] 67%|   | 292/435 [00:06<00:03, 44.62it/s] 68%|   | 297/435 [00:06<00:03, 44.57it/s] 69%|   | 302/435 [00:06<00:02, 44.63it/s] 71%|   | 307/435 [00:06<00:02, 44.73it/s] 72%|  | 312/435 [00:06<00:02, 44.88it/s] 73%|  | 317/435 [00:07<00:02, 44.83it/s] 74%|  | 322/435 [00:07<00:02, 44.82it/s] 75%|  | 327/435 [00:07<00:02, 44.73it/s] 76%|  | 332/435 [00:07<00:02, 44.71it/s] 77%|  | 337/435 [00:07<00:02, 44.89it/s] 79%|  | 342/435 [00:07<00:02, 44.83it/s] 80%|  | 347/435 [00:07<00:01, 44.81it/s] 81%|  | 352/435 [00:07<00:01, 44.94it/s] 82%| | 357/435 [00:07<00:01, 44.88it/s] 83%| | 362/435 [00:08<00:01, 44.95it/s] 84%| | 367/435 [00:08<00:01, 44.86it/s] 86%| | 372/435 [00:08<00:01, 44.89it/s] 87%| | 377/435 [00:08<00:01, 43.30it/s] 88%| | 382/435 [00:08<00:01, 43.90it/s] 89%| | 387/435 [00:08<00:01, 44.23it/s] 90%| | 392/435 [00:08<00:00, 44.43it/s] 91%|| 397/435 [00:08<00:00, 44.56it/s] 92%|| 402/435 [00:08<00:00, 44.71it/s] 94%|| 407/435 [00:09<00:00, 44.79it/s] 95%|| 412/435 [00:09<00:00, 44.83it/s] 96%|| 417/435 [00:09<00:00, 44.65it/s] 97%|| 422/435 [00:09<00:00, 44.64it/s] 98%|| 427/435 [00:09<00:00, 44.77it/s] 99%|| 432/435 [00:09<00:00, 44.84it/s]100%|| 435/435 [00:09<00:00, 44.70it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:24:34,397 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:34,397 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:34,397 >>   eval_loss               =     0.9974
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:34,397 >>   eval_runtime            = 0:00:09.74
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:34,397 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:34,397 >>   eval_samples_per_second =    356.641
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:34,397 >>   eval_steps_per_second   =     44.619
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:24:34,397 >>   perplexity              =     2.7113
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:43,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:43,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:43,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:43,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:43,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:24:44,986 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:24:44,987 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:24:45,653 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:24:46,847 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:24:46,905 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:50,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:50,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:50,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:50,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:24:50,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:24:50,951 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:24:50,952 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:24:51,566 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:24:51,823 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:24:51,823 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-664
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-332
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-498
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-166
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/checkpoint-830
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.77it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.75it/s]Extractor Predicting: 4it [00:02,  1.74it/s]Extractor Predicting: 5it [00:02,  1.74it/s]Extractor Predicting: 6it [00:03,  1.73it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.66it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:08,  1.66it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.68it/s]Extractor Predicting: 21it [00:12,  1.69it/s]Extractor Predicting: 22it [00:13,  1.70it/s]Extractor Predicting: 23it [00:13,  1.69it/s]Extractor Predicting: 24it [00:14,  1.70it/s]Extractor Predicting: 25it [00:14,  1.66it/s]Extractor Predicting: 26it [00:15,  1.66it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:16,  1.59it/s]Extractor Predicting: 29it [00:17,  1.56it/s]Extractor Predicting: 30it [00:18,  1.59it/s]Extractor Predicting: 31it [00:18,  1.60it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:19,  1.60it/s]Extractor Predicting: 34it [00:20,  1.59it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:22,  1.68it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:25,  1.67it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:27,  1.65it/s]Extractor Predicting: 47it [00:28,  1.68it/s]Extractor Predicting: 48it [00:28,  1.68it/s]Extractor Predicting: 49it [00:29,  1.71it/s]Extractor Predicting: 50it [00:30,  1.76it/s]Extractor Predicting: 51it [00:30,  1.70it/s]Extractor Predicting: 52it [00:31,  1.66it/s]Extractor Predicting: 53it [00:31,  1.66it/s]Extractor Predicting: 54it [00:32,  1.70it/s]Extractor Predicting: 55it [00:33,  1.68it/s]Extractor Predicting: 56it [00:33,  1.64it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:34,  1.68it/s]Extractor Predicting: 59it [00:35,  1.72it/s]Extractor Predicting: 60it [00:35,  1.75it/s]Extractor Predicting: 61it [00:36,  1.75it/s]Extractor Predicting: 62it [00:37,  1.76it/s]Extractor Predicting: 63it [00:37,  1.75it/s]Extractor Predicting: 64it [00:38,  1.75it/s]Extractor Predicting: 65it [00:38,  1.76it/s]Extractor Predicting: 66it [00:39,  1.73it/s]Extractor Predicting: 67it [00:39,  1.74it/s]Extractor Predicting: 68it [00:40,  1.72it/s]Extractor Predicting: 69it [00:41,  1.73it/s]Extractor Predicting: 70it [00:41,  1.72it/s]Extractor Predicting: 71it [00:42,  1.71it/s]Extractor Predicting: 72it [00:42,  1.74it/s]Extractor Predicting: 73it [00:43,  1.73it/s]Extractor Predicting: 74it [00:44,  1.73it/s]Extractor Predicting: 75it [00:44,  1.76it/s]Extractor Predicting: 76it [00:45,  1.74it/s]Extractor Predicting: 77it [00:45,  1.70it/s]Extractor Predicting: 78it [00:46,  1.70it/s]Extractor Predicting: 79it [00:46,  1.69it/s]Extractor Predicting: 80it [00:47,  1.68it/s]Extractor Predicting: 81it [00:48,  1.61it/s]Extractor Predicting: 82it [00:48,  1.66it/s]Extractor Predicting: 83it [00:49,  1.68it/s]Extractor Predicting: 84it [00:49,  1.70it/s]Extractor Predicting: 85it [00:50,  1.70it/s]Extractor Predicting: 86it [00:51,  1.72it/s]Extractor Predicting: 87it [00:51,  1.67it/s]Extractor Predicting: 88it [00:52,  1.67it/s]Extractor Predicting: 89it [00:52,  1.71it/s]Extractor Predicting: 90it [00:53,  1.71it/s]Extractor Predicting: 91it [00:54,  1.76it/s]Extractor Predicting: 92it [00:54,  1.81it/s]Extractor Predicting: 93it [00:55,  1.77it/s]Extractor Predicting: 94it [00:55,  1.74it/s]Extractor Predicting: 95it [00:56,  1.79it/s]Extractor Predicting: 96it [00:56,  1.79it/s]Extractor Predicting: 97it [00:57,  1.78it/s]Extractor Predicting: 98it [00:57,  1.78it/s]Extractor Predicting: 99it [00:58,  1.69it/s]Extractor Predicting: 100it [00:59,  1.67it/s]Extractor Predicting: 101it [00:59,  1.68it/s]Extractor Predicting: 102it [01:00,  1.77it/s]Extractor Predicting: 103it [01:00,  1.80it/s]Extractor Predicting: 104it [01:01,  1.79it/s]Extractor Predicting: 105it [01:02,  1.76it/s]Extractor Predicting: 106it [01:02,  1.76it/s]Extractor Predicting: 107it [01:03,  1.78it/s]Extractor Predicting: 108it [01:03,  1.77it/s]Extractor Predicting: 109it [01:04,  1.76it/s]Extractor Predicting: 110it [01:04,  1.77it/s]Extractor Predicting: 111it [01:05,  1.77it/s]Extractor Predicting: 112it [01:05,  1.77it/s]Extractor Predicting: 113it [01:06,  1.83it/s]Extractor Predicting: 114it [01:07,  1.67it/s]Extractor Predicting: 115it [01:07,  1.70it/s]Extractor Predicting: 116it [01:08,  1.73it/s]Extractor Predicting: 117it [01:08,  1.73it/s]Extractor Predicting: 118it [01:09,  1.73it/s]Extractor Predicting: 119it [01:10,  1.75it/s]Extractor Predicting: 120it [01:10,  1.71it/s]Extractor Predicting: 121it [01:11,  1.70it/s]Extractor Predicting: 122it [01:11,  1.71it/s]Extractor Predicting: 123it [01:12,  1.68it/s]Extractor Predicting: 124it [01:13,  1.65it/s]Extractor Predicting: 125it [01:13,  1.68it/s]Extractor Predicting: 126it [01:14,  1.69it/s]Extractor Predicting: 127it [01:14,  1.68it/s]Extractor Predicting: 128it [01:15,  1.66it/s]Extractor Predicting: 129it [01:15,  1.69it/s]Extractor Predicting: 130it [01:16,  1.68it/s]Extractor Predicting: 131it [01:17,  1.70it/s]Extractor Predicting: 132it [01:17,  1.67it/s]Extractor Predicting: 133it [01:18,  1.71it/s]Extractor Predicting: 134it [01:18,  1.69it/s]Extractor Predicting: 135it [01:19,  1.70it/s]Extractor Predicting: 136it [01:20,  1.68it/s]Extractor Predicting: 137it [01:20,  1.73it/s]Extractor Predicting: 138it [01:21,  1.71it/s]Extractor Predicting: 139it [01:21,  1.69it/s]Extractor Predicting: 140it [01:22,  1.69it/s]Extractor Predicting: 141it [01:23,  1.72it/s]Extractor Predicting: 142it [01:23,  1.71it/s]Extractor Predicting: 143it [01:24,  1.75it/s]Extractor Predicting: 144it [01:24,  1.79it/s]Extractor Predicting: 144it [01:24,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:29,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:29,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:29,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:29,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:29,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:26:29,831 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:26:29,832 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:26:30,411 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:26:31,477 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:26:31,477 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:34,533 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:34,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:34,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:34,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:34,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:26:35,209 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:26:35,210 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:26:35,790 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:26:35,948 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:26:35,948 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.74it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.74it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.79it/s]Extractor Predicting: 12it [00:07,  1.76it/s]Extractor Predicting: 13it [00:07,  1.75it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.75it/s]Extractor Predicting: 16it [00:09,  1.75it/s]Extractor Predicting: 17it [00:09,  1.74it/s]Extractor Predicting: 18it [00:10,  1.78it/s]Extractor Predicting: 19it [00:10,  1.82it/s]Extractor Predicting: 20it [00:11,  1.73it/s]Extractor Predicting: 21it [00:12,  1.77it/s]Extractor Predicting: 22it [00:12,  1.79it/s]Extractor Predicting: 23it [00:13,  1.79it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:15,  1.71it/s]Extractor Predicting: 28it [00:16,  1.73it/s]Extractor Predicting: 29it [00:16,  1.75it/s]Extractor Predicting: 30it [00:17,  1.78it/s]Extractor Predicting: 31it [00:17,  1.75it/s]Extractor Predicting: 32it [00:18,  1.78it/s]Extractor Predicting: 33it [00:18,  1.76it/s]Extractor Predicting: 34it [00:19,  1.72it/s]Extractor Predicting: 35it [00:20,  1.72it/s]Extractor Predicting: 36it [00:20,  1.74it/s]Extractor Predicting: 37it [00:21,  1.78it/s]Extractor Predicting: 38it [00:21,  1.79it/s]Extractor Predicting: 39it [00:22,  1.78it/s]Extractor Predicting: 40it [00:22,  1.76it/s]Extractor Predicting: 41it [00:23,  1.77it/s]Extractor Predicting: 42it [00:24,  1.81it/s]Extractor Predicting: 43it [00:24,  1.75it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:25,  1.70it/s]Extractor Predicting: 46it [00:26,  1.70it/s]Extractor Predicting: 47it [00:27,  1.74it/s]Extractor Predicting: 48it [00:27,  1.72it/s]Extractor Predicting: 49it [00:28,  1.76it/s]Extractor Predicting: 50it [00:28,  1.71it/s]Extractor Predicting: 51it [00:29,  1.71it/s]Extractor Predicting: 52it [00:29,  1.70it/s]Extractor Predicting: 53it [00:30,  1.74it/s]Extractor Predicting: 54it [00:31,  1.71it/s]Extractor Predicting: 55it [00:31,  1.77it/s]Extractor Predicting: 56it [00:32,  1.70it/s]Extractor Predicting: 57it [00:32,  1.70it/s]Extractor Predicting: 58it [00:33,  1.68it/s]Extractor Predicting: 59it [00:34,  1.67it/s]Extractor Predicting: 60it [00:34,  1.71it/s]Extractor Predicting: 61it [00:35,  1.67it/s]Extractor Predicting: 62it [00:35,  1.68it/s]Extractor Predicting: 63it [00:36,  1.72it/s]Extractor Predicting: 64it [00:36,  1.71it/s]Extractor Predicting: 65it [00:37,  1.78it/s]Extractor Predicting: 66it [00:38,  1.74it/s]Extractor Predicting: 67it [00:38,  1.70it/s]Extractor Predicting: 68it [00:39,  1.69it/s]Extractor Predicting: 69it [00:39,  1.71it/s]Extractor Predicting: 70it [00:40,  1.67it/s]Extractor Predicting: 71it [00:41,  1.70it/s]Extractor Predicting: 72it [00:41,  1.70it/s]Extractor Predicting: 73it [00:42,  1.66it/s]Extractor Predicting: 74it [00:42,  1.69it/s]Extractor Predicting: 75it [00:43,  1.70it/s]Extractor Predicting: 76it [00:44,  1.72it/s]Extractor Predicting: 77it [00:44,  1.72it/s]Extractor Predicting: 78it [00:45,  1.75it/s]Extractor Predicting: 79it [00:45,  1.72it/s]Extractor Predicting: 80it [00:46,  1.80it/s]Extractor Predicting: 81it [00:46,  1.80it/s]Extractor Predicting: 82it [00:47,  1.80it/s]Extractor Predicting: 83it [00:47,  1.79it/s]Extractor Predicting: 84it [00:48,  1.78it/s]Extractor Predicting: 85it [00:49,  1.73it/s]Extractor Predicting: 86it [00:49,  1.67it/s]Extractor Predicting: 87it [00:50,  1.65it/s]Extractor Predicting: 88it [00:50,  1.69it/s]Extractor Predicting: 89it [00:51,  1.68it/s]Extractor Predicting: 90it [00:52,  1.64it/s]Extractor Predicting: 91it [00:52,  1.63it/s]Extractor Predicting: 92it [00:53,  1.66it/s]Extractor Predicting: 93it [00:53,  1.68it/s]Extractor Predicting: 94it [00:54,  1.70it/s]Extractor Predicting: 95it [00:55,  1.70it/s]Extractor Predicting: 96it [00:55,  1.68it/s]Extractor Predicting: 97it [00:56,  1.66it/s]Extractor Predicting: 98it [00:56,  1.64it/s]Extractor Predicting: 99it [00:57,  1.65it/s]Extractor Predicting: 100it [00:58,  1.68it/s]Extractor Predicting: 101it [00:58,  1.67it/s]Extractor Predicting: 102it [00:59,  1.70it/s]Extractor Predicting: 103it [00:59,  1.68it/s]Extractor Predicting: 104it [01:00,  1.66it/s]Extractor Predicting: 105it [01:01,  1.70it/s]Extractor Predicting: 106it [01:01,  1.69it/s]Extractor Predicting: 107it [01:02,  1.67it/s]Extractor Predicting: 108it [01:02,  1.70it/s]Extractor Predicting: 109it [01:03,  1.68it/s]Extractor Predicting: 110it [01:04,  1.68it/s]Extractor Predicting: 111it [01:04,  1.70it/s]Extractor Predicting: 112it [01:05,  1.69it/s]Extractor Predicting: 113it [01:05,  1.69it/s]Extractor Predicting: 114it [01:06,  1.69it/s]Extractor Predicting: 115it [01:07,  1.68it/s]Extractor Predicting: 116it [01:07,  1.64it/s]Extractor Predicting: 117it [01:08,  1.44it/s]Extractor Predicting: 118it [01:09,  1.51it/s]Extractor Predicting: 119it [01:09,  1.51it/s]Extractor Predicting: 120it [01:10,  1.54it/s]Extractor Predicting: 121it [01:11,  1.58it/s]Extractor Predicting: 122it [01:11,  1.64it/s]Extractor Predicting: 123it [01:12,  1.66it/s]Extractor Predicting: 124it [01:12,  1.61it/s]Extractor Predicting: 125it [01:13,  1.62it/s]Extractor Predicting: 126it [01:13,  1.71it/s]Extractor Predicting: 127it [01:14,  1.72it/s]Extractor Predicting: 128it [01:15,  1.69it/s]Extractor Predicting: 129it [01:15,  1.75it/s]Extractor Predicting: 130it [01:16,  1.68it/s]Extractor Predicting: 131it [01:16,  1.69it/s]Extractor Predicting: 132it [01:17,  1.71it/s]Extractor Predicting: 133it [01:17,  1.78it/s]Extractor Predicting: 134it [01:18,  1.79it/s]Extractor Predicting: 135it [01:19,  1.73it/s]Extractor Predicting: 136it [01:19,  1.69it/s]Extractor Predicting: 137it [01:20,  1.73it/s]Extractor Predicting: 138it [01:20,  1.79it/s]Extractor Predicting: 139it [01:21,  1.77it/s]Extractor Predicting: 140it [01:22,  1.77it/s]Extractor Predicting: 141it [01:22,  1.75it/s]Extractor Predicting: 142it [01:23,  1.73it/s]Extractor Predicting: 143it [01:23,  1.70it/s]Extractor Predicting: 144it [01:24,  1.68it/s]Extractor Predicting: 145it [01:24,  1.69it/s]Extractor Predicting: 146it [01:25,  1.73it/s]Extractor Predicting: 147it [01:26,  1.79it/s]Extractor Predicting: 148it [01:26,  1.75it/s]Extractor Predicting: 149it [01:27,  1.78it/s]Extractor Predicting: 150it [01:27,  1.83it/s]Extractor Predicting: 151it [01:28,  1.84it/s]Extractor Predicting: 152it [01:28,  1.84it/s]Extractor Predicting: 153it [01:29,  1.84it/s]Extractor Predicting: 154it [01:29,  1.84it/s]Extractor Predicting: 155it [01:30,  1.82it/s]Extractor Predicting: 156it [01:30,  1.84it/s]Extractor Predicting: 157it [01:31,  1.89it/s]Extractor Predicting: 158it [01:31,  1.88it/s]Extractor Predicting: 159it [01:32,  1.94it/s]Extractor Predicting: 160it [01:32,  1.97it/s]Extractor Predicting: 161it [01:33,  1.90it/s]Extractor Predicting: 162it [01:34,  1.86it/s]Extractor Predicting: 163it [01:34,  1.86it/s]Extractor Predicting: 164it [01:35,  1.87it/s]Extractor Predicting: 165it [01:35,  1.91it/s]Extractor Predicting: 166it [01:36,  1.88it/s]Extractor Predicting: 167it [01:36,  1.89it/s]Extractor Predicting: 168it [01:37,  1.86it/s]Extractor Predicting: 169it [01:37,  1.91it/s]Extractor Predicting: 170it [01:38,  1.90it/s]Extractor Predicting: 171it [01:38,  1.94it/s]Extractor Predicting: 172it [01:39,  1.91it/s]Extractor Predicting: 173it [01:39,  1.92it/s]Extractor Predicting: 174it [01:40,  1.82it/s]Extractor Predicting: 175it [01:41,  1.81it/s]Extractor Predicting: 176it [01:41,  1.79it/s]Extractor Predicting: 177it [01:42,  1.77it/s]Extractor Predicting: 178it [01:42,  1.71it/s]Extractor Predicting: 179it [01:43,  1.70it/s]Extractor Predicting: 180it [01:43,  1.72it/s]Extractor Predicting: 181it [01:44,  1.71it/s]Extractor Predicting: 182it [01:45,  1.72it/s]Extractor Predicting: 183it [01:45,  1.74it/s]Extractor Predicting: 184it [01:46,  1.74it/s]Extractor Predicting: 185it [01:46,  1.74it/s]Extractor Predicting: 186it [01:47,  1.74it/s]Extractor Predicting: 187it [01:48,  1.73it/s]Extractor Predicting: 188it [01:48,  1.71it/s]Extractor Predicting: 189it [01:49,  1.73it/s]Extractor Predicting: 190it [01:49,  1.72it/s]Extractor Predicting: 191it [01:50,  1.69it/s]Extractor Predicting: 192it [01:51,  1.66it/s]Extractor Predicting: 193it [01:51,  1.65it/s]Extractor Predicting: 194it [01:52,  1.65it/s]Extractor Predicting: 195it [01:52,  1.67it/s]Extractor Predicting: 196it [01:53,  1.72it/s]Extractor Predicting: 197it [01:53,  1.73it/s]Extractor Predicting: 198it [01:54,  1.71it/s]Extractor Predicting: 199it [01:55,  1.73it/s]Extractor Predicting: 200it [01:55,  1.69it/s]Extractor Predicting: 201it [01:56,  1.68it/s]Extractor Predicting: 202it [01:56,  1.65it/s]Extractor Predicting: 203it [01:57,  1.61it/s]Extractor Predicting: 204it [01:58,  1.60it/s]Extractor Predicting: 205it [01:58,  1.60it/s]Extractor Predicting: 206it [01:59,  1.60it/s]Extractor Predicting: 207it [02:00,  1.63it/s]Extractor Predicting: 208it [02:00,  1.59it/s]Extractor Predicting: 209it [02:01,  1.55it/s]Extractor Predicting: 210it [02:02,  1.56it/s]Extractor Predicting: 211it [02:02,  1.57it/s]Extractor Predicting: 212it [02:03,  1.60it/s]Extractor Predicting: 213it [02:03,  1.58it/s]Extractor Predicting: 214it [02:04,  1.62it/s]Extractor Predicting: 215it [02:05,  1.59it/s]Extractor Predicting: 216it [02:05,  1.60it/s]Extractor Predicting: 217it [02:06,  1.62it/s]Extractor Predicting: 218it [02:07,  1.57it/s]Extractor Predicting: 219it [02:07,  1.57it/s]Extractor Predicting: 220it [02:08,  1.55it/s]Extractor Predicting: 221it [02:09,  1.54it/s]Extractor Predicting: 222it [02:09,  1.55it/s]Extractor Predicting: 223it [02:10,  1.56it/s]Extractor Predicting: 224it [02:10,  1.59it/s]Extractor Predicting: 225it [02:11,  1.60it/s]Extractor Predicting: 226it [02:12,  1.59it/s]Extractor Predicting: 227it [02:12,  1.61it/s]Extractor Predicting: 228it [02:13,  1.58it/s]Extractor Predicting: 229it [02:14,  1.41it/s]Extractor Predicting: 230it [02:14,  1.49it/s]Extractor Predicting: 231it [02:15,  1.57it/s]Extractor Predicting: 232it [02:15,  1.65it/s]Extractor Predicting: 233it [02:16,  1.69it/s]Extractor Predicting: 234it [02:17,  1.66it/s]Extractor Predicting: 235it [02:17,  1.65it/s]Extractor Predicting: 236it [02:18,  1.68it/s]Extractor Predicting: 237it [02:18,  1.69it/s]Extractor Predicting: 238it [02:19,  1.70it/s]Extractor Predicting: 239it [02:20,  1.67it/s]Extractor Predicting: 240it [02:20,  1.73it/s]Extractor Predicting: 241it [02:21,  1.71it/s]Extractor Predicting: 242it [02:21,  1.75it/s]Extractor Predicting: 243it [02:22,  1.75it/s]Extractor Predicting: 244it [02:22,  1.82it/s]Extractor Predicting: 245it [02:23,  1.77it/s]Extractor Predicting: 246it [02:24,  1.74it/s]Extractor Predicting: 247it [02:24,  1.69it/s]Extractor Predicting: 248it [02:25,  1.68it/s]Extractor Predicting: 249it [02:25,  1.72it/s]Extractor Predicting: 250it [02:26,  1.72it/s]Extractor Predicting: 251it [02:26,  1.75it/s]Extractor Predicting: 252it [02:27,  1.79it/s]Extractor Predicting: 253it [02:28,  1.73it/s]Extractor Predicting: 254it [02:28,  1.69it/s]Extractor Predicting: 255it [02:29,  1.71it/s]Extractor Predicting: 256it [02:29,  1.70it/s]Extractor Predicting: 257it [02:30,  1.67it/s]Extractor Predicting: 258it [02:31,  1.66it/s]Extractor Predicting: 259it [02:31,  1.68it/s]Extractor Predicting: 260it [02:32,  1.67it/s]Extractor Predicting: 261it [02:32,  1.68it/s]Extractor Predicting: 262it [02:33,  1.69it/s]Extractor Predicting: 263it [02:34,  1.72it/s]Extractor Predicting: 264it [02:34,  1.67it/s]Extractor Predicting: 265it [02:35,  1.68it/s]Extractor Predicting: 266it [02:35,  1.66it/s]Extractor Predicting: 267it [02:36,  1.66it/s]Extractor Predicting: 268it [02:37,  1.64it/s]Extractor Predicting: 269it [02:37,  1.63it/s]Extractor Predicting: 270it [02:38,  1.64it/s]Extractor Predicting: 271it [02:38,  1.66it/s]Extractor Predicting: 272it [02:39,  1.70it/s]Extractor Predicting: 273it [02:40,  1.65it/s]Extractor Predicting: 274it [02:40,  1.60it/s]Extractor Predicting: 275it [02:41,  1.63it/s]Extractor Predicting: 276it [02:42,  1.63it/s]Extractor Predicting: 277it [02:42,  1.65it/s]Extractor Predicting: 278it [02:43,  1.67it/s]Extractor Predicting: 279it [02:43,  1.62it/s]Extractor Predicting: 280it [02:44,  1.64it/s]Extractor Predicting: 281it [02:45,  1.68it/s]Extractor Predicting: 282it [02:45,  1.67it/s]Extractor Predicting: 283it [02:46,  1.63it/s]Extractor Predicting: 284it [02:46,  1.71it/s]Extractor Predicting: 285it [02:47,  1.69it/s]Extractor Predicting: 286it [02:47,  1.72it/s]Extractor Predicting: 287it [02:48,  1.67it/s]Extractor Predicting: 288it [02:49,  1.67it/s]Extractor Predicting: 289it [02:49,  1.67it/s]Extractor Predicting: 290it [02:50,  1.66it/s]Extractor Predicting: 291it [02:51,  1.61it/s]Extractor Predicting: 292it [02:51,  1.58it/s]Extractor Predicting: 293it [02:52,  1.63it/s]Extractor Predicting: 294it [02:52,  1.61it/s]Extractor Predicting: 295it [02:53,  1.64it/s]Extractor Predicting: 296it [02:54,  1.63it/s]Extractor Predicting: 297it [02:54,  1.64it/s]Extractor Predicting: 298it [02:55,  1.63it/s]Extractor Predicting: 299it [02:55,  1.62it/s]Extractor Predicting: 300it [02:56,  1.66it/s]Extractor Predicting: 301it [02:57,  1.64it/s]Extractor Predicting: 302it [02:57,  1.66it/s]Extractor Predicting: 303it [02:58,  1.62it/s]Extractor Predicting: 304it [02:59,  1.62it/s]Extractor Predicting: 305it [02:59,  1.63it/s]Extractor Predicting: 306it [03:00,  1.67it/s]Extractor Predicting: 307it [03:00,  1.61it/s]Extractor Predicting: 308it [03:01,  1.65it/s]Extractor Predicting: 309it [03:02,  1.66it/s]Extractor Predicting: 310it [03:02,  1.69it/s]Extractor Predicting: 311it [03:03,  1.67it/s]Extractor Predicting: 312it [03:03,  1.70it/s]Extractor Predicting: 313it [03:04,  1.76it/s]Extractor Predicting: 314it [03:04,  1.78it/s]Extractor Predicting: 315it [03:05,  1.80it/s]Extractor Predicting: 316it [03:05,  1.76it/s]Extractor Predicting: 317it [03:06,  1.74it/s]Extractor Predicting: 318it [03:07,  1.72it/s]Extractor Predicting: 319it [03:07,  1.71it/s]Extractor Predicting: 320it [03:08,  1.70it/s]Extractor Predicting: 321it [03:08,  1.70it/s]Extractor Predicting: 322it [03:09,  1.71it/s]Extractor Predicting: 323it [03:10,  1.66it/s]Extractor Predicting: 324it [03:10,  1.63it/s]Extractor Predicting: 325it [03:11,  1.64it/s]Extractor Predicting: 326it [03:11,  1.67it/s]Extractor Predicting: 327it [03:12,  1.71it/s]Extractor Predicting: 328it [03:13,  1.70it/s]Extractor Predicting: 329it [03:13,  1.72it/s]Extractor Predicting: 330it [03:14,  1.66it/s]Extractor Predicting: 331it [03:14,  1.66it/s]Extractor Predicting: 332it [03:15,  1.67it/s]Extractor Predicting: 333it [03:16,  1.66it/s]Extractor Predicting: 334it [03:16,  1.67it/s]Extractor Predicting: 335it [03:17,  1.71it/s]Extractor Predicting: 336it [03:18,  1.47it/s]Extractor Predicting: 337it [03:18,  1.54it/s]Extractor Predicting: 338it [03:19,  1.53it/s]Extractor Predicting: 339it [03:19,  1.62it/s]Extractor Predicting: 340it [03:20,  1.67it/s]Extractor Predicting: 341it [03:21,  1.69it/s]Extractor Predicting: 342it [03:21,  1.68it/s]Extractor Predicting: 343it [03:22,  1.68it/s]Extractor Predicting: 344it [03:22,  1.67it/s]Extractor Predicting: 345it [03:23,  1.64it/s]Extractor Predicting: 346it [03:24,  1.66it/s]Extractor Predicting: 347it [03:24,  1.67it/s]Extractor Predicting: 348it [03:25,  1.68it/s]Extractor Predicting: 349it [03:25,  1.66it/s]Extractor Predicting: 350it [03:26,  1.69it/s]Extractor Predicting: 351it [03:27,  1.71it/s]Extractor Predicting: 352it [03:27,  1.65it/s]Extractor Predicting: 353it [03:28,  1.68it/s]Extractor Predicting: 354it [03:28,  1.72it/s]Extractor Predicting: 355it [03:29,  1.69it/s]Extractor Predicting: 356it [03:30,  1.69it/s]Extractor Predicting: 357it [03:30,  1.69it/s]Extractor Predicting: 358it [03:31,  1.72it/s]Extractor Predicting: 359it [03:31,  1.71it/s]Extractor Predicting: 360it [03:32,  1.72it/s]Extractor Predicting: 361it [03:32,  1.70it/s]Extractor Predicting: 362it [03:33,  1.71it/s]Extractor Predicting: 363it [03:34,  1.74it/s]Extractor Predicting: 364it [03:34,  1.77it/s]Extractor Predicting: 365it [03:35,  1.79it/s]Extractor Predicting: 366it [03:35,  1.75it/s]Extractor Predicting: 367it [03:36,  1.65it/s]Extractor Predicting: 368it [03:37,  1.67it/s]Extractor Predicting: 369it [03:37,  1.68it/s]Extractor Predicting: 370it [03:38,  1.75it/s]Extractor Predicting: 371it [03:38,  1.76it/s]Extractor Predicting: 372it [03:39,  1.77it/s]Extractor Predicting: 373it [03:39,  1.67it/s]Extractor Predicting: 374it [03:40,  1.70it/s]Extractor Predicting: 375it [03:41,  1.73it/s]Extractor Predicting: 376it [03:41,  1.76it/s]Extractor Predicting: 377it [03:42,  1.77it/s]Extractor Predicting: 378it [03:42,  1.80it/s]Extractor Predicting: 379it [03:43,  1.73it/s]Extractor Predicting: 380it [03:43,  1.73it/s]Extractor Predicting: 381it [03:44,  1.76it/s]Extractor Predicting: 382it [03:45,  1.76it/s]Extractor Predicting: 383it [03:45,  1.81it/s]Extractor Predicting: 384it [03:46,  1.84it/s]Extractor Predicting: 385it [03:46,  1.78it/s]Extractor Predicting: 386it [03:47,  1.79it/s]Extractor Predicting: 387it [03:47,  1.78it/s]Extractor Predicting: 388it [03:48,  1.76it/s]Extractor Predicting: 389it [03:48,  1.77it/s]Extractor Predicting: 390it [03:49,  1.76it/s]Extractor Predicting: 391it [03:50,  1.72it/s]Extractor Predicting: 392it [03:50,  1.74it/s]Extractor Predicting: 393it [03:51,  1.78it/s]Extractor Predicting: 394it [03:51,  1.70it/s]Extractor Predicting: 395it [03:52,  1.65it/s]Extractor Predicting: 396it [03:53,  1.62it/s]Extractor Predicting: 397it [03:53,  1.62it/s]Extractor Predicting: 398it [03:54,  1.63it/s]Extractor Predicting: 399it [03:54,  1.62it/s]Extractor Predicting: 400it [03:55,  1.68it/s]Extractor Predicting: 401it [03:56,  1.64it/s]Extractor Predicting: 402it [03:56,  1.61it/s]Extractor Predicting: 403it [03:57,  1.63it/s]Extractor Predicting: 404it [03:58,  1.58it/s]Extractor Predicting: 405it [03:58,  1.58it/s]Extractor Predicting: 406it [03:59,  1.57it/s]Extractor Predicting: 407it [04:00,  1.56it/s]Extractor Predicting: 408it [04:00,  1.60it/s]Extractor Predicting: 409it [04:01,  1.62it/s]Extractor Predicting: 410it [04:01,  1.60it/s]Extractor Predicting: 411it [04:02,  1.58it/s]Extractor Predicting: 412it [04:03,  1.61it/s]Extractor Predicting: 413it [04:03,  1.64it/s]Extractor Predicting: 414it [04:04,  1.67it/s]Extractor Predicting: 415it [04:04,  1.72it/s]Extractor Predicting: 416it [04:05,  1.71it/s]Extractor Predicting: 417it [04:05,  1.69it/s]Extractor Predicting: 418it [04:06,  1.68it/s]Extractor Predicting: 419it [04:07,  1.62it/s]Extractor Predicting: 420it [04:07,  1.64it/s]Extractor Predicting: 421it [04:08,  1.74it/s]Extractor Predicting: 421it [04:08,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,774 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:30:58,418 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:30:58,419 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:30:59,038 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:31:00,117 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:31:00,118 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:03,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:03,067 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:03,067 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:03,067 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:03,067 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:31:03,717 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:31:03,718 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:31:04,365 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:31:04,539 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:31:04,539 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  2.06it/s]Extractor Predicting: 9it [00:05,  1.69it/s]
[INFO|configuration_utils.py:515] 2023-08-28 10:31:10,989 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:31:10,991 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:31:11,036 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:31:11,037 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 10:31:11,061 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:31:21,172 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 10:31:21,195 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 10:31:21,285 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:31:21,285 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:31:21,337 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:31:21,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:31:21,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:31:21,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:31:21,374 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:31:21,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:31:21,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 10:31:21,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:22,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:22,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:23,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:24,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:24,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:25,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:26,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:26,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:27,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:28,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:28,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:29,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:30,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:30,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:31,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:31,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:32,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:33,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:34,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:35,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:36,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:36,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:37,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:37,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:38,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:17<05:30, 17.39s/it][WARNING|generation_utils.py:914] 2023-08-28 10:31:39,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:39,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:40,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:41,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:41,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:42,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:43,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:43,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:44,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:45,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:45,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:46,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:47,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:47,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:48,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:48,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:49,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:50,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:50,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:51,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:52,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:52,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:53,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:54,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:54,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:55,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:34<05:07, 17.07s/it][WARNING|generation_utils.py:914] 2023-08-28 10:31:55,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:56,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:57,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:58,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:58,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:59,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:31:59,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:00,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:01,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:01,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:02,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:03,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:03,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:04,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:05,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:05,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:06,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:07,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:07,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:08,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:08,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:09,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:10,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:10,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:11,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:12,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:51<04:49, 17.05s/it][WARNING|generation_utils.py:914] 2023-08-28 10:32:12,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:13,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:14,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:14,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:15,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:16,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:17,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:17,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:18,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:19,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:19,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:20,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:20,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:21,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:22,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:22,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:23,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:24,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:25,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:25,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:26,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:27,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:27,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:28,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:29,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:07<04:30, 16.89s/it][WARNING|generation_utils.py:914] 2023-08-28 10:32:29,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:30,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:30,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:31,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:32,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:32,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:33,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:33,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:34,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:34,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:35,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:36,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:36,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:37,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:38,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:38,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:39,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:39,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:40,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:41,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:41,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:42,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:42,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:43,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:44,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:44,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:45,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:24<04:11, 16.75s/it][WARNING|generation_utils.py:914] 2023-08-28 10:32:46,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:46,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:47,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:47,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:48,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:49,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:50,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:50,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:51,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:52,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:52,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:53,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:54,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:54,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:55,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:56,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:56,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:57,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:58,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:58,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:32:59,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:00,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:00,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:39<03:47, 16.25s/it][WARNING|generation_utils.py:914] 2023-08-28 10:33:01,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:02,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:02,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:03,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:04,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:04,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:05,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:06,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:06,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:07,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:08,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:08,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:09,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:10,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:10,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:11,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:12,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:12,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:13,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:14,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:15,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:15,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:16,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:17,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:56<03:34, 16.54s/it][WARNING|generation_utils.py:914] 2023-08-28 10:33:18,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:19,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:20,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:20,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:21,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:22,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:22,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:23,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:24,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:25,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:25,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:26,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:26,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:27,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:28,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:29,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:29,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:30,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:31,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:32,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:32,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:33,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:34,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:35,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:35,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:14<03:24, 17.04s/it][WARNING|generation_utils.py:914] 2023-08-28 10:33:36,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:37,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:37,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:38,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:39,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:39,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:40,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:41,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:42,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:42,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:43,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:44,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:44,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:45,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:46,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:47,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:47,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:48,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:49,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:49,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:50,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:51,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:51,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:30<03:03, 16.71s/it][WARNING|generation_utils.py:914] 2023-08-28 10:33:52,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:53,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:54,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:54,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:55,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:56,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:57,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:57,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:58,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:33:59,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:00,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:00,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:01,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:02,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:02,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:03,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:04,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:05,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:05,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:06,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:06,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:07,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:08,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:08,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:47<02:48, 16.81s/it][WARNING|generation_utils.py:914] 2023-08-28 10:34:09,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:10,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:11,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:12,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:12,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:13,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:14,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:15,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:16,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:17,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:17,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:18,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:19,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:19,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:20,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:21,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:21,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:22,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:23,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:23,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:24,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:25,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:26,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:05<02:32, 16.92s/it][WARNING|generation_utils.py:914] 2023-08-28 10:34:26,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:27,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:27,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:28,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:29,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:30,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:30,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:31,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:32,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:32,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:33,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:34,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:35,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:35,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:36,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:37,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:37,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:38,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:39,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:39,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:40,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:41,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:41,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:20<02:12, 16.57s/it][WARNING|generation_utils.py:914] 2023-08-28 10:34:42,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:43,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:44,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:44,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:45,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:46,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:47,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:47,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:48,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:49,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:49,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:50,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:51,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:52,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:52,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:53,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:54,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:54,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:55,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:56,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:56,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:57,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:58,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:58,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:34:59,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:38<01:58, 16.89s/it][WARNING|generation_utils.py:914] 2023-08-28 10:35:00,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:01,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:01,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:02,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:03,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:03,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:04,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:04,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:05,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:05,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:06,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:06,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:07,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:08,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:08,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:09,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:10,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:10,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:11,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:12,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:12,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:13,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:14,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:53<01:37, 16.19s/it][WARNING|generation_utils.py:914] 2023-08-28 10:35:14,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:15,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:16,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:16,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:17,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:17,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:18,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:19,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:19,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:20,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:21,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:21,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:22,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:23,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:23,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:24,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:25,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:25,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:26,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:27,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:27,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:28,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:29,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:29,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:30,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:31,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:09<01:21, 16.38s/it][WARNING|generation_utils.py:914] 2023-08-28 10:35:31,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:32,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:32,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:33,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:34,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:34,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:35,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:36,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:36,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:37,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:38,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:38,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:39,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:39,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:40,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:41,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:41,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:42,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:43,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:44,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:44,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:45,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:46,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:46,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:47,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:26<01:05, 16.39s/it][WARNING|generation_utils.py:914] 2023-08-28 10:35:48,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:48,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:49,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:49,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:50,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:51,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:52,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:52,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:53,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:54,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:54,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:55,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:56,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:57,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:57,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:58,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:59,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:59,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:00,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:00,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:01,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:02,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:02,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:41<00:48, 16.09s/it][WARNING|generation_utils.py:914] 2023-08-28 10:36:03,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:04,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:04,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:05,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:06,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:06,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:07,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:08,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:08,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:09,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:10,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:10,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:11,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:12,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:12,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:13,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:13,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:14,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:15,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:15,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:16,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:17,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:17,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:18,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:18,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:19,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:19,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:20,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:21,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:21,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:00<00:33, 16.93s/it][WARNING|generation_utils.py:914] 2023-08-28 10:36:22,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:23,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:23,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:24,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:25,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:25,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:26,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:27,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:27,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:28,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:29,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:29,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:30,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:31,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:31,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:32,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:33,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:33,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:34,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:35,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:35,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:36,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:37,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:16<00:16, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-28 10:36:37,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:38,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:39,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:39,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:40,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:41,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:41,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:42,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:43,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:43,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:44,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:45,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:45,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:46,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:47,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:47,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:48,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:49,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:49,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:50,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:51,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:51,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:52,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:31<00:00, 16.21s/it]Generating: 100%|| 20/20 [05:31<00:00, 16.58s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:01,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:01,996 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:01,996 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:01,996 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:01,996 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:37:02,646 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:37:02,647 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:37:03,240 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:37:04,316 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:37:04,316 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:07,314 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:07,316 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:07,316 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:07,316 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:07,316 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:37:07,990 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:37:07,991 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:37:08,623 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:37:08,797 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:37:08,797 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 441, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 512, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7379807692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 234, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 625, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7512019230769231, 'errors': {'', "('Hans', 'head of government', '', 'He was succeeded by his brother , Dr . Hans .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : military branch .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('British ships', 'military branch', '', 'The Battle of Bataillon was the battle of the Battle of the Bastille , where British ships sank at least 1,400 French ships .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 181, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 274, 'raw': 384}
{'target': 600, 'success': 295, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 339, 'raw': 480}
{'target': 600, 'success': 361, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 428, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 602, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.6967592592592593, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('European Championships', 'winner', '', 'The previous year , he won the European Championships , and finished runner in 5th place in the category of medals .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : crosses . Context : Later in the year ( 11411231 ) he married Brigadier John B. Stoughton , sister of King James VI , the King of England . Head Entity : Robert Stoughton , Tail Entity : John B . Stoughton .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : crosses .', 'success_rate': 0.77, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('FreeBSD', 'operating system', '', 'The operating system is based on FreeBSD 2.1 .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 519, 'raw': 704}
{'target': 600, 'success': 544, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : participant .', 'success_rate': 0.7415865384615384, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : position played on team / speciality . Context : Later in 2008 , he played in the United States national team squad for the 2002 FIFA World Cup and 2010 FIFA World Cup games . Head Entity : Walter , Tail Entity : United States national team .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 99, 'raw': 160}
{'target': 600, 'success': 119, 'raw': 192}
{'target': 600, 'success': 141, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 183, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 224, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 266, 'raw': 416}
{'target': 600, 'success': 287, 'raw': 448}
{'target': 600, 'success': 312, 'raw': 480}
{'target': 600, 'success': 330, 'raw': 512}
{'target': 600, 'success': 348, 'raw': 544}
{'target': 600, 'success': 367, 'raw': 576}
{'target': 600, 'success': 386, 'raw': 608}
{'target': 600, 'success': 404, 'raw': 640}
{'target': 600, 'success': 421, 'raw': 672}
{'target': 600, 'success': 448, 'raw': 704}
{'target': 600, 'success': 465, 'raw': 736}
{'target': 600, 'success': 482, 'raw': 768}
{'target': 600, 'success': 499, 'raw': 800}
{'target': 600, 'success': 525, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 574, 'raw': 896}
{'target': 600, 'success': 594, 'raw': 928}
{'target': 600, 'success': 611, 'raw': 960}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.6364583333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : publisher . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : season 2 , Tail Entity : the Walking Dead .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8288043478260869, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 17454
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17554, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.45it/s]Extractor Estimating: 2it [00:01,  1.45it/s]Extractor Estimating: 3it [00:02,  1.48it/s]Extractor Estimating: 4it [00:02,  1.49it/s]Extractor Estimating: 5it [00:03,  1.52it/s]Extractor Estimating: 6it [00:04,  1.50it/s]Extractor Estimating: 7it [00:04,  1.56it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.59it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.59it/s]Extractor Estimating: 14it [00:08,  1.61it/s]Extractor Estimating: 15it [00:09,  1.55it/s]Extractor Estimating: 16it [00:10,  1.57it/s]Extractor Estimating: 17it [00:10,  1.56it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:12,  1.56it/s]Extractor Estimating: 20it [00:12,  1.65it/s]Extractor Estimating: 21it [00:13,  1.67it/s]Extractor Estimating: 22it [00:14,  1.60it/s]Extractor Estimating: 23it [00:14,  1.61it/s]Extractor Estimating: 24it [00:15,  1.64it/s]Extractor Estimating: 25it [00:15,  1.67it/s]Extractor Estimating: 26it [00:16,  1.57it/s]Extractor Estimating: 27it [00:17,  1.55it/s]Extractor Estimating: 28it [00:17,  1.48it/s]Extractor Estimating: 29it [00:18,  1.39it/s]Extractor Estimating: 30it [00:19,  1.39it/s]Extractor Estimating: 31it [00:20,  1.39it/s]Extractor Estimating: 32it [00:20,  1.41it/s]Extractor Estimating: 33it [00:21,  1.46it/s]Extractor Estimating: 34it [00:22,  1.42it/s]Extractor Estimating: 35it [00:22,  1.44it/s]Extractor Estimating: 36it [00:23,  1.46it/s]Extractor Estimating: 37it [00:24,  1.46it/s]Extractor Estimating: 38it [00:24,  1.50it/s]Extractor Estimating: 39it [00:25,  1.53it/s]Extractor Estimating: 40it [00:26,  1.51it/s]Extractor Estimating: 41it [00:26,  1.52it/s]Extractor Estimating: 42it [00:27,  1.53it/s]Extractor Estimating: 43it [00:28,  1.47it/s]Extractor Estimating: 44it [00:28,  1.50it/s]Extractor Estimating: 45it [00:29,  1.50it/s]Extractor Estimating: 46it [00:30,  1.51it/s]Extractor Estimating: 47it [00:30,  1.52it/s]Extractor Estimating: 48it [00:31,  1.49it/s]Extractor Estimating: 49it [00:32,  1.54it/s]Extractor Estimating: 50it [00:32,  1.56it/s]Extractor Estimating: 51it [00:33,  1.46it/s]Extractor Estimating: 52it [00:34,  1.48it/s]Extractor Estimating: 53it [00:34,  1.45it/s]Extractor Estimating: 54it [00:35,  1.51it/s]Extractor Estimating: 55it [00:36,  1.59it/s]Extractor Estimating: 56it [00:36,  1.67it/s]Extractor Estimating: 57it [00:37,  1.68it/s]Extractor Estimating: 58it [00:37,  1.66it/s]Extractor Estimating: 59it [00:38,  1.59it/s]Extractor Estimating: 60it [00:39,  1.65it/s]Extractor Estimating: 61it [00:39,  1.63it/s]Extractor Estimating: 62it [00:40,  1.66it/s]Extractor Estimating: 63it [00:40,  1.60it/s]Extractor Estimating: 64it [00:41,  1.57it/s]Extractor Estimating: 65it [00:42,  1.57it/s]Extractor Estimating: 66it [00:42,  1.63it/s]Extractor Estimating: 67it [00:43,  1.57it/s]Extractor Estimating: 68it [00:44,  1.60it/s]Extractor Estimating: 69it [00:44,  1.53it/s]Extractor Estimating: 70it [00:45,  1.59it/s]Extractor Estimating: 71it [00:45,  1.59it/s]Extractor Estimating: 72it [00:46,  1.61it/s]Extractor Estimating: 73it [00:47,  1.53it/s]Extractor Estimating: 74it [00:48,  1.51it/s]Extractor Estimating: 75it [00:48,  1.53it/s]Extractor Estimating: 76it [00:49,  1.54it/s]Extractor Estimating: 77it [00:49,  1.51it/s]Extractor Estimating: 78it [00:50,  1.54it/s]Extractor Estimating: 79it [00:51,  1.47it/s]Extractor Estimating: 80it [00:52,  1.46it/s]Extractor Estimating: 81it [00:52,  1.47it/s]Extractor Estimating: 82it [00:53,  1.51it/s]Extractor Estimating: 83it [00:53,  1.51it/s]Extractor Estimating: 84it [00:54,  1.47it/s]Extractor Estimating: 85it [00:55,  1.50it/s]Extractor Estimating: 86it [00:55,  1.54it/s]Extractor Estimating: 87it [00:56,  1.52it/s]Extractor Estimating: 88it [00:57,  1.53it/s]Extractor Estimating: 89it [00:57,  1.59it/s]Extractor Estimating: 90it [00:58,  1.58it/s]Extractor Estimating: 91it [00:59,  1.41it/s]Extractor Estimating: 92it [00:59,  1.46it/s]Extractor Estimating: 93it [01:00,  1.49it/s]Extractor Estimating: 94it [01:01,  1.57it/s]Extractor Estimating: 95it [01:01,  1.57it/s]Extractor Estimating: 96it [01:02,  1.52it/s]Extractor Estimating: 97it [01:03,  1.51it/s]Extractor Estimating: 98it [01:03,  1.55it/s]Extractor Estimating: 99it [01:04,  1.54it/s]Extractor Estimating: 100it [01:05,  1.55it/s]Extractor Estimating: 101it [01:05,  1.55it/s]Extractor Estimating: 102it [01:06,  1.56it/s]Extractor Estimating: 103it [01:07,  1.54it/s]Extractor Estimating: 104it [01:07,  1.56it/s]Extractor Estimating: 105it [01:08,  1.60it/s]Extractor Estimating: 106it [01:08,  1.62it/s]Extractor Estimating: 107it [01:09,  1.63it/s]Extractor Estimating: 108it [01:10,  1.63it/s]Extractor Estimating: 109it [01:10,  1.70it/s]Extractor Estimating: 110it [01:11,  1.64it/s]Extractor Estimating: 111it [01:11,  1.62it/s]Extractor Estimating: 112it [01:12,  1.61it/s]Extractor Estimating: 113it [01:13,  1.64it/s]Extractor Estimating: 114it [01:13,  1.62it/s]Extractor Estimating: 115it [01:14,  1.65it/s]Extractor Estimating: 116it [01:14,  1.63it/s]Extractor Estimating: 117it [01:15,  1.59it/s]Extractor Estimating: 118it [01:16,  1.55it/s]Extractor Estimating: 119it [01:16,  1.58it/s]Extractor Estimating: 120it [01:17,  1.63it/s]Extractor Estimating: 121it [01:18,  1.59it/s]Extractor Estimating: 122it [01:18,  1.58it/s]Extractor Estimating: 123it [01:19,  1.59it/s]Extractor Estimating: 124it [01:19,  1.62it/s]Extractor Estimating: 125it [01:20,  1.65it/s]Extractor Estimating: 126it [01:21,  1.61it/s]Extractor Estimating: 127it [01:21,  1.59it/s]Extractor Estimating: 128it [01:22,  1.58it/s]Extractor Estimating: 129it [01:23,  1.54it/s]Extractor Estimating: 130it [01:24,  1.40it/s]Extractor Estimating: 131it [01:24,  1.44it/s]Extractor Estimating: 132it [01:25,  1.42it/s]Extractor Estimating: 133it [01:26,  1.40it/s]Extractor Estimating: 134it [01:26,  1.43it/s]Extractor Estimating: 135it [01:27,  1.44it/s]Extractor Estimating: 136it [01:28,  1.47it/s]Extractor Estimating: 137it [01:28,  1.49it/s]Extractor Estimating: 138it [01:29,  1.53it/s]Extractor Estimating: 139it [01:30,  1.38it/s]Extractor Estimating: 140it [01:31,  1.38it/s]Extractor Estimating: 141it [01:31,  1.44it/s]Extractor Estimating: 142it [01:32,  1.45it/s]Extractor Estimating: 143it [01:33,  1.42it/s]Extractor Estimating: 144it [01:33,  1.43it/s]Extractor Estimating: 145it [01:34,  1.50it/s]Extractor Estimating: 146it [01:35,  1.49it/s]Extractor Estimating: 147it [01:35,  1.44it/s]Extractor Estimating: 148it [01:36,  1.46it/s]Extractor Estimating: 149it [01:37,  1.47it/s]Extractor Estimating: 150it [01:37,  1.48it/s]Extractor Estimating: 151it [01:38,  1.53it/s]Extractor Estimating: 152it [01:39,  1.52it/s]Extractor Estimating: 153it [01:39,  1.56it/s]Extractor Estimating: 154it [01:40,  1.50it/s]Extractor Estimating: 155it [01:40,  1.56it/s]Extractor Estimating: 156it [01:41,  1.54it/s]Extractor Estimating: 157it [01:42,  1.51it/s]Extractor Estimating: 158it [01:42,  1.59it/s]Extractor Estimating: 159it [01:43,  1.56it/s]Extractor Estimating: 160it [01:44,  1.51it/s]Extractor Estimating: 161it [01:44,  1.54it/s]Extractor Estimating: 162it [01:45,  1.63it/s]Extractor Estimating: 163it [01:45,  1.67it/s]Extractor Estimating: 164it [01:46,  1.66it/s]Extractor Estimating: 165it [01:47,  1.60it/s]Extractor Estimating: 166it [01:47,  1.60it/s]Extractor Estimating: 167it [01:48,  1.61it/s]Extractor Estimating: 168it [01:49,  1.62it/s]Extractor Estimating: 169it [01:49,  1.68it/s]Extractor Estimating: 170it [01:50,  1.53it/s]Extractor Estimating: 171it [01:51,  1.54it/s]Extractor Estimating: 172it [01:51,  1.56it/s]Extractor Estimating: 173it [01:52,  1.60it/s]Extractor Estimating: 174it [01:52,  1.58it/s]Extractor Estimating: 175it [01:53,  1.60it/s]Extractor Estimating: 176it [01:54,  1.58it/s]Extractor Estimating: 177it [01:54,  1.55it/s]Extractor Estimating: 178it [01:55,  1.46it/s]Extractor Estimating: 179it [01:56,  1.49it/s]Extractor Estimating: 180it [01:56,  1.54it/s]Extractor Estimating: 181it [01:57,  1.57it/s]Extractor Estimating: 182it [01:58,  1.58it/s]Extractor Estimating: 183it [01:58,  1.54it/s]Extractor Estimating: 184it [01:59,  1.49it/s]Extractor Estimating: 185it [02:00,  1.45it/s]Extractor Estimating: 186it [02:01,  1.40it/s]Extractor Estimating: 187it [02:01,  1.43it/s]Extractor Estimating: 188it [02:02,  1.44it/s]Extractor Estimating: 189it [02:03,  1.41it/s]Extractor Estimating: 190it [02:03,  1.44it/s]Extractor Estimating: 191it [02:04,  1.51it/s]Extractor Estimating: 192it [02:05,  1.47it/s]Extractor Estimating: 193it [02:05,  1.40it/s]Extractor Estimating: 194it [02:06,  1.48it/s]Extractor Estimating: 195it [02:07,  1.37it/s]Extractor Estimating: 196it [02:08,  1.36it/s]Extractor Estimating: 197it [02:08,  1.42it/s]Extractor Estimating: 198it [02:09,  1.46it/s]Extractor Estimating: 199it [02:10,  1.50it/s]Extractor Estimating: 200it [02:10,  1.53it/s]Extractor Estimating: 201it [02:11,  1.55it/s]Extractor Estimating: 202it [02:11,  1.52it/s]Extractor Estimating: 203it [02:12,  1.47it/s]Extractor Estimating: 204it [02:13,  1.51it/s]Extractor Estimating: 205it [02:13,  1.51it/s]Extractor Estimating: 206it [02:14,  1.54it/s]Extractor Estimating: 207it [02:15,  1.57it/s]Extractor Estimating: 208it [02:15,  1.58it/s]Extractor Estimating: 209it [02:16,  1.52it/s]Extractor Estimating: 210it [02:17,  1.47it/s]Extractor Estimating: 211it [02:17,  1.53it/s]Extractor Estimating: 212it [02:18,  1.39it/s]Extractor Estimating: 213it [02:19,  1.40it/s]Extractor Estimating: 214it [02:20,  1.37it/s]Extractor Estimating: 215it [02:20,  1.39it/s]Extractor Estimating: 216it [02:21,  1.40it/s]Extractor Estimating: 217it [02:22,  1.43it/s]Extractor Estimating: 218it [02:22,  1.43it/s]Extractor Estimating: 219it [02:23,  1.39it/s]Extractor Estimating: 220it [02:24,  1.43it/s]Extractor Estimating: 221it [02:25,  1.44it/s]Extractor Estimating: 222it [02:25,  1.43it/s]Extractor Estimating: 223it [02:26,  1.43it/s]Extractor Estimating: 224it [02:27,  1.46it/s]Extractor Estimating: 225it [02:27,  1.47it/s]Extractor Estimating: 226it [02:28,  1.45it/s]Extractor Estimating: 227it [02:29,  1.44it/s]Extractor Estimating: 228it [02:29,  1.46it/s]Extractor Estimating: 229it [02:30,  1.47it/s]Extractor Estimating: 230it [02:31,  1.42it/s]Extractor Estimating: 231it [02:31,  1.43it/s]Extractor Estimating: 232it [02:32,  1.46it/s]Extractor Estimating: 233it [02:33,  1.50it/s]Extractor Estimating: 234it [02:34,  1.36it/s]Extractor Estimating: 235it [02:34,  1.38it/s]Extractor Estimating: 236it [02:35,  1.41it/s]Extractor Estimating: 237it [02:36,  1.45it/s]Extractor Estimating: 238it [02:36,  1.46it/s]Extractor Estimating: 239it [02:37,  1.50it/s]Extractor Estimating: 240it [02:38,  1.44it/s]Extractor Estimating: 241it [02:38,  1.48it/s]Extractor Estimating: 242it [02:39,  1.53it/s]Extractor Estimating: 243it [02:40,  1.55it/s]Extractor Estimating: 244it [02:40,  1.54it/s]Extractor Estimating: 245it [02:41,  1.50it/s]Extractor Estimating: 246it [02:42,  1.49it/s]Extractor Estimating: 247it [02:42,  1.52it/s]Extractor Estimating: 248it [02:43,  1.53it/s]Extractor Estimating: 249it [02:44,  1.51it/s]Extractor Estimating: 250it [02:44,  1.49it/s]Extractor Estimating: 251it [02:45,  1.49it/s]Extractor Estimating: 252it [02:46,  1.53it/s]Extractor Estimating: 253it [02:46,  1.50it/s]Extractor Estimating: 254it [02:47,  1.48it/s]Extractor Estimating: 255it [02:48,  1.49it/s]Extractor Estimating: 256it [02:48,  1.45it/s]Extractor Estimating: 257it [02:49,  1.44it/s]Extractor Estimating: 258it [02:50,  1.43it/s]Extractor Estimating: 259it [02:50,  1.43it/s]Extractor Estimating: 260it [02:51,  1.38it/s]Extractor Estimating: 261it [02:52,  1.39it/s]Extractor Estimating: 262it [02:53,  1.41it/s]Extractor Estimating: 263it [02:53,  1.39it/s]Extractor Estimating: 264it [02:54,  1.44it/s]Extractor Estimating: 265it [02:55,  1.45it/s]Extractor Estimating: 266it [02:55,  1.49it/s]Extractor Estimating: 267it [02:56,  1.50it/s]Extractor Estimating: 268it [02:57,  1.46it/s]Extractor Estimating: 269it [02:57,  1.48it/s]Extractor Estimating: 270it [02:58,  1.43it/s]Extractor Estimating: 271it [02:59,  1.46it/s]Extractor Estimating: 272it [02:59,  1.45it/s]Extractor Estimating: 273it [03:00,  1.51it/s]Extractor Estimating: 274it [03:01,  1.48it/s]Extractor Estimating: 275it [03:02,  1.44it/s]Extractor Estimating: 276it [03:02,  1.50it/s]Extractor Estimating: 277it [03:03,  1.54it/s]Extractor Estimating: 278it [03:03,  1.55it/s]Extractor Estimating: 279it [03:04,  1.56it/s]Extractor Estimating: 280it [03:05,  1.55it/s]Extractor Estimating: 281it [03:05,  1.58it/s]Extractor Estimating: 282it [03:06,  1.58it/s]Extractor Estimating: 283it [03:06,  1.58it/s]Extractor Estimating: 284it [03:07,  1.63it/s]Extractor Estimating: 285it [03:08,  1.55it/s]Extractor Estimating: 286it [03:08,  1.58it/s]Extractor Estimating: 287it [03:09,  1.54it/s]Extractor Estimating: 288it [03:10,  1.55it/s]Extractor Estimating: 289it [03:10,  1.56it/s]Extractor Estimating: 290it [03:11,  1.59it/s]Extractor Estimating: 291it [03:12,  1.65it/s]Extractor Estimating: 292it [03:12,  1.47it/s]Extractor Estimating: 293it [03:13,  1.48it/s]Extractor Estimating: 294it [03:14,  1.49it/s]Extractor Estimating: 295it [03:14,  1.53it/s]Extractor Estimating: 296it [03:15,  1.58it/s]Extractor Estimating: 297it [03:16,  1.54it/s]Extractor Estimating: 298it [03:16,  1.58it/s]Extractor Estimating: 299it [03:17,  1.57it/s]Extractor Estimating: 300it [03:17,  1.56it/s]Extractor Estimating: 301it [03:18,  1.59it/s]Extractor Estimating: 302it [03:19,  1.49it/s]Extractor Estimating: 303it [03:19,  1.49it/s]Extractor Estimating: 304it [03:20,  1.52it/s]Extractor Estimating: 305it [03:21,  1.54it/s]Extractor Estimating: 306it [03:21,  1.51it/s]Extractor Estimating: 307it [03:22,  1.45it/s]Extractor Estimating: 308it [03:23,  1.45it/s]Extractor Estimating: 309it [03:24,  1.50it/s]Extractor Estimating: 310it [03:24,  1.53it/s]Extractor Estimating: 311it [03:25,  1.57it/s]Extractor Estimating: 312it [03:25,  1.57it/s]Extractor Estimating: 313it [03:26,  1.58it/s]Extractor Estimating: 314it [03:27,  1.58it/s]Extractor Estimating: 315it [03:27,  1.55it/s]Extractor Estimating: 316it [03:28,  1.58it/s]Extractor Estimating: 317it [03:29,  1.57it/s]Extractor Estimating: 318it [03:29,  1.55it/s]Extractor Estimating: 319it [03:30,  1.55it/s]Extractor Estimating: 320it [03:30,  1.62it/s]Extractor Estimating: 321it [03:31,  1.61it/s]Extractor Estimating: 322it [03:32,  1.63it/s]Extractor Estimating: 323it [03:32,  1.63it/s]Extractor Estimating: 324it [03:33,  1.59it/s]Extractor Estimating: 325it [03:34,  1.57it/s]Extractor Estimating: 326it [03:34,  1.45it/s]Extractor Estimating: 327it [03:35,  1.55it/s]Extractor Estimating: 328it [03:35,  1.63it/s]Extractor Estimating: 329it [03:36,  1.56it/s]Extractor Estimating: 330it [03:37,  1.60it/s]Extractor Estimating: 331it [03:37,  1.63it/s]Extractor Estimating: 332it [03:38,  1.62it/s]Extractor Estimating: 333it [03:39,  1.65it/s]Extractor Estimating: 334it [03:39,  1.64it/s]Extractor Estimating: 335it [03:40,  1.67it/s]Extractor Estimating: 336it [03:40,  1.71it/s]Extractor Estimating: 337it [03:41,  1.70it/s]Extractor Estimating: 338it [03:42,  1.67it/s]Extractor Estimating: 339it [03:42,  1.56it/s]Extractor Estimating: 340it [03:43,  1.59it/s]Extractor Estimating: 341it [03:43,  1.59it/s]Extractor Estimating: 342it [03:44,  1.68it/s]Extractor Estimating: 343it [03:45,  1.64it/s]Extractor Estimating: 344it [03:45,  1.60it/s]Extractor Estimating: 345it [03:46,  1.61it/s]Extractor Estimating: 346it [03:47,  1.59it/s]Extractor Estimating: 347it [03:47,  1.62it/s]Extractor Estimating: 348it [03:48,  1.60it/s]Extractor Estimating: 349it [03:48,  1.55it/s]Extractor Estimating: 350it [03:49,  1.60it/s]Extractor Estimating: 351it [03:50,  1.61it/s]Extractor Estimating: 352it [03:50,  1.60it/s]Extractor Estimating: 353it [03:51,  1.59it/s]Extractor Estimating: 354it [03:52,  1.60it/s]Extractor Estimating: 355it [03:52,  1.60it/s]Extractor Estimating: 356it [03:53,  1.58it/s]Extractor Estimating: 357it [03:53,  1.61it/s]Extractor Estimating: 358it [03:54,  1.59it/s]Extractor Estimating: 359it [03:55,  1.61it/s]Extractor Estimating: 360it [03:55,  1.60it/s]Extractor Estimating: 361it [03:56,  1.62it/s]Extractor Estimating: 362it [03:56,  1.69it/s]Extractor Estimating: 363it [03:57,  1.71it/s]Extractor Estimating: 364it [03:58,  1.69it/s]Extractor Estimating: 365it [03:58,  1.69it/s]Extractor Estimating: 366it [03:59,  1.63it/s]Extractor Estimating: 367it [03:59,  1.64it/s]Extractor Estimating: 368it [04:00,  1.58it/s]Extractor Estimating: 369it [04:01,  1.44it/s]Extractor Estimating: 370it [04:02,  1.51it/s]Extractor Estimating: 371it [04:02,  1.57it/s]Extractor Estimating: 372it [04:03,  1.54it/s]Extractor Estimating: 373it [04:03,  1.59it/s]Extractor Estimating: 374it [04:04,  1.62it/s]Extractor Estimating: 375it [04:05,  1.62it/s]Extractor Estimating: 376it [04:05,  1.63it/s]Extractor Estimating: 377it [04:06,  1.64it/s]Extractor Estimating: 378it [04:06,  1.69it/s]Extractor Estimating: 379it [04:07,  1.69it/s]Extractor Estimating: 380it [04:08,  1.68it/s]Extractor Estimating: 381it [04:08,  1.61it/s]Extractor Estimating: 382it [04:09,  1.62it/s]Extractor Estimating: 383it [04:09,  1.61it/s]Extractor Estimating: 384it [04:10,  1.60it/s]Extractor Estimating: 385it [04:11,  1.60it/s]Extractor Estimating: 386it [04:11,  1.55it/s]Extractor Estimating: 387it [04:12,  1.60it/s]Extractor Estimating: 388it [04:13,  1.63it/s]Extractor Estimating: 389it [04:13,  1.67it/s]Extractor Estimating: 390it [04:14,  1.60it/s]Extractor Estimating: 391it [04:15,  1.58it/s]Extractor Estimating: 392it [04:15,  1.60it/s]Extractor Estimating: 393it [04:16,  1.62it/s]Extractor Estimating: 394it [04:16,  1.65it/s]Extractor Estimating: 395it [04:17,  1.65it/s]Extractor Estimating: 396it [04:18,  1.59it/s]Extractor Estimating: 397it [04:18,  1.58it/s]Extractor Estimating: 398it [04:19,  1.58it/s]Extractor Estimating: 399it [04:19,  1.59it/s]Extractor Estimating: 400it [04:20,  1.58it/s]Extractor Estimating: 401it [04:21,  1.54it/s]Extractor Estimating: 402it [04:22,  1.50it/s]Extractor Estimating: 403it [04:22,  1.52it/s]Extractor Estimating: 404it [04:23,  1.52it/s]Extractor Estimating: 405it [04:23,  1.54it/s]Extractor Estimating: 406it [04:24,  1.51it/s]Extractor Estimating: 407it [04:25,  1.52it/s]Extractor Estimating: 408it [04:25,  1.51it/s]Extractor Estimating: 409it [04:26,  1.59it/s]Extractor Estimating: 410it [04:27,  1.58it/s]Extractor Estimating: 411it [04:27,  1.55it/s]Extractor Estimating: 412it [04:28,  1.51it/s]Extractor Estimating: 413it [04:29,  1.62it/s]Extractor Estimating: 414it [04:29,  1.61it/s]Extractor Estimating: 415it [04:30,  1.56it/s]Extractor Estimating: 416it [04:31,  1.52it/s]Extractor Estimating: 417it [04:31,  1.55it/s]Extractor Estimating: 418it [04:32,  1.59it/s]Extractor Estimating: 419it [04:32,  1.61it/s]Extractor Estimating: 420it [04:33,  1.60it/s]Extractor Estimating: 421it [04:34,  1.60it/s]Extractor Estimating: 422it [04:34,  1.59it/s]Extractor Estimating: 423it [04:35,  1.57it/s]Extractor Estimating: 424it [04:36,  1.57it/s]Extractor Estimating: 425it [04:36,  1.57it/s]Extractor Estimating: 426it [04:37,  1.59it/s]Extractor Estimating: 427it [04:37,  1.57it/s]Extractor Estimating: 428it [04:38,  1.58it/s]Extractor Estimating: 429it [04:39,  1.57it/s]Extractor Estimating: 430it [04:39,  1.54it/s]Extractor Estimating: 431it [04:40,  1.56it/s]Extractor Estimating: 432it [04:41,  1.64it/s]Extractor Estimating: 433it [04:41,  1.66it/s]Extractor Estimating: 434it [04:42,  1.60it/s]Extractor Estimating: 435it [04:42,  1.63it/s]Extractor Estimating: 436it [04:43,  1.60it/s]Extractor Estimating: 437it [04:44,  1.62it/s]Extractor Estimating: 438it [04:44,  1.62it/s]Extractor Estimating: 439it [04:45,  1.59it/s]Extractor Estimating: 440it [04:46,  1.60it/s]Extractor Estimating: 441it [04:46,  1.65it/s]Extractor Estimating: 442it [04:47,  1.62it/s]Extractor Estimating: 443it [04:47,  1.62it/s]Extractor Estimating: 444it [04:48,  1.62it/s]Extractor Estimating: 445it [04:49,  1.66it/s]Extractor Estimating: 446it [04:49,  1.67it/s]Extractor Estimating: 447it [04:50,  1.68it/s]Extractor Estimating: 448it [04:50,  1.64it/s]Extractor Estimating: 449it [04:51,  1.68it/s]Extractor Estimating: 450it [04:52,  1.44it/s]Extractor Estimating: 451it [04:52,  1.48it/s]Extractor Estimating: 452it [04:53,  1.44it/s]Extractor Estimating: 453it [04:54,  1.33it/s]Extractor Estimating: 454it [04:55,  1.34it/s]Extractor Estimating: 455it [04:55,  1.41it/s]Extractor Estimating: 456it [04:56,  1.49it/s]Extractor Estimating: 457it [04:57,  1.48it/s]Extractor Estimating: 458it [04:57,  1.47it/s]Extractor Estimating: 459it [04:58,  1.42it/s]Extractor Estimating: 460it [04:59,  1.42it/s]Extractor Estimating: 461it [05:00,  1.45it/s]Extractor Estimating: 462it [05:00,  1.44it/s]Extractor Estimating: 463it [05:01,  1.46it/s]Extractor Estimating: 464it [05:02,  1.42it/s]Extractor Estimating: 465it [05:02,  1.40it/s]Extractor Estimating: 466it [05:03,  1.40it/s]Extractor Estimating: 467it [05:04,  1.40it/s]Extractor Estimating: 468it [05:05,  1.41it/s]Extractor Estimating: 469it [05:05,  1.39it/s]Extractor Estimating: 470it [05:06,  1.44it/s]Extractor Estimating: 471it [05:07,  1.42it/s]Extractor Estimating: 472it [05:07,  1.41it/s]Extractor Estimating: 473it [05:08,  1.42it/s]Extractor Estimating: 474it [05:09,  1.44it/s]Extractor Estimating: 475it [05:09,  1.43it/s]Extractor Estimating: 476it [05:10,  1.47it/s]Extractor Estimating: 477it [05:11,  1.51it/s]Extractor Estimating: 478it [05:11,  1.51it/s]Extractor Estimating: 479it [05:12,  1.58it/s]Extractor Estimating: 480it [05:13,  1.57it/s]Extractor Estimating: 481it [05:13,  1.61it/s]Extractor Estimating: 482it [05:14,  1.60it/s]Extractor Estimating: 483it [05:14,  1.56it/s]Extractor Estimating: 484it [05:15,  1.55it/s]Extractor Estimating: 485it [05:16,  1.56it/s]Extractor Estimating: 486it [05:16,  1.50it/s]Extractor Estimating: 487it [05:17,  1.54it/s]Extractor Estimating: 488it [05:18,  1.61it/s]Extractor Estimating: 489it [05:18,  1.66it/s]Extractor Estimating: 490it [05:19,  1.58it/s]Extractor Estimating: 491it [05:19,  1.63it/s]Extractor Estimating: 492it [05:20,  1.58it/s]Extractor Estimating: 493it [05:21,  1.58it/s]Extractor Estimating: 494it [05:21,  1.53it/s]Extractor Estimating: 495it [05:22,  1.58it/s]Extractor Estimating: 496it [05:23,  1.55it/s]Extractor Estimating: 497it [05:23,  1.58it/s]Extractor Estimating: 498it [05:24,  1.58it/s]Extractor Estimating: 499it [05:25,  1.59it/s]Extractor Estimating: 500it [05:25,  1.67it/s]Extractor Estimating: 500it [05:25,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:50,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:50,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:50,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:50,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:50,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:42:50,720 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:42:50,721 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:42:51,362 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:42:52,446 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:42:52,472 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:55,503 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:55,523 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:55,524 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:55,524 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:42:55,524 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:42:56,171 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:42:56,173 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:42:56,761 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:42:56,934 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:42:56,934 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 13:54:05,021 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 13:54:05,363 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 10462 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 25199
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25299, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25299, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.102, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.148, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.099, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.114, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.103, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 2.195, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 264, avg_time 1.118, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 364, avg_time 1.107, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.120, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.131, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 228, avg_time 2.148, loss:nan
g_step 1200, step 328, avg_time 1.104, loss:nan
g_step 1300, step 428, avg_time 1.123, loss:nan
g_step 1400, step 92, avg_time 1.099, loss:nan
g_step 1500, step 192, avg_time 1.109, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 292, avg_time 2.173, loss:nan
g_step 1700, step 392, avg_time 1.117, loss:nan
g_step 1800, step 56, avg_time 1.135, loss:nan
g_step 1900, step 156, avg_time 1.122, loss:nan
g_step 2000, step 256, avg_time 1.098, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 356, avg_time 2.147, loss:nan
g_step 2200, step 20, avg_time 1.100, loss:nan
g_step 2300, step 120, avg_time 1.115, loss:nan
g_step 2400, step 220, avg_time 1.096, loss:nan
g_step 2500, step 320, avg_time 1.122, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 420, avg_time 2.158, loss:nan
g_step 2700, step 84, avg_time 1.104, loss:nan
g_step 2800, step 184, avg_time 1.101, loss:nan
g_step 2900, step 284, avg_time 1.119, loss:nan
g_step 3000, step 384, avg_time 1.123, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.140, loss:nan
g_step 3200, step 148, avg_time 1.104, loss:nan
g_step 3300, step 248, avg_time 1.100, loss:nan
g_step 3400, step 348, avg_time 1.108, loss:nan
g_step 3500, step 12, avg_time 1.099, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 2.171, loss:nan
g_step 3700, step 212, avg_time 1.103, loss:nan
g_step 3800, step 312, avg_time 1.084, loss:nan
g_step 3900, step 412, avg_time 1.120, loss:nan
g_step 4000, step 76, avg_time 1.112, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 2.151, loss:nan
g_step 4200, step 276, avg_time 1.091, loss:nan
g_step 4300, step 376, avg_time 1.102, loss:nan
g_step 4400, step 40, avg_time 1.089, loss:nan
g_step 4500, step 140, avg_time 1.095, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 240, avg_time 2.148, loss:nan
g_step 4700, step 340, avg_time 1.102, loss:nan
g_step 4800, step 4, avg_time 1.108, loss:nan
g_step 4900, step 104, avg_time 1.110, loss:nan
g_step 5000, step 204, avg_time 1.119, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 304, avg_time 2.136, loss:nan
g_step 5200, step 404, avg_time 1.102, loss:nan
g_step 5300, step 68, avg_time 1.114, loss:nan
g_step 5400, step 168, avg_time 1.096, loss:nan
g_step 5500, step 268, avg_time 1.138, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 368, avg_time 2.157, loss:nan
g_step 5700, step 32, avg_time 1.081, loss:nan
g_step 5800, step 132, avg_time 1.135, loss:nan
g_step 5900, step 232, avg_time 1.091, loss:nan
g_step 6000, step 332, avg_time 1.102, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 432, avg_time 2.157, loss:nan
g_step 6200, step 96, avg_time 1.102, loss:nan
g_step 6300, step 196, avg_time 1.112, loss:nan
g_step 6400, step 296, avg_time 1.094, loss:nan
g_step 6500, step 396, avg_time 1.116, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 60, avg_time 2.186, loss:nan
g_step 6700, step 160, avg_time 1.084, loss:nan
g_step 6800, step 260, avg_time 1.118, loss:nan
g_step 6900, step 360, avg_time 1.107, loss:nan
g_step 7000, step 24, avg_time 1.152, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 124, avg_time 2.174, loss:nan
g_step 7200, step 224, avg_time 1.108, loss:nan
g_step 7300, step 324, avg_time 1.100, loss:nan
g_step 7400, step 424, avg_time 1.095, loss:nan
g_step 7500, step 88, avg_time 1.094, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 188, avg_time 2.175, loss:nan
g_step 7700, step 288, avg_time 1.115, loss:nan
g_step 7800, step 388, avg_time 1.119, loss:nan
g_step 7900, step 52, avg_time 1.131, loss:nan
g_step 8000, step 152, avg_time 1.095, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 252, avg_time 2.157, loss:nan
g_step 8200, step 352, avg_time 1.100, loss:nan
g_step 8300, step 16, avg_time 1.139, loss:nan
g_step 8400, step 116, avg_time 1.111, loss:nan
g_step 8500, step 216, avg_time 1.120, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 316, avg_time 2.160, loss:nan
g_step 8700, step 416, avg_time 1.116, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:54:05 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:54:05 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-54-05_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:54:06 - WARNING - datasets.builder -   Using custom data configuration default-8667cedf444c3056
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8667cedf444c3056/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:54:09,082 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:54:09,084 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:54:09,084 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:54:09,085 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:54:09,166 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:54:09,216 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:54:09,216 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:54:09,216 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:54:09,216 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:54:09,216 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:54:09,216 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:54:09,585 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:54:12,691 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:54:12,719 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8667cedf444c3056/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.75ba/s] 18%|        | 2/11 [00:00<00:02,  3.66ba/s] 27%|       | 3/11 [00:00<00:01,  4.06ba/s] 36%|      | 4/11 [00:01<00:02,  3.39ba/s] 45%|     | 5/11 [00:01<00:01,  3.74ba/s] 55%|    | 6/11 [00:01<00:01,  3.99ba/s] 64%|   | 7/11 [00:01<00:00,  4.14ba/s] 73%|  | 8/11 [00:02<00:00,  4.27ba/s] 82%| | 9/11 [00:02<00:00,  4.37ba/s] 91%| | 10/11 [00:02<00:00,  4.43ba/s]100%|| 11/11 [00:02<00:00,  5.25ba/s]100%|| 11/11 [00:02<00:00,  4.26ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.16ba/s] 50%|     | 2/4 [00:00<00:00,  3.83ba/s] 75%|  | 3/4 [00:00<00:00,  4.13ba/s]100%|| 4/4 [00:00<00:00,  5.25ba/s]100%|| 4/4 [00:00<00:00,  4.60ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:02,  4.76ba/s] 27%|       | 3/11 [00:00<00:01,  7.91ba/s] 45%|     | 5/11 [00:00<00:00,  8.92ba/s] 64%|   | 7/11 [00:00<00:00,  9.42ba/s] 82%| | 9/11 [00:00<00:00,  9.74ba/s]100%|| 11/11 [00:01<00:00, 10.80ba/s]100%|| 11/11 [00:01<00:00,  9.66ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  6.14ba/s] 75%|  | 3/4 [00:00<00:00,  8.71ba/s]100%|| 4/4 [00:00<00:00,  9.82ba/s]
[INFO|trainer.py:414] 2023-08-28 13:54:19,476 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:54:19,564 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:54:19,564 >>   Num examples = 10479
[INFO|trainer.py:1149] 2023-08-28 13:54:19,564 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:54:19,564 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:54:19,564 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:54:19,564 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:54:19,565 >>   Total optimization steps = 820
  0%|          | 0/820 [00:00<?, ?it/s]  0%|          | 1/820 [00:00<12:05,  1.13it/s]  0%|          | 2/820 [00:01<09:25,  1.45it/s]  0%|          | 3/820 [00:01<08:29,  1.60it/s]  0%|          | 4/820 [00:02<07:02,  1.93it/s]  1%|          | 5/820 [00:02<06:18,  2.16it/s]  1%|          | 6/820 [00:03<05:50,  2.32it/s]  1%|          | 7/820 [00:03<05:10,  2.62it/s]  1%|          | 8/820 [00:03<04:55,  2.75it/s]  1%|          | 9/820 [00:04<04:52,  2.78it/s]  1%|          | 10/820 [00:04<04:32,  2.97it/s]  1%|         | 11/820 [00:04<04:19,  3.12it/s]  1%|         | 12/820 [00:04<04:09,  3.23it/s]  2%|         | 13/820 [00:05<04:13,  3.18it/s]  2%|         | 14/820 [00:05<04:05,  3.28it/s]  2%|         | 15/820 [00:05<04:00,  3.35it/s]  2%|         | 16/820 [00:06<03:57,  3.38it/s]  2%|         | 17/820 [00:06<03:54,  3.43it/s]  2%|         | 18/820 [00:06<03:51,  3.47it/s]  2%|         | 19/820 [00:06<03:48,  3.51it/s]  2%|         | 20/820 [00:07<03:45,  3.54it/s]  3%|         | 21/820 [00:07<03:44,  3.55it/s]  3%|         | 22/820 [00:07<03:43,  3.57it/s]  3%|         | 23/820 [00:08<03:43,  3.57it/s]  3%|         | 24/820 [00:08<03:42,  3.58it/s]  3%|         | 25/820 [00:08<03:42,  3.58it/s]  3%|         | 26/820 [00:08<03:41,  3.59it/s]  3%|         | 27/820 [00:09<03:49,  3.46it/s]  3%|         | 28/820 [00:09<03:46,  3.50it/s]  4%|         | 29/820 [00:09<03:44,  3.53it/s]  4%|         | 30/820 [00:10<03:42,  3.55it/s]  4%|         | 31/820 [00:10<03:41,  3.56it/s]  4%|         | 32/820 [00:10<03:40,  3.57it/s]  4%|         | 33/820 [00:10<03:39,  3.58it/s]  4%|         | 34/820 [00:11<03:57,  3.31it/s]  4%|         | 35/820 [00:11<04:03,  3.23it/s]  4%|         | 36/820 [00:11<03:55,  3.33it/s]  5%|         | 37/820 [00:12<03:50,  3.40it/s]  5%|         | 38/820 [00:12<03:46,  3.45it/s]  5%|         | 39/820 [00:12<03:43,  3.49it/s]  5%|         | 40/820 [00:12<03:41,  3.52it/s]  5%|         | 41/820 [00:13<03:39,  3.54it/s]  5%|         | 42/820 [00:13<03:38,  3.55it/s]  5%|         | 43/820 [00:13<03:38,  3.56it/s]  5%|         | 44/820 [00:14<03:37,  3.57it/s]  5%|         | 45/820 [00:14<03:44,  3.46it/s]  6%|         | 46/820 [00:14<03:41,  3.49it/s]  6%|         | 47/820 [00:14<03:39,  3.52it/s]  6%|         | 48/820 [00:15<03:38,  3.54it/s]  6%|         | 49/820 [00:15<03:36,  3.56it/s]  6%|         | 50/820 [00:15<03:36,  3.56it/s]  6%|         | 51/820 [00:16<03:35,  3.57it/s]  6%|         | 52/820 [00:16<03:35,  3.57it/s]  6%|         | 53/820 [00:16<03:34,  3.57it/s]  7%|         | 54/820 [00:16<03:34,  3.57it/s]  7%|         | 55/820 [00:17<03:34,  3.57it/s]  7%|         | 56/820 [00:17<03:33,  3.57it/s]  7%|         | 57/820 [00:17<03:33,  3.57it/s]  7%|         | 58/820 [00:17<03:33,  3.58it/s]  7%|         | 59/820 [00:18<03:33,  3.57it/s]  7%|         | 60/820 [00:18<03:32,  3.57it/s]  7%|         | 61/820 [00:18<03:32,  3.57it/s]  8%|         | 62/820 [00:19<03:32,  3.57it/s]  8%|         | 63/820 [00:19<03:45,  3.36it/s]  8%|         | 64/820 [00:19<03:40,  3.42it/s]  8%|         | 65/820 [00:19<03:37,  3.46it/s]  8%|         | 66/820 [00:20<03:35,  3.50it/s]  8%|         | 67/820 [00:20<03:34,  3.52it/s]  8%|         | 68/820 [00:20<03:32,  3.53it/s]  8%|         | 69/820 [00:21<03:31,  3.54it/s]  9%|         | 70/820 [00:21<03:52,  3.22it/s]  9%|         | 71/820 [00:21<03:45,  3.32it/s]  9%|         | 72/820 [00:22<03:40,  3.39it/s]  9%|         | 73/820 [00:22<03:36,  3.45it/s]  9%|         | 74/820 [00:22<03:33,  3.49it/s]  9%|         | 75/820 [00:22<03:31,  3.52it/s]  9%|         | 76/820 [00:23<03:30,  3.54it/s]  9%|         | 77/820 [00:23<03:29,  3.55it/s] 10%|         | 78/820 [00:23<03:28,  3.56it/s] 10%|         | 79/820 [00:24<03:27,  3.57it/s] 10%|         | 80/820 [00:24<03:26,  3.58it/s] 10%|         | 81/820 [00:24<03:38,  3.39it/s] 10%|         | 82/820 [00:24<03:34,  3.44it/s] 10%|         | 83/820 [00:25<03:31,  3.48it/s] 10%|         | 84/820 [00:25<03:29,  3.51it/s] 10%|         | 85/820 [00:25<03:28,  3.53it/s] 10%|         | 86/820 [00:26<03:27,  3.54it/s] 11%|         | 87/820 [00:26<03:26,  3.55it/s] 11%|         | 88/820 [00:26<03:33,  3.44it/s] 11%|         | 89/820 [00:26<03:30,  3.48it/s] 11%|         | 90/820 [00:27<03:27,  3.51it/s] 11%|         | 91/820 [00:27<03:26,  3.53it/s] 11%|         | 92/820 [00:27<03:24,  3.55it/s] 11%|        | 93/820 [00:27<03:24,  3.56it/s] 11%|        | 94/820 [00:28<03:23,  3.56it/s] 12%|        | 95/820 [00:28<03:23,  3.57it/s] 12%|        | 96/820 [00:28<03:22,  3.57it/s] 12%|        | 97/820 [00:29<03:21,  3.58it/s] 12%|        | 98/820 [00:29<03:21,  3.58it/s] 12%|        | 99/820 [00:29<03:41,  3.26it/s] 12%|        | 100/820 [00:30<03:35,  3.35it/s] 12%|        | 101/820 [00:30<03:30,  3.41it/s] 12%|        | 102/820 [00:30<03:27,  3.46it/s] 13%|        | 103/820 [00:30<03:25,  3.50it/s] 13%|        | 104/820 [00:31<03:22,  3.53it/s] 13%|        | 105/820 [00:31<03:21,  3.54it/s] 13%|        | 106/820 [00:31<03:21,  3.55it/s] 13%|        | 107/820 [00:31<03:20,  3.56it/s] 13%|        | 108/820 [00:32<03:19,  3.57it/s] 13%|        | 109/820 [00:32<03:19,  3.57it/s] 13%|        | 110/820 [00:32<03:18,  3.57it/s] 14%|        | 111/820 [00:33<03:18,  3.58it/s] 14%|        | 112/820 [00:33<03:17,  3.58it/s] 14%|        | 113/820 [00:33<03:17,  3.58it/s] 14%|        | 114/820 [00:33<03:17,  3.58it/s] 14%|        | 115/820 [00:34<03:17,  3.58it/s] 14%|        | 116/820 [00:34<03:16,  3.58it/s] 14%|        | 117/820 [00:34<03:25,  3.43it/s] 14%|        | 118/820 [00:35<03:22,  3.47it/s] 15%|        | 119/820 [00:35<03:20,  3.50it/s] 15%|        | 120/820 [00:35<03:18,  3.53it/s] 15%|        | 121/820 [00:35<03:17,  3.54it/s] 15%|        | 122/820 [00:36<03:16,  3.55it/s] 15%|        | 123/820 [00:36<03:15,  3.56it/s] 15%|        | 124/820 [00:36<03:15,  3.57it/s] 15%|        | 125/820 [00:37<03:14,  3.57it/s] 15%|        | 126/820 [00:37<03:13,  3.58it/s] 15%|        | 127/820 [00:37<03:13,  3.58it/s] 16%|        | 128/820 [00:37<03:12,  3.59it/s] 16%|        | 129/820 [00:38<03:12,  3.59it/s] 16%|        | 130/820 [00:38<03:12,  3.59it/s] 16%|        | 131/820 [00:38<03:11,  3.59it/s] 16%|        | 132/820 [00:39<03:15,  3.52it/s] 16%|        | 133/820 [00:39<03:14,  3.53it/s] 16%|        | 134/820 [00:39<03:13,  3.55it/s] 16%|        | 135/820 [00:39<03:17,  3.47it/s] 17%|        | 136/820 [00:40<03:14,  3.51it/s] 17%|        | 137/820 [00:40<03:13,  3.54it/s] 17%|        | 138/820 [00:40<03:11,  3.55it/s] 17%|        | 139/820 [00:41<03:10,  3.57it/s] 17%|        | 140/820 [00:41<03:10,  3.57it/s] 17%|        | 141/820 [00:41<03:10,  3.57it/s] 17%|        | 142/820 [00:41<03:21,  3.36it/s] 17%|        | 143/820 [00:42<03:17,  3.42it/s] 18%|        | 144/820 [00:42<03:15,  3.46it/s] 18%|        | 145/820 [00:42<03:12,  3.50it/s] 18%|        | 146/820 [00:43<03:11,  3.53it/s] 18%|        | 147/820 [00:43<03:15,  3.45it/s] 18%|        | 148/820 [00:43<03:13,  3.48it/s] 18%|        | 149/820 [00:43<03:11,  3.51it/s] 18%|        | 150/820 [00:44<03:09,  3.53it/s] 18%|        | 151/820 [00:44<03:08,  3.54it/s] 19%|        | 152/820 [00:44<03:08,  3.55it/s] 19%|        | 153/820 [00:45<03:15,  3.40it/s] 19%|        | 154/820 [00:45<03:12,  3.45it/s] 19%|        | 155/820 [00:45<04:15,  2.60it/s] 19%|        | 156/820 [00:46<03:54,  2.83it/s] 19%|        | 157/820 [00:46<03:39,  3.02it/s] 19%|        | 158/820 [00:46<03:28,  3.17it/s] 19%|        | 159/820 [00:47<03:21,  3.29it/s] 20%|        | 160/820 [00:47<03:15,  3.37it/s] 20%|        | 161/820 [00:47<03:11,  3.44it/s] 20%|        | 162/820 [00:47<03:09,  3.48it/s] 20%|        | 163/820 [00:48<03:07,  3.51it/s] 20%|        | 164/820 [00:48<02:52,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 13:55:07,948 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:55:07,948 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 13:55:07,948 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.80it/s][A
  3%|         | 12/435 [00:00<00:08, 49.34it/s][A
  4%|         | 17/435 [00:00<00:08, 47.41it/s][A
  5%|         | 22/435 [00:00<00:08, 46.56it/s][A
  6%|         | 27/435 [00:00<00:08, 45.96it/s][A
  7%|         | 32/435 [00:00<00:08, 45.46it/s][A
  9%|         | 37/435 [00:00<00:08, 44.88it/s][A
 10%|         | 42/435 [00:00<00:08, 44.66it/s][A
 11%|         | 47/435 [00:01<00:08, 44.71it/s][A
 12%|        | 52/435 [00:01<00:09, 42.32it/s][A
 13%|        | 57/435 [00:01<00:08, 43.01it/s][A
 14%|        | 62/435 [00:01<00:08, 43.66it/s][A
 15%|        | 67/435 [00:01<00:08, 42.09it/s][A
 17%|        | 72/435 [00:01<00:08, 43.11it/s][A
 18%|        | 77/435 [00:01<00:08, 43.70it/s][A
 19%|        | 82/435 [00:01<00:08, 43.99it/s][A
 20%|        | 87/435 [00:01<00:07, 44.21it/s][A
 21%|        | 92/435 [00:02<00:07, 44.11it/s][A
 22%|       | 97/435 [00:02<00:07, 44.31it/s][A
 23%|       | 102/435 [00:02<00:07, 44.55it/s][A
 25%|       | 107/435 [00:02<00:07, 44.56it/s][A
 26%|       | 112/435 [00:02<00:07, 44.66it/s][A
 27%|       | 117/435 [00:02<00:07, 44.80it/s][A
 28%|       | 122/435 [00:02<00:06, 44.84it/s][A
 29%|       | 127/435 [00:02<00:06, 44.95it/s][A
 30%|       | 132/435 [00:02<00:06, 44.84it/s][A
 31%|      | 137/435 [00:03<00:06, 44.57it/s][A
 33%|      | 142/435 [00:03<00:06, 44.71it/s][A
 34%|      | 147/435 [00:03<00:06, 44.73it/s][A
 35%|      | 152/435 [00:03<00:06, 44.79it/s][A
 36%|      | 157/435 [00:03<00:06, 39.83it/s][A
 37%|      | 162/435 [00:03<00:06, 41.37it/s][A
 38%|      | 167/435 [00:03<00:06, 42.48it/s][A
 40%|      | 172/435 [00:03<00:06, 43.34it/s][A
 41%|      | 177/435 [00:04<00:06, 42.89it/s][A
 42%|     | 182/435 [00:04<00:05, 43.61it/s][A
 43%|     | 187/435 [00:04<00:05, 43.94it/s][A
 44%|     | 192/435 [00:04<00:05, 44.15it/s][A
 45%|     | 197/435 [00:04<00:05, 44.08it/s][A
 46%|     | 202/435 [00:04<00:05, 44.12it/s][A
 48%|     | 207/435 [00:04<00:05, 44.22it/s][A
 49%|     | 212/435 [00:04<00:05, 44.53it/s][A
 50%|     | 217/435 [00:04<00:04, 44.65it/s][A
 51%|     | 222/435 [00:05<00:04, 44.77it/s][A
 52%|    | 227/435 [00:05<00:04, 44.79it/s][A
 53%|    | 232/435 [00:05<00:04, 44.89it/s][A
 54%|    | 237/435 [00:05<00:04, 44.91it/s][A
 56%|    | 242/435 [00:05<00:04, 44.82it/s][A
 57%|    | 247/435 [00:05<00:04, 44.71it/s][A
 58%|    | 252/435 [00:05<00:04, 44.66it/s][A
 59%|    | 257/435 [00:05<00:03, 44.77it/s][A
 60%|    | 262/435 [00:05<00:03, 44.77it/s][A
 61%|   | 267/435 [00:06<00:03, 44.89it/s][A
 63%|   | 272/435 [00:06<00:03, 44.85it/s][A
 64%|   | 277/435 [00:06<00:03, 44.95it/s][A
 65%|   | 282/435 [00:06<00:03, 44.86it/s][A
 66%|   | 287/435 [00:06<00:03, 44.79it/s][A
 67%|   | 292/435 [00:06<00:03, 43.65it/s][A
 68%|   | 297/435 [00:06<00:03, 44.10it/s][A
 69%|   | 302/435 [00:06<00:03, 42.94it/s][A
 71%|   | 307/435 [00:06<00:02, 43.68it/s][A
 72%|  | 312/435 [00:07<00:02, 44.12it/s][A
 73%|  | 317/435 [00:07<00:02, 44.39it/s][A
 74%|  | 322/435 [00:07<00:02, 44.49it/s][A
 75%|  | 327/435 [00:07<00:02, 44.53it/s][A
 76%|  | 332/435 [00:07<00:02, 44.48it/s][A
 77%|  | 337/435 [00:07<00:02, 44.49it/s][A
 79%|  | 342/435 [00:07<00:02, 44.46it/s][A
 80%|  | 347/435 [00:07<00:01, 44.65it/s][A
 81%|  | 352/435 [00:07<00:01, 44.76it/s][A
 82%| | 357/435 [00:08<00:01, 44.93it/s][A
 83%| | 362/435 [00:08<00:01, 44.98it/s][A
 84%| | 367/435 [00:08<00:01, 44.87it/s][A
 86%| | 372/435 [00:08<00:01, 44.87it/s][A
 87%| | 377/435 [00:08<00:01, 44.62it/s][A
 88%| | 382/435 [00:08<00:01, 44.54it/s][A
 89%| | 387/435 [00:08<00:01, 44.54it/s][A
 90%| | 392/435 [00:08<00:00, 44.69it/s][A
 91%|| 397/435 [00:08<00:00, 44.90it/s][A
 92%|| 402/435 [00:09<00:00, 44.88it/s][A
 94%|| 407/435 [00:09<00:00, 44.98it/s][A
 95%|| 412/435 [00:09<00:00, 44.96it/s][A
 96%|| 417/435 [00:09<00:00, 44.88it/s][A
 97%|| 422/435 [00:09<00:00, 44.73it/s][A
 98%|| 427/435 [00:09<00:00, 44.53it/s][A
 99%|| 432/435 [00:09<00:00, 44.49it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.49it/s][A 20%|        | 164/820 [00:58<02:52,  3.80it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:55:18,067 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 13:55:18,301 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:55:21,873 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:55:21,967 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:55:22,000 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-164/special_tokens_map.json
 20%|        | 165/820 [01:03<52:23,  4.80s/it] 20%|        | 166/820 [01:04<37:32,  3.44s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|        | 167/820 [01:04<27:09,  2.50s/it] 20%|        | 168/820 [01:04<19:54,  1.83s/it] 21%|        | 169/820 [01:04<14:49,  1.37s/it] 21%|        | 170/820 [01:05<11:17,  1.04s/it] 21%|        | 171/820 [01:05<08:48,  1.23it/s] 21%|        | 172/820 [01:05<07:09,  1.51it/s] 21%|        | 173/820 [01:06<05:54,  1.82it/s] 21%|        | 174/820 [01:06<05:02,  2.13it/s] 21%|       | 175/820 [01:06<04:26,  2.42it/s] 21%|       | 176/820 [01:06<04:00,  2.67it/s] 22%|       | 177/820 [01:07<03:42,  2.88it/s] 22%|       | 178/820 [01:07<03:30,  3.05it/s] 22%|       | 179/820 [01:07<03:21,  3.18it/s] 22%|       | 180/820 [01:08<03:15,  3.28it/s] 22%|       | 181/820 [01:08<03:10,  3.35it/s] 22%|       | 182/820 [01:08<03:07,  3.40it/s] 22%|       | 183/820 [01:08<03:11,  3.34it/s] 22%|       | 184/820 [01:09<03:07,  3.39it/s] 23%|       | 185/820 [01:09<03:05,  3.43it/s] 23%|       | 186/820 [01:09<03:03,  3.46it/s] 23%|       | 187/820 [01:10<03:01,  3.48it/s] 23%|       | 188/820 [01:10<03:00,  3.50it/s] 23%|       | 189/820 [01:10<02:59,  3.51it/s] 23%|       | 190/820 [01:10<02:59,  3.52it/s] 23%|       | 191/820 [01:11<02:58,  3.52it/s] 23%|       | 192/820 [01:11<02:59,  3.51it/s] 24%|       | 193/820 [01:11<02:58,  3.51it/s] 24%|       | 194/820 [01:12<03:04,  3.39it/s] 24%|       | 195/820 [01:12<03:02,  3.42it/s] 24%|       | 196/820 [01:12<03:00,  3.46it/s] 24%|       | 197/820 [01:12<02:59,  3.48it/s] 24%|       | 198/820 [01:13<02:58,  3.49it/s] 24%|       | 199/820 [01:13<02:57,  3.51it/s] 24%|       | 200/820 [01:13<02:56,  3.51it/s] 25%|       | 201/820 [01:14<02:55,  3.52it/s] 25%|       | 202/820 [01:14<02:54,  3.53it/s] 25%|       | 203/820 [01:14<02:54,  3.53it/s] 25%|       | 204/820 [01:14<02:54,  3.53it/s] 25%|       | 205/820 [01:15<03:00,  3.41it/s] 25%|       | 206/820 [01:15<02:58,  3.45it/s] 25%|       | 207/820 [01:15<02:56,  3.47it/s] 25%|       | 208/820 [01:16<02:55,  3.49it/s] 25%|       | 209/820 [01:16<02:54,  3.51it/s] 26%|       | 210/820 [01:16<02:53,  3.52it/s] 26%|       | 211/820 [01:16<02:52,  3.52it/s] 26%|       | 212/820 [01:17<02:52,  3.52it/s] 26%|       | 213/820 [01:17<02:52,  3.52it/s] 26%|       | 214/820 [01:17<02:52,  3.52it/s] 26%|       | 215/820 [01:18<02:51,  3.53it/s] 26%|       | 216/820 [01:18<03:01,  3.33it/s] 26%|       | 217/820 [01:18<02:58,  3.39it/s] 27%|       | 218/820 [01:18<02:55,  3.43it/s] 27%|       | 219/820 [01:19<02:53,  3.46it/s] 27%|       | 220/820 [01:19<02:52,  3.48it/s] 27%|       | 221/820 [01:19<02:51,  3.49it/s] 27%|       | 222/820 [01:20<02:50,  3.50it/s] 27%|       | 223/820 [01:20<02:49,  3.52it/s] 27%|       | 224/820 [01:20<02:49,  3.51it/s] 27%|       | 225/820 [01:20<02:49,  3.52it/s] 28%|       | 226/820 [01:21<02:48,  3.53it/s] 28%|       | 227/820 [01:21<02:56,  3.36it/s] 28%|       | 228/820 [01:21<02:52,  3.43it/s] 28%|       | 229/820 [01:22<02:50,  3.47it/s] 28%|       | 230/820 [01:22<02:47,  3.51it/s] 28%|       | 231/820 [01:22<02:46,  3.53it/s] 28%|       | 232/820 [01:22<02:45,  3.55it/s] 28%|       | 233/820 [01:23<02:44,  3.56it/s] 29%|       | 234/820 [01:23<02:44,  3.57it/s] 29%|       | 235/820 [01:23<02:43,  3.57it/s] 29%|       | 236/820 [01:24<02:43,  3.57it/s] 29%|       | 237/820 [01:24<02:43,  3.57it/s] 29%|       | 238/820 [01:24<02:49,  3.43it/s] 29%|       | 239/820 [01:24<02:47,  3.47it/s] 29%|       | 240/820 [01:25<02:45,  3.50it/s] 29%|       | 241/820 [01:25<02:44,  3.52it/s] 30%|       | 242/820 [01:25<02:43,  3.54it/s] 30%|       | 243/820 [01:26<02:42,  3.55it/s] 30%|       | 244/820 [01:26<02:41,  3.56it/s] 30%|       | 245/820 [01:26<02:40,  3.57it/s] 30%|       | 246/820 [01:26<02:40,  3.58it/s] 30%|       | 247/820 [01:27<02:40,  3.58it/s] 30%|       | 248/820 [01:27<02:39,  3.59it/s] 30%|       | 249/820 [01:27<02:50,  3.36it/s] 30%|       | 250/820 [01:28<02:46,  3.43it/s] 31%|       | 251/820 [01:28<02:43,  3.48it/s] 31%|       | 252/820 [01:28<02:41,  3.51it/s] 31%|       | 253/820 [01:28<02:40,  3.53it/s] 31%|       | 254/820 [01:29<02:39,  3.55it/s] 31%|       | 255/820 [01:29<02:38,  3.56it/s] 31%|       | 256/820 [01:29<02:38,  3.57it/s] 31%|      | 257/820 [01:30<02:37,  3.57it/s] 31%|      | 258/820 [01:30<02:37,  3.58it/s] 32%|      | 259/820 [01:30<02:36,  3.58it/s] 32%|      | 260/820 [01:30<02:36,  3.58it/s] 32%|      | 261/820 [01:31<02:41,  3.46it/s] 32%|      | 262/820 [01:31<02:39,  3.50it/s] 32%|      | 263/820 [01:31<02:37,  3.53it/s] 32%|      | 264/820 [01:31<02:36,  3.55it/s] 32%|      | 265/820 [01:32<02:36,  3.55it/s] 32%|      | 266/820 [01:32<02:35,  3.57it/s] 33%|      | 267/820 [01:32<02:34,  3.57it/s] 33%|      | 268/820 [01:33<02:34,  3.58it/s] 33%|      | 269/820 [01:33<02:34,  3.57it/s] 33%|      | 270/820 [01:33<02:33,  3.58it/s] 33%|      | 271/820 [01:33<02:33,  3.58it/s] 33%|      | 272/820 [01:34<02:38,  3.47it/s] 33%|      | 273/820 [01:34<02:36,  3.50it/s] 33%|      | 274/820 [01:34<02:34,  3.53it/s] 34%|      | 275/820 [01:35<02:33,  3.55it/s] 34%|      | 276/820 [01:35<02:32,  3.56it/s] 34%|      | 277/820 [01:35<02:32,  3.57it/s] 34%|      | 278/820 [01:35<02:31,  3.57it/s] 34%|      | 279/820 [01:36<02:31,  3.57it/s] 34%|      | 280/820 [01:36<02:31,  3.57it/s] 34%|      | 281/820 [01:36<02:30,  3.58it/s] 34%|      | 282/820 [01:37<02:30,  3.58it/s] 35%|      | 283/820 [01:37<02:35,  3.45it/s] 35%|      | 284/820 [01:37<02:33,  3.48it/s] 35%|      | 285/820 [01:37<02:32,  3.51it/s] 35%|      | 286/820 [01:38<02:31,  3.53it/s] 35%|      | 287/820 [01:38<02:30,  3.54it/s] 35%|      | 288/820 [01:38<02:29,  3.55it/s] 35%|      | 289/820 [01:39<02:32,  3.48it/s] 35%|      | 290/820 [01:39<02:31,  3.50it/s] 35%|      | 291/820 [01:39<02:30,  3.52it/s] 36%|      | 292/820 [01:39<02:29,  3.54it/s] 36%|      | 293/820 [01:40<02:28,  3.56it/s] 36%|      | 294/820 [01:40<02:32,  3.45it/s] 36%|      | 295/820 [01:40<02:30,  3.49it/s] 36%|      | 296/820 [01:41<02:28,  3.52it/s] 36%|      | 297/820 [01:41<02:27,  3.54it/s] 36%|      | 298/820 [01:41<02:27,  3.55it/s] 36%|      | 299/820 [01:41<02:26,  3.56it/s] 37%|      | 300/820 [01:42<02:25,  3.57it/s] 37%|      | 301/820 [01:42<02:25,  3.57it/s] 37%|      | 302/820 [01:42<02:25,  3.57it/s] 37%|      | 303/820 [01:43<02:24,  3.57it/s] 37%|      | 304/820 [01:43<02:24,  3.57it/s] 37%|      | 305/820 [01:43<02:33,  3.36it/s] 37%|      | 306/820 [01:43<02:30,  3.41it/s] 37%|      | 307/820 [01:44<02:28,  3.46it/s] 38%|      | 308/820 [01:44<02:26,  3.49it/s] 38%|      | 309/820 [01:44<02:25,  3.52it/s] 38%|      | 310/820 [01:45<02:24,  3.54it/s] 38%|      | 311/820 [01:45<02:23,  3.55it/s] 38%|      | 312/820 [01:45<02:22,  3.56it/s] 38%|      | 313/820 [01:46<03:03,  2.76it/s] 38%|      | 314/820 [01:46<02:51,  2.95it/s] 38%|      | 315/820 [01:46<02:48,  2.99it/s] 39%|      | 316/820 [01:47<02:40,  3.14it/s] 39%|      | 317/820 [01:47<02:34,  3.26it/s] 39%|      | 318/820 [01:47<02:29,  3.36it/s] 39%|      | 319/820 [01:47<02:26,  3.42it/s] 39%|      | 320/820 [01:48<02:23,  3.47it/s] 39%|      | 321/820 [01:48<02:22,  3.51it/s] 39%|      | 322/820 [01:48<02:21,  3.52it/s] 39%|      | 323/820 [01:48<02:20,  3.54it/s] 40%|      | 324/820 [01:49<02:19,  3.56it/s] 40%|      | 325/820 [01:49<02:18,  3.56it/s] 40%|      | 326/820 [01:49<02:22,  3.47it/s] 40%|      | 327/820 [01:50<02:20,  3.51it/s] 40%|      | 328/820 [01:50<02:09,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 13:56:09,902 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:56:09,902 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 13:56:09,902 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.8129, 'eval_samples_per_second': 354.328, 'eval_steps_per_second': 44.329, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.37it/s][A
  3%|         | 12/435 [00:00<00:08, 49.10it/s][A
  4%|         | 17/435 [00:00<00:08, 47.45it/s][A
  5%|         | 22/435 [00:00<00:08, 46.53it/s][A
  6%|         | 27/435 [00:00<00:08, 45.92it/s][A
  7%|         | 32/435 [00:00<00:08, 45.44it/s][A
  9%|         | 37/435 [00:00<00:08, 45.06it/s][A
 10%|         | 42/435 [00:00<00:08, 44.68it/s][A
 11%|         | 47/435 [00:01<00:08, 44.66it/s][A
 12%|        | 52/435 [00:01<00:08, 44.77it/s][A
 13%|        | 57/435 [00:01<00:08, 44.84it/s][A
 14%|        | 62/435 [00:01<00:08, 44.97it/s][A
 15%|        | 67/435 [00:01<00:08, 45.02it/s][A
 17%|        | 72/435 [00:01<00:08, 44.94it/s][A
 18%|        | 77/435 [00:01<00:07, 44.87it/s][A
 19%|        | 82/435 [00:01<00:07, 44.68it/s][A
 20%|        | 87/435 [00:01<00:07, 44.53it/s][A
 21%|        | 92/435 [00:02<00:07, 44.51it/s][A
 22%|       | 97/435 [00:02<00:07, 44.60it/s][A
 23%|       | 102/435 [00:02<00:07, 44.77it/s][A
 25%|       | 107/435 [00:02<00:07, 43.03it/s][A
 26%|       | 112/435 [00:02<00:07, 43.61it/s][A
 27%|       | 117/435 [00:02<00:07, 43.98it/s][A
 28%|       | 122/435 [00:02<00:07, 44.31it/s][A
 29%|       | 127/435 [00:02<00:06, 44.38it/s][A
 30%|       | 132/435 [00:02<00:06, 44.43it/s][A
 31%|      | 137/435 [00:03<00:06, 44.43it/s][A
 33%|      | 142/435 [00:03<00:06, 44.46it/s][A
 34%|      | 147/435 [00:03<00:06, 44.43it/s][A
 35%|      | 152/435 [00:03<00:06, 44.62it/s][A
 36%|      | 157/435 [00:03<00:06, 44.82it/s][A
 37%|      | 162/435 [00:03<00:06, 44.83it/s][A
 38%|      | 167/435 [00:03<00:05, 44.92it/s][A
 40%|      | 172/435 [00:03<00:05, 44.88it/s][A
 41%|      | 177/435 [00:03<00:05, 44.78it/s][A
 42%|     | 182/435 [00:04<00:05, 44.70it/s][A
 43%|     | 187/435 [00:04<00:05, 44.59it/s][A
 44%|     | 192/435 [00:04<00:05, 44.58it/s][A
 45%|     | 197/435 [00:04<00:05, 44.63it/s][A
 46%|     | 202/435 [00:04<00:05, 44.55it/s][A
 48%|     | 207/435 [00:04<00:05, 44.65it/s][A
 49%|     | 212/435 [00:04<00:04, 44.77it/s][A
 50%|     | 217/435 [00:04<00:04, 44.83it/s][A
 51%|     | 222/435 [00:04<00:04, 44.85it/s][A
 52%|    | 227/435 [00:05<00:04, 44.73it/s][A
 53%|    | 232/435 [00:05<00:04, 44.73it/s][A
 54%|    | 237/435 [00:05<00:04, 44.77it/s][A
 56%|    | 242/435 [00:05<00:04, 42.48it/s][A
 57%|    | 247/435 [00:05<00:04, 43.28it/s][A
 58%|    | 252/435 [00:05<00:04, 43.72it/s][A
 59%|    | 257/435 [00:05<00:04, 44.20it/s][A
 60%|    | 262/435 [00:05<00:03, 44.43it/s][A
 61%|   | 267/435 [00:05<00:03, 44.62it/s][A
 63%|   | 272/435 [00:06<00:03, 44.66it/s][A
 64%|   | 277/435 [00:06<00:03, 44.67it/s][A
 65%|   | 282/435 [00:06<00:03, 44.49it/s][A
 66%|   | 287/435 [00:06<00:03, 44.46it/s][A
 67%|   | 292/435 [00:06<00:03, 44.59it/s][A
 68%|   | 297/435 [00:06<00:03, 44.71it/s][A
 69%|   | 302/435 [00:06<00:02, 44.85it/s][A
 71%|   | 307/435 [00:06<00:02, 44.91it/s][A
 72%|  | 312/435 [00:06<00:02, 44.82it/s][A
 73%|  | 317/435 [00:07<00:02, 44.82it/s][A
 74%|  | 322/435 [00:07<00:02, 44.71it/s][A
 75%|  | 327/435 [00:07<00:02, 44.64it/s][A
 76%|  | 332/435 [00:07<00:02, 44.66it/s][A
 77%|  | 337/435 [00:07<00:02, 44.69it/s][A
 79%|  | 342/435 [00:07<00:02, 44.71it/s][A
 80%|  | 347/435 [00:07<00:01, 44.82it/s][A
 81%|  | 352/435 [00:07<00:01, 44.87it/s][A
 82%| | 357/435 [00:07<00:01, 44.87it/s][A
 83%| | 362/435 [00:08<00:01, 44.80it/s][A
 84%| | 367/435 [00:08<00:01, 44.83it/s][A
 86%| | 372/435 [00:08<00:01, 44.71it/s][A
 87%| | 377/435 [00:08<00:01, 44.63it/s][A
 88%| | 382/435 [00:08<00:01, 44.73it/s][A
 89%| | 387/435 [00:08<00:01, 44.73it/s][A
 90%| | 392/435 [00:08<00:00, 44.83it/s][A
 91%|| 397/435 [00:08<00:00, 44.83it/s][A
 92%|| 402/435 [00:08<00:00, 44.85it/s][A
 94%|| 407/435 [00:09<00:00, 44.81it/s][A
 95%|| 412/435 [00:09<00:00, 44.87it/s][A
 96%|| 417/435 [00:09<00:00, 44.82it/s][A
 97%|| 422/435 [00:09<00:00, 44.70it/s][A
 98%|| 427/435 [00:09<00:00, 44.67it/s][A
 99%|| 432/435 [00:09<00:00, 44.69it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.69it/s][A 40%|      | 328/820 [02:00<02:09,  3.80it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:56:19,796 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 13:56:19,903 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:56:22,746 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:56:22,874 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:56:22,934 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-328/special_tokens_map.json
 40%|      | 329/820 [02:04<36:28,  4.46s/it] 40%|      | 330/820 [02:04<26:09,  3.20s/it] 40%|      | 331/820 [02:05<18:57,  2.33s/it] 40%|      | 332/820 [02:05<13:55,  1.71s/it] 41%|      | 333/820 [02:05<10:23,  1.28s/it] 41%|      | 334/820 [02:05<07:56,  1.02it/s] 41%|      | 335/820 [02:06<06:13,  1.30it/s] 41%|      | 336/820 [02:06<05:05,  1.58it/s] 41%|      | 337/820 [02:06<04:13,  1.90it/s] 41%|      | 338/820 [02:07<03:37,  2.22it/s] 41%|     | 339/820 [02:07<03:11,  2.51it/s] 41%|     | 340/820 [02:07<02:54,  2.76it/s] 42%|     | 341/820 [02:07<02:41,  2.96it/s] 42%|     | 342/820 [02:08<02:32,  3.13it/s] 42%|     | 343/820 [02:08<02:26,  3.25it/s] 42%|     | 344/820 [02:08<02:22,  3.35it/s] 42%|     | 345/820 [02:09<02:18,  3.42it/s] 42%|     | 346/820 [02:09<02:16,  3.47it/s] 42%|     | 347/820 [02:09<02:16,  3.46it/s] 42%|     | 348/820 [02:09<02:14,  3.50it/s] 43%|     | 349/820 [02:10<02:13,  3.53it/s] 43%|     | 350/820 [02:10<02:12,  3.55it/s] 43%|     | 351/820 [02:10<02:11,  3.56it/s] 43%|     | 352/820 [02:11<02:11,  3.57it/s] 43%|     | 353/820 [02:11<02:10,  3.58it/s] 43%|     | 354/820 [02:11<02:09,  3.58it/s] 43%|     | 355/820 [02:11<02:09,  3.59it/s] 43%|     | 356/820 [02:12<02:09,  3.59it/s] 44%|     | 357/820 [02:12<02:08,  3.59it/s] 44%|     | 358/820 [02:12<02:11,  3.52it/s] 44%|     | 359/820 [02:12<02:10,  3.54it/s] 44%|     | 360/820 [02:13<02:09,  3.56it/s] 44%|     | 361/820 [02:13<02:08,  3.57it/s] 44%|     | 362/820 [02:13<02:08,  3.58it/s] 44%|     | 363/820 [02:14<02:07,  3.58it/s] 44%|     | 364/820 [02:14<02:07,  3.59it/s] 45%|     | 365/820 [02:14<02:06,  3.59it/s] 45%|     | 366/820 [02:14<02:06,  3.58it/s] 45%|     | 367/820 [02:15<02:06,  3.59it/s] 45%|     | 368/820 [02:15<02:05,  3.59it/s] 45%|     | 369/820 [02:15<02:08,  3.52it/s] 45%|     | 370/820 [02:16<02:06,  3.55it/s] 45%|     | 371/820 [02:16<02:06,  3.56it/s] 45%|     | 372/820 [02:16<02:05,  3.57it/s] 45%|     | 373/820 [02:16<02:04,  3.58it/s] 46%|     | 374/820 [02:17<02:04,  3.58it/s] 46%|     | 375/820 [02:17<02:04,  3.59it/s] 46%|     | 376/820 [02:17<02:03,  3.59it/s] 46%|     | 377/820 [02:18<02:03,  3.59it/s] 46%|     | 378/820 [02:18<02:02,  3.60it/s] 46%|     | 379/820 [02:18<02:02,  3.60it/s] 46%|     | 380/820 [02:18<02:08,  3.44it/s] 46%|     | 381/820 [02:19<02:06,  3.48it/s] 47%|     | 382/820 [02:19<02:04,  3.52it/s] 47%|     | 383/820 [02:19<02:03,  3.54it/s] 47%|     | 384/820 [02:19<02:02,  3.55it/s] 47%|     | 385/820 [02:20<02:02,  3.56it/s] 47%|     | 386/820 [02:20<02:01,  3.57it/s] 47%|     | 387/820 [02:20<02:00,  3.58it/s] 47%|     | 388/820 [02:21<02:00,  3.58it/s] 47%|     | 389/820 [02:21<02:00,  3.59it/s] 48%|     | 390/820 [02:21<01:59,  3.59it/s] 48%|     | 391/820 [02:22<02:07,  3.36it/s] 48%|     | 392/820 [02:22<02:04,  3.43it/s] 48%|     | 393/820 [02:22<02:02,  3.48it/s] 48%|     | 394/820 [02:22<02:01,  3.51it/s] 48%|     | 395/820 [02:23<02:00,  3.54it/s] 48%|     | 396/820 [02:23<01:59,  3.55it/s] 48%|     | 397/820 [02:23<01:58,  3.57it/s] 49%|     | 398/820 [02:23<01:58,  3.57it/s] 49%|     | 399/820 [02:24<01:57,  3.58it/s] 49%|     | 400/820 [02:24<01:57,  3.58it/s] 49%|     | 401/820 [02:24<01:56,  3.59it/s] 49%|     | 402/820 [02:25<02:04,  3.37it/s] 49%|     | 403/820 [02:25<02:01,  3.43it/s] 49%|     | 404/820 [02:25<01:59,  3.48it/s] 49%|     | 405/820 [02:25<01:57,  3.52it/s] 50%|     | 406/820 [02:26<01:56,  3.54it/s] 50%|     | 407/820 [02:26<01:56,  3.56it/s] 50%|     | 408/820 [02:26<01:55,  3.57it/s] 50%|     | 409/820 [02:27<01:54,  3.57it/s] 50%|     | 410/820 [02:27<01:54,  3.58it/s] 50%|     | 411/820 [02:27<01:54,  3.59it/s] 50%|     | 412/820 [02:27<01:53,  3.59it/s] 50%|     | 413/820 [02:28<02:02,  3.33it/s] 50%|     | 414/820 [02:28<01:59,  3.40it/s] 51%|     | 415/820 [02:28<01:57,  3.46it/s] 51%|     | 416/820 [02:29<01:55,  3.50it/s] 51%|     | 417/820 [02:29<02:07,  3.16it/s] 51%|     | 418/820 [02:29<02:02,  3.28it/s] 51%|     | 419/820 [02:30<01:58,  3.37it/s] 51%|     | 420/820 [02:30<01:56,  3.43it/s] 51%|    | 421/820 [02:30<01:54,  3.48it/s] 51%|    | 422/820 [02:30<01:53,  3.51it/s] 52%|    | 423/820 [02:31<01:52,  3.54it/s] 52%|    | 424/820 [02:31<01:51,  3.55it/s] 52%|    | 425/820 [02:31<01:50,  3.56it/s] 52%|    | 426/820 [02:31<01:50,  3.57it/s] 52%|    | 427/820 [02:32<01:49,  3.58it/s] 52%|    | 428/820 [02:32<01:53,  3.47it/s] 52%|    | 429/820 [02:32<01:51,  3.50it/s] 52%|    | 430/820 [02:33<01:50,  3.53it/s] 53%|    | 431/820 [02:33<01:49,  3.54it/s] 53%|    | 432/820 [02:33<01:48,  3.56it/s] 53%|    | 433/820 [02:33<01:48,  3.57it/s] 53%|    | 434/820 [02:34<01:47,  3.58it/s] 53%|    | 435/820 [02:34<01:49,  3.51it/s] 53%|    | 436/820 [02:34<01:48,  3.54it/s] 53%|    | 437/820 [02:35<01:47,  3.56it/s] 53%|    | 438/820 [02:35<01:46,  3.57it/s] 54%|    | 439/820 [02:35<01:46,  3.58it/s] 54%|    | 440/820 [02:35<01:46,  3.58it/s] 54%|    | 441/820 [02:36<01:45,  3.59it/s] 54%|    | 442/820 [02:36<01:45,  3.59it/s] 54%|    | 443/820 [02:36<01:45,  3.59it/s] 54%|    | 444/820 [02:37<01:44,  3.59it/s] 54%|    | 445/820 [02:37<01:44,  3.59it/s] 54%|    | 446/820 [02:37<01:49,  3.43it/s] 55%|    | 447/820 [02:37<01:47,  3.48it/s] 55%|    | 448/820 [02:38<01:45,  3.52it/s] 55%|    | 449/820 [02:38<01:44,  3.54it/s] 55%|    | 450/820 [02:38<01:43,  3.56it/s] 55%|    | 451/820 [02:39<01:43,  3.57it/s] 55%|    | 452/820 [02:39<01:42,  3.58it/s] 55%|    | 453/820 [02:39<01:42,  3.58it/s] 55%|    | 454/820 [02:39<01:42,  3.58it/s] 55%|    | 455/820 [02:40<01:41,  3.59it/s] 56%|    | 456/820 [02:40<01:41,  3.59it/s] 56%|    | 457/820 [02:40<01:41,  3.59it/s] 56%|    | 458/820 [02:40<01:40,  3.59it/s] 56%|    | 459/820 [02:41<01:40,  3.60it/s] 56%|    | 460/820 [02:41<01:40,  3.60it/s] 56%|    | 461/820 [02:41<01:39,  3.60it/s] 56%|    | 462/820 [02:42<01:39,  3.60it/s] 56%|    | 463/820 [02:42<01:39,  3.60it/s] 57%|    | 464/820 [02:42<01:42,  3.48it/s] 57%|    | 465/820 [02:42<01:40,  3.52it/s] 57%|    | 466/820 [02:43<01:39,  3.55it/s] 57%|    | 467/820 [02:43<01:39,  3.56it/s] 57%|    | 468/820 [02:43<01:41,  3.47it/s] 57%|    | 469/820 [02:44<01:40,  3.49it/s] 57%|    | 470/820 [02:44<01:39,  3.53it/s] 57%|    | 471/820 [02:44<01:38,  3.55it/s] 58%|    | 472/820 [02:44<01:37,  3.56it/s] 58%|    | 473/820 [02:45<01:37,  3.57it/s] 58%|    | 474/820 [02:45<01:36,  3.58it/s] 58%|    | 475/820 [02:45<01:36,  3.58it/s] 58%|    | 476/820 [02:46<02:21,  2.44it/s] 58%|    | 477/820 [02:46<02:07,  2.69it/s] 58%|    | 478/820 [02:47<01:57,  2.91it/s] 58%|    | 479/820 [02:47<01:50,  3.09it/s] 59%|    | 480/820 [02:47<01:45,  3.23it/s] 59%|    | 481/820 [02:47<01:44,  3.25it/s] 59%|    | 482/820 [02:48<01:40,  3.35it/s] 59%|    | 483/820 [02:48<01:38,  3.42it/s] 59%|    | 484/820 [02:48<01:36,  3.47it/s] 59%|    | 485/820 [02:49<01:35,  3.51it/s] 59%|    | 486/820 [02:49<01:34,  3.53it/s] 59%|    | 487/820 [02:49<01:33,  3.55it/s] 60%|    | 488/820 [02:49<01:36,  3.43it/s] 60%|    | 489/820 [02:50<01:35,  3.48it/s] 60%|    | 490/820 [02:50<01:33,  3.51it/s] 60%|    | 491/820 [02:50<01:32,  3.54it/s] 60%|    | 492/820 [02:50<01:25,  3.82it/s][INFO|trainer.py:2140] 2023-08-28 13:57:10,498 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:57:10,498 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 13:57:10,498 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7434, 'eval_samples_per_second': 356.856, 'eval_steps_per_second': 44.646, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.97it/s][A
  3%|         | 12/435 [00:00<00:08, 48.99it/s][A
  4%|         | 17/435 [00:00<00:08, 47.24it/s][A
  5%|         | 22/435 [00:00<00:08, 46.39it/s][A
  6%|         | 27/435 [00:00<00:08, 45.91it/s][A
  7%|         | 32/435 [00:00<00:08, 45.39it/s][A
  9%|         | 37/435 [00:00<00:08, 45.13it/s][A
 10%|         | 42/435 [00:00<00:08, 44.84it/s][A
 11%|         | 47/435 [00:01<00:08, 44.83it/s][A
 12%|        | 52/435 [00:01<00:08, 45.03it/s][A
 13%|        | 57/435 [00:01<00:08, 44.95it/s][A
 14%|        | 62/435 [00:01<00:08, 44.87it/s][A
 15%|        | 67/435 [00:01<00:08, 44.92it/s][A
 17%|        | 72/435 [00:01<00:08, 44.92it/s][A
 18%|        | 77/435 [00:01<00:07, 44.78it/s][A
 19%|        | 82/435 [00:01<00:08, 42.03it/s][A
 20%|        | 87/435 [00:01<00:08, 42.85it/s][A
 21%|        | 92/435 [00:02<00:07, 43.49it/s][A
 22%|       | 97/435 [00:02<00:07, 43.94it/s][A
 23%|       | 102/435 [00:02<00:07, 44.28it/s][A
 25%|       | 107/435 [00:02<00:07, 44.50it/s][A
 26%|       | 112/435 [00:02<00:07, 44.63it/s][A
 27%|       | 117/435 [00:02<00:07, 44.56it/s][A
 28%|       | 122/435 [00:02<00:07, 44.37it/s][A
 29%|       | 127/435 [00:02<00:06, 44.42it/s][A
 30%|       | 132/435 [00:02<00:06, 44.52it/s][A
 31%|      | 137/435 [00:03<00:06, 44.61it/s][A
 33%|      | 142/435 [00:03<00:06, 44.78it/s][A
 34%|      | 147/435 [00:03<00:06, 44.88it/s][A
 35%|      | 152/435 [00:03<00:06, 44.94it/s][A
 36%|      | 157/435 [00:03<00:06, 44.92it/s][A
 37%|      | 162/435 [00:03<00:06, 44.86it/s][A
 38%|      | 167/435 [00:03<00:05, 44.71it/s][A
 40%|      | 172/435 [00:03<00:05, 44.75it/s][A
 41%|      | 177/435 [00:03<00:05, 44.69it/s][A
 42%|     | 182/435 [00:04<00:05, 44.40it/s][A
 43%|     | 187/435 [00:04<00:05, 44.64it/s][A
 44%|     | 192/435 [00:04<00:05, 44.70it/s][A
 45%|     | 197/435 [00:04<00:05, 44.81it/s][A
 46%|     | 202/435 [00:04<00:05, 44.90it/s][A
 48%|     | 207/435 [00:04<00:05, 44.84it/s][A
 49%|     | 212/435 [00:04<00:04, 44.79it/s][A
 50%|     | 217/435 [00:04<00:04, 44.68it/s][A
 51%|     | 222/435 [00:04<00:04, 44.77it/s][A
 52%|    | 227/435 [00:05<00:04, 44.71it/s][A
 53%|    | 232/435 [00:05<00:04, 44.78it/s][A
 54%|    | 237/435 [00:05<00:04, 44.83it/s][A
 56%|    | 242/435 [00:05<00:04, 44.92it/s][A
 57%|    | 247/435 [00:05<00:04, 44.94it/s][A
 58%|    | 252/435 [00:05<00:04, 44.84it/s][A
 59%|    | 257/435 [00:05<00:03, 44.75it/s][A
 60%|    | 262/435 [00:05<00:03, 44.75it/s][A
 61%|   | 267/435 [00:05<00:03, 44.76it/s][A
 63%|   | 272/435 [00:06<00:03, 44.77it/s][A
 64%|   | 277/435 [00:06<00:03, 44.71it/s][A
 65%|   | 282/435 [00:06<00:03, 44.86it/s][A
 66%|   | 287/435 [00:06<00:03, 44.90it/s][A
 67%|   | 292/435 [00:06<00:03, 44.94it/s][A
 68%|   | 297/435 [00:06<00:03, 44.89it/s][A
 69%|   | 302/435 [00:06<00:02, 44.74it/s][A
 71%|   | 307/435 [00:06<00:02, 43.15it/s][A
 72%|  | 312/435 [00:06<00:02, 43.71it/s][A
 73%|  | 317/435 [00:07<00:02, 44.07it/s][A
 74%|  | 322/435 [00:07<00:02, 44.16it/s][A
 75%|  | 327/435 [00:07<00:02, 44.38it/s][A
 76%|  | 332/435 [00:07<00:02, 44.60it/s][A
 77%|  | 337/435 [00:07<00:02, 44.63it/s][A
 79%|  | 342/435 [00:07<00:02, 44.69it/s][A
 80%|  | 347/435 [00:07<00:01, 44.52it/s][A
 81%|  | 352/435 [00:07<00:01, 44.56it/s][A
 82%| | 357/435 [00:07<00:01, 44.69it/s][A
 83%| | 362/435 [00:08<00:01, 44.84it/s][A
 84%| | 367/435 [00:08<00:01, 44.69it/s][A
 86%| | 372/435 [00:08<00:01, 44.86it/s][A
 87%| | 377/435 [00:08<00:01, 44.88it/s][A
 88%| | 382/435 [00:08<00:01, 44.81it/s][A
 89%| | 387/435 [00:08<00:01, 44.76it/s][A
 90%| | 392/435 [00:08<00:00, 44.69it/s][A
 91%|| 397/435 [00:08<00:00, 44.67it/s][A
 92%|| 402/435 [00:09<00:00, 41.50it/s][A
 94%|| 407/435 [00:09<00:00, 42.52it/s][A
 95%|| 412/435 [00:09<00:00, 43.32it/s][A
 96%|| 417/435 [00:09<00:00, 43.79it/s][A
 97%|| 422/435 [00:09<00:00, 44.22it/s][A
 98%|| 427/435 [00:09<00:00, 44.50it/s][A
 99%|| 432/435 [00:09<00:00, 44.70it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.70it/s][A 60%|    | 492/820 [03:00<01:25,  3.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:57:20,515 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 13:57:20,672 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:57:24,492 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:57:24,661 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:57:24,769 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-492/special_tokens_map.json
 60%|    | 493/820 [03:06<26:20,  4.83s/it] 60%|    | 494/820 [03:06<18:50,  3.47s/it] 60%|    | 495/820 [03:06<13:36,  2.51s/it] 60%|    | 496/820 [03:07<09:57,  1.84s/it] 61%|    | 497/820 [03:07<07:24,  1.38s/it] 61%|    | 498/820 [03:07<05:37,  1.05s/it] 61%|    | 499/820 [03:08<04:22,  1.22it/s] 61%|    | 500/820 [03:08<03:32,  1.51it/s]                                                  61%|    | 500/820 [03:08<03:32,  1.51it/s] 61%|    | 501/820 [03:08<02:54,  1.82it/s] 61%|    | 502/820 [03:08<02:28,  2.13it/s] 61%|   | 503/820 [03:09<02:10,  2.43it/s] 61%|   | 504/820 [03:09<01:57,  2.68it/s] 62%|   | 505/820 [03:09<01:48,  2.89it/s] 62%|   | 506/820 [03:10<01:42,  3.06it/s] 62%|   | 507/820 [03:10<01:38,  3.19it/s] 62%|   | 508/820 [03:10<01:34,  3.29it/s] 62%|   | 509/820 [03:10<01:32,  3.36it/s] 62%|   | 510/820 [03:11<01:30,  3.41it/s] 62%|   | 511/820 [03:11<01:31,  3.38it/s] 62%|   | 512/820 [03:11<01:29,  3.43it/s] 63%|   | 513/820 [03:12<01:28,  3.46it/s] 63%|   | 514/820 [03:12<01:27,  3.49it/s] 63%|   | 515/820 [03:12<01:27,  3.50it/s] 63%|   | 516/820 [03:12<01:26,  3.51it/s] 63%|   | 517/820 [03:13<01:26,  3.52it/s] 63%|   | 518/820 [03:13<01:25,  3.52it/s] 63%|   | 519/820 [03:13<01:25,  3.53it/s] 63%|   | 520/820 [03:14<01:24,  3.53it/s] 64%|   | 521/820 [03:14<01:24,  3.54it/s] 64%|   | 522/820 [03:14<01:27,  3.42it/s] 64%|   | 523/820 [03:14<01:25,  3.45it/s] 64%|   | 524/820 [03:15<01:25,  3.48it/s] 64%|   | 525/820 [03:15<01:24,  3.50it/s] 64%|   | 526/820 [03:15<01:23,  3.51it/s] 64%|   | 527/820 [03:16<01:23,  3.52it/s] 64%|   | 528/820 [03:16<01:22,  3.52it/s] 65%|   | 529/820 [03:16<01:22,  3.53it/s] 65%|   | 530/820 [03:16<01:22,  3.53it/s] 65%|   | 531/820 [03:17<01:21,  3.54it/s] 65%|   | 532/820 [03:17<01:21,  3.54it/s] 65%|   | 533/820 [03:17<01:24,  3.38it/s] 65%|   | 534/820 [03:18<01:23,  3.43it/s] 65%|   | 535/820 [03:18<01:22,  3.46it/s] 65%|   | 536/820 [03:18<01:21,  3.49it/s] 65%|   | 537/820 [03:18<01:20,  3.50it/s] 66%|   | 538/820 [03:19<01:20,  3.52it/s] 66%|   | 539/820 [03:19<01:19,  3.52it/s] 66%|   | 540/820 [03:19<01:19,  3.53it/s] 66%|   | 541/820 [03:20<01:18,  3.53it/s] 66%|   | 542/820 [03:20<01:18,  3.53it/s] 66%|   | 543/820 [03:20<01:18,  3.54it/s] 66%|   | 544/820 [03:21<01:23,  3.31it/s] 66%|   | 545/820 [03:21<01:21,  3.37it/s] 67%|   | 546/820 [03:21<01:20,  3.42it/s] 67%|   | 547/820 [03:21<01:19,  3.46it/s] 67%|   | 548/820 [03:22<01:18,  3.48it/s] 67%|   | 549/820 [03:22<01:17,  3.50it/s] 67%|   | 550/820 [03:22<01:16,  3.52it/s] 67%|   | 551/820 [03:22<01:16,  3.53it/s] 67%|   | 552/820 [03:23<01:15,  3.53it/s] 67%|   | 553/820 [03:23<01:15,  3.54it/s] 68%|   | 554/820 [03:23<01:15,  3.54it/s] 68%|   | 555/820 [03:24<01:21,  3.27it/s] 68%|   | 556/820 [03:24<01:18,  3.35it/s] 68%|   | 557/820 [03:24<01:17,  3.40it/s] 68%|   | 558/820 [03:25<01:16,  3.44it/s] 68%|   | 559/820 [03:25<01:15,  3.47it/s] 68%|   | 560/820 [03:25<01:14,  3.50it/s] 68%|   | 561/820 [03:25<01:13,  3.51it/s] 69%|   | 562/820 [03:26<01:13,  3.52it/s] 69%|   | 563/820 [03:26<01:12,  3.53it/s] 69%|   | 564/820 [03:26<01:12,  3.53it/s] 69%|   | 565/820 [03:27<01:12,  3.54it/s] 69%|   | 566/820 [03:27<01:16,  3.31it/s] 69%|   | 567/820 [03:27<01:14,  3.38it/s] 69%|   | 568/820 [03:27<01:13,  3.43it/s] 69%|   | 569/820 [03:28<01:12,  3.47it/s] 70%|   | 570/820 [03:28<01:11,  3.49it/s] 70%|   | 571/820 [03:28<01:10,  3.51it/s] 70%|   | 572/820 [03:29<01:10,  3.52it/s] 70%|   | 573/820 [03:29<01:10,  3.52it/s] 70%|   | 574/820 [03:29<01:09,  3.53it/s] 70%|   | 575/820 [03:29<01:09,  3.53it/s] 70%|   | 576/820 [03:30<01:09,  3.53it/s] 70%|   | 577/820 [03:30<01:11,  3.39it/s] 70%|   | 578/820 [03:30<01:10,  3.43it/s] 71%|   | 579/820 [03:31<01:09,  3.46it/s] 71%|   | 580/820 [03:31<01:08,  3.49it/s] 71%|   | 581/820 [03:31<01:08,  3.50it/s] 71%|   | 582/820 [03:31<01:07,  3.50it/s] 71%|   | 583/820 [03:32<01:07,  3.52it/s] 71%|   | 584/820 [03:32<01:06,  3.53it/s] 71%|  | 585/820 [03:32<01:06,  3.53it/s] 71%|  | 586/820 [03:33<01:06,  3.54it/s] 72%|  | 587/820 [03:33<01:05,  3.54it/s] 72%|  | 588/820 [03:33<01:05,  3.54it/s] 72%|  | 589/820 [03:33<01:07,  3.41it/s] 72%|  | 590/820 [03:34<01:06,  3.45it/s] 72%|  | 591/820 [03:34<01:05,  3.47it/s] 72%|  | 592/820 [03:34<01:05,  3.50it/s] 72%|  | 593/820 [03:35<01:04,  3.52it/s] 72%|  | 594/820 [03:35<01:03,  3.55it/s] 73%|  | 595/820 [03:35<01:03,  3.56it/s] 73%|  | 596/820 [03:35<01:02,  3.57it/s] 73%|  | 597/820 [03:36<01:02,  3.58it/s] 73%|  | 598/820 [03:36<01:01,  3.58it/s] 73%|  | 599/820 [03:36<01:01,  3.59it/s] 73%|  | 600/820 [03:36<01:01,  3.60it/s] 73%|  | 601/820 [03:37<01:00,  3.60it/s] 73%|  | 602/820 [03:37<01:00,  3.60it/s] 74%|  | 603/820 [03:37<01:00,  3.60it/s] 74%|  | 604/820 [03:38<01:00,  3.60it/s] 74%|  | 605/820 [03:38<00:59,  3.60it/s] 74%|  | 606/820 [03:38<00:59,  3.59it/s] 74%|  | 607/820 [03:38<01:01,  3.46it/s] 74%|  | 608/820 [03:39<01:00,  3.51it/s] 74%|  | 609/820 [03:39<00:59,  3.53it/s] 74%|  | 610/820 [03:39<00:59,  3.55it/s] 75%|  | 611/820 [03:40<00:58,  3.56it/s] 75%|  | 612/820 [03:40<00:58,  3.57it/s] 75%|  | 613/820 [03:40<00:57,  3.58it/s] 75%|  | 614/820 [03:40<00:57,  3.58it/s] 75%|  | 615/820 [03:41<00:57,  3.59it/s] 75%|  | 616/820 [03:41<00:56,  3.59it/s] 75%|  | 617/820 [03:41<00:56,  3.60it/s] 75%|  | 618/820 [03:42<00:56,  3.60it/s] 75%|  | 619/820 [03:42<00:55,  3.60it/s] 76%|  | 620/820 [03:42<00:55,  3.60it/s] 76%|  | 621/820 [03:42<00:55,  3.60it/s] 76%|  | 622/820 [03:43<00:55,  3.60it/s] 76%|  | 623/820 [03:43<00:54,  3.60it/s] 76%|  | 624/820 [03:43<00:54,  3.60it/s] 76%|  | 625/820 [03:44<00:55,  3.50it/s] 76%|  | 626/820 [03:44<00:56,  3.42it/s] 76%|  | 627/820 [03:44<00:55,  3.47it/s] 77%|  | 628/820 [03:44<00:54,  3.51it/s] 77%|  | 629/820 [03:45<00:54,  3.54it/s] 77%|  | 630/820 [03:45<00:53,  3.55it/s] 77%|  | 631/820 [03:45<00:53,  3.56it/s] 77%|  | 632/820 [03:45<00:52,  3.57it/s] 77%|  | 633/820 [03:46<01:11,  2.62it/s] 77%|  | 634/820 [03:46<01:05,  2.83it/s] 77%|  | 635/820 [03:47<01:01,  3.03it/s] 78%|  | 636/820 [03:47<00:57,  3.18it/s] 78%|  | 637/820 [03:47<00:55,  3.29it/s] 78%|  | 638/820 [03:48<00:53,  3.38it/s] 78%|  | 639/820 [03:48<00:52,  3.44it/s] 78%|  | 640/820 [03:48<00:51,  3.49it/s] 78%|  | 641/820 [03:48<00:50,  3.52it/s] 78%|  | 642/820 [03:49<00:50,  3.55it/s] 78%|  | 643/820 [03:49<00:51,  3.41it/s] 79%|  | 644/820 [03:49<00:50,  3.47it/s] 79%|  | 645/820 [03:49<00:49,  3.50it/s] 79%|  | 646/820 [03:50<00:49,  3.53it/s] 79%|  | 647/820 [03:50<00:48,  3.55it/s] 79%|  | 648/820 [03:50<00:48,  3.57it/s] 79%|  | 649/820 [03:51<00:47,  3.57it/s] 79%|  | 650/820 [03:51<00:47,  3.58it/s] 79%|  | 651/820 [03:51<00:47,  3.59it/s] 80%|  | 652/820 [03:51<00:46,  3.59it/s] 80%|  | 653/820 [03:52<00:46,  3.60it/s] 80%|  | 654/820 [03:52<00:48,  3.45it/s] 80%|  | 655/820 [03:52<00:47,  3.50it/s] 80%|  | 656/820 [03:53<00:43,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 13:58:12,587 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:58:12,587 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 13:58:12,587 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7652, 'eval_samples_per_second': 356.059, 'eval_steps_per_second': 44.546, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.222560975609756e-05, 'epoch': 3.05}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.29it/s][A
  3%|         | 12/435 [00:00<00:08, 49.14it/s][A
  4%|         | 17/435 [00:00<00:08, 47.43it/s][A
  5%|         | 22/435 [00:00<00:08, 46.53it/s][A
  6%|         | 27/435 [00:00<00:08, 45.62it/s][A
  7%|         | 32/435 [00:00<00:08, 45.32it/s][A
  9%|         | 37/435 [00:00<00:08, 44.95it/s][A
 10%|         | 42/435 [00:00<00:08, 44.59it/s][A
 11%|         | 47/435 [00:01<00:08, 44.67it/s][A
 12%|        | 52/435 [00:01<00:08, 44.77it/s][A
 13%|        | 57/435 [00:01<00:08, 44.77it/s][A
 14%|        | 62/435 [00:01<00:08, 44.81it/s][A
 15%|        | 67/435 [00:01<00:08, 44.87it/s][A
 17%|        | 72/435 [00:01<00:08, 44.94it/s][A
 18%|        | 77/435 [00:01<00:07, 44.85it/s][A
 19%|        | 82/435 [00:01<00:07, 44.69it/s][A
 20%|        | 87/435 [00:01<00:07, 44.58it/s][A
 21%|        | 92/435 [00:02<00:07, 44.59it/s][A
 22%|       | 97/435 [00:02<00:07, 44.65it/s][A
 23%|       | 102/435 [00:02<00:07, 44.75it/s][A
 25%|       | 107/435 [00:02<00:07, 42.04it/s][A
 26%|       | 112/435 [00:02<00:07, 42.93it/s][A
 27%|       | 117/435 [00:02<00:07, 43.56it/s][A
 28%|       | 122/435 [00:02<00:07, 44.09it/s][A
 29%|       | 127/435 [00:02<00:06, 44.20it/s][A
 30%|       | 132/435 [00:02<00:06, 44.33it/s][A
 31%|      | 137/435 [00:03<00:06, 44.35it/s][A
 33%|      | 142/435 [00:03<00:06, 44.44it/s][A
 34%|      | 147/435 [00:03<00:06, 44.35it/s][A
 35%|      | 152/435 [00:03<00:06, 44.40it/s][A
 36%|      | 157/435 [00:03<00:06, 44.55it/s][A
 37%|      | 162/435 [00:03<00:06, 44.72it/s][A
 38%|      | 167/435 [00:03<00:05, 44.86it/s][A
 40%|      | 172/435 [00:03<00:05, 44.88it/s][A
 41%|      | 177/435 [00:03<00:05, 44.88it/s][A
 42%|     | 182/435 [00:04<00:05, 44.86it/s][A
 43%|     | 187/435 [00:04<00:05, 44.78it/s][A
 44%|     | 192/435 [00:04<00:05, 44.74it/s][A
 45%|     | 197/435 [00:04<00:05, 44.67it/s][A
 46%|     | 202/435 [00:04<00:05, 44.76it/s][A
 48%|     | 207/435 [00:04<00:05, 44.83it/s][A
 49%|     | 212/435 [00:04<00:04, 44.79it/s][A
 50%|     | 217/435 [00:04<00:04, 44.87it/s][A
 51%|     | 222/435 [00:04<00:04, 44.87it/s][A
 52%|    | 227/435 [00:05<00:04, 44.88it/s][A
 53%|    | 232/435 [00:05<00:04, 44.80it/s][A
 54%|    | 237/435 [00:05<00:04, 44.71it/s][A
 56%|    | 242/435 [00:05<00:04, 44.70it/s][A
 57%|    | 247/435 [00:05<00:04, 44.60it/s][A
 58%|    | 252/435 [00:05<00:04, 44.73it/s][A
 59%|    | 257/435 [00:05<00:03, 44.73it/s][A
 60%|    | 262/435 [00:05<00:03, 44.75it/s][A
 61%|   | 267/435 [00:05<00:03, 44.83it/s][A
 63%|   | 272/435 [00:06<00:03, 44.76it/s][A
 64%|   | 277/435 [00:06<00:03, 44.79it/s][A
 65%|   | 282/435 [00:06<00:03, 43.27it/s][A
 66%|   | 287/435 [00:06<00:03, 43.69it/s][A
 67%|   | 292/435 [00:06<00:03, 44.03it/s][A
 68%|   | 297/435 [00:06<00:03, 44.28it/s][A
 69%|   | 302/435 [00:06<00:02, 44.44it/s][A
 71%|   | 307/435 [00:06<00:02, 44.63it/s][A
 72%|  | 312/435 [00:06<00:02, 44.64it/s][A
 73%|  | 317/435 [00:07<00:02, 44.69it/s][A
 74%|  | 322/435 [00:07<00:02, 44.53it/s][A
 75%|  | 327/435 [00:07<00:02, 44.55it/s][A
 76%|  | 332/435 [00:07<00:02, 44.64it/s][A
 77%|  | 337/435 [00:07<00:02, 44.82it/s][A
 79%|  | 342/435 [00:07<00:02, 44.87it/s][A
 80%|  | 347/435 [00:07<00:01, 44.92it/s][A
 81%|  | 352/435 [00:07<00:01, 44.87it/s][A
 82%| | 357/435 [00:07<00:01, 44.88it/s][A
 83%| | 362/435 [00:08<00:01, 44.81it/s][A
 84%| | 367/435 [00:08<00:01, 44.77it/s][A
 86%| | 372/435 [00:08<00:01, 40.63it/s][A
 87%| | 377/435 [00:08<00:01, 41.89it/s][A
 88%| | 382/435 [00:08<00:01, 42.73it/s][A
 89%| | 387/435 [00:08<00:01, 43.43it/s][A
 90%| | 392/435 [00:08<00:00, 43.98it/s][A
 91%|| 397/435 [00:08<00:00, 44.32it/s][A
 92%|| 402/435 [00:09<00:00, 44.49it/s][A
 94%|| 407/435 [00:09<00:00, 44.67it/s][A
 95%|| 412/435 [00:09<00:00, 44.41it/s][A
 96%|| 417/435 [00:09<00:00, 44.34it/s][A
 97%|| 422/435 [00:09<00:00, 44.48it/s][A
 98%|| 427/435 [00:09<00:00, 44.64it/s][A
 99%|| 432/435 [00:09<00:00, 44.82it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.82it/s][A 80%|  | 656/820 [04:02<00:43,  3.79it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:58:22,548 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-656
[INFO|configuration_utils.py:351] 2023-08-28 13:58:22,746 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-656/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:58:25,845 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-656/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:58:26,004 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-656/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:58:26,091 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-656/special_tokens_map.json
 80%|  | 657/820 [04:07<12:33,  4.63s/it] 80%|  | 658/820 [04:08<08:58,  3.32s/it] 80%|  | 659/820 [04:08<06:28,  2.41s/it] 80%|  | 660/820 [04:08<04:43,  1.77s/it] 81%|  | 661/820 [04:08<03:30,  1.32s/it] 81%|  | 662/820 [04:09<02:39,  1.01s/it] 81%|  | 663/820 [04:09<02:03,  1.27it/s] 81%|  | 664/820 [04:09<01:40,  1.55it/s] 81%|  | 665/820 [04:10<01:22,  1.87it/s] 81%|  | 666/820 [04:10<01:10,  2.19it/s] 81%| | 667/820 [04:10<01:01,  2.48it/s] 81%| | 668/820 [04:10<00:55,  2.74it/s] 82%| | 669/820 [04:11<00:51,  2.95it/s] 82%| | 670/820 [04:11<00:48,  3.12it/s] 82%| | 671/820 [04:11<00:45,  3.25it/s] 82%| | 672/820 [04:12<00:44,  3.35it/s] 82%| | 673/820 [04:12<00:43,  3.42it/s] 82%| | 674/820 [04:12<00:42,  3.47it/s] 82%| | 675/820 [04:12<00:42,  3.38it/s] 82%| | 676/820 [04:13<00:41,  3.44it/s] 83%| | 677/820 [04:13<00:40,  3.49it/s] 83%| | 678/820 [04:13<00:40,  3.52it/s] 83%| | 679/820 [04:13<00:39,  3.54it/s] 83%| | 680/820 [04:14<00:39,  3.56it/s] 83%| | 681/820 [04:14<00:38,  3.57it/s] 83%| | 682/820 [04:14<00:38,  3.58it/s] 83%| | 683/820 [04:15<00:38,  3.58it/s] 83%| | 684/820 [04:15<00:37,  3.59it/s] 84%| | 685/820 [04:15<00:37,  3.60it/s] 84%| | 686/820 [04:15<00:38,  3.52it/s] 84%| | 687/820 [04:16<00:37,  3.54it/s] 84%| | 688/820 [04:16<00:37,  3.56it/s] 84%| | 689/820 [04:16<00:36,  3.57it/s] 84%| | 690/820 [04:17<00:36,  3.58it/s] 84%| | 691/820 [04:17<00:36,  3.58it/s] 84%| | 692/820 [04:17<00:35,  3.58it/s] 85%| | 693/820 [04:17<00:35,  3.58it/s] 85%| | 694/820 [04:18<00:35,  3.59it/s] 85%| | 695/820 [04:18<00:34,  3.59it/s] 85%| | 696/820 [04:18<00:34,  3.60it/s] 85%| | 697/820 [04:19<00:34,  3.52it/s] 85%| | 698/820 [04:19<00:34,  3.54it/s] 85%| | 699/820 [04:19<00:34,  3.56it/s] 85%| | 700/820 [04:19<00:33,  3.56it/s] 85%| | 701/820 [04:20<00:33,  3.57it/s] 86%| | 702/820 [04:20<00:32,  3.58it/s] 86%| | 703/820 [04:20<00:32,  3.57it/s] 86%| | 704/820 [04:20<00:32,  3.58it/s] 86%| | 705/820 [04:21<00:32,  3.58it/s] 86%| | 706/820 [04:21<00:31,  3.59it/s] 86%| | 707/820 [04:21<00:31,  3.59it/s] 86%| | 708/820 [04:22<00:32,  3.48it/s] 86%| | 709/820 [04:22<00:31,  3.51it/s] 87%| | 710/820 [04:22<00:31,  3.54it/s] 87%| | 711/820 [04:22<00:30,  3.55it/s] 87%| | 712/820 [04:23<00:30,  3.57it/s] 87%| | 713/820 [04:23<00:29,  3.58it/s] 87%| | 714/820 [04:23<00:29,  3.59it/s] 87%| | 715/820 [04:24<00:29,  3.59it/s] 87%| | 716/820 [04:24<00:28,  3.60it/s] 87%| | 717/820 [04:24<00:28,  3.60it/s] 88%| | 718/820 [04:24<00:28,  3.60it/s] 88%| | 719/820 [04:25<00:29,  3.43it/s] 88%| | 720/820 [04:25<00:28,  3.48it/s] 88%| | 721/820 [04:25<00:28,  3.51it/s] 88%| | 722/820 [04:26<00:27,  3.54it/s] 88%| | 723/820 [04:26<00:27,  3.55it/s] 88%| | 724/820 [04:26<00:26,  3.57it/s] 88%| | 725/820 [04:26<00:26,  3.57it/s] 89%| | 726/820 [04:27<00:26,  3.58it/s] 89%| | 727/820 [04:27<00:25,  3.59it/s] 89%| | 728/820 [04:27<00:25,  3.59it/s] 89%| | 729/820 [04:28<00:25,  3.60it/s] 89%| | 730/820 [04:28<00:26,  3.36it/s] 89%| | 731/820 [04:28<00:25,  3.43it/s] 89%| | 732/820 [04:28<00:25,  3.48it/s] 89%| | 733/820 [04:29<00:24,  3.51it/s] 90%| | 734/820 [04:29<00:24,  3.54it/s] 90%| | 735/820 [04:29<00:23,  3.55it/s] 90%| | 736/820 [04:30<00:23,  3.57it/s] 90%| | 737/820 [04:30<00:23,  3.58it/s] 90%| | 738/820 [04:30<00:22,  3.58it/s] 90%| | 739/820 [04:30<00:22,  3.59it/s] 90%| | 740/820 [04:31<00:22,  3.59it/s] 90%| | 741/820 [04:31<00:23,  3.34it/s] 90%| | 742/820 [04:31<00:22,  3.42it/s] 91%| | 743/820 [04:32<00:22,  3.47it/s] 91%| | 744/820 [04:32<00:21,  3.49it/s] 91%| | 745/820 [04:32<00:21,  3.52it/s] 91%| | 746/820 [04:32<00:20,  3.55it/s] 91%| | 747/820 [04:33<00:20,  3.56it/s] 91%| | 748/820 [04:33<00:20,  3.57it/s] 91%|| 749/820 [04:33<00:19,  3.58it/s] 91%|| 750/820 [04:33<00:19,  3.58it/s] 92%|| 751/820 [04:34<00:19,  3.59it/s] 92%|| 752/820 [04:34<00:18,  3.59it/s] 92%|| 753/820 [04:34<00:18,  3.59it/s] 92%|| 754/820 [04:35<00:18,  3.59it/s] 92%|| 755/820 [04:35<00:18,  3.45it/s] 92%|| 756/820 [04:35<00:18,  3.49it/s] 92%|| 757/820 [04:35<00:17,  3.52it/s] 92%|| 758/820 [04:36<00:17,  3.55it/s] 93%|| 759/820 [04:36<00:17,  3.56it/s] 93%|| 760/820 [04:36<00:16,  3.57it/s] 93%|| 761/820 [04:37<00:16,  3.58it/s] 93%|| 762/820 [04:37<00:16,  3.58it/s] 93%|| 763/820 [04:37<00:15,  3.59it/s] 93%|| 764/820 [04:37<00:15,  3.59it/s] 93%|| 765/820 [04:38<00:15,  3.59it/s] 93%|| 766/820 [04:38<00:15,  3.49it/s] 94%|| 767/820 [04:38<00:15,  3.52it/s] 94%|| 768/820 [04:39<00:14,  3.54it/s] 94%|| 769/820 [04:39<00:14,  3.56it/s] 94%|| 770/820 [04:39<00:13,  3.57it/s] 94%|| 771/820 [04:39<00:13,  3.58it/s] 94%|| 772/820 [04:40<00:13,  3.59it/s] 94%|| 773/820 [04:40<00:13,  3.59it/s] 94%|| 774/820 [04:40<00:12,  3.60it/s] 95%|| 775/820 [04:41<00:12,  3.59it/s] 95%|| 776/820 [04:41<00:12,  3.59it/s] 95%|| 777/820 [04:41<00:12,  3.46it/s] 95%|| 778/820 [04:41<00:11,  3.50it/s] 95%|| 779/820 [04:42<00:11,  3.53it/s] 95%|| 780/820 [04:42<00:11,  3.55it/s] 95%|| 781/820 [04:42<00:10,  3.57it/s] 95%|| 782/820 [04:42<00:10,  3.58it/s] 95%|| 783/820 [04:43<00:10,  3.58it/s] 96%|| 784/820 [04:43<00:10,  3.59it/s] 96%|| 785/820 [04:43<00:09,  3.59it/s] 96%|| 786/820 [04:44<00:09,  3.59it/s] 96%|| 787/820 [04:44<00:09,  3.59it/s] 96%|| 788/820 [04:44<00:09,  3.47it/s] 96%|| 789/820 [04:44<00:08,  3.51it/s] 96%|| 790/820 [04:45<00:08,  3.53it/s] 96%|| 791/820 [04:45<00:08,  3.55it/s] 97%|| 792/820 [04:45<00:07,  3.56it/s] 97%|| 793/820 [04:46<00:07,  3.57it/s] 97%|| 794/820 [04:46<00:08,  2.94it/s] 97%|| 795/820 [04:46<00:08,  3.03it/s] 97%|| 796/820 [04:47<00:07,  3.16it/s] 97%|| 797/820 [04:47<00:07,  3.28it/s] 97%|| 798/820 [04:47<00:06,  3.31it/s] 97%|| 799/820 [04:48<00:06,  3.39it/s] 98%|| 800/820 [04:48<00:05,  3.45it/s] 98%|| 801/820 [04:48<00:05,  3.49it/s] 98%|| 802/820 [04:48<00:05,  3.52it/s] 98%|| 803/820 [04:49<00:04,  3.55it/s] 98%|| 804/820 [04:49<00:04,  3.56it/s] 98%|| 805/820 [04:49<00:04,  3.58it/s] 98%|| 806/820 [04:49<00:03,  3.58it/s] 98%|| 807/820 [04:50<00:03,  3.59it/s] 99%|| 808/820 [04:50<00:03,  3.59it/s] 99%|| 809/820 [04:50<00:03,  3.49it/s] 99%|| 810/820 [04:51<00:02,  3.52it/s] 99%|| 811/820 [04:51<00:02,  3.54it/s] 99%|| 812/820 [04:51<00:02,  3.56it/s] 99%|| 813/820 [04:51<00:01,  3.57it/s] 99%|| 814/820 [04:52<00:01,  3.58it/s] 99%|| 815/820 [04:52<00:01,  3.58it/s]100%|| 816/820 [04:52<00:01,  3.59it/s]100%|| 817/820 [04:53<00:00,  3.59it/s]100%|| 818/820 [04:53<00:00,  3.60it/s]100%|| 819/820 [04:53<00:00,  3.60it/s]100%|| 820/820 [04:53<00:00,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 13:59:13,398 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:59:13,399 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 13:59:13,399 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.779, 'eval_samples_per_second': 355.559, 'eval_steps_per_second': 44.483, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.98it/s][A
  3%|         | 12/435 [00:00<00:08, 49.11it/s][A
  4%|         | 17/435 [00:00<00:08, 47.36it/s][A
  5%|         | 22/435 [00:00<00:08, 46.52it/s][A
  6%|         | 27/435 [00:00<00:08, 46.04it/s][A
  7%|         | 32/435 [00:00<00:08, 45.59it/s][A
  9%|         | 37/435 [00:00<00:08, 45.14it/s][A
 10%|         | 42/435 [00:00<00:08, 44.68it/s][A
 11%|         | 47/435 [00:01<00:08, 44.63it/s][A
 12%|        | 52/435 [00:01<00:08, 44.84it/s][A
 13%|        | 57/435 [00:01<00:08, 44.93it/s][A
 14%|        | 62/435 [00:01<00:08, 44.99it/s][A
 15%|        | 67/435 [00:01<00:08, 45.06it/s][A
 17%|        | 72/435 [00:01<00:08, 45.08it/s][A
 18%|        | 77/435 [00:01<00:07, 44.97it/s][A
 19%|        | 82/435 [00:01<00:07, 44.63it/s][A
 20%|        | 87/435 [00:01<00:07, 44.58it/s][A
 21%|        | 92/435 [00:02<00:07, 44.50it/s][A
 22%|       | 97/435 [00:02<00:07, 44.66it/s][A
 23%|       | 102/435 [00:02<00:07, 44.75it/s][A
 25%|       | 107/435 [00:02<00:07, 44.88it/s][A
 26%|       | 112/435 [00:02<00:07, 44.98it/s][A
 27%|       | 117/435 [00:02<00:07, 45.01it/s][A
 28%|       | 122/435 [00:02<00:06, 44.98it/s][A
 29%|       | 127/435 [00:02<00:06, 44.85it/s][A
 30%|       | 132/435 [00:02<00:07, 42.53it/s][A
 31%|      | 137/435 [00:03<00:06, 43.27it/s][A
 33%|      | 142/435 [00:03<00:06, 43.70it/s][A
 34%|      | 147/435 [00:03<00:06, 44.10it/s][A
 35%|      | 152/435 [00:03<00:06, 44.37it/s][A
 36%|      | 157/435 [00:03<00:06, 44.60it/s][A
 37%|      | 162/435 [00:03<00:06, 44.64it/s][A
 38%|      | 167/435 [00:03<00:05, 44.70it/s][A
 40%|      | 172/435 [00:03<00:05, 44.44it/s][A
 41%|      | 177/435 [00:03<00:05, 44.32it/s][A
 42%|     | 182/435 [00:04<00:05, 44.46it/s][A
 43%|     | 187/435 [00:04<00:05, 44.45it/s][A
 44%|     | 192/435 [00:04<00:05, 44.52it/s][A
 45%|     | 197/435 [00:04<00:05, 44.62it/s][A
 46%|     | 202/435 [00:04<00:05, 44.83it/s][A
 48%|     | 207/435 [00:04<00:05, 44.92it/s][A
 49%|     | 212/435 [00:04<00:04, 44.80it/s][A
 50%|     | 217/435 [00:04<00:04, 44.76it/s][A
 51%|     | 222/435 [00:04<00:04, 44.68it/s][A
 52%|    | 227/435 [00:05<00:04, 44.70it/s][A
 53%|    | 232/435 [00:05<00:04, 44.80it/s][A
 54%|    | 237/435 [00:05<00:04, 44.77it/s][A
 56%|    | 242/435 [00:05<00:04, 44.86it/s][A
 57%|    | 247/435 [00:05<00:04, 44.87it/s][A
 58%|    | 252/435 [00:05<00:04, 44.91it/s][A
 59%|    | 257/435 [00:05<00:03, 44.84it/s][A
 60%|    | 262/435 [00:05<00:03, 44.71it/s][A
 61%|   | 267/435 [00:05<00:04, 41.80it/s][A
 63%|   | 272/435 [00:06<00:03, 42.74it/s][A
 64%|   | 277/435 [00:06<00:03, 43.46it/s][A
 65%|   | 282/435 [00:06<00:03, 43.92it/s][A
 66%|   | 287/435 [00:06<00:03, 44.31it/s][A
 67%|   | 292/435 [00:06<00:03, 44.34it/s][A
 68%|   | 297/435 [00:06<00:03, 44.77it/s][A
 69%|   | 302/435 [00:06<00:02, 44.64it/s][A
 71%|   | 307/435 [00:06<00:02, 44.49it/s][A
 72%|  | 312/435 [00:06<00:02, 44.42it/s][A
 73%|  | 317/435 [00:07<00:02, 44.49it/s][A
 74%|  | 322/435 [00:07<00:02, 44.67it/s][A
 75%|  | 327/435 [00:07<00:02, 44.79it/s][A
 76%|  | 332/435 [00:07<00:02, 44.91it/s][A
 77%|  | 337/435 [00:07<00:02, 44.98it/s][A
 79%|  | 342/435 [00:07<00:02, 44.95it/s][A
 80%|  | 347/435 [00:07<00:01, 44.70it/s][A
 81%|  | 352/435 [00:07<00:01, 44.61it/s][A
 82%| | 357/435 [00:07<00:01, 44.57it/s][A
 83%| | 362/435 [00:08<00:01, 44.56it/s][A
 84%| | 367/435 [00:08<00:01, 44.71it/s][A
 86%| | 372/435 [00:08<00:01, 44.84it/s][A
 87%| | 377/435 [00:08<00:01, 44.98it/s][A
 88%| | 382/435 [00:08<00:01, 45.03it/s][A
 89%| | 387/435 [00:08<00:01, 44.97it/s][A
 90%| | 392/435 [00:08<00:00, 44.82it/s][A
 91%|| 397/435 [00:08<00:00, 44.78it/s][A
 92%|| 402/435 [00:08<00:00, 44.74it/s][A
 94%|| 407/435 [00:09<00:00, 44.71it/s][A
 95%|| 412/435 [00:09<00:00, 44.72it/s][A
 96%|| 417/435 [00:09<00:00, 44.82it/s][A
 97%|| 422/435 [00:09<00:00, 44.86it/s][A
 98%|| 427/435 [00:09<00:00, 44.91it/s][A
 99%|| 432/435 [00:09<00:00, 44.87it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.87it/s][A100%|| 820/820 [05:03<00:00,  3.77it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:59:23,363 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-820
[INFO|configuration_utils.py:351] 2023-08-28 13:59:23,532 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-820/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:59:26,482 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-820/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:59:26,611 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:59:26,661 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-820/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:59:27,271 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:59:27,272 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-164 (score: 0.9974275827407837).
                                                 100%|| 820/820 [05:15<00:00,  3.77it/s]100%|| 820/820 [05:15<00:00,  2.60it/s]
[INFO|trainer.py:1894] 2023-08-28 13:59:35,263 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 13:59:35,457 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:59:38,935 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:59:39,052 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:59:39,115 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:59:39,640 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:39,641 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:39,641 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:39,641 >>   train_runtime            = 0:05:15.62
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:39,641 >>   train_samples            =      10479
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:39,641 >>   train_samples_per_second =    166.002
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:39,641 >>   train_steps_per_second   =      2.598
{'eval_loss': 0.9974275827407837, 'eval_runtime': 9.7465, 'eval_samples_per_second': 356.743, 'eval_steps_per_second': 44.631, 'epoch': 5.0}
{'train_runtime': 315.629, 'train_samples_per_second': 166.002, 'train_steps_per_second': 2.598, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 13:59:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:59:39,949 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:59:39,949 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 13:59:39,949 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 55.21it/s]  3%|         | 12/435 [00:00<00:08, 48.94it/s]  4%|         | 17/435 [00:00<00:08, 47.44it/s]  5%|         | 22/435 [00:00<00:08, 46.70it/s]  6%|         | 27/435 [00:00<00:08, 46.23it/s]  7%|         | 32/435 [00:00<00:08, 45.99it/s]  9%|         | 37/435 [00:00<00:08, 45.80it/s] 10%|         | 42/435 [00:00<00:08, 45.42it/s] 11%|         | 47/435 [00:01<00:08, 44.88it/s] 12%|        | 52/435 [00:01<00:08, 44.80it/s] 13%|        | 57/435 [00:01<00:08, 44.87it/s] 14%|        | 62/435 [00:01<00:08, 44.97it/s] 15%|        | 67/435 [00:01<00:08, 45.07it/s] 17%|        | 72/435 [00:01<00:08, 45.21it/s] 18%|        | 77/435 [00:01<00:07, 45.24it/s] 19%|        | 82/435 [00:01<00:07, 45.22it/s] 20%|        | 87/435 [00:01<00:07, 44.86it/s] 21%|        | 92/435 [00:02<00:07, 44.71it/s] 22%|       | 97/435 [00:02<00:07, 42.90it/s] 23%|       | 102/435 [00:02<00:07, 43.52it/s] 25%|       | 107/435 [00:02<00:07, 43.93it/s] 26%|       | 112/435 [00:02<00:07, 44.40it/s] 27%|       | 117/435 [00:02<00:07, 44.67it/s] 28%|       | 122/435 [00:02<00:06, 44.86it/s] 29%|       | 127/435 [00:02<00:06, 44.91it/s] 30%|       | 132/435 [00:02<00:06, 44.90it/s] 31%|      | 137/435 [00:03<00:06, 44.69it/s] 33%|      | 142/435 [00:03<00:06, 44.51it/s] 34%|      | 147/435 [00:03<00:06, 44.70it/s] 35%|      | 152/435 [00:03<00:06, 44.87it/s] 36%|      | 157/435 [00:03<00:06, 44.96it/s] 37%|      | 162/435 [00:03<00:06, 45.04it/s] 38%|      | 167/435 [00:03<00:05, 45.07it/s] 40%|      | 172/435 [00:03<00:05, 45.09it/s] 41%|      | 177/435 [00:03<00:05, 44.90it/s] 42%|     | 182/435 [00:04<00:05, 44.76it/s] 43%|     | 187/435 [00:04<00:05, 44.64it/s] 44%|     | 192/435 [00:04<00:05, 44.65it/s] 45%|     | 197/435 [00:04<00:05, 44.78it/s] 46%|     | 202/435 [00:04<00:05, 44.84it/s] 48%|     | 207/435 [00:04<00:05, 45.01it/s] 49%|     | 212/435 [00:04<00:04, 45.06it/s] 50%|     | 217/435 [00:04<00:04, 45.04it/s] 51%|     | 222/435 [00:04<00:04, 45.03it/s] 52%|    | 227/435 [00:05<00:04, 44.92it/s] 53%|    | 232/435 [00:05<00:04, 41.63it/s] 54%|    | 237/435 [00:05<00:04, 42.71it/s] 56%|    | 242/435 [00:05<00:04, 43.41it/s] 57%|    | 247/435 [00:05<00:04, 43.89it/s] 58%|    | 252/435 [00:05<00:04, 44.31it/s] 59%|    | 257/435 [00:05<00:03, 44.57it/s] 60%|    | 262/435 [00:05<00:03, 44.73it/s] 61%|   | 267/435 [00:05<00:03, 44.84it/s] 63%|   | 272/435 [00:06<00:03, 44.58it/s] 64%|   | 277/435 [00:06<00:03, 44.50it/s] 65%|   | 282/435 [00:06<00:03, 44.70it/s] 66%|   | 287/435 [00:06<00:03, 44.79it/s] 67%|   | 292/435 [00:06<00:03, 44.93it/s] 68%|   | 297/435 [00:06<00:03, 44.99it/s] 69%|   | 302/435 [00:06<00:02, 44.90it/s] 71%|   | 307/435 [00:06<00:02, 44.99it/s] 72%|  | 312/435 [00:06<00:02, 44.90it/s] 73%|  | 317/435 [00:07<00:02, 44.68it/s] 74%|  | 322/435 [00:07<00:02, 44.64it/s] 75%|  | 327/435 [00:07<00:02, 44.79it/s] 76%|  | 332/435 [00:07<00:02, 44.85it/s] 77%|  | 337/435 [00:07<00:02, 44.91it/s] 79%|  | 342/435 [00:07<00:02, 44.92it/s] 80%|  | 347/435 [00:07<00:01, 44.92it/s] 81%|  | 352/435 [00:07<00:01, 44.98it/s] 82%| | 357/435 [00:07<00:01, 44.83it/s] 83%| | 362/435 [00:08<00:01, 44.72it/s] 84%| | 367/435 [00:08<00:01, 39.23it/s] 86%| | 372/435 [00:08<00:01, 40.89it/s] 87%| | 377/435 [00:08<00:01, 42.00it/s] 88%| | 382/435 [00:08<00:01, 42.97it/s] 89%| | 387/435 [00:08<00:01, 43.61it/s] 90%| | 392/435 [00:08<00:00, 44.13it/s] 91%|| 397/435 [00:08<00:00, 44.37it/s] 92%|| 402/435 [00:09<00:00, 44.58it/s] 94%|| 407/435 [00:09<00:00, 44.34it/s] 95%|| 412/435 [00:09<00:00, 44.42it/s] 96%|| 417/435 [00:09<00:00, 44.58it/s] 97%|| 422/435 [00:09<00:00, 44.73it/s] 98%|| 427/435 [00:09<00:00, 44.90it/s] 99%|| 432/435 [00:09<00:00, 44.98it/s]100%|| 435/435 [00:09<00:00, 44.59it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:59:49,720 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:49,720 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:49,720 >>   eval_loss               =     0.9974
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:49,720 >>   eval_runtime            = 0:00:09.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:49,720 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:49,720 >>   eval_samples_per_second =    355.857
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:49,720 >>   eval_steps_per_second   =     44.521
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:59:49,720 >>   perplexity              =     2.7113
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:03,162 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:03,219 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:03,219 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:03,219 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:03,219 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:00:04,193 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:00:04,194 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:00:04,822 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:00:05,975 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:00:05,975 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:09,137 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:09,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:09,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:09,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:00:09,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:00:09,929 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:00:09,930 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:00:10,546 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:00:10,783 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:00:10,784 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-656
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-820
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/checkpoint-328
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.78it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.75it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.66it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:08,  1.64it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:10,  1.70it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:11,  1.68it/s]Extractor Predicting: 21it [00:12,  1.69it/s]Extractor Predicting: 22it [00:13,  1.70it/s]Extractor Predicting: 23it [00:13,  1.73it/s]Extractor Predicting: 24it [00:14,  1.73it/s]Extractor Predicting: 25it [00:14,  1.66it/s]Extractor Predicting: 26it [00:15,  1.65it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:16,  1.64it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:17,  1.61it/s]Extractor Predicting: 31it [00:18,  1.61it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:19,  1.59it/s]Extractor Predicting: 34it [00:20,  1.62it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:22,  1.68it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.66it/s]Extractor Predicting: 40it [00:24,  1.66it/s]Extractor Predicting: 41it [00:24,  1.67it/s]Extractor Predicting: 42it [00:25,  1.67it/s]Extractor Predicting: 43it [00:25,  1.65it/s]Extractor Predicting: 44it [00:26,  1.65it/s]Extractor Predicting: 45it [00:27,  1.63it/s]Extractor Predicting: 46it [00:27,  1.64it/s]Extractor Predicting: 47it [00:28,  1.66it/s]Extractor Predicting: 48it [00:28,  1.67it/s]Extractor Predicting: 49it [00:29,  1.70it/s]Extractor Predicting: 50it [00:29,  1.75it/s]Extractor Predicting: 51it [00:30,  1.74it/s]Extractor Predicting: 52it [00:31,  1.62it/s]Extractor Predicting: 53it [00:31,  1.63it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.53it/s]Extractor Predicting: 56it [00:33,  1.56it/s]Extractor Predicting: 57it [00:34,  1.54it/s]Extractor Predicting: 58it [00:35,  1.60it/s]Extractor Predicting: 59it [00:35,  1.67it/s]Extractor Predicting: 60it [00:36,  1.71it/s]Extractor Predicting: 61it [00:36,  1.72it/s]Extractor Predicting: 62it [00:37,  1.75it/s]Extractor Predicting: 63it [00:37,  1.74it/s]Extractor Predicting: 64it [00:38,  1.75it/s]Extractor Predicting: 65it [00:39,  1.70it/s]Extractor Predicting: 66it [00:39,  1.69it/s]Extractor Predicting: 67it [00:40,  1.69it/s]Extractor Predicting: 68it [00:40,  1.72it/s]Extractor Predicting: 69it [00:41,  1.72it/s]Extractor Predicting: 70it [00:41,  1.72it/s]Extractor Predicting: 71it [00:42,  1.71it/s]Extractor Predicting: 72it [00:43,  1.73it/s]Extractor Predicting: 73it [00:43,  1.73it/s]Extractor Predicting: 74it [00:44,  1.73it/s]Extractor Predicting: 75it [00:44,  1.76it/s]Extractor Predicting: 76it [00:45,  1.74it/s]Extractor Predicting: 77it [00:45,  1.70it/s]Extractor Predicting: 78it [00:46,  1.70it/s]Extractor Predicting: 79it [00:47,  1.68it/s]Extractor Predicting: 80it [00:47,  1.63it/s]Extractor Predicting: 81it [00:48,  1.62it/s]Extractor Predicting: 82it [00:49,  1.66it/s]Extractor Predicting: 83it [00:49,  1.67it/s]Extractor Predicting: 84it [00:50,  1.68it/s]Extractor Predicting: 85it [00:50,  1.65it/s]Extractor Predicting: 86it [00:51,  1.66it/s]Extractor Predicting: 87it [00:52,  1.66it/s]Extractor Predicting: 88it [00:52,  1.64it/s]Extractor Predicting: 89it [00:53,  1.68it/s]Extractor Predicting: 90it [00:53,  1.68it/s]Extractor Predicting: 91it [00:54,  1.70it/s]Extractor Predicting: 92it [00:54,  1.74it/s]Extractor Predicting: 93it [00:55,  1.75it/s]Extractor Predicting: 94it [00:56,  1.72it/s]Extractor Predicting: 95it [00:56,  1.76it/s]Extractor Predicting: 96it [00:57,  1.75it/s]Extractor Predicting: 97it [00:57,  1.74it/s]Extractor Predicting: 98it [00:58,  1.74it/s]Extractor Predicting: 99it [00:59,  1.70it/s]Extractor Predicting: 100it [00:59,  1.63it/s]Extractor Predicting: 101it [01:00,  1.64it/s]Extractor Predicting: 102it [01:00,  1.73it/s]Extractor Predicting: 103it [01:01,  1.74it/s]Extractor Predicting: 104it [01:01,  1.74it/s]Extractor Predicting: 105it [01:02,  1.74it/s]Extractor Predicting: 106it [01:03,  1.74it/s]Extractor Predicting: 107it [01:03,  1.76it/s]Extractor Predicting: 108it [01:04,  1.75it/s]Extractor Predicting: 109it [01:04,  1.71it/s]Extractor Predicting: 110it [01:05,  1.73it/s]Extractor Predicting: 111it [01:05,  1.74it/s]Extractor Predicting: 112it [01:06,  1.74it/s]Extractor Predicting: 113it [01:07,  1.80it/s]Extractor Predicting: 114it [01:07,  1.83it/s]Extractor Predicting: 115it [01:08,  1.75it/s]Extractor Predicting: 116it [01:08,  1.77it/s]Extractor Predicting: 117it [01:09,  1.78it/s]Extractor Predicting: 118it [01:09,  1.77it/s]Extractor Predicting: 119it [01:10,  1.78it/s]Extractor Predicting: 120it [01:11,  1.73it/s]Extractor Predicting: 121it [01:11,  1.68it/s]Extractor Predicting: 122it [01:12,  1.70it/s]Extractor Predicting: 123it [01:12,  1.71it/s]Extractor Predicting: 124it [01:13,  1.69it/s]Extractor Predicting: 125it [01:13,  1.71it/s]Extractor Predicting: 126it [01:14,  1.73it/s]Extractor Predicting: 127it [01:15,  1.69it/s]Extractor Predicting: 128it [01:15,  1.71it/s]Extractor Predicting: 129it [01:16,  1.72it/s]Extractor Predicting: 130it [01:16,  1.70it/s]Extractor Predicting: 131it [01:17,  1.72it/s]Extractor Predicting: 132it [01:18,  1.70it/s]Extractor Predicting: 133it [01:18,  1.71it/s]Extractor Predicting: 134it [01:19,  1.75it/s]Extractor Predicting: 135it [01:19,  1.74it/s]Extractor Predicting: 136it [01:20,  1.71it/s]Extractor Predicting: 137it [01:20,  1.76it/s]Extractor Predicting: 138it [01:21,  1.58it/s]Extractor Predicting: 139it [01:22,  1.58it/s]Extractor Predicting: 140it [01:22,  1.63it/s]Extractor Predicting: 141it [01:23,  1.68it/s]Extractor Predicting: 142it [01:24,  1.68it/s]Extractor Predicting: 143it [01:24,  1.72it/s]Extractor Predicting: 144it [01:25,  1.76it/s]Extractor Predicting: 144it [01:25,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:48,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:48,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:48,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:48,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:48,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:01:49,734 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:01:49,735 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:01:50,383 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:01:51,562 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:01:51,562 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:54,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:54,769 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:54,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:54,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:54,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:01:55,752 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:01:55,753 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:01:56,433 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:01:56,684 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:01:56,684 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.74it/s]Extractor Predicting: 8it [00:04,  1.80it/s]Extractor Predicting: 9it [00:05,  1.75it/s]Extractor Predicting: 10it [00:05,  1.78it/s]Extractor Predicting: 11it [00:06,  1.78it/s]Extractor Predicting: 12it [00:06,  1.76it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:08,  1.75it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.72it/s]Extractor Predicting: 17it [00:09,  1.71it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:10,  1.79it/s]Extractor Predicting: 20it [00:11,  1.75it/s]Extractor Predicting: 21it [00:12,  1.74it/s]Extractor Predicting: 22it [00:12,  1.77it/s]Extractor Predicting: 23it [00:13,  1.77it/s]Extractor Predicting: 24it [00:13,  1.76it/s]Extractor Predicting: 25it [00:14,  1.74it/s]Extractor Predicting: 26it [00:14,  1.76it/s]Extractor Predicting: 27it [00:15,  1.71it/s]Extractor Predicting: 28it [00:16,  1.73it/s]Extractor Predicting: 29it [00:16,  1.75it/s]Extractor Predicting: 30it [00:17,  1.63it/s]Extractor Predicting: 31it [00:17,  1.65it/s]Extractor Predicting: 32it [00:18,  1.73it/s]Extractor Predicting: 33it [00:19,  1.72it/s]Extractor Predicting: 34it [00:19,  1.70it/s]Extractor Predicting: 35it [00:20,  1.71it/s]Extractor Predicting: 36it [00:20,  1.74it/s]Extractor Predicting: 37it [00:21,  1.77it/s]Extractor Predicting: 38it [00:21,  1.74it/s]Extractor Predicting: 39it [00:22,  1.75it/s]Extractor Predicting: 40it [00:23,  1.74it/s]Extractor Predicting: 41it [00:23,  1.76it/s]Extractor Predicting: 42it [00:24,  1.80it/s]Extractor Predicting: 43it [00:24,  1.74it/s]Extractor Predicting: 44it [00:25,  1.73it/s]Extractor Predicting: 45it [00:26,  1.71it/s]Extractor Predicting: 46it [00:26,  1.69it/s]Extractor Predicting: 47it [00:27,  1.74it/s]Extractor Predicting: 48it [00:27,  1.72it/s]Extractor Predicting: 49it [00:28,  1.75it/s]Extractor Predicting: 50it [00:28,  1.70it/s]Extractor Predicting: 51it [00:29,  1.70it/s]Extractor Predicting: 52it [00:30,  1.69it/s]Extractor Predicting: 53it [00:30,  1.72it/s]Extractor Predicting: 54it [00:31,  1.69it/s]Extractor Predicting: 55it [00:31,  1.75it/s]Extractor Predicting: 56it [00:32,  1.70it/s]Extractor Predicting: 57it [00:33,  1.69it/s]Extractor Predicting: 58it [00:33,  1.67it/s]Extractor Predicting: 59it [00:34,  1.65it/s]Extractor Predicting: 60it [00:34,  1.69it/s]Extractor Predicting: 61it [00:35,  1.67it/s]Extractor Predicting: 62it [00:36,  1.68it/s]Extractor Predicting: 63it [00:36,  1.70it/s]Extractor Predicting: 64it [00:37,  1.70it/s]Extractor Predicting: 65it [00:37,  1.76it/s]Extractor Predicting: 66it [00:38,  1.72it/s]Extractor Predicting: 67it [00:38,  1.69it/s]Extractor Predicting: 68it [00:39,  1.65it/s]Extractor Predicting: 69it [00:40,  1.67it/s]Extractor Predicting: 70it [00:40,  1.64it/s]Extractor Predicting: 71it [00:41,  1.67it/s]Extractor Predicting: 72it [00:41,  1.67it/s]Extractor Predicting: 73it [00:42,  1.63it/s]Extractor Predicting: 74it [00:43,  1.66it/s]Extractor Predicting: 75it [00:43,  1.67it/s]Extractor Predicting: 76it [00:44,  1.70it/s]Extractor Predicting: 77it [00:44,  1.70it/s]Extractor Predicting: 78it [00:45,  1.73it/s]Extractor Predicting: 79it [00:46,  1.70it/s]Extractor Predicting: 80it [00:46,  1.76it/s]Extractor Predicting: 81it [00:47,  1.77it/s]Extractor Predicting: 82it [00:47,  1.77it/s]Extractor Predicting: 83it [00:48,  1.76it/s]Extractor Predicting: 84it [00:48,  1.75it/s]Extractor Predicting: 85it [00:49,  1.65it/s]Extractor Predicting: 86it [00:50,  1.62it/s]Extractor Predicting: 87it [00:50,  1.61it/s]Extractor Predicting: 88it [00:51,  1.65it/s]Extractor Predicting: 89it [00:52,  1.64it/s]Extractor Predicting: 90it [00:52,  1.57it/s]Extractor Predicting: 91it [00:53,  1.58it/s]Extractor Predicting: 92it [00:53,  1.61it/s]Extractor Predicting: 93it [00:54,  1.64it/s]Extractor Predicting: 94it [00:55,  1.67it/s]Extractor Predicting: 95it [00:55,  1.63it/s]Extractor Predicting: 96it [00:56,  1.64it/s]Extractor Predicting: 97it [00:56,  1.63it/s]Extractor Predicting: 98it [00:57,  1.61it/s]Extractor Predicting: 99it [00:58,  1.63it/s]Extractor Predicting: 100it [00:58,  1.63it/s]Extractor Predicting: 101it [00:59,  1.67it/s]Extractor Predicting: 102it [00:59,  1.68it/s]Extractor Predicting: 103it [01:00,  1.67it/s]Extractor Predicting: 104it [01:01,  1.66it/s]Extractor Predicting: 105it [01:01,  1.69it/s]Extractor Predicting: 106it [01:02,  1.64it/s]Extractor Predicting: 107it [01:03,  1.67it/s]Extractor Predicting: 108it [01:03,  1.68it/s]Extractor Predicting: 109it [01:04,  1.68it/s]Extractor Predicting: 110it [01:04,  1.67it/s]Extractor Predicting: 111it [01:05,  1.66it/s]Extractor Predicting: 112it [01:06,  1.66it/s]Extractor Predicting: 113it [01:06,  1.68it/s]Extractor Predicting: 114it [01:07,  1.71it/s]Extractor Predicting: 115it [01:07,  1.70it/s]Extractor Predicting: 116it [01:08,  1.65it/s]Extractor Predicting: 117it [01:09,  1.64it/s]Extractor Predicting: 118it [01:09,  1.67it/s]Extractor Predicting: 119it [01:10,  1.62it/s]Extractor Predicting: 120it [01:10,  1.63it/s]Extractor Predicting: 121it [01:11,  1.65it/s]Extractor Predicting: 122it [01:11,  1.70it/s]Extractor Predicting: 123it [01:12,  1.71it/s]Extractor Predicting: 124it [01:13,  1.71it/s]Extractor Predicting: 125it [01:13,  1.68it/s]Extractor Predicting: 126it [01:14,  1.76it/s]Extractor Predicting: 127it [01:14,  1.77it/s]Extractor Predicting: 128it [01:15,  1.73it/s]Extractor Predicting: 129it [01:15,  1.78it/s]Extractor Predicting: 130it [01:16,  1.77it/s]Extractor Predicting: 131it [01:17,  1.72it/s]Extractor Predicting: 132it [01:17,  1.75it/s]Extractor Predicting: 133it [01:18,  1.81it/s]Extractor Predicting: 134it [01:18,  1.83it/s]Extractor Predicting: 135it [01:19,  1.76it/s]Extractor Predicting: 136it [01:19,  1.78it/s]Extractor Predicting: 137it [01:20,  1.70it/s]Extractor Predicting: 138it [01:21,  1.78it/s]Extractor Predicting: 139it [01:21,  1.77it/s]Extractor Predicting: 140it [01:22,  1.78it/s]Extractor Predicting: 141it [01:23,  1.53it/s]Extractor Predicting: 142it [01:23,  1.55it/s]Extractor Predicting: 143it [01:24,  1.58it/s]Extractor Predicting: 144it [01:24,  1.61it/s]Extractor Predicting: 145it [01:25,  1.63it/s]Extractor Predicting: 146it [01:26,  1.68it/s]Extractor Predicting: 147it [01:26,  1.76it/s]Extractor Predicting: 148it [01:27,  1.75it/s]Extractor Predicting: 149it [01:27,  1.78it/s]Extractor Predicting: 150it [01:28,  1.83it/s]Extractor Predicting: 151it [01:28,  1.83it/s]Extractor Predicting: 152it [01:29,  1.84it/s]Extractor Predicting: 153it [01:29,  1.84it/s]Extractor Predicting: 154it [01:30,  1.82it/s]Extractor Predicting: 155it [01:30,  1.81it/s]Extractor Predicting: 156it [01:31,  1.84it/s]Extractor Predicting: 157it [01:31,  1.88it/s]Extractor Predicting: 158it [01:32,  1.87it/s]Extractor Predicting: 159it [01:32,  1.94it/s]Extractor Predicting: 160it [01:33,  1.95it/s]Extractor Predicting: 161it [01:34,  1.89it/s]Extractor Predicting: 162it [01:34,  1.85it/s]Extractor Predicting: 163it [01:35,  1.84it/s]Extractor Predicting: 164it [01:35,  1.85it/s]Extractor Predicting: 165it [01:36,  1.89it/s]Extractor Predicting: 166it [01:36,  1.87it/s]Extractor Predicting: 167it [01:37,  1.87it/s]Extractor Predicting: 168it [01:37,  1.84it/s]Extractor Predicting: 169it [01:38,  1.87it/s]Extractor Predicting: 170it [01:38,  1.86it/s]Extractor Predicting: 171it [01:39,  1.91it/s]Extractor Predicting: 172it [01:39,  1.90it/s]Extractor Predicting: 173it [01:40,  1.90it/s]Extractor Predicting: 174it [01:41,  1.73it/s]Extractor Predicting: 175it [01:41,  1.75it/s]Extractor Predicting: 176it [01:42,  1.73it/s]Extractor Predicting: 177it [01:42,  1.71it/s]Extractor Predicting: 178it [01:43,  1.67it/s]Extractor Predicting: 179it [01:44,  1.63it/s]Extractor Predicting: 180it [01:44,  1.67it/s]Extractor Predicting: 181it [01:45,  1.66it/s]Extractor Predicting: 182it [01:45,  1.67it/s]Extractor Predicting: 183it [01:46,  1.70it/s]Extractor Predicting: 184it [01:47,  1.70it/s]Extractor Predicting: 185it [01:47,  1.66it/s]Extractor Predicting: 186it [01:48,  1.70it/s]Extractor Predicting: 187it [01:48,  1.70it/s]Extractor Predicting: 188it [01:49,  1.67it/s]Extractor Predicting: 189it [01:50,  1.70it/s]Extractor Predicting: 190it [01:50,  1.69it/s]Extractor Predicting: 191it [01:51,  1.61it/s]Extractor Predicting: 192it [01:51,  1.62it/s]Extractor Predicting: 193it [01:52,  1.62it/s]Extractor Predicting: 194it [01:53,  1.61it/s]Extractor Predicting: 195it [01:53,  1.64it/s]Extractor Predicting: 196it [01:54,  1.65it/s]Extractor Predicting: 197it [01:54,  1.67it/s]Extractor Predicting: 198it [01:55,  1.68it/s]Extractor Predicting: 199it [01:56,  1.70it/s]Extractor Predicting: 200it [01:56,  1.66it/s]Extractor Predicting: 201it [01:57,  1.65it/s]Extractor Predicting: 202it [01:58,  1.58it/s]Extractor Predicting: 203it [01:58,  1.57it/s]Extractor Predicting: 204it [01:59,  1.57it/s]Extractor Predicting: 205it [01:59,  1.58it/s]Extractor Predicting: 206it [02:00,  1.59it/s]Extractor Predicting: 207it [02:01,  1.56it/s]Extractor Predicting: 208it [02:01,  1.57it/s]Extractor Predicting: 209it [02:02,  1.54it/s]Extractor Predicting: 210it [02:03,  1.55it/s]Extractor Predicting: 211it [02:03,  1.56it/s]Extractor Predicting: 212it [02:04,  1.56it/s]Extractor Predicting: 213it [02:05,  1.58it/s]Extractor Predicting: 214it [02:05,  1.61it/s]Extractor Predicting: 215it [02:06,  1.59it/s]Extractor Predicting: 216it [02:06,  1.59it/s]Extractor Predicting: 217it [02:07,  1.59it/s]Extractor Predicting: 218it [02:08,  1.59it/s]Extractor Predicting: 219it [02:08,  1.59it/s]Extractor Predicting: 220it [02:09,  1.56it/s]Extractor Predicting: 221it [02:10,  1.56it/s]Extractor Predicting: 222it [02:10,  1.55it/s]Extractor Predicting: 223it [02:11,  1.57it/s]Extractor Predicting: 224it [02:11,  1.60it/s]Extractor Predicting: 225it [02:12,  1.62it/s]Extractor Predicting: 226it [02:13,  1.60it/s]Extractor Predicting: 227it [02:13,  1.62it/s]Extractor Predicting: 228it [02:14,  1.59it/s]Extractor Predicting: 229it [02:15,  1.62it/s]Extractor Predicting: 230it [02:15,  1.65it/s]Extractor Predicting: 231it [02:16,  1.71it/s]Extractor Predicting: 232it [02:16,  1.76it/s]Extractor Predicting: 233it [02:17,  1.78it/s]Extractor Predicting: 234it [02:17,  1.68it/s]Extractor Predicting: 235it [02:18,  1.72it/s]Extractor Predicting: 236it [02:19,  1.73it/s]Extractor Predicting: 237it [02:19,  1.73it/s]Extractor Predicting: 238it [02:20,  1.74it/s]Extractor Predicting: 239it [02:20,  1.71it/s]Extractor Predicting: 240it [02:21,  1.72it/s]Extractor Predicting: 241it [02:21,  1.74it/s]Extractor Predicting: 242it [02:22,  1.78it/s]Extractor Predicting: 243it [02:23,  1.78it/s]Extractor Predicting: 244it [02:23,  1.85it/s]Extractor Predicting: 245it [02:24,  1.79it/s]Extractor Predicting: 246it [02:24,  1.72it/s]Extractor Predicting: 247it [02:25,  1.71it/s]Extractor Predicting: 248it [02:25,  1.70it/s]Extractor Predicting: 249it [02:26,  1.74it/s]Extractor Predicting: 250it [02:27,  1.74it/s]Extractor Predicting: 251it [02:27,  1.77it/s]Extractor Predicting: 252it [02:28,  1.78it/s]Extractor Predicting: 253it [02:28,  1.76it/s]Extractor Predicting: 254it [02:29,  1.73it/s]Extractor Predicting: 255it [02:29,  1.74it/s]Extractor Predicting: 256it [02:30,  1.73it/s]Extractor Predicting: 257it [02:31,  1.68it/s]Extractor Predicting: 258it [02:31,  1.66it/s]Extractor Predicting: 259it [02:32,  1.69it/s]Extractor Predicting: 260it [02:32,  1.68it/s]Extractor Predicting: 261it [02:33,  1.46it/s]Extractor Predicting: 262it [02:34,  1.52it/s]Extractor Predicting: 263it [02:35,  1.56it/s]Extractor Predicting: 264it [02:35,  1.58it/s]Extractor Predicting: 265it [02:36,  1.61it/s]Extractor Predicting: 266it [02:36,  1.61it/s]Extractor Predicting: 267it [02:37,  1.61it/s]Extractor Predicting: 268it [02:38,  1.57it/s]Extractor Predicting: 269it [02:38,  1.60it/s]Extractor Predicting: 270it [02:39,  1.62it/s]Extractor Predicting: 271it [02:39,  1.64it/s]Extractor Predicting: 272it [02:40,  1.67it/s]Extractor Predicting: 273it [02:41,  1.62it/s]Extractor Predicting: 274it [02:41,  1.61it/s]Extractor Predicting: 275it [02:42,  1.59it/s]Extractor Predicting: 276it [02:43,  1.60it/s]Extractor Predicting: 277it [02:43,  1.61it/s]Extractor Predicting: 278it [02:44,  1.63it/s]Extractor Predicting: 279it [02:44,  1.62it/s]Extractor Predicting: 280it [02:45,  1.61it/s]Extractor Predicting: 281it [02:46,  1.65it/s]Extractor Predicting: 282it [02:46,  1.64it/s]Extractor Predicting: 283it [02:47,  1.61it/s]Extractor Predicting: 284it [02:47,  1.69it/s]Extractor Predicting: 285it [02:48,  1.66it/s]Extractor Predicting: 286it [02:49,  1.65it/s]Extractor Predicting: 287it [02:49,  1.66it/s]Extractor Predicting: 288it [02:50,  1.65it/s]Extractor Predicting: 289it [02:50,  1.66it/s]Extractor Predicting: 290it [02:51,  1.65it/s]Extractor Predicting: 291it [02:52,  1.55it/s]Extractor Predicting: 292it [02:52,  1.55it/s]Extractor Predicting: 293it [02:53,  1.60it/s]Extractor Predicting: 294it [02:54,  1.58it/s]Extractor Predicting: 295it [02:54,  1.61it/s]Extractor Predicting: 296it [02:55,  1.54it/s]Extractor Predicting: 297it [02:56,  1.58it/s]Extractor Predicting: 298it [02:56,  1.58it/s]Extractor Predicting: 299it [02:57,  1.58it/s]Extractor Predicting: 300it [02:57,  1.61it/s]Extractor Predicting: 301it [02:58,  1.53it/s]Extractor Predicting: 302it [02:59,  1.59it/s]Extractor Predicting: 303it [02:59,  1.57it/s]Extractor Predicting: 304it [03:00,  1.57it/s]Extractor Predicting: 305it [03:01,  1.58it/s]Extractor Predicting: 306it [03:01,  1.60it/s]Extractor Predicting: 307it [03:02,  1.60it/s]Extractor Predicting: 308it [03:02,  1.63it/s]Extractor Predicting: 309it [03:03,  1.65it/s]Extractor Predicting: 310it [03:04,  1.67it/s]Extractor Predicting: 311it [03:04,  1.66it/s]Extractor Predicting: 312it [03:05,  1.68it/s]Extractor Predicting: 313it [03:05,  1.74it/s]Extractor Predicting: 314it [03:06,  1.76it/s]Extractor Predicting: 315it [03:06,  1.79it/s]Extractor Predicting: 316it [03:07,  1.74it/s]Extractor Predicting: 317it [03:08,  1.71it/s]Extractor Predicting: 318it [03:08,  1.67it/s]Extractor Predicting: 319it [03:09,  1.68it/s]Extractor Predicting: 320it [03:09,  1.68it/s]Extractor Predicting: 321it [03:10,  1.68it/s]Extractor Predicting: 322it [03:11,  1.69it/s]Extractor Predicting: 323it [03:11,  1.65it/s]Extractor Predicting: 324it [03:12,  1.65it/s]Extractor Predicting: 325it [03:13,  1.62it/s]Extractor Predicting: 326it [03:13,  1.64it/s]Extractor Predicting: 327it [03:14,  1.69it/s]Extractor Predicting: 328it [03:14,  1.68it/s]Extractor Predicting: 329it [03:15,  1.71it/s]Extractor Predicting: 330it [03:15,  1.71it/s]Extractor Predicting: 331it [03:16,  1.65it/s]Extractor Predicting: 332it [03:17,  1.66it/s]Extractor Predicting: 333it [03:17,  1.70it/s]Extractor Predicting: 334it [03:18,  1.70it/s]Extractor Predicting: 335it [03:18,  1.72it/s]Extractor Predicting: 336it [03:19,  1.69it/s]Extractor Predicting: 337it [03:20,  1.67it/s]Extractor Predicting: 338it [03:20,  1.70it/s]Extractor Predicting: 339it [03:21,  1.75it/s]Extractor Predicting: 340it [03:21,  1.77it/s]Extractor Predicting: 341it [03:22,  1.75it/s]Extractor Predicting: 342it [03:22,  1.71it/s]Extractor Predicting: 343it [03:23,  1.68it/s]Extractor Predicting: 344it [03:24,  1.72it/s]Extractor Predicting: 345it [03:24,  1.68it/s]Extractor Predicting: 346it [03:25,  1.69it/s]Extractor Predicting: 347it [03:25,  1.70it/s]Extractor Predicting: 348it [03:26,  1.71it/s]Extractor Predicting: 349it [03:27,  1.70it/s]Extractor Predicting: 350it [03:27,  1.72it/s]Extractor Predicting: 351it [03:28,  1.75it/s]Extractor Predicting: 352it [03:28,  1.71it/s]Extractor Predicting: 353it [03:29,  1.73it/s]Extractor Predicting: 354it [03:29,  1.76it/s]Extractor Predicting: 355it [03:30,  1.77it/s]Extractor Predicting: 356it [03:31,  1.75it/s]Extractor Predicting: 357it [03:31,  1.75it/s]Extractor Predicting: 358it [03:32,  1.76it/s]Extractor Predicting: 359it [03:33,  1.51it/s]Extractor Predicting: 360it [03:33,  1.55it/s]Extractor Predicting: 361it [03:34,  1.61it/s]Extractor Predicting: 362it [03:34,  1.65it/s]Extractor Predicting: 363it [03:35,  1.70it/s]Extractor Predicting: 364it [03:35,  1.73it/s]Extractor Predicting: 365it [03:36,  1.76it/s]Extractor Predicting: 366it [03:37,  1.70it/s]Extractor Predicting: 367it [03:37,  1.67it/s]Extractor Predicting: 368it [03:38,  1.68it/s]Extractor Predicting: 369it [03:38,  1.69it/s]Extractor Predicting: 370it [03:39,  1.76it/s]Extractor Predicting: 371it [03:39,  1.77it/s]Extractor Predicting: 372it [03:40,  1.77it/s]Extractor Predicting: 373it [03:41,  1.74it/s]Extractor Predicting: 374it [03:41,  1.75it/s]Extractor Predicting: 375it [03:42,  1.75it/s]Extractor Predicting: 376it [03:42,  1.77it/s]Extractor Predicting: 377it [03:43,  1.74it/s]Extractor Predicting: 378it [03:43,  1.79it/s]Extractor Predicting: 379it [03:44,  1.74it/s]Extractor Predicting: 380it [03:45,  1.74it/s]Extractor Predicting: 381it [03:45,  1.76it/s]Extractor Predicting: 382it [03:46,  1.76it/s]Extractor Predicting: 383it [03:46,  1.78it/s]Extractor Predicting: 384it [03:47,  1.82it/s]Extractor Predicting: 385it [03:47,  1.78it/s]Extractor Predicting: 386it [03:48,  1.78it/s]Extractor Predicting: 387it [03:49,  1.77it/s]Extractor Predicting: 388it [03:49,  1.75it/s]Extractor Predicting: 389it [03:50,  1.73it/s]Extractor Predicting: 390it [03:50,  1.73it/s]Extractor Predicting: 391it [03:51,  1.71it/s]Extractor Predicting: 392it [03:51,  1.73it/s]Extractor Predicting: 393it [03:52,  1.76it/s]Extractor Predicting: 394it [03:53,  1.68it/s]Extractor Predicting: 395it [03:53,  1.57it/s]Extractor Predicting: 396it [03:54,  1.58it/s]Extractor Predicting: 397it [03:55,  1.58it/s]Extractor Predicting: 398it [03:55,  1.60it/s]Extractor Predicting: 399it [03:56,  1.59it/s]Extractor Predicting: 400it [03:57,  1.59it/s]Extractor Predicting: 401it [03:57,  1.58it/s]Extractor Predicting: 402it [03:58,  1.57it/s]Extractor Predicting: 403it [03:58,  1.58it/s]Extractor Predicting: 404it [03:59,  1.54it/s]Extractor Predicting: 405it [04:00,  1.50it/s]Extractor Predicting: 406it [04:00,  1.53it/s]Extractor Predicting: 407it [04:01,  1.52it/s]Extractor Predicting: 408it [04:02,  1.57it/s]Extractor Predicting: 409it [04:02,  1.59it/s]Extractor Predicting: 410it [04:03,  1.54it/s]Extractor Predicting: 411it [04:04,  1.56it/s]Extractor Predicting: 412it [04:04,  1.59it/s]Extractor Predicting: 413it [04:05,  1.62it/s]Extractor Predicting: 414it [04:05,  1.65it/s]Extractor Predicting: 415it [04:06,  1.66it/s]Extractor Predicting: 416it [04:07,  1.66it/s]Extractor Predicting: 417it [04:07,  1.67it/s]Extractor Predicting: 418it [04:08,  1.66it/s]Extractor Predicting: 419it [04:09,  1.59it/s]Extractor Predicting: 420it [04:09,  1.57it/s]Extractor Predicting: 421it [04:10,  1.68it/s]Extractor Predicting: 421it [04:10,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:20,281 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:20,488 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:20,488 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:20,488 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:20,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:06:21,125 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:06:21,126 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:06:21,728 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:06:22,796 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:06:22,796 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:24,902 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:24,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:24,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:24,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:24,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:06:25,620 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:06:25,621 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:06:26,214 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:06:26,387 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:06:26,388 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  2.09it/s]Extractor Predicting: 9it [00:05,  1.71it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:06:33,640 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:06:33,641 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:06:33,713 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:06:33,714 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:06:33,750 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:06:46,775 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:06:46,806 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:06:46,983 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:06:46,984 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:06:47,074 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:47,128 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:47,128 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:47,128 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:47,128 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:47,128 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:47,128 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:06:47,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:48,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:48,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:49,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:49,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:50,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:51,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:51,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:52,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:53,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:54,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:54,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:55,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:56,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:56,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:57,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:57,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:58,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:59,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:59,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:00,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:01,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:02,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:03,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:03,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:04,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:17<05:32, 17.50s/it][WARNING|generation_utils.py:914] 2023-08-28 14:07:05,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:05,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:06,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:07,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:07,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:08,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:09,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:09,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:10,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:10,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:11,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:12,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:13,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:13,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:14,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:14,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:15,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:16,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:17,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:17,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:18,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:18,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:19,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:20,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:20,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:21,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:34<05:08, 17.12s/it][WARNING|generation_utils.py:914] 2023-08-28 14:07:21,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:22,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:23,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:24,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:24,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:25,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:25,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:26,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:27,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:27,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:28,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:29,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:29,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:30,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:31,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:31,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:32,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:33,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:33,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:34,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:34,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:35,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:36,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:37,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:37,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:38,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:51<04:50, 17.10s/it][WARNING|generation_utils.py:914] 2023-08-28 14:07:39,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:39,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:40,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:40,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:41,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:42,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:43,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:43,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:44,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:45,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:45,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:46,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:46,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:47,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:48,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:48,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:49,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:50,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:51,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:51,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:52,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:52,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:53,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:54,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:55,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:08<04:30, 16.92s/it][WARNING|generation_utils.py:914] 2023-08-28 14:07:55,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:56,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:56,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:57,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:58,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:58,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:59,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:07:59,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:00,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:01,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:01,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:02,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:02,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:03,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:04,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:04,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:05,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:06,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:06,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:07,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:07,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:08,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:08,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:09,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:10,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:11,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:11,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:24<04:12, 16.81s/it][WARNING|generation_utils.py:914] 2023-08-28 14:08:12,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:12,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:13,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:14,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:14,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:15,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:16,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:16,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:17,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:18,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:18,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:19,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:20,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:20,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:21,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:22,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:22,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:23,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:24,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:24,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:25,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:26,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:26,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:39<03:47, 16.24s/it][WARNING|generation_utils.py:914] 2023-08-28 14:08:27,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:28,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:28,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:29,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:30,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:30,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:31,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:32,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:32,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:33,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:34,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:34,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:35,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:36,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:36,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:37,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:38,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:38,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:39,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:40,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:41,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:41,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:42,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:43,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:56<03:34, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-28 14:08:44,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:45,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:45,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:46,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:47,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:48,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:48,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:49,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:50,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:50,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:51,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:52,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:52,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:53,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:54,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:54,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:55,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:56,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:57,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:58,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:58,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:08:59,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:00,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:01,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:01,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:14<03:24, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 14:09:02,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:03,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:03,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:04,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:05,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:05,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:06,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:07,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:07,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:08,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:09,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:09,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:10,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:11,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:12,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:12,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:13,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:14,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:15,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:15,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:16,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:17,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:17,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:31<03:03, 16.71s/it][WARNING|generation_utils.py:914] 2023-08-28 14:09:18,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:19,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:20,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:20,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:21,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:22,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:22,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:23,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:24,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:25,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:25,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:26,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:27,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:28,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:28,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:29,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:30,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:31,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:31,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:32,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:32,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:33,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:34,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:34,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:47<02:47, 16.79s/it][WARNING|generation_utils.py:914] 2023-08-28 14:09:35,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:36,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:37,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:38,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:38,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:39,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:40,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:41,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:42,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:42,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:43,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:44,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:44,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:45,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:46,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:47,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:47,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:48,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:49,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:49,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:50,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:51,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:51,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:05<02:32, 16.89s/it][WARNING|generation_utils.py:914] 2023-08-28 14:09:52,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:53,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:53,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:54,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:55,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:56,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:56,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:57,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:58,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:58,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:09:59,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:00,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:00,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:01,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:02,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:02,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:03,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:04,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:04,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:05,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:06,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:07,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:07,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:20<02:12, 16.57s/it][WARNING|generation_utils.py:914] 2023-08-28 14:10:08,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:09,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:10,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:10,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:11,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:12,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:13,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:13,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:14,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:15,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:15,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:16,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:17,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:18,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:18,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:19,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:20,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:20,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:21,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:22,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:22,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:23,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:23,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:24,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:25,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:38<01:58, 16.86s/it][WARNING|generation_utils.py:914] 2023-08-28 14:10:26,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:26,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:27,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:28,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:28,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:29,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:30,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:30,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:31,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:31,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:32,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:32,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:33,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:34,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:34,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:35,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:36,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:36,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:37,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:38,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:38,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:39,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:40,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:53<01:37, 16.18s/it][WARNING|generation_utils.py:914] 2023-08-28 14:10:40,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:41,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:41,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:42,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:43,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:43,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:44,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:45,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:45,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:46,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:47,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:47,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:48,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:49,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:49,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:50,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:50,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:51,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:52,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:52,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:53,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:54,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:54,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:55,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:56,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:56,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:09<01:21, 16.36s/it][WARNING|generation_utils.py:914] 2023-08-28 14:10:57,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:57,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:58,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:59,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:10:59,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:00,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:01,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:01,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:02,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:03,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:03,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:04,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:05,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:05,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:06,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:07,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:07,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:08,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:09,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:09,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:10,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:11,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:11,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:12,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:13,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:26<01:05, 16.39s/it][WARNING|generation_utils.py:914] 2023-08-28 14:11:13,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:14,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:15,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:15,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:16,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:16,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:18,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:18,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:19,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:20,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:20,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:21,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:21,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:22,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:23,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:24,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:24,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:25,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:26,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:26,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:27,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:28,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:28,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:41<00:48, 16.06s/it][WARNING|generation_utils.py:914] 2023-08-28 14:11:29,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:29,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:30,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:31,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:31,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:32,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:33,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:33,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:34,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:34,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:35,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:36,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:36,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:37,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:38,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:38,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:39,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:40,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:40,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:41,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:42,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:42,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:43,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:43,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:44,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:45,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:45,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:46,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:46,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:47,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:00<00:33, 16.88s/it][WARNING|generation_utils.py:914] 2023-08-28 14:11:47,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:48,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:49,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:50,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:51,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:51,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:52,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:52,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:53,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:54,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:54,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:55,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:56,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:56,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:57,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:58,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:58,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:59,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:00,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:00,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:01,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:02,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:02,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:15<00:16, 16.43s/it][WARNING|generation_utils.py:914] 2023-08-28 14:12:03,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:03,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:04,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:05,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:05,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:06,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:07,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:07,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:08,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:09,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:10,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:10,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:11,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:11,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:12,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:13,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:13,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:14,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:15,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:15,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:16,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:17,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:18,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:31<00:00, 16.13s/it]Generating: 100%|| 20/20 [05:31<00:00, 16.56s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:27,449 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:27,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:27,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:27,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:27,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:12:28,216 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:12:28,217 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:12:28,738 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:12:29,896 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:12:29,896 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:31,403 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:31,438 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:31,438 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:31,438 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:12:31,438 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:12:32,004 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:12:32,005 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:12:32,361 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:12:32,644 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:12:32,644 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 441, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 512, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7379807692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 234, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 625, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7512019230769231, 'errors': {'', "('Hans', 'head of government', '', 'He was succeeded by his brother , Dr . Hans .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : military branch .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('British ships', 'military branch', '', 'The Battle of Bataillon was the battle of the Battle of the Bastille , where British ships sank at least 1,400 French ships .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 181, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 274, 'raw': 384}
{'target': 600, 'success': 295, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 339, 'raw': 480}
{'target': 600, 'success': 361, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 428, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 602, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.6967592592592593, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('European Championships', 'winner', '', 'The previous year , he won the European Championships , and finished runner in 5th place in the category of medals .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : crosses . Context : Later in the year ( 11411231 ) he married Brigadier John B. Stoughton , sister of King James VI , the King of England . Head Entity : Robert Stoughton , Tail Entity : John B . Stoughton .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : crosses .', 'success_rate': 0.77, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('FreeBSD', 'operating system', '', 'The operating system is based on FreeBSD 2.1 .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 519, 'raw': 704}
{'target': 600, 'success': 544, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : participant .', 'success_rate': 0.7415865384615384, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : position played on team / speciality . Context : Later in 2008 , he played in the United States national team squad for the 2002 FIFA World Cup and 2010 FIFA World Cup games . Head Entity : Walter , Tail Entity : United States national team .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 99, 'raw': 160}
{'target': 600, 'success': 119, 'raw': 192}
{'target': 600, 'success': 141, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 183, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 224, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 266, 'raw': 416}
{'target': 600, 'success': 287, 'raw': 448}
{'target': 600, 'success': 312, 'raw': 480}
{'target': 600, 'success': 330, 'raw': 512}
{'target': 600, 'success': 348, 'raw': 544}
{'target': 600, 'success': 367, 'raw': 576}
{'target': 600, 'success': 386, 'raw': 608}
{'target': 600, 'success': 404, 'raw': 640}
{'target': 600, 'success': 421, 'raw': 672}
{'target': 600, 'success': 448, 'raw': 704}
{'target': 600, 'success': 465, 'raw': 736}
{'target': 600, 'success': 482, 'raw': 768}
{'target': 600, 'success': 499, 'raw': 800}
{'target': 600, 'success': 525, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 574, 'raw': 896}
{'target': 600, 'success': 594, 'raw': 928}
{'target': 600, 'success': 611, 'raw': 960}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.6364583333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : publisher . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : season 2 , Tail Entity : the Walking Dead .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8288043478260869, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 17454
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17554, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.42it/s]Extractor Estimating: 2it [00:01,  1.44it/s]Extractor Estimating: 3it [00:02,  1.47it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.43it/s]Extractor Estimating: 6it [00:04,  1.43it/s]Extractor Estimating: 7it [00:04,  1.50it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:07,  1.57it/s]Extractor Estimating: 12it [00:07,  1.58it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:09,  1.60it/s]Extractor Estimating: 15it [00:09,  1.54it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:11,  1.56it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:12,  1.64it/s]Extractor Estimating: 21it [00:13,  1.68it/s]Extractor Estimating: 22it [00:14,  1.61it/s]Extractor Estimating: 23it [00:14,  1.60it/s]Extractor Estimating: 24it [00:15,  1.64it/s]Extractor Estimating: 25it [00:15,  1.66it/s]Extractor Estimating: 26it [00:16,  1.57it/s]Extractor Estimating: 27it [00:17,  1.55it/s]Extractor Estimating: 28it [00:18,  1.47it/s]Extractor Estimating: 29it [00:18,  1.38it/s]Extractor Estimating: 30it [00:19,  1.39it/s]Extractor Estimating: 31it [00:20,  1.40it/s]Extractor Estimating: 32it [00:21,  1.41it/s]Extractor Estimating: 33it [00:21,  1.46it/s]Extractor Estimating: 34it [00:22,  1.42it/s]Extractor Estimating: 35it [00:23,  1.44it/s]Extractor Estimating: 36it [00:23,  1.45it/s]Extractor Estimating: 37it [00:24,  1.46it/s]Extractor Estimating: 38it [00:25,  1.50it/s]Extractor Estimating: 39it [00:25,  1.52it/s]Extractor Estimating: 40it [00:26,  1.51it/s]Extractor Estimating: 41it [00:27,  1.46it/s]Extractor Estimating: 42it [00:27,  1.49it/s]Extractor Estimating: 43it [00:28,  1.46it/s]Extractor Estimating: 44it [00:29,  1.50it/s]Extractor Estimating: 45it [00:29,  1.50it/s]Extractor Estimating: 46it [00:30,  1.44it/s]Extractor Estimating: 47it [00:31,  1.47it/s]Extractor Estimating: 48it [00:31,  1.47it/s]Extractor Estimating: 49it [00:32,  1.52it/s]Extractor Estimating: 50it [00:33,  1.54it/s]Extractor Estimating: 51it [00:33,  1.50it/s]Extractor Estimating: 52it [00:34,  1.51it/s]Extractor Estimating: 53it [00:35,  1.49it/s]Extractor Estimating: 54it [00:35,  1.54it/s]Extractor Estimating: 55it [00:36,  1.62it/s]Extractor Estimating: 56it [00:36,  1.64it/s]Extractor Estimating: 57it [00:37,  1.66it/s]Extractor Estimating: 58it [00:38,  1.64it/s]Extractor Estimating: 59it [00:38,  1.60it/s]Extractor Estimating: 60it [00:39,  1.65it/s]Extractor Estimating: 61it [00:40,  1.60it/s]Extractor Estimating: 62it [00:40,  1.64it/s]Extractor Estimating: 63it [00:41,  1.59it/s]Extractor Estimating: 64it [00:41,  1.58it/s]Extractor Estimating: 65it [00:42,  1.60it/s]Extractor Estimating: 66it [00:43,  1.61it/s]Extractor Estimating: 67it [00:43,  1.57it/s]Extractor Estimating: 68it [00:44,  1.61it/s]Extractor Estimating: 69it [00:45,  1.57it/s]Extractor Estimating: 70it [00:45,  1.63it/s]Extractor Estimating: 71it [00:46,  1.63it/s]Extractor Estimating: 72it [00:46,  1.64it/s]Extractor Estimating: 73it [00:47,  1.52it/s]Extractor Estimating: 74it [00:48,  1.54it/s]Extractor Estimating: 75it [00:48,  1.55it/s]Extractor Estimating: 76it [00:49,  1.57it/s]Extractor Estimating: 77it [00:50,  1.54it/s]Extractor Estimating: 78it [00:50,  1.52it/s]Extractor Estimating: 79it [00:51,  1.52it/s]Extractor Estimating: 80it [00:52,  1.51it/s]Extractor Estimating: 81it [00:52,  1.51it/s]Extractor Estimating: 82it [00:53,  1.55it/s]Extractor Estimating: 83it [00:54,  1.46it/s]Extractor Estimating: 84it [00:55,  1.34it/s]Extractor Estimating: 85it [00:55,  1.40it/s]Extractor Estimating: 86it [00:56,  1.46it/s]Extractor Estimating: 87it [00:57,  1.47it/s]Extractor Estimating: 88it [00:57,  1.47it/s]Extractor Estimating: 89it [00:58,  1.55it/s]Extractor Estimating: 90it [00:58,  1.54it/s]Extractor Estimating: 91it [00:59,  1.43it/s]Extractor Estimating: 92it [01:00,  1.47it/s]Extractor Estimating: 93it [01:01,  1.45it/s]Extractor Estimating: 94it [01:01,  1.53it/s]Extractor Estimating: 95it [01:02,  1.54it/s]Extractor Estimating: 96it [01:02,  1.51it/s]Extractor Estimating: 97it [01:03,  1.50it/s]Extractor Estimating: 98it [01:04,  1.50it/s]Extractor Estimating: 99it [01:04,  1.51it/s]Extractor Estimating: 100it [01:05,  1.52it/s]Extractor Estimating: 101it [01:06,  1.54it/s]Extractor Estimating: 102it [01:06,  1.55it/s]Extractor Estimating: 103it [01:07,  1.48it/s]Extractor Estimating: 104it [01:08,  1.51it/s]Extractor Estimating: 105it [01:08,  1.56it/s]Extractor Estimating: 106it [01:09,  1.60it/s]Extractor Estimating: 107it [01:10,  1.62it/s]Extractor Estimating: 108it [01:10,  1.57it/s]Extractor Estimating: 109it [01:11,  1.65it/s]Extractor Estimating: 110it [01:11,  1.60it/s]Extractor Estimating: 111it [01:12,  1.62it/s]Extractor Estimating: 112it [01:13,  1.60it/s]Extractor Estimating: 113it [01:13,  1.59it/s]Extractor Estimating: 114it [01:14,  1.58it/s]Extractor Estimating: 115it [01:15,  1.62it/s]Extractor Estimating: 116it [01:15,  1.62it/s]Extractor Estimating: 117it [01:16,  1.57it/s]Extractor Estimating: 118it [01:17,  1.54it/s]Extractor Estimating: 119it [01:17,  1.52it/s]Extractor Estimating: 120it [01:18,  1.58it/s]Extractor Estimating: 121it [01:18,  1.57it/s]Extractor Estimating: 122it [01:19,  1.57it/s]Extractor Estimating: 123it [01:20,  1.57it/s]Extractor Estimating: 124it [01:20,  1.58it/s]Extractor Estimating: 125it [01:21,  1.62it/s]Extractor Estimating: 126it [01:21,  1.62it/s]Extractor Estimating: 127it [01:22,  1.59it/s]Extractor Estimating: 128it [01:23,  1.59it/s]Extractor Estimating: 129it [01:24,  1.52it/s]Extractor Estimating: 130it [01:24,  1.39it/s]Extractor Estimating: 131it [01:25,  1.45it/s]Extractor Estimating: 132it [01:26,  1.43it/s]Extractor Estimating: 133it [01:26,  1.41it/s]Extractor Estimating: 134it [01:27,  1.40it/s]Extractor Estimating: 135it [01:28,  1.42it/s]Extractor Estimating: 136it [01:29,  1.46it/s]Extractor Estimating: 137it [01:29,  1.48it/s]Extractor Estimating: 138it [01:30,  1.52it/s]Extractor Estimating: 139it [01:30,  1.49it/s]Extractor Estimating: 140it [01:31,  1.46it/s]Extractor Estimating: 141it [01:32,  1.50it/s]Extractor Estimating: 142it [01:32,  1.50it/s]Extractor Estimating: 143it [01:33,  1.46it/s]Extractor Estimating: 144it [01:34,  1.44it/s]Extractor Estimating: 145it [01:35,  1.51it/s]Extractor Estimating: 146it [01:35,  1.50it/s]Extractor Estimating: 147it [01:36,  1.45it/s]Extractor Estimating: 148it [01:37,  1.47it/s]Extractor Estimating: 149it [01:37,  1.47it/s]Extractor Estimating: 150it [01:38,  1.49it/s]Extractor Estimating: 151it [01:39,  1.54it/s]Extractor Estimating: 152it [01:39,  1.54it/s]Extractor Estimating: 153it [01:40,  1.59it/s]Extractor Estimating: 154it [01:40,  1.51it/s]Extractor Estimating: 155it [01:41,  1.57it/s]Extractor Estimating: 156it [01:42,  1.55it/s]Extractor Estimating: 157it [01:42,  1.54it/s]Extractor Estimating: 158it [01:43,  1.62it/s]Extractor Estimating: 159it [01:44,  1.54it/s]Extractor Estimating: 160it [01:44,  1.50it/s]Extractor Estimating: 161it [01:45,  1.54it/s]Extractor Estimating: 162it [01:45,  1.63it/s]Extractor Estimating: 163it [01:46,  1.68it/s]Extractor Estimating: 164it [01:47,  1.51it/s]Extractor Estimating: 165it [01:48,  1.50it/s]Extractor Estimating: 166it [01:48,  1.50it/s]Extractor Estimating: 167it [01:49,  1.54it/s]Extractor Estimating: 168it [01:49,  1.57it/s]Extractor Estimating: 169it [01:50,  1.64it/s]Extractor Estimating: 170it [01:51,  1.55it/s]Extractor Estimating: 171it [01:51,  1.50it/s]Extractor Estimating: 172it [01:52,  1.54it/s]Extractor Estimating: 173it [01:53,  1.58it/s]Extractor Estimating: 174it [01:53,  1.57it/s]Extractor Estimating: 175it [01:54,  1.58it/s]Extractor Estimating: 176it [01:55,  1.53it/s]Extractor Estimating: 177it [01:55,  1.52it/s]Extractor Estimating: 178it [01:56,  1.46it/s]Extractor Estimating: 179it [01:57,  1.49it/s]Extractor Estimating: 180it [01:57,  1.54it/s]Extractor Estimating: 181it [01:58,  1.54it/s]Extractor Estimating: 182it [01:59,  1.56it/s]Extractor Estimating: 183it [01:59,  1.52it/s]Extractor Estimating: 184it [02:00,  1.47it/s]Extractor Estimating: 185it [02:01,  1.43it/s]Extractor Estimating: 186it [02:01,  1.38it/s]Extractor Estimating: 187it [02:02,  1.40it/s]Extractor Estimating: 188it [02:03,  1.47it/s]Extractor Estimating: 189it [02:04,  1.42it/s]Extractor Estimating: 190it [02:04,  1.45it/s]Extractor Estimating: 191it [02:05,  1.49it/s]Extractor Estimating: 192it [02:06,  1.45it/s]Extractor Estimating: 193it [02:06,  1.39it/s]Extractor Estimating: 194it [02:07,  1.47it/s]Extractor Estimating: 195it [02:08,  1.35it/s]Extractor Estimating: 196it [02:09,  1.32it/s]Extractor Estimating: 197it [02:09,  1.39it/s]Extractor Estimating: 198it [02:10,  1.43it/s]Extractor Estimating: 199it [02:11,  1.46it/s]Extractor Estimating: 200it [02:11,  1.51it/s]Extractor Estimating: 201it [02:12,  1.49it/s]Extractor Estimating: 202it [02:13,  1.48it/s]Extractor Estimating: 203it [02:13,  1.44it/s]Extractor Estimating: 204it [02:14,  1.49it/s]Extractor Estimating: 205it [02:15,  1.49it/s]Extractor Estimating: 206it [02:15,  1.48it/s]Extractor Estimating: 207it [02:16,  1.52it/s]Extractor Estimating: 208it [02:16,  1.55it/s]Extractor Estimating: 209it [02:17,  1.50it/s]Extractor Estimating: 210it [02:18,  1.45it/s]Extractor Estimating: 211it [02:19,  1.52it/s]Extractor Estimating: 212it [02:19,  1.50it/s]Extractor Estimating: 213it [02:20,  1.47it/s]Extractor Estimating: 214it [02:21,  1.43it/s]Extractor Estimating: 215it [02:21,  1.46it/s]Extractor Estimating: 216it [02:22,  1.45it/s]Extractor Estimating: 217it [02:23,  1.45it/s]Extractor Estimating: 218it [02:23,  1.45it/s]Extractor Estimating: 219it [02:24,  1.41it/s]Extractor Estimating: 220it [02:25,  1.45it/s]Extractor Estimating: 221it [02:25,  1.46it/s]Extractor Estimating: 222it [02:26,  1.42it/s]Extractor Estimating: 223it [02:27,  1.43it/s]Extractor Estimating: 224it [02:28,  1.47it/s]Extractor Estimating: 225it [02:28,  1.49it/s]Extractor Estimating: 226it [02:29,  1.46it/s]Extractor Estimating: 227it [02:30,  1.42it/s]Extractor Estimating: 228it [02:30,  1.45it/s]Extractor Estimating: 229it [02:31,  1.46it/s]Extractor Estimating: 230it [02:32,  1.45it/s]Extractor Estimating: 231it [02:32,  1.45it/s]Extractor Estimating: 232it [02:33,  1.43it/s]Extractor Estimating: 233it [02:34,  1.49it/s]Extractor Estimating: 234it [02:35,  1.36it/s]Extractor Estimating: 235it [02:35,  1.38it/s]Extractor Estimating: 236it [02:36,  1.41it/s]Extractor Estimating: 237it [02:37,  1.37it/s]Extractor Estimating: 238it [02:37,  1.40it/s]Extractor Estimating: 239it [02:38,  1.47it/s]Extractor Estimating: 240it [02:39,  1.43it/s]Extractor Estimating: 241it [02:39,  1.48it/s]Extractor Estimating: 242it [02:40,  1.49it/s]Extractor Estimating: 243it [02:41,  1.38it/s]Extractor Estimating: 244it [02:42,  1.42it/s]Extractor Estimating: 245it [02:42,  1.45it/s]Extractor Estimating: 246it [02:43,  1.46it/s]Extractor Estimating: 247it [02:44,  1.43it/s]Extractor Estimating: 248it [02:44,  1.46it/s]Extractor Estimating: 249it [02:45,  1.46it/s]Extractor Estimating: 250it [02:46,  1.44it/s]Extractor Estimating: 251it [02:46,  1.45it/s]Extractor Estimating: 252it [02:47,  1.50it/s]Extractor Estimating: 253it [02:48,  1.48it/s]Extractor Estimating: 254it [02:48,  1.47it/s]Extractor Estimating: 255it [02:49,  1.48it/s]Extractor Estimating: 256it [02:50,  1.40it/s]Extractor Estimating: 257it [02:51,  1.39it/s]Extractor Estimating: 258it [02:51,  1.40it/s]Extractor Estimating: 259it [02:52,  1.40it/s]Extractor Estimating: 260it [02:53,  1.38it/s]Extractor Estimating: 261it [02:53,  1.36it/s]Extractor Estimating: 262it [02:54,  1.39it/s]Extractor Estimating: 263it [02:55,  1.37it/s]Extractor Estimating: 264it [02:56,  1.42it/s]Extractor Estimating: 265it [02:56,  1.43it/s]Extractor Estimating: 266it [02:57,  1.45it/s]Extractor Estimating: 267it [02:58,  1.47it/s]Extractor Estimating: 268it [02:58,  1.46it/s]Extractor Estimating: 269it [02:59,  1.48it/s]Extractor Estimating: 270it [03:00,  1.42it/s]Extractor Estimating: 271it [03:00,  1.42it/s]Extractor Estimating: 272it [03:01,  1.42it/s]Extractor Estimating: 273it [03:02,  1.48it/s]Extractor Estimating: 274it [03:02,  1.45it/s]Extractor Estimating: 275it [03:03,  1.45it/s]Extractor Estimating: 276it [03:04,  1.48it/s]Extractor Estimating: 277it [03:04,  1.51it/s]Extractor Estimating: 278it [03:05,  1.53it/s]Extractor Estimating: 279it [03:06,  1.54it/s]Extractor Estimating: 280it [03:06,  1.53it/s]Extractor Estimating: 281it [03:07,  1.55it/s]Extractor Estimating: 282it [03:08,  1.56it/s]Extractor Estimating: 283it [03:08,  1.59it/s]Extractor Estimating: 284it [03:09,  1.64it/s]Extractor Estimating: 285it [03:09,  1.55it/s]Extractor Estimating: 286it [03:10,  1.56it/s]Extractor Estimating: 287it [03:11,  1.53it/s]Extractor Estimating: 288it [03:11,  1.55it/s]Extractor Estimating: 289it [03:12,  1.56it/s]Extractor Estimating: 290it [03:13,  1.60it/s]Extractor Estimating: 291it [03:13,  1.64it/s]Extractor Estimating: 292it [03:14,  1.63it/s]Extractor Estimating: 293it [03:14,  1.61it/s]Extractor Estimating: 294it [03:15,  1.58it/s]Extractor Estimating: 295it [03:16,  1.60it/s]Extractor Estimating: 296it [03:16,  1.61it/s]Extractor Estimating: 297it [03:17,  1.58it/s]Extractor Estimating: 298it [03:18,  1.61it/s]Extractor Estimating: 299it [03:18,  1.62it/s]Extractor Estimating: 300it [03:19,  1.60it/s]Extractor Estimating: 301it [03:19,  1.62it/s]Extractor Estimating: 302it [03:20,  1.51it/s]Extractor Estimating: 303it [03:21,  1.52it/s]Extractor Estimating: 304it [03:21,  1.54it/s]Extractor Estimating: 305it [03:22,  1.56it/s]Extractor Estimating: 306it [03:23,  1.53it/s]Extractor Estimating: 307it [03:24,  1.47it/s]Extractor Estimating: 308it [03:24,  1.47it/s]Extractor Estimating: 309it [03:25,  1.52it/s]Extractor Estimating: 310it [03:25,  1.56it/s]Extractor Estimating: 311it [03:26,  1.59it/s]Extractor Estimating: 312it [03:27,  1.55it/s]Extractor Estimating: 313it [03:27,  1.58it/s]Extractor Estimating: 314it [03:28,  1.59it/s]Extractor Estimating: 315it [03:29,  1.43it/s]Extractor Estimating: 316it [03:29,  1.50it/s]Extractor Estimating: 317it [03:30,  1.47it/s]Extractor Estimating: 318it [03:31,  1.48it/s]Extractor Estimating: 319it [03:31,  1.49it/s]Extractor Estimating: 320it [03:32,  1.58it/s]Extractor Estimating: 321it [03:33,  1.59it/s]Extractor Estimating: 322it [03:33,  1.56it/s]Extractor Estimating: 323it [03:34,  1.59it/s]Extractor Estimating: 324it [03:35,  1.55it/s]Extractor Estimating: 325it [03:35,  1.54it/s]Extractor Estimating: 326it [03:36,  1.42it/s]Extractor Estimating: 327it [03:37,  1.48it/s]Extractor Estimating: 328it [03:37,  1.57it/s]Extractor Estimating: 329it [03:38,  1.52it/s]Extractor Estimating: 330it [03:38,  1.56it/s]Extractor Estimating: 331it [03:39,  1.63it/s]Extractor Estimating: 332it [03:40,  1.61it/s]Extractor Estimating: 333it [03:40,  1.65it/s]Extractor Estimating: 334it [03:41,  1.63it/s]Extractor Estimating: 335it [03:41,  1.65it/s]Extractor Estimating: 336it [03:42,  1.68it/s]Extractor Estimating: 337it [03:43,  1.67it/s]Extractor Estimating: 338it [03:43,  1.65it/s]Extractor Estimating: 339it [03:44,  1.52it/s]Extractor Estimating: 340it [03:45,  1.55it/s]Extractor Estimating: 341it [03:45,  1.57it/s]Extractor Estimating: 342it [03:46,  1.65it/s]Extractor Estimating: 343it [03:46,  1.61it/s]Extractor Estimating: 344it [03:47,  1.54it/s]Extractor Estimating: 345it [03:48,  1.56it/s]Extractor Estimating: 346it [03:48,  1.55it/s]Extractor Estimating: 347it [03:49,  1.58it/s]Extractor Estimating: 348it [03:50,  1.59it/s]Extractor Estimating: 349it [03:50,  1.52it/s]Extractor Estimating: 350it [03:51,  1.57it/s]Extractor Estimating: 351it [03:52,  1.58it/s]Extractor Estimating: 352it [03:52,  1.57it/s]Extractor Estimating: 353it [03:53,  1.57it/s]Extractor Estimating: 354it [03:54,  1.55it/s]Extractor Estimating: 355it [03:54,  1.56it/s]Extractor Estimating: 356it [03:55,  1.59it/s]Extractor Estimating: 357it [03:55,  1.62it/s]Extractor Estimating: 358it [03:56,  1.59it/s]Extractor Estimating: 359it [03:57,  1.58it/s]Extractor Estimating: 360it [03:57,  1.58it/s]Extractor Estimating: 361it [03:58,  1.61it/s]Extractor Estimating: 362it [03:58,  1.68it/s]Extractor Estimating: 363it [03:59,  1.71it/s]Extractor Estimating: 364it [04:00,  1.75it/s]Extractor Estimating: 365it [04:00,  1.69it/s]Extractor Estimating: 366it [04:01,  1.63it/s]Extractor Estimating: 367it [04:01,  1.64it/s]Extractor Estimating: 368it [04:02,  1.58it/s]Extractor Estimating: 369it [04:03,  1.61it/s]Extractor Estimating: 370it [04:03,  1.60it/s]Extractor Estimating: 371it [04:04,  1.65it/s]Extractor Estimating: 372it [04:05,  1.65it/s]Extractor Estimating: 373it [04:05,  1.67it/s]Extractor Estimating: 374it [04:06,  1.68it/s]Extractor Estimating: 375it [04:06,  1.67it/s]Extractor Estimating: 376it [04:07,  1.66it/s]Extractor Estimating: 377it [04:08,  1.67it/s]Extractor Estimating: 378it [04:08,  1.72it/s]Extractor Estimating: 379it [04:09,  1.72it/s]Extractor Estimating: 380it [04:09,  1.72it/s]Extractor Estimating: 381it [04:10,  1.68it/s]Extractor Estimating: 382it [04:10,  1.68it/s]Extractor Estimating: 383it [04:11,  1.66it/s]Extractor Estimating: 384it [04:12,  1.61it/s]Extractor Estimating: 385it [04:12,  1.62it/s]Extractor Estimating: 386it [04:13,  1.61it/s]Extractor Estimating: 387it [04:14,  1.66it/s]Extractor Estimating: 388it [04:14,  1.68it/s]Extractor Estimating: 389it [04:15,  1.70it/s]Extractor Estimating: 390it [04:15,  1.62it/s]Extractor Estimating: 391it [04:16,  1.65it/s]Extractor Estimating: 392it [04:17,  1.65it/s]Extractor Estimating: 393it [04:17,  1.67it/s]Extractor Estimating: 394it [04:18,  1.66it/s]Extractor Estimating: 395it [04:18,  1.67it/s]Extractor Estimating: 396it [04:19,  1.66it/s]Extractor Estimating: 397it [04:20,  1.64it/s]Extractor Estimating: 398it [04:20,  1.63it/s]Extractor Estimating: 399it [04:21,  1.61it/s]Extractor Estimating: 400it [04:21,  1.60it/s]Extractor Estimating: 401it [04:22,  1.43it/s]Extractor Estimating: 402it [04:23,  1.42it/s]Extractor Estimating: 403it [04:24,  1.48it/s]Extractor Estimating: 404it [04:24,  1.48it/s]Extractor Estimating: 405it [04:25,  1.51it/s]Extractor Estimating: 406it [04:26,  1.52it/s]Extractor Estimating: 407it [04:26,  1.53it/s]Extractor Estimating: 408it [04:27,  1.52it/s]Extractor Estimating: 409it [04:27,  1.58it/s]Extractor Estimating: 410it [04:28,  1.57it/s]Extractor Estimating: 411it [04:29,  1.58it/s]Extractor Estimating: 412it [04:29,  1.53it/s]Extractor Estimating: 413it [04:30,  1.64it/s]Extractor Estimating: 414it [04:31,  1.58it/s]Extractor Estimating: 415it [04:31,  1.55it/s]Extractor Estimating: 416it [04:32,  1.53it/s]Extractor Estimating: 417it [04:33,  1.54it/s]Extractor Estimating: 418it [04:33,  1.60it/s]Extractor Estimating: 419it [04:34,  1.57it/s]Extractor Estimating: 420it [04:35,  1.56it/s]Extractor Estimating: 421it [04:35,  1.59it/s]Extractor Estimating: 422it [04:36,  1.58it/s]Extractor Estimating: 423it [04:36,  1.56it/s]Extractor Estimating: 424it [04:37,  1.52it/s]Extractor Estimating: 425it [04:38,  1.53it/s]Extractor Estimating: 426it [04:38,  1.56it/s]Extractor Estimating: 427it [04:39,  1.51it/s]Extractor Estimating: 428it [04:40,  1.53it/s]Extractor Estimating: 429it [04:40,  1.55it/s]Extractor Estimating: 430it [04:41,  1.53it/s]Extractor Estimating: 431it [04:42,  1.56it/s]Extractor Estimating: 432it [04:42,  1.61it/s]Extractor Estimating: 433it [04:43,  1.63it/s]Extractor Estimating: 434it [04:43,  1.61it/s]Extractor Estimating: 435it [04:44,  1.63it/s]Extractor Estimating: 436it [04:45,  1.60it/s]Extractor Estimating: 437it [04:45,  1.56it/s]Extractor Estimating: 438it [04:46,  1.58it/s]Extractor Estimating: 439it [04:47,  1.59it/s]Extractor Estimating: 440it [04:47,  1.60it/s]Extractor Estimating: 441it [04:48,  1.65it/s]Extractor Estimating: 442it [04:48,  1.57it/s]Extractor Estimating: 443it [04:49,  1.59it/s]Extractor Estimating: 444it [04:50,  1.61it/s]Extractor Estimating: 445it [04:50,  1.66it/s]Extractor Estimating: 446it [04:51,  1.67it/s]Extractor Estimating: 447it [04:51,  1.65it/s]Extractor Estimating: 448it [04:52,  1.62it/s]Extractor Estimating: 449it [04:53,  1.70it/s]Extractor Estimating: 450it [04:53,  1.62it/s]Extractor Estimating: 451it [04:54,  1.61it/s]Extractor Estimating: 452it [04:55,  1.50it/s]Extractor Estimating: 453it [04:56,  1.36it/s]Extractor Estimating: 454it [04:56,  1.40it/s]Extractor Estimating: 455it [04:57,  1.46it/s]Extractor Estimating: 456it [04:57,  1.53it/s]Extractor Estimating: 457it [04:58,  1.48it/s]Extractor Estimating: 458it [04:59,  1.48it/s]Extractor Estimating: 459it [05:00,  1.47it/s]Extractor Estimating: 460it [05:00,  1.45it/s]Extractor Estimating: 461it [05:01,  1.49it/s]Extractor Estimating: 462it [05:02,  1.45it/s]Extractor Estimating: 463it [05:02,  1.48it/s]Extractor Estimating: 464it [05:03,  1.48it/s]Extractor Estimating: 465it [05:04,  1.45it/s]Extractor Estimating: 466it [05:05,  1.31it/s]Extractor Estimating: 467it [05:05,  1.32it/s]Extractor Estimating: 468it [05:06,  1.36it/s]Extractor Estimating: 469it [05:07,  1.38it/s]Extractor Estimating: 470it [05:07,  1.44it/s]Extractor Estimating: 471it [05:08,  1.42it/s]Extractor Estimating: 472it [05:09,  1.38it/s]Extractor Estimating: 473it [05:10,  1.40it/s]Extractor Estimating: 474it [05:10,  1.43it/s]Extractor Estimating: 475it [05:11,  1.46it/s]Extractor Estimating: 476it [05:12,  1.48it/s]Extractor Estimating: 477it [05:12,  1.52it/s]Extractor Estimating: 478it [05:13,  1.49it/s]Extractor Estimating: 479it [05:13,  1.56it/s]Extractor Estimating: 480it [05:14,  1.57it/s]Extractor Estimating: 481it [05:15,  1.60it/s]Extractor Estimating: 482it [05:15,  1.59it/s]Extractor Estimating: 483it [05:16,  1.52it/s]Extractor Estimating: 484it [05:17,  1.52it/s]Extractor Estimating: 485it [05:17,  1.56it/s]Extractor Estimating: 486it [05:18,  1.50it/s]Extractor Estimating: 487it [05:19,  1.54it/s]Extractor Estimating: 488it [05:19,  1.55it/s]Extractor Estimating: 489it [05:20,  1.61it/s]Extractor Estimating: 490it [05:20,  1.57it/s]Extractor Estimating: 491it [05:21,  1.62it/s]Extractor Estimating: 492it [05:22,  1.57it/s]Extractor Estimating: 493it [05:22,  1.53it/s]Extractor Estimating: 494it [05:23,  1.49it/s]Extractor Estimating: 495it [05:24,  1.55it/s]Extractor Estimating: 496it [05:24,  1.53it/s]Extractor Estimating: 497it [05:25,  1.57it/s]Extractor Estimating: 498it [05:26,  1.51it/s]Extractor Estimating: 499it [05:26,  1.53it/s]Extractor Estimating: 500it [05:27,  1.64it/s]Extractor Estimating: 500it [05:27,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:32,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:32,791 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:32,792 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:32,792 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:32,792 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:18:33,523 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:18:33,524 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:18:34,145 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:18:35,215 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:18:35,215 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:38,411 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:38,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:38,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:38,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:38,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:18:39,255 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:18:39,257 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:18:39,856 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:18:40,029 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:18:40,029 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:29:37,058 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:29:37,112 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 10292 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 22750
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22850, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22850, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.107, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.131, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.115, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.153, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 71, avg_time 1.123, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 171, avg_time 2.181, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 271, avg_time 1.145, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 371, avg_time 1.141, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 42, avg_time 1.113, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 142, avg_time 1.128, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 242, avg_time 2.196, loss:nan
g_step 1200, step 342, avg_time 1.120, loss:nan
g_step 1300, step 13, avg_time 1.120, loss:nan
g_step 1400, step 113, avg_time 1.131, loss:nan
g_step 1500, step 213, avg_time 1.139, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 313, avg_time 2.169, loss:nan
g_step 1700, step 413, avg_time 1.127, loss:nan
g_step 1800, step 84, avg_time 1.137, loss:nan
g_step 1900, step 184, avg_time 1.114, loss:nan
g_step 2000, step 284, avg_time 1.108, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 384, avg_time 2.190, loss:nan
g_step 2200, step 55, avg_time 1.172, loss:nan
g_step 2300, step 155, avg_time 1.096, loss:nan
g_step 2400, step 255, avg_time 1.131, loss:nan
g_step 2500, step 355, avg_time 1.103, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 26, avg_time 2.176, loss:nan
g_step 2700, step 126, avg_time 1.121, loss:nan
g_step 2800, step 226, avg_time 1.124, loss:nan
g_step 2900, step 326, avg_time 1.144, loss:nan
g_step 3000, step 426, avg_time 1.107, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 97, avg_time 2.170, loss:nan
g_step 3200, step 197, avg_time 1.112, loss:nan
g_step 3300, step 297, avg_time 1.134, loss:nan
g_step 3400, step 397, avg_time 1.141, loss:nan
g_step 3500, step 68, avg_time 1.131, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 168, avg_time 2.182, loss:nan
g_step 3700, step 268, avg_time 1.116, loss:nan
g_step 3800, step 368, avg_time 1.136, loss:nan
g_step 3900, step 39, avg_time 1.120, loss:nan
g_step 4000, step 139, avg_time 1.114, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 239, avg_time 2.187, loss:nan
g_step 4200, step 339, avg_time 1.123, loss:nan
g_step 4300, step 10, avg_time 1.129, loss:nan
g_step 4400, step 110, avg_time 1.142, loss:nan
g_step 4500, step 210, avg_time 1.114, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 310, avg_time 2.166, loss:nan
g_step 4700, step 410, avg_time 1.126, loss:nan
g_step 4800, step 81, avg_time 1.107, loss:nan
g_step 4900, step 181, avg_time 1.150, loss:nan
g_step 5000, step 281, avg_time 1.122, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 381, avg_time 2.185, loss:nan
g_step 5200, step 52, avg_time 1.123, loss:nan
g_step 5300, step 152, avg_time 1.090, loss:nan
g_step 5400, step 252, avg_time 1.120, loss:nan
g_step 5500, step 352, avg_time 1.164, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 23, avg_time 2.202, loss:nan
g_step 5700, step 123, avg_time 1.119, loss:nan
g_step 5800, step 223, avg_time 1.122, loss:nan
g_step 5900, step 323, avg_time 1.108, loss:nan
g_step 6000, step 423, avg_time 1.144, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 94, avg_time 2.209, loss:nan
g_step 6200, step 194, avg_time 1.122, loss:nan
g_step 6300, step 294, avg_time 1.107, loss:nan
g_step 6400, step 394, avg_time 1.104, loss:nan
g_step 6500, step 65, avg_time 1.115, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 165, avg_time 2.191, loss:nan
g_step 6700, step 265, avg_time 1.114, loss:nan
g_step 6800, step 365, avg_time 1.141, loss:nan
g_step 6900, step 36, avg_time 1.119, loss:nan
g_step 7000, step 136, avg_time 1.138, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 236, avg_time 2.181, loss:nan
g_step 7200, step 336, avg_time 1.130, loss:nan
g_step 7300, step 7, avg_time 1.091, loss:nan
g_step 7400, step 107, avg_time 1.131, loss:nan
g_step 7500, step 207, avg_time 1.095, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 307, avg_time 2.159, loss:nan
g_step 7700, step 407, avg_time 1.147, loss:nan
g_step 7800, step 78, avg_time 1.114, loss:nan
g_step 7900, step 178, avg_time 1.124, loss:nan
g_step 8000, step 278, avg_time 1.128, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 378, avg_time 2.172, loss:nan
g_step 8200, step 49, avg_time 1.131, loss:nan
g_step 8300, step 149, avg_time 1.113, loss:nan
g_step 8400, step 249, avg_time 1.156, loss:nan
g_step 8500, step 349, avg_time 1.134, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:29:37 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:29:37 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-29-37_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:29:39 - WARNING - datasets.builder -   Using custom data configuration default-f654e05ec280f9b2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f654e05ec280f9b2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:29:52,751 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:29:52,752 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:29:52,753 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:29:52,754 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:29:53,161 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:29:53,253 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:29:53,253 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:29:53,253 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:29:53,253 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:29:53,253 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:29:53,253 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:29:54,744 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:29:58,201 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:29:58,202 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f654e05ec280f9b2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:06,  1.54ba/s] 18%|        | 2/11 [00:00<00:03,  2.38ba/s] 27%|       | 3/11 [00:01<00:02,  3.10ba/s] 36%|      | 4/11 [00:01<00:01,  3.56ba/s] 45%|     | 5/11 [00:01<00:01,  3.87ba/s] 55%|    | 6/11 [00:01<00:01,  4.11ba/s] 64%|   | 7/11 [00:01<00:00,  4.28ba/s] 73%|  | 8/11 [00:02<00:00,  3.61ba/s] 82%| | 9/11 [00:02<00:00,  3.88ba/s] 91%| | 10/11 [00:02<00:00,  4.08ba/s]100%|| 11/11 [00:02<00:00,  3.82ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:01<00:03,  1.06s/ba] 50%|     | 2/4 [00:01<00:01,  1.77ba/s] 75%|  | 3/4 [00:01<00:00,  2.45ba/s]100%|| 4/4 [00:01<00:00,  2.67ba/s]100%|| 4/4 [00:01<00:00,  2.19ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:01<00:11,  1.13s/ba] 27%|       | 3/11 [00:01<00:02,  2.74ba/s] 45%|     | 5/11 [00:01<00:01,  4.39ba/s] 55%|    | 6/11 [00:01<00:01,  3.82ba/s] 73%|  | 8/11 [00:02<00:00,  5.27ba/s] 91%| | 10/11 [00:02<00:00,  6.47ba/s]100%|| 11/11 [00:02<00:00,  4.80ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.47ba/s] 50%|     | 2/4 [00:00<00:00,  2.51ba/s] 75%|  | 3/4 [00:00<00:00,  3.77ba/s]100%|| 4/4 [00:00<00:00,  4.36ba/s]
[INFO|trainer.py:414] 2023-08-28 17:30:11,702 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:30:11,843 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:30:11,843 >>   Num examples = 10300
[INFO|trainer.py:1149] 2023-08-28 17:30:11,843 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:30:11,843 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:30:11,843 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:30:11,843 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:30:11,844 >>   Total optimization steps = 805
  0%|          | 0/805 [00:00<?, ?it/s]  0%|          | 1/805 [00:00<03:52,  3.45it/s]  0%|          | 2/805 [00:00<03:48,  3.51it/s]  0%|          | 3/805 [00:00<03:47,  3.53it/s]  0%|          | 4/805 [00:01<03:46,  3.53it/s]  1%|          | 5/805 [00:01<03:45,  3.54it/s]  1%|          | 6/805 [00:01<03:45,  3.55it/s]  1%|          | 7/805 [00:01<03:44,  3.55it/s]  1%|          | 8/805 [00:02<05:09,  2.57it/s]  1%|          | 9/805 [00:02<04:43,  2.81it/s]  1%|          | 10/805 [00:03<04:37,  2.86it/s]  1%|         | 11/805 [00:03<04:20,  3.04it/s]  1%|         | 12/805 [00:03<04:08,  3.19it/s]  2%|         | 13/805 [00:04<03:59,  3.30it/s]  2%|         | 14/805 [00:04<03:53,  3.39it/s]  2%|         | 15/805 [00:04<03:48,  3.45it/s]  2%|         | 16/805 [00:04<03:45,  3.49it/s]  2%|         | 17/805 [00:05<03:43,  3.53it/s]  2%|         | 18/805 [00:05<03:41,  3.55it/s]  2%|         | 19/805 [00:05<03:40,  3.57it/s]  2%|         | 20/805 [00:05<03:39,  3.58it/s]  3%|         | 21/805 [00:06<03:38,  3.59it/s]  3%|         | 22/805 [00:06<03:37,  3.59it/s]  3%|         | 23/805 [00:06<03:37,  3.59it/s]  3%|         | 24/805 [00:07<03:36,  3.60it/s]  3%|         | 25/805 [00:07<03:36,  3.61it/s]  3%|         | 26/805 [00:07<03:36,  3.60it/s]  3%|         | 27/805 [00:07<03:35,  3.60it/s]  3%|         | 28/805 [00:08<03:35,  3.61it/s]  4%|         | 29/805 [00:08<04:04,  3.18it/s]  4%|         | 30/805 [00:08<03:55,  3.30it/s]  4%|         | 31/805 [00:09<03:48,  3.38it/s]  4%|         | 32/805 [00:09<03:44,  3.45it/s]  4%|         | 33/805 [00:09<03:41,  3.49it/s]  4%|         | 34/805 [00:10<03:38,  3.53it/s]  4%|         | 35/805 [00:10<03:36,  3.55it/s]  4%|         | 36/805 [00:10<03:49,  3.35it/s]  5%|         | 37/805 [00:10<03:44,  3.42it/s]  5%|         | 38/805 [00:11<03:40,  3.47it/s]  5%|         | 39/805 [00:11<03:38,  3.51it/s]  5%|         | 40/805 [00:11<03:37,  3.52it/s]  5%|         | 41/805 [00:12<03:35,  3.54it/s]  5%|         | 42/805 [00:12<03:34,  3.56it/s]  5%|         | 43/805 [00:12<03:33,  3.57it/s]  5%|         | 44/805 [00:12<03:33,  3.57it/s]  6%|         | 45/805 [00:13<03:32,  3.58it/s]  6%|         | 46/805 [00:13<03:44,  3.38it/s]  6%|         | 47/805 [00:13<03:40,  3.44it/s]  6%|         | 48/805 [00:14<03:37,  3.48it/s]  6%|         | 49/805 [00:14<03:34,  3.52it/s]  6%|         | 50/805 [00:14<03:50,  3.27it/s]  6%|         | 51/805 [00:14<03:45,  3.34it/s]  6%|         | 52/805 [00:15<03:40,  3.41it/s]  7%|         | 53/805 [00:15<03:37,  3.46it/s]  7%|         | 54/805 [00:16<06:39,  1.88it/s]  7%|         | 55/805 [00:16<05:42,  2.19it/s]  7%|         | 56/805 [00:17<05:01,  2.48it/s]  7%|         | 57/805 [00:17<04:33,  2.74it/s]  7%|         | 58/805 [00:17<04:13,  2.94it/s]  7%|         | 59/805 [00:17<03:59,  3.11it/s]  7%|         | 60/805 [00:18<05:12,  2.39it/s]  8%|         | 61/805 [00:19<05:15,  2.35it/s]  8%|         | 62/805 [00:19<04:42,  2.63it/s]  8%|         | 63/805 [00:19<04:20,  2.85it/s]  8%|         | 64/805 [00:19<04:03,  3.04it/s]  8%|         | 65/805 [00:20<03:52,  3.19it/s]  8%|         | 66/805 [00:20<03:44,  3.29it/s]  8%|         | 67/805 [00:20<03:38,  3.38it/s]  8%|         | 68/805 [00:21<03:34,  3.44it/s]  9%|         | 69/805 [00:21<03:31,  3.49it/s]  9%|         | 70/805 [00:21<03:55,  3.13it/s]  9%|         | 71/805 [00:22<04:04,  3.00it/s]  9%|         | 72/805 [00:22<03:53,  3.14it/s]  9%|         | 73/805 [00:22<03:44,  3.26it/s]  9%|         | 74/805 [00:22<03:37,  3.35it/s]  9%|         | 75/805 [00:23<03:33,  3.42it/s]  9%|         | 76/805 [00:23<03:30,  3.46it/s] 10%|         | 77/805 [00:23<03:28,  3.50it/s] 10%|         | 78/805 [00:24<03:25,  3.53it/s] 10%|         | 79/805 [00:24<03:24,  3.55it/s] 10%|         | 80/805 [00:24<03:52,  3.12it/s] 10%|         | 81/805 [00:24<03:42,  3.25it/s] 10%|         | 82/805 [00:25<03:36,  3.34it/s] 10%|         | 83/805 [00:25<03:31,  3.41it/s] 10%|         | 84/805 [00:25<03:28,  3.46it/s] 11%|         | 85/805 [00:26<03:25,  3.50it/s] 11%|         | 86/805 [00:26<03:23,  3.53it/s] 11%|         | 87/805 [00:26<03:22,  3.54it/s] 11%|         | 88/805 [00:27<05:08,  2.32it/s] 11%|         | 89/805 [00:27<04:35,  2.60it/s] 11%|         | 90/805 [00:27<04:12,  2.83it/s] 11%|        | 91/805 [00:28<03:56,  3.02it/s] 11%|        | 92/805 [00:28<03:44,  3.17it/s] 12%|        | 93/805 [00:28<03:36,  3.29it/s] 12%|        | 94/805 [00:29<03:30,  3.37it/s] 12%|        | 95/805 [00:29<03:26,  3.43it/s] 12%|        | 96/805 [00:29<03:24,  3.47it/s] 12%|        | 97/805 [00:30<03:39,  3.23it/s] 12%|        | 98/805 [00:30<03:32,  3.33it/s] 12%|        | 99/805 [00:30<03:27,  3.40it/s] 12%|        | 100/805 [00:30<03:24,  3.45it/s] 13%|        | 101/805 [00:31<03:21,  3.49it/s] 13%|        | 102/805 [00:31<03:19,  3.52it/s] 13%|        | 103/805 [00:31<03:18,  3.54it/s] 13%|        | 104/805 [00:31<03:17,  3.55it/s] 13%|        | 105/805 [00:32<04:24,  2.65it/s] 13%|        | 106/805 [00:32<04:03,  2.87it/s] 13%|        | 107/805 [00:33<03:48,  3.06it/s] 13%|        | 108/805 [00:33<03:37,  3.20it/s] 14%|        | 109/805 [00:33<03:30,  3.31it/s] 14%|        | 110/805 [00:33<03:24,  3.39it/s] 14%|        | 111/805 [00:34<03:21,  3.45it/s] 14%|        | 112/805 [00:34<03:18,  3.49it/s] 14%|        | 113/805 [00:34<03:16,  3.52it/s] 14%|        | 114/805 [00:35<03:15,  3.54it/s] 14%|        | 115/805 [00:35<03:58,  2.89it/s] 14%|        | 116/805 [00:35<03:44,  3.06it/s] 15%|        | 117/805 [00:36<03:34,  3.21it/s] 15%|        | 118/805 [00:36<03:27,  3.31it/s] 15%|        | 119/805 [00:36<03:22,  3.39it/s] 15%|        | 120/805 [00:36<03:18,  3.46it/s] 15%|        | 121/805 [00:37<03:15,  3.50it/s] 15%|        | 122/805 [00:37<04:21,  2.61it/s] 15%|        | 123/805 [00:38<03:59,  2.84it/s] 15%|        | 124/805 [00:38<03:44,  3.03it/s] 16%|        | 125/805 [00:38<03:33,  3.19it/s] 16%|        | 126/805 [00:38<03:25,  3.30it/s] 16%|        | 127/805 [00:39<03:20,  3.38it/s] 16%|        | 128/805 [00:39<03:17,  3.43it/s] 16%|        | 129/805 [00:39<03:14,  3.48it/s] 16%|        | 130/805 [00:40<03:11,  3.52it/s] 16%|        | 131/805 [00:40<03:37,  3.10it/s] 16%|        | 132/805 [00:40<04:11,  2.67it/s] 17%|        | 133/805 [00:41<03:53,  2.88it/s] 17%|        | 134/805 [00:41<03:39,  3.06it/s] 17%|        | 135/805 [00:41<03:29,  3.20it/s] 17%|        | 136/805 [00:42<03:22,  3.31it/s] 17%|        | 137/805 [00:42<03:16,  3.39it/s] 17%|        | 138/805 [00:42<03:13,  3.45it/s] 17%|        | 139/805 [00:43<04:47,  2.32it/s] 17%|        | 140/805 [00:43<04:17,  2.59it/s] 18%|        | 141/805 [00:43<03:54,  2.83it/s] 18%|        | 142/805 [00:44<03:39,  3.02it/s] 18%|        | 143/805 [00:44<03:28,  3.17it/s] 18%|        | 144/805 [00:44<03:21,  3.29it/s] 18%|        | 145/805 [00:45<03:15,  3.37it/s] 18%|        | 146/805 [00:45<03:12,  3.43it/s] 18%|        | 147/805 [00:45<03:09,  3.47it/s] 18%|        | 148/805 [00:46<03:33,  3.08it/s] 19%|        | 149/805 [00:46<03:23,  3.22it/s] 19%|        | 150/805 [00:46<03:17,  3.32it/s] 19%|        | 151/805 [00:46<03:12,  3.40it/s] 19%|        | 152/805 [00:47<03:09,  3.45it/s] 19%|        | 153/805 [00:47<03:07,  3.49it/s] 19%|        | 154/805 [00:47<03:05,  3.52it/s] 19%|        | 155/805 [00:48<04:36,  2.35it/s] 19%|        | 156/805 [00:48<04:07,  2.62it/s] 20%|        | 157/805 [00:49<03:47,  2.85it/s] 20%|        | 158/805 [00:49<03:32,  3.04it/s] 20%|        | 159/805 [00:49<03:23,  3.18it/s] 20%|        | 160/805 [00:49<03:15,  3.29it/s] 20%|        | 161/805 [00:50<03:08,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 17:31:02,006 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:31:02,006 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:31:02,006 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.86it/s][A
  3%|         | 12/435 [00:00<00:08, 49.12it/s][A
  4%|         | 17/435 [00:00<00:08, 47.36it/s][A
  5%|         | 22/435 [00:00<00:08, 46.55it/s][A
  6%|         | 27/435 [00:01<00:08, 46.05it/s][A
  7%|         | 32/435 [00:01<00:25, 15.71it/s][A
  9%|         | 37/435 [00:01<00:20, 19.88it/s][A
 10%|         | 42/435 [00:01<00:16, 24.14it/s][A
 11%|         | 47/435 [00:01<00:13, 28.19it/s][A
 12%|        | 52/435 [00:01<00:12, 31.84it/s][A
 13%|        | 57/435 [00:01<00:10, 35.00it/s][A
 14%|        | 62/435 [00:01<00:09, 37.53it/s][A
 15%|        | 67/435 [00:02<00:09, 39.54it/s][A
 17%|        | 72/435 [00:02<00:08, 40.62it/s][A
 18%|        | 77/435 [00:02<00:08, 41.75it/s][A
 19%|        | 82/435 [00:02<00:08, 42.63it/s][A
 20%|        | 87/435 [00:02<00:08, 43.38it/s][A
 21%|        | 92/435 [00:02<00:07, 43.71it/s][A
 22%|       | 97/435 [00:03<00:07, 44.11it/s][A
 23%|       | 102/435 [00:03<00:17, 19.45it/s][A
 25%|       | 107/435 [00:03<00:13, 23.45it/s][A
 26%|       | 112/435 [00:03<00:11, 27.40it/s][A
 27%|       | 117/435 [00:03<00:10, 31.03it/s][A
 28%|       | 122/435 [00:03<00:09, 34.27it/s][A
 29%|       | 127/435 [00:03<00:08, 36.97it/s][A
 30%|       | 132/435 [00:04<00:07, 39.10it/s][A
 31%|      | 137/435 [00:04<00:07, 40.71it/s][A
 33%|      | 142/435 [00:04<00:07, 41.47it/s][A
 34%|      | 147/435 [00:04<00:06, 42.34it/s][A
 35%|      | 152/435 [00:04<00:06, 43.05it/s][A
 36%|      | 157/435 [00:04<00:06, 43.63it/s][A
 37%|      | 162/435 [00:04<00:06, 44.10it/s][A
 38%|      | 167/435 [00:04<00:06, 44.40it/s][A
 40%|      | 172/435 [00:04<00:05, 44.55it/s][A
 41%|      | 177/435 [00:05<00:05, 44.72it/s][A
 42%|     | 182/435 [00:05<00:05, 44.69it/s][A
 43%|     | 187/435 [00:05<00:05, 44.46it/s][A
 44%|     | 192/435 [00:05<00:05, 44.38it/s][A
 45%|     | 197/435 [00:05<00:05, 44.38it/s][A
 46%|     | 202/435 [00:05<00:05, 44.55it/s][A
 48%|     | 207/435 [00:05<00:05, 44.74it/s][A
 49%|     | 212/435 [00:05<00:06, 32.71it/s][A
 50%|     | 217/435 [00:06<00:06, 35.68it/s][A
 51%|     | 222/435 [00:06<00:05, 38.07it/s][A
 52%|    | 227/435 [00:06<00:05, 39.98it/s][A
 53%|    | 232/435 [00:06<00:04, 41.41it/s][A
 54%|    | 237/435 [00:06<00:04, 42.49it/s][A
 56%|    | 242/435 [00:06<00:04, 43.18it/s][A
 57%|    | 247/435 [00:06<00:04, 43.68it/s][A
 58%|    | 252/435 [00:06<00:04, 43.56it/s][A
 59%|    | 257/435 [00:06<00:04, 43.78it/s][A
 60%|    | 262/435 [00:07<00:03, 44.14it/s][A
 61%|   | 267/435 [00:07<00:03, 44.38it/s][A
 63%|   | 272/435 [00:07<00:03, 44.59it/s][A
 64%|   | 277/435 [00:07<00:03, 44.67it/s][A
 65%|   | 282/435 [00:07<00:03, 44.85it/s][A
 66%|   | 287/435 [00:07<00:03, 44.96it/s][A
 67%|   | 292/435 [00:07<00:03, 44.88it/s][A
 68%|   | 297/435 [00:07<00:03, 44.61it/s][A
 69%|   | 302/435 [00:07<00:02, 44.57it/s][A
 71%|   | 307/435 [00:08<00:02, 44.42it/s][A
 72%|  | 312/435 [00:08<00:05, 23.58it/s][A
 73%|  | 317/435 [00:08<00:04, 27.53it/s][A
 74%|  | 322/435 [00:08<00:03, 31.20it/s][A
 75%|  | 327/435 [00:08<00:03, 34.36it/s][A
 76%|  | 332/435 [00:08<00:02, 37.02it/s][A
 77%|  | 337/435 [00:09<00:02, 39.15it/s][A
 79%|  | 342/435 [00:09<00:04, 21.97it/s][A
 80%|  | 347/435 [00:09<00:03, 26.07it/s][A
 81%|  | 352/435 [00:09<00:02, 29.84it/s][A
 82%| | 357/435 [00:09<00:02, 33.26it/s][A
 83%| | 362/435 [00:09<00:02, 36.11it/s][A
 84%| | 367/435 [00:10<00:01, 38.42it/s][A
 86%| | 372/435 [00:10<00:01, 40.21it/s][A
 87%| | 377/435 [00:10<00:01, 41.59it/s][A
 88%| | 382/435 [00:10<00:01, 42.32it/s][A
 89%| | 387/435 [00:10<00:01, 42.70it/s][A
 90%| | 392/435 [00:10<00:00, 43.23it/s][A
 91%|| 397/435 [00:10<00:00, 43.77it/s][A
 92%|| 402/435 [00:10<00:00, 44.20it/s][A
 94%|| 407/435 [00:10<00:00, 44.40it/s][A
 95%|| 412/435 [00:11<00:00, 44.68it/s][A
 96%|| 417/435 [00:11<00:01, 17.27it/s][A
 97%|| 422/435 [00:11<00:00, 21.21it/s][A
 98%|| 427/435 [00:12<00:00, 25.22it/s][A
 99%|| 432/435 [00:12<00:00, 29.09it/s][A
                                                 [A                                                 
100%|| 435/435 [00:12<00:00, 29.09it/s][A 20%|        | 161/805 [01:02<03:08,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:31:15,470 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-161
[INFO|configuration_utils.py:351] 2023-08-28 17:31:16,503 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-161/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:31:26,552 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-161/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:31:27,221 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-161/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:31:27,351 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-161/special_tokens_map.json
 20%|        | 162/805 [01:20<1:38:14,  9.17s/it] 20%|        | 163/805 [01:20<1:09:43,  6.52s/it] 20%|        | 164/805 [01:20<49:38,  4.65s/it]   20%|        | 165/805 [01:20<35:35,  3.34s/it] 21%|        | 166/805 [01:21<25:46,  2.42s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 21%|        | 167/805 [01:21<18:55,  1.78s/it] 21%|        | 168/805 [01:21<14:07,  1.33s/it] 21%|        | 169/805 [01:22<11:33,  1.09s/it] 21%|        | 170/805 [01:22<08:59,  1.18it/s] 21%|        | 171/805 [01:22<07:10,  1.47it/s] 21%|       | 172/805 [01:23<05:54,  1.79it/s] 21%|       | 173/805 [01:23<05:13,  2.02it/s] 22%|       | 174/805 [01:23<04:32,  2.31it/s] 22%|       | 175/805 [01:24<04:04,  2.58it/s] 22%|       | 176/805 [01:24<03:43,  2.81it/s] 22%|       | 177/805 [01:24<03:29,  3.00it/s] 22%|       | 178/805 [01:24<03:19,  3.14it/s] 22%|       | 179/805 [01:25<03:12,  3.25it/s] 22%|       | 180/805 [01:25<03:07,  3.33it/s] 22%|       | 181/805 [01:25<03:04,  3.39it/s] 23%|       | 182/805 [01:26<03:01,  3.43it/s] 23%|       | 183/805 [01:26<02:59,  3.46it/s] 23%|       | 184/805 [01:27<04:10,  2.48it/s] 23%|       | 185/805 [01:27<03:47,  2.72it/s] 23%|       | 186/805 [01:27<03:31,  2.92it/s] 23%|       | 187/805 [01:27<03:20,  3.09it/s] 23%|       | 188/805 [01:28<03:12,  3.21it/s] 23%|       | 189/805 [01:28<03:06,  3.31it/s] 24%|       | 190/805 [01:28<03:02,  3.37it/s] 24%|       | 191/805 [01:28<02:59,  3.42it/s] 24%|       | 192/805 [01:29<02:57,  3.46it/s] 24%|       | 193/805 [01:29<02:55,  3.48it/s] 24%|       | 194/805 [01:29<03:24,  2.99it/s] 24%|       | 195/805 [01:30<03:14,  3.14it/s] 24%|       | 196/805 [01:30<03:07,  3.25it/s] 24%|       | 197/805 [01:30<03:02,  3.34it/s] 25%|       | 198/805 [01:31<02:58,  3.40it/s] 25%|       | 199/805 [01:31<02:56,  3.44it/s] 25%|       | 200/805 [01:31<02:54,  3.47it/s] 25%|       | 201/805 [01:31<02:53,  3.49it/s] 25%|       | 202/805 [01:32<02:52,  3.50it/s] 25%|       | 203/805 [01:32<02:51,  3.51it/s] 25%|       | 204/805 [01:32<02:50,  3.52it/s] 25%|       | 205/805 [01:33<03:05,  3.24it/s] 26%|       | 206/805 [01:33<03:00,  3.32it/s] 26%|       | 207/805 [01:33<02:56,  3.38it/s] 26%|       | 208/805 [01:34<02:54,  3.43it/s] 26%|       | 209/805 [01:34<02:52,  3.46it/s] 26%|       | 210/805 [01:34<02:50,  3.48it/s] 26%|       | 211/805 [01:35<03:56,  2.52it/s] 26%|       | 212/805 [01:35<03:35,  2.76it/s] 26%|       | 213/805 [01:35<03:20,  2.95it/s] 27%|       | 214/805 [01:36<03:10,  3.11it/s] 27%|       | 215/805 [01:36<03:02,  3.23it/s] 27%|       | 216/805 [01:36<02:57,  3.32it/s] 27%|       | 217/805 [01:36<02:53,  3.39it/s] 27%|       | 218/805 [01:37<02:50,  3.45it/s] 27%|       | 219/805 [01:37<02:47,  3.49it/s] 27%|       | 220/805 [01:37<03:21,  2.90it/s] 27%|       | 221/805 [01:38<03:09,  3.08it/s] 28%|       | 222/805 [01:38<03:01,  3.22it/s] 28%|       | 223/805 [01:38<02:55,  3.32it/s] 28%|       | 224/805 [01:39<02:50,  3.40it/s] 28%|       | 225/805 [01:39<02:47,  3.46it/s] 28%|       | 226/805 [01:39<02:45,  3.50it/s] 28%|       | 227/805 [01:39<02:43,  3.53it/s] 28%|       | 228/805 [01:40<02:42,  3.54it/s] 28%|       | 229/805 [01:40<03:03,  3.14it/s] 29%|       | 230/805 [01:40<02:56,  3.26it/s] 29%|       | 231/805 [01:41<02:51,  3.35it/s] 29%|       | 232/805 [01:41<02:47,  3.42it/s] 29%|       | 233/805 [01:41<02:44,  3.47it/s] 29%|       | 234/805 [01:42<02:42,  3.51it/s] 29%|       | 235/805 [01:42<02:41,  3.53it/s] 29%|       | 236/805 [01:42<02:40,  3.55it/s] 29%|       | 237/805 [01:43<03:21,  2.82it/s] 30%|       | 238/805 [01:43<04:00,  2.35it/s] 30%|       | 239/805 [01:43<03:36,  2.62it/s] 30%|       | 240/805 [01:44<03:18,  2.85it/s] 30%|       | 241/805 [01:44<03:05,  3.04it/s] 30%|       | 242/805 [01:44<02:56,  3.19it/s] 30%|       | 243/805 [01:45<02:50,  3.30it/s] 30%|       | 244/805 [01:45<02:45,  3.38it/s] 30%|       | 245/805 [01:45<02:42,  3.44it/s] 31%|       | 246/805 [01:45<02:40,  3.48it/s] 31%|       | 247/805 [01:46<02:38,  3.52it/s] 31%|       | 248/805 [01:46<02:37,  3.54it/s] 31%|       | 249/805 [01:46<02:36,  3.56it/s] 31%|       | 250/805 [01:47<02:35,  3.57it/s] 31%|       | 251/805 [01:47<02:34,  3.58it/s] 31%|      | 252/805 [01:47<02:34,  3.58it/s] 31%|      | 253/805 [01:47<02:55,  3.14it/s] 32%|      | 254/805 [01:48<02:48,  3.26it/s] 32%|      | 255/805 [01:48<02:43,  3.35it/s] 32%|      | 256/805 [01:48<02:40,  3.42it/s] 32%|      | 257/805 [01:49<02:37,  3.47it/s] 32%|      | 258/805 [01:49<02:35,  3.51it/s] 32%|      | 259/805 [01:49<02:34,  3.53it/s] 32%|      | 260/805 [01:49<02:33,  3.55it/s] 32%|      | 261/805 [01:50<02:32,  3.57it/s] 33%|      | 262/805 [01:50<02:31,  3.57it/s] 33%|      | 263/805 [01:50<02:31,  3.58it/s] 33%|      | 264/805 [01:51<02:31,  3.58it/s] 33%|      | 265/805 [01:51<02:30,  3.58it/s] 33%|      | 266/805 [01:51<02:30,  3.59it/s] 33%|      | 267/805 [01:51<02:29,  3.59it/s] 33%|      | 268/805 [01:52<02:29,  3.59it/s] 33%|      | 269/805 [01:52<02:29,  3.59it/s] 34%|      | 270/805 [01:52<02:28,  3.59it/s] 34%|      | 271/805 [01:53<02:37,  3.39it/s] 34%|      | 272/805 [01:53<02:34,  3.45it/s] 34%|      | 273/805 [01:53<02:32,  3.49it/s] 34%|      | 274/805 [01:53<02:30,  3.52it/s] 34%|      | 275/805 [01:54<02:29,  3.54it/s] 34%|      | 276/805 [01:54<02:28,  3.55it/s] 34%|      | 277/805 [01:54<02:28,  3.56it/s] 35%|      | 278/805 [01:54<02:27,  3.57it/s] 35%|      | 279/805 [01:55<02:27,  3.58it/s] 35%|      | 280/805 [01:55<02:26,  3.58it/s] 35%|      | 281/805 [01:55<02:26,  3.58it/s] 35%|      | 282/805 [01:56<02:25,  3.59it/s] 35%|      | 283/805 [01:56<02:25,  3.59it/s] 35%|      | 284/805 [01:56<02:25,  3.59it/s] 35%|      | 285/805 [01:56<02:24,  3.59it/s] 36%|      | 286/805 [01:57<02:24,  3.59it/s] 36%|      | 287/805 [01:57<02:24,  3.59it/s] 36%|      | 288/805 [01:57<02:23,  3.59it/s] 36%|      | 289/805 [01:58<02:23,  3.59it/s] 36%|      | 290/805 [01:58<02:30,  3.42it/s] 36%|      | 291/805 [01:58<02:28,  3.47it/s] 36%|      | 292/805 [01:58<02:26,  3.51it/s] 36%|      | 293/805 [01:59<02:25,  3.53it/s] 37%|      | 294/805 [01:59<02:45,  3.08it/s] 37%|      | 295/805 [01:59<02:39,  3.20it/s] 37%|      | 296/805 [02:00<02:33,  3.31it/s] 37%|      | 297/805 [02:00<02:29,  3.39it/s] 37%|      | 298/805 [02:00<02:27,  3.45it/s] 37%|      | 299/805 [02:01<02:25,  3.49it/s] 37%|      | 300/805 [02:01<02:23,  3.52it/s] 37%|      | 301/805 [02:01<02:22,  3.54it/s] 38%|      | 302/805 [02:01<02:21,  3.55it/s] 38%|      | 303/805 [02:02<02:21,  3.56it/s] 38%|      | 304/805 [02:02<02:20,  3.57it/s] 38%|      | 305/805 [02:02<02:19,  3.58it/s] 38%|      | 306/805 [02:02<02:19,  3.58it/s] 38%|      | 307/805 [02:03<02:32,  3.26it/s] 38%|      | 308/805 [02:03<02:28,  3.35it/s] 38%|      | 309/805 [02:03<02:24,  3.42it/s] 39%|      | 310/805 [02:04<02:22,  3.47it/s] 39%|      | 311/805 [02:04<02:20,  3.50it/s] 39%|      | 312/805 [02:04<02:19,  3.53it/s] 39%|      | 313/805 [02:05<02:18,  3.55it/s] 39%|      | 314/805 [02:05<02:17,  3.56it/s] 39%|      | 315/805 [02:06<03:45,  2.17it/s] 39%|      | 316/805 [02:06<03:18,  2.47it/s] 39%|      | 317/805 [02:06<02:59,  2.72it/s] 40%|      | 318/805 [02:07<02:45,  2.94it/s] 40%|      | 319/805 [02:07<02:36,  3.11it/s] 40%|      | 320/805 [02:07<02:29,  3.24it/s] 40%|      | 321/805 [02:07<02:24,  3.34it/s] 40%|      | 322/805 [02:08<02:19,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 17:32:20,017 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:32:20,017 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:32:20,017 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 12.2022, 'eval_samples_per_second': 284.95, 'eval_steps_per_second': 35.649, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.83it/s][A
  3%|         | 12/435 [00:00<00:08, 49.18it/s][A
  4%|         | 17/435 [00:00<00:08, 47.34it/s][A
  5%|         | 22/435 [00:00<00:08, 46.41it/s][A
  6%|         | 27/435 [00:00<00:08, 46.06it/s][A
  7%|         | 32/435 [00:00<00:08, 45.82it/s][A
  9%|         | 37/435 [00:00<00:08, 45.55it/s][A
 10%|         | 42/435 [00:00<00:08, 45.01it/s][A
 11%|         | 47/435 [00:01<00:08, 44.71it/s][A
 12%|        | 52/435 [00:01<00:08, 44.69it/s][A
 13%|        | 57/435 [00:01<00:08, 44.78it/s][A
 14%|        | 62/435 [00:01<00:08, 44.84it/s][A
 15%|        | 67/435 [00:01<00:08, 44.90it/s][A
 17%|        | 72/435 [00:01<00:08, 44.97it/s][A
 18%|        | 77/435 [00:01<00:07, 45.08it/s][A
 19%|        | 82/435 [00:01<00:07, 45.02it/s][A
 20%|        | 87/435 [00:01<00:07, 44.68it/s][A
 21%|        | 92/435 [00:02<00:07, 44.61it/s][A
 22%|       | 97/435 [00:02<00:07, 44.57it/s][A
 23%|       | 102/435 [00:02<00:07, 44.63it/s][A
 25%|       | 107/435 [00:02<00:13, 23.81it/s][A
 26%|       | 112/435 [00:02<00:11, 27.77it/s][A
 27%|       | 117/435 [00:02<00:10, 31.39it/s][A
 28%|       | 122/435 [00:03<00:09, 34.50it/s][A
 29%|       | 127/435 [00:03<00:08, 37.17it/s][A
 30%|       | 132/435 [00:03<00:07, 39.20it/s][A
 31%|      | 137/435 [00:03<00:07, 40.84it/s][A
 33%|      | 142/435 [00:03<00:06, 41.94it/s][A
 34%|      | 147/435 [00:03<00:06, 42.45it/s][A
 35%|      | 152/435 [00:03<00:06, 42.96it/s][A
 36%|      | 157/435 [00:03<00:06, 43.47it/s][A
 37%|      | 162/435 [00:03<00:06, 43.93it/s][A
 38%|      | 167/435 [00:04<00:06, 44.18it/s][A
 40%|      | 172/435 [00:04<00:05, 44.29it/s][A
 41%|      | 177/435 [00:04<00:05, 44.70it/s][A
 42%|     | 182/435 [00:04<00:05, 44.90it/s][A
 43%|     | 187/435 [00:04<00:05, 44.74it/s][A
 44%|     | 192/435 [00:04<00:05, 44.59it/s][A
 45%|     | 197/435 [00:04<00:05, 44.54it/s][A
 46%|     | 202/435 [00:04<00:05, 44.59it/s][A
 48%|     | 207/435 [00:04<00:05, 44.71it/s][A
 49%|     | 212/435 [00:05<00:04, 44.69it/s][A
 50%|     | 217/435 [00:05<00:04, 44.81it/s][A
 51%|     | 222/435 [00:05<00:04, 44.90it/s][A
 52%|    | 227/435 [00:05<00:09, 22.60it/s][A
 53%|    | 232/435 [00:05<00:07, 26.62it/s][A
 54%|    | 237/435 [00:05<00:06, 30.34it/s][A
 56%|    | 242/435 [00:06<00:05, 33.62it/s][A
 57%|    | 247/435 [00:06<00:05, 36.44it/s][A
 58%|    | 252/435 [00:06<00:04, 38.67it/s][A
 59%|    | 257/435 [00:06<00:04, 40.42it/s][A
 60%|    | 262/435 [00:06<00:04, 41.63it/s][A
 61%|   | 267/435 [00:06<00:03, 42.18it/s][A
 63%|   | 272/435 [00:06<00:03, 42.77it/s][A
 64%|   | 277/435 [00:06<00:04, 36.82it/s][A
 65%|   | 283/435 [00:07<00:03, 40.49it/s][A
 66%|   | 288/435 [00:07<00:03, 41.81it/s][A
 67%|   | 293/435 [00:07<00:03, 42.81it/s][A
 69%|   | 298/435 [00:07<00:03, 43.57it/s][A
 70%|   | 303/435 [00:07<00:03, 43.89it/s][A
 71%|   | 308/435 [00:07<00:02, 44.32it/s][A
 72%|  | 313/435 [00:08<00:02, 44.63it/s][A
 73%|  | 318/435 [00:08<00:05, 22.12it/s][A
 74%|  | 323/435 [00:08<00:04, 26.12it/s][A
 75%|  | 328/435 [00:08<00:03, 29.87it/s][A
 77%|  | 333/435 [00:08<00:03, 33.27it/s][A
 78%|  | 338/435 [00:08<00:02, 36.08it/s][A
 79%|  | 343/435 [00:08<00:02, 38.39it/s][A
 80%|  | 348/435 [00:08<00:02, 40.23it/s][A
 81%|  | 353/435 [00:08<00:01, 41.52it/s][A
 82%| | 358/435 [00:09<00:01, 42.06it/s][A
 83%| | 363/435 [00:09<00:01, 42.72it/s][A
 85%| | 368/435 [00:09<00:01, 43.25it/s][A
 86%| | 373/435 [00:09<00:01, 43.82it/s][A
 87%| | 378/435 [00:09<00:01, 44.18it/s][A
 88%| | 383/435 [00:09<00:01, 44.45it/s][A
 89%| | 388/435 [00:09<00:01, 44.65it/s][A
 90%| | 393/435 [00:09<00:00, 44.79it/s][A
 91%|| 398/435 [00:09<00:00, 44.57it/s][A
 93%|| 403/435 [00:10<00:00, 44.35it/s][A
 94%|| 408/435 [00:10<00:00, 44.33it/s][A
 95%|| 413/435 [00:10<00:00, 44.38it/s][A
 96%|| 418/435 [00:10<00:00, 44.60it/s][A
 97%|| 423/435 [00:10<00:00, 44.80it/s][A
 98%|| 428/435 [00:10<00:00, 27.47it/s][A
 99%|| 432/435 [00:11<00:00, 28.43it/s][A
                                                 [A                                                 
100%|| 435/435 [00:11<00:00, 28.43it/s][A 40%|      | 322/805 [02:19<02:19,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:32:31,613 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-322
[INFO|configuration_utils.py:351] 2023-08-28 17:32:31,908 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-322/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:32:42,157 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-322/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:32:42,951 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-322/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:32:43,393 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-322/special_tokens_map.json
 40%|      | 323/805 [02:42<1:23:28, 10.39s/it] 40%|      | 324/805 [02:42<59:07,  7.38s/it]   40%|      | 325/805 [02:42<41:58,  5.25s/it] 40%|      | 326/805 [02:42<30:00,  3.76s/it] 41%|      | 327/805 [02:43<21:37,  2.72s/it] 41%|      | 328/805 [02:43<16:28,  2.07s/it] 41%|      | 329/805 [02:44<12:11,  1.54s/it] 41%|      | 330/805 [02:44<09:10,  1.16s/it] 41%|      | 331/805 [02:44<07:04,  1.12it/s] 41%|      | 332/805 [02:44<05:36,  1.41it/s] 41%|     | 333/805 [02:45<04:34,  1.72it/s] 41%|     | 334/805 [02:45<04:13,  1.86it/s] 42%|     | 335/805 [02:45<03:36,  2.17it/s] 42%|     | 336/805 [02:46<03:10,  2.46it/s] 42%|     | 337/805 [02:46<02:53,  2.70it/s] 42%|     | 338/805 [02:46<02:40,  2.91it/s] 42%|     | 339/805 [02:47<02:31,  3.08it/s] 42%|     | 340/805 [02:47<02:25,  3.21it/s] 42%|     | 341/805 [02:47<02:20,  3.30it/s] 42%|     | 342/805 [02:47<02:17,  3.37it/s] 43%|     | 343/805 [02:48<02:15,  3.42it/s] 43%|     | 344/805 [02:48<02:13,  3.45it/s] 43%|     | 345/805 [02:48<02:23,  3.21it/s] 43%|     | 346/805 [02:49<02:18,  3.31it/s] 43%|     | 347/805 [02:49<02:15,  3.38it/s] 43%|     | 348/805 [02:49<02:13,  3.43it/s] 43%|     | 349/805 [02:49<02:11,  3.46it/s] 43%|     | 350/805 [02:50<02:10,  3.49it/s] 44%|     | 351/805 [02:50<02:09,  3.50it/s] 44%|     | 352/805 [02:50<02:09,  3.51it/s] 44%|     | 353/805 [02:51<02:08,  3.52it/s] 44%|     | 354/805 [02:51<02:07,  3.53it/s] 44%|     | 355/805 [02:51<02:07,  3.53it/s] 44%|     | 356/805 [02:52<02:19,  3.22it/s] 44%|     | 357/805 [02:52<02:43,  2.73it/s] 44%|     | 358/805 [02:52<02:32,  2.93it/s] 45%|     | 359/805 [02:53<02:24,  3.09it/s] 45%|     | 360/805 [02:53<02:18,  3.21it/s] 45%|     | 361/805 [02:53<02:14,  3.31it/s] 45%|     | 362/805 [02:53<02:11,  3.38it/s] 45%|     | 363/805 [02:54<02:09,  3.42it/s] 45%|     | 364/805 [02:54<02:07,  3.46it/s] 45%|     | 365/805 [02:54<02:06,  3.48it/s] 45%|     | 366/805 [02:55<02:05,  3.49it/s] 46%|     | 367/805 [02:55<02:43,  2.68it/s] 46%|     | 368/805 [02:55<02:31,  2.89it/s] 46%|     | 369/805 [02:56<02:22,  3.06it/s] 46%|     | 370/805 [02:56<02:16,  3.19it/s] 46%|     | 371/805 [02:56<02:11,  3.29it/s] 46%|     | 372/805 [02:57<02:08,  3.36it/s] 46%|     | 373/805 [02:57<02:45,  2.61it/s] 46%|     | 374/805 [02:57<02:31,  2.85it/s] 47%|     | 375/805 [02:58<02:21,  3.04it/s] 47%|     | 376/805 [02:58<02:14,  3.19it/s] 47%|     | 377/805 [02:58<02:09,  3.30it/s] 47%|     | 378/805 [02:59<02:06,  3.39it/s] 47%|     | 379/805 [02:59<02:03,  3.45it/s] 47%|     | 380/805 [03:00<03:17,  2.16it/s] 47%|     | 381/805 [03:00<03:04,  2.30it/s] 47%|     | 382/805 [03:00<02:43,  2.58it/s] 48%|     | 383/805 [03:01<02:29,  2.82it/s] 48%|     | 384/805 [03:01<02:19,  3.02it/s] 48%|     | 385/805 [03:01<02:12,  3.17it/s] 48%|     | 386/805 [03:01<02:07,  3.29it/s] 48%|     | 387/805 [03:02<02:03,  3.38it/s] 48%|     | 388/805 [03:02<02:01,  3.44it/s] 48%|     | 389/805 [03:02<02:19,  2.97it/s] 48%|     | 390/805 [03:03<02:12,  3.13it/s] 49%|     | 391/805 [03:03<02:07,  3.25it/s] 49%|     | 392/805 [03:03<02:03,  3.35it/s] 49%|     | 393/805 [03:04<02:00,  3.42it/s] 49%|     | 394/805 [03:04<01:58,  3.47it/s] 49%|     | 395/805 [03:04<01:56,  3.51it/s] 49%|     | 396/805 [03:04<01:55,  3.53it/s] 49%|     | 397/805 [03:05<01:54,  3.56it/s] 49%|     | 398/805 [03:05<01:54,  3.57it/s] 50%|     | 399/805 [03:06<02:36,  2.59it/s] 50%|     | 400/805 [03:06<02:23,  2.83it/s] 50%|     | 401/805 [03:06<02:13,  3.02it/s] 50%|     | 402/805 [03:06<02:07,  3.17it/s] 50%|     | 403/805 [03:07<02:02,  3.29it/s] 50%|     | 404/805 [03:07<01:58,  3.37it/s] 50%|     | 405/805 [03:07<01:56,  3.44it/s] 50%|     | 406/805 [03:08<02:18,  2.87it/s] 51%|     | 407/805 [03:08<02:10,  3.06it/s] 51%|     | 408/805 [03:08<02:03,  3.20it/s] 51%|     | 409/805 [03:09<01:59,  3.31it/s] 51%|     | 410/805 [03:09<01:56,  3.39it/s] 51%|     | 411/805 [03:09<01:54,  3.45it/s] 51%|     | 412/805 [03:09<01:52,  3.49it/s] 51%|    | 413/805 [03:10<01:51,  3.52it/s] 51%|    | 414/805 [03:10<01:50,  3.54it/s] 52%|    | 415/805 [03:10<01:49,  3.56it/s] 52%|    | 416/805 [03:11<02:04,  3.13it/s] 52%|    | 417/805 [03:11<01:59,  3.26it/s] 52%|    | 418/805 [03:11<01:55,  3.35it/s] 52%|    | 419/805 [03:11<01:52,  3.42it/s] 52%|    | 420/805 [03:12<01:50,  3.47it/s] 52%|    | 421/805 [03:12<01:49,  3.50it/s] 52%|    | 422/805 [03:12<01:48,  3.53it/s] 53%|    | 423/805 [03:13<01:47,  3.55it/s] 53%|    | 424/805 [03:13<01:47,  3.56it/s] 53%|    | 425/805 [03:13<01:46,  3.57it/s] 53%|    | 426/805 [03:13<01:45,  3.58it/s] 53%|    | 427/805 [03:14<01:45,  3.59it/s] 53%|    | 428/805 [03:14<01:45,  3.59it/s] 53%|    | 429/805 [03:14<01:44,  3.59it/s] 53%|    | 430/805 [03:15<01:49,  3.44it/s] 54%|    | 431/805 [03:15<01:47,  3.47it/s] 54%|    | 432/805 [03:15<01:46,  3.51it/s] 54%|    | 433/805 [03:15<01:45,  3.53it/s] 54%|    | 434/805 [03:16<01:49,  3.39it/s] 54%|    | 435/805 [03:16<01:47,  3.45it/s] 54%|    | 436/805 [03:16<01:45,  3.49it/s] 54%|    | 437/805 [03:17<01:44,  3.52it/s] 54%|    | 438/805 [03:17<01:43,  3.54it/s] 55%|    | 439/805 [03:17<01:43,  3.55it/s] 55%|    | 440/805 [03:17<01:42,  3.56it/s] 55%|    | 441/805 [03:18<01:41,  3.57it/s] 55%|    | 442/805 [03:18<01:41,  3.58it/s] 55%|    | 443/805 [03:18<01:40,  3.58it/s] 55%|    | 444/805 [03:19<01:52,  3.21it/s] 55%|    | 445/805 [03:19<01:57,  3.07it/s] 55%|    | 446/805 [03:19<02:01,  2.96it/s] 56%|    | 447/805 [03:20<01:54,  3.12it/s] 56%|    | 448/805 [03:20<01:49,  3.25it/s] 56%|    | 449/805 [03:20<01:46,  3.35it/s] 56%|    | 450/805 [03:20<01:43,  3.41it/s] 56%|    | 451/805 [03:21<01:42,  3.46it/s] 56%|    | 452/805 [03:21<01:40,  3.50it/s] 56%|    | 453/805 [03:21<01:39,  3.53it/s] 56%|    | 454/805 [03:22<01:38,  3.55it/s] 57%|    | 455/805 [03:22<01:38,  3.56it/s] 57%|    | 456/805 [03:22<01:51,  3.12it/s] 57%|    | 457/805 [03:23<01:47,  3.24it/s] 57%|    | 458/805 [03:23<01:43,  3.34it/s] 57%|    | 459/805 [03:23<01:53,  3.06it/s] 57%|    | 460/805 [03:24<01:47,  3.20it/s] 57%|    | 461/805 [03:24<01:43,  3.31it/s] 57%|    | 462/805 [03:24<01:41,  3.39it/s] 58%|    | 463/805 [03:24<01:39,  3.45it/s] 58%|    | 464/805 [03:25<01:37,  3.49it/s] 58%|    | 465/805 [03:25<01:36,  3.53it/s] 58%|    | 466/805 [03:25<01:35,  3.55it/s] 58%|    | 467/805 [03:25<01:35,  3.55it/s] 58%|    | 468/805 [03:26<01:34,  3.56it/s] 58%|    | 469/805 [03:26<02:03,  2.71it/s] 58%|    | 470/805 [03:27<01:54,  2.93it/s] 59%|    | 471/805 [03:27<01:47,  3.10it/s] 59%|    | 472/805 [03:27<01:43,  3.23it/s] 59%|    | 473/805 [03:27<01:40,  3.31it/s] 59%|    | 474/805 [03:28<01:38,  3.37it/s] 59%|    | 475/805 [03:28<01:36,  3.42it/s] 59%|    | 476/805 [03:29<01:55,  2.84it/s] 59%|    | 477/805 [03:29<01:48,  3.02it/s] 59%|    | 478/805 [03:29<01:43,  3.15it/s] 60%|    | 479/805 [03:29<01:40,  3.26it/s] 60%|    | 480/805 [03:30<01:37,  3.34it/s] 60%|    | 481/805 [03:30<01:35,  3.40it/s] 60%|    | 482/805 [03:30<01:33,  3.44it/s] 60%|    | 483/805 [03:30<01:31,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 17:33:42,823 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:33:42,823 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:33:42,823 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 11.0926, 'eval_samples_per_second': 313.452, 'eval_steps_per_second': 39.215, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.93it/s][A
  3%|         | 12/435 [00:00<00:08, 49.02it/s][A
  4%|         | 17/435 [00:00<00:08, 47.38it/s][A
  5%|         | 22/435 [00:00<00:08, 46.61it/s][A
  6%|         | 27/435 [00:00<00:08, 46.08it/s][A
  7%|         | 32/435 [00:00<00:10, 38.08it/s][A
  9%|         | 37/435 [00:00<00:09, 40.11it/s][A
 10%|         | 42/435 [00:00<00:09, 41.57it/s][A
 11%|         | 47/435 [00:01<00:09, 42.54it/s][A
 12%|        | 52/435 [00:01<00:08, 43.29it/s][A
 13%|        | 57/435 [00:01<00:08, 43.86it/s][A
 14%|        | 62/435 [00:01<00:08, 44.30it/s][A
 15%|        | 67/435 [00:01<00:08, 44.48it/s][A
 17%|        | 72/435 [00:01<00:08, 44.16it/s][A
 18%|        | 77/435 [00:01<00:08, 44.36it/s][A
 19%|        | 82/435 [00:01<00:07, 44.50it/s][A
 20%|        | 87/435 [00:01<00:07, 44.67it/s][A
 21%|        | 92/435 [00:02<00:07, 44.75it/s][A
 22%|       | 97/435 [00:02<00:07, 44.84it/s][A
 23%|       | 102/435 [00:02<00:07, 44.86it/s][A
 25%|       | 107/435 [00:02<00:07, 44.94it/s][A
 26%|       | 112/435 [00:02<00:07, 44.83it/s][A
 27%|       | 117/435 [00:03<00:07, 44.65it/s][A
 28%|       | 122/435 [00:03<00:13, 22.60it/s][A
 29%|       | 127/435 [00:03<00:11, 26.58it/s][A
 30%|       | 132/435 [00:03<00:09, 30.34it/s][A
 31%|      | 137/435 [00:03<00:08, 33.64it/s][A
 33%|      | 142/435 [00:03<00:08, 36.43it/s][A
 34%|      | 147/435 [00:03<00:07, 38.69it/s][A
 35%|      | 152/435 [00:03<00:06, 40.44it/s][A
 36%|      | 157/435 [00:03<00:06, 41.66it/s][A
 37%|      | 162/435 [00:04<00:06, 42.20it/s][A
 38%|      | 167/435 [00:04<00:06, 42.82it/s][A
 40%|      | 172/435 [00:04<00:06, 43.49it/s][A
 41%|      | 177/435 [00:04<00:05, 44.00it/s][A
 42%|     | 182/435 [00:04<00:05, 44.26it/s][A
 43%|     | 187/435 [00:04<00:05, 44.55it/s][A
 44%|     | 192/435 [00:04<00:05, 44.71it/s][A
 45%|     | 197/435 [00:04<00:05, 44.77it/s][A
 46%|     | 202/435 [00:04<00:05, 44.64it/s][A
 48%|     | 207/435 [00:05<00:05, 44.44it/s][A
 49%|     | 212/435 [00:05<00:05, 44.40it/s][A
 50%|     | 217/435 [00:05<00:04, 44.48it/s][A
 51%|     | 222/435 [00:05<00:04, 44.69it/s][A
 52%|    | 227/435 [00:05<00:04, 44.85it/s][A
 53%|    | 232/435 [00:05<00:04, 44.95it/s][A
 54%|    | 237/435 [00:05<00:04, 44.96it/s][A
 56%|    | 242/435 [00:05<00:04, 39.68it/s][A
 57%|    | 247/435 [00:05<00:04, 41.18it/s][A
 58%|    | 252/435 [00:06<00:04, 42.27it/s][A
 59%|    | 257/435 [00:06<00:04, 43.10it/s][A
 60%|    | 262/435 [00:06<00:03, 43.66it/s][A
 61%|   | 267/435 [00:06<00:03, 44.12it/s][A
 63%|   | 272/435 [00:06<00:03, 44.37it/s][A
 64%|   | 277/435 [00:06<00:03, 44.57it/s][A
 65%|   | 282/435 [00:06<00:03, 44.26it/s][A
 66%|   | 287/435 [00:06<00:03, 44.32it/s][A
 67%|   | 292/435 [00:06<00:03, 44.46it/s][A
 68%|   | 297/435 [00:07<00:03, 44.66it/s][A
 69%|   | 302/435 [00:07<00:02, 44.79it/s][A
 71%|   | 307/435 [00:07<00:02, 44.94it/s][A
 72%|  | 312/435 [00:07<00:02, 44.98it/s][A
 73%|  | 317/435 [00:07<00:02, 45.02it/s][A
 74%|  | 322/435 [00:07<00:02, 44.89it/s][A
 75%|  | 327/435 [00:07<00:02, 44.76it/s][A
 76%|  | 332/435 [00:07<00:02, 44.71it/s][A
 77%|  | 337/435 [00:07<00:02, 44.66it/s][A
 79%|  | 342/435 [00:08<00:02, 44.79it/s][A
 80%|  | 347/435 [00:08<00:05, 15.37it/s][A
 81%|  | 352/435 [00:09<00:04, 19.13it/s][A
 82%| | 357/435 [00:09<00:03, 23.12it/s][A
 83%| | 362/435 [00:09<00:02, 27.11it/s][A
 84%| | 367/435 [00:09<00:02, 30.81it/s][A
 86%| | 372/435 [00:09<00:01, 34.03it/s][A
 87%| | 377/435 [00:09<00:01, 36.79it/s][A
 88%| | 382/435 [00:09<00:01, 38.88it/s][A
 89%| | 387/435 [00:09<00:01, 40.25it/s][A
 90%| | 392/435 [00:09<00:01, 41.38it/s][A
 91%|| 397/435 [00:10<00:00, 42.31it/s][A
 92%|| 402/435 [00:10<00:00, 43.14it/s][A
 94%|| 407/435 [00:10<00:00, 43.66it/s][A
 95%|| 412/435 [00:10<00:00, 34.98it/s][A
 96%|| 417/435 [00:10<00:00, 38.11it/s][A
 97%|| 422/435 [00:10<00:00, 40.03it/s][A
 98%|| 427/435 [00:10<00:00, 33.15it/s][A
 99%|| 432/435 [00:10<00:00, 36.09it/s][A
                                                 [A                                                 
100%|| 435/435 [00:11<00:00, 36.09it/s][A 60%|    | 483/805 [03:42<01:31,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:33:55,711 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-483
[INFO|configuration_utils.py:351] 2023-08-28 17:33:56,149 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-483/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:34:03,207 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-483/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:34:04,023 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-483/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:34:04,294 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-483/special_tokens_map.json
 60%|    | 484/805 [03:55<40:29,  7.57s/it] 60%|    | 485/805 [03:55<28:51,  5.41s/it] 60%|    | 486/805 [03:56<20:35,  3.87s/it] 60%|    | 487/805 [03:56<14:49,  2.80s/it] 61%|    | 488/805 [03:56<10:47,  2.04s/it] 61%|    | 489/805 [03:57<07:58,  1.51s/it] 61%|    | 490/805 [03:57<06:00,  1.14s/it] 61%|    | 491/805 [03:57<04:38,  1.13it/s] 61%|    | 492/805 [03:57<03:40,  1.42it/s] 61%|    | 493/805 [03:58<03:00,  1.73it/s] 61%|   | 494/805 [03:58<02:32,  2.04it/s] 61%|   | 495/805 [03:58<02:12,  2.34it/s] 62%|   | 496/805 [03:59<02:03,  2.50it/s] 62%|   | 497/805 [03:59<01:52,  2.74it/s] 62%|   | 498/805 [03:59<02:13,  2.30it/s] 62%|   | 499/805 [04:00<02:05,  2.43it/s] 62%|   | 500/805 [04:00<01:53,  2.68it/s]                                                  62%|   | 500/805 [04:00<01:53,  2.68it/s] 62%|   | 501/805 [04:00<01:45,  2.89it/s] 62%|   | 502/805 [04:01<01:39,  3.06it/s] 62%|   | 503/805 [04:01<01:34,  3.19it/s] 63%|   | 504/805 [04:01<01:31,  3.29it/s] 63%|   | 505/805 [04:02<01:29,  3.36it/s] 63%|   | 506/805 [04:02<01:39,  2.99it/s] 63%|   | 507/805 [04:02<01:34,  3.14it/s] 63%|   | 508/805 [04:03<02:19,  2.14it/s] 63%|   | 509/805 [04:03<02:02,  2.42it/s] 63%|   | 510/805 [04:04<01:50,  2.67it/s] 63%|   | 511/805 [04:04<01:41,  2.89it/s] 64%|   | 512/805 [04:04<01:35,  3.06it/s] 64%|   | 513/805 [04:04<01:31,  3.19it/s] 64%|   | 514/805 [04:05<01:28,  3.29it/s] 64%|   | 515/805 [04:05<01:56,  2.49it/s] 64%|   | 516/805 [04:06<01:45,  2.73it/s] 64%|   | 517/805 [04:06<01:38,  2.94it/s] 64%|   | 518/805 [04:06<01:32,  3.10it/s] 64%|   | 519/805 [04:06<01:28,  3.22it/s] 65%|   | 520/805 [04:07<01:25,  3.31it/s] 65%|   | 521/805 [04:07<01:23,  3.38it/s] 65%|   | 522/805 [04:07<01:22,  3.43it/s] 65%|   | 523/805 [04:08<01:21,  3.46it/s] 65%|   | 524/805 [04:08<01:20,  3.49it/s] 65%|   | 525/805 [04:08<01:35,  2.92it/s] 65%|   | 526/805 [04:09<01:30,  3.09it/s] 65%|   | 527/805 [04:09<01:26,  3.21it/s] 66%|   | 528/805 [04:09<01:23,  3.30it/s] 66%|   | 529/805 [04:09<01:21,  3.37it/s] 66%|   | 530/805 [04:10<01:20,  3.42it/s] 66%|   | 531/805 [04:10<01:19,  3.46it/s] 66%|   | 532/805 [04:10<01:18,  3.48it/s] 66%|   | 533/805 [04:11<01:17,  3.50it/s] 66%|   | 534/805 [04:11<01:17,  3.52it/s] 66%|   | 535/805 [04:11<01:28,  3.06it/s] 67%|   | 536/805 [04:12<01:23,  3.21it/s] 67%|   | 537/805 [04:12<01:20,  3.32it/s] 67%|   | 538/805 [04:12<01:18,  3.40it/s] 67%|   | 539/805 [04:12<01:16,  3.46it/s] 67%|   | 540/805 [04:13<01:15,  3.50it/s] 67%|   | 541/805 [04:13<01:14,  3.53it/s] 67%|   | 542/805 [04:13<01:14,  3.55it/s] 67%|   | 543/805 [04:14<01:13,  3.56it/s] 68%|   | 544/805 [04:14<01:13,  3.57it/s] 68%|   | 545/805 [04:14<01:12,  3.58it/s] 68%|   | 546/805 [04:14<01:17,  3.34it/s] 68%|   | 547/805 [04:15<01:45,  2.44it/s] 68%|   | 548/805 [04:16<01:46,  2.41it/s] 68%|   | 549/805 [04:16<01:35,  2.67it/s] 68%|   | 550/805 [04:16<01:28,  2.89it/s] 68%|   | 551/805 [04:16<01:22,  3.07it/s] 69%|   | 552/805 [04:17<01:18,  3.21it/s] 69%|   | 553/805 [04:17<01:15,  3.32it/s] 69%|   | 554/805 [04:17<01:13,  3.40it/s] 69%|   | 555/805 [04:17<01:12,  3.46it/s] 69%|   | 556/805 [04:18<01:11,  3.50it/s] 69%|   | 557/805 [04:18<01:10,  3.53it/s] 69%|   | 558/805 [04:19<01:44,  2.37it/s] 69%|   | 559/805 [04:19<01:33,  2.63it/s] 70%|   | 560/805 [04:20<02:09,  1.90it/s] 70%|   | 561/805 [04:20<01:50,  2.20it/s] 70%|   | 562/805 [04:21<01:37,  2.49it/s] 70%|   | 563/805 [04:21<01:28,  2.74it/s] 70%|   | 564/805 [04:21<01:21,  2.95it/s] 70%|   | 565/805 [04:21<01:23,  2.88it/s] 70%|   | 566/805 [04:22<01:18,  3.06it/s] 70%|   | 567/805 [04:22<01:14,  3.21it/s] 71%|   | 568/805 [04:22<01:11,  3.31it/s] 71%|   | 569/805 [04:23<01:32,  2.56it/s] 71%|   | 570/805 [04:23<01:24,  2.79it/s] 71%|   | 571/805 [04:23<01:18,  2.99it/s] 71%|   | 572/805 [04:24<01:14,  3.15it/s] 71%|   | 573/805 [04:24<01:10,  3.27it/s] 71%|  | 574/805 [04:24<01:08,  3.36it/s] 71%|  | 575/805 [04:25<01:09,  3.30it/s] 72%|  | 576/805 [04:25<01:07,  3.38it/s] 72%|  | 577/805 [04:25<01:06,  3.43it/s] 72%|  | 578/805 [04:25<01:05,  3.47it/s] 72%|  | 579/805 [04:26<01:04,  3.51it/s] 72%|  | 580/805 [04:26<01:03,  3.53it/s] 72%|  | 581/805 [04:26<01:02,  3.56it/s] 72%|  | 582/805 [04:27<01:02,  3.57it/s] 72%|  | 583/805 [04:27<01:01,  3.58it/s] 73%|  | 584/805 [04:27<01:01,  3.58it/s] 73%|  | 585/805 [04:27<01:01,  3.58it/s] 73%|  | 586/805 [04:28<01:11,  3.07it/s] 73%|  | 587/805 [04:28<01:07,  3.21it/s] 73%|  | 588/805 [04:28<01:05,  3.32it/s] 73%|  | 589/805 [04:29<01:03,  3.40it/s] 73%|  | 590/805 [04:29<01:02,  3.46it/s] 73%|  | 591/805 [04:29<01:01,  3.50it/s] 74%|  | 592/805 [04:29<01:00,  3.53it/s] 74%|  | 593/805 [04:30<00:59,  3.55it/s] 74%|  | 594/805 [04:30<00:59,  3.56it/s] 74%|  | 595/805 [04:30<00:58,  3.57it/s] 74%|  | 596/805 [04:31<00:58,  3.58it/s] 74%|  | 597/805 [04:31<01:08,  3.03it/s] 74%|  | 598/805 [04:31<01:05,  3.18it/s] 74%|  | 599/805 [04:32<01:02,  3.29it/s] 75%|  | 600/805 [04:32<01:00,  3.38it/s] 75%|  | 601/805 [04:32<00:59,  3.44it/s] 75%|  | 602/805 [04:32<00:58,  3.48it/s] 75%|  | 603/805 [04:33<00:57,  3.51it/s] 75%|  | 604/805 [04:33<00:56,  3.54it/s] 75%|  | 605/805 [04:33<00:56,  3.55it/s] 75%|  | 606/805 [04:34<00:55,  3.57it/s] 75%|  | 607/805 [04:34<00:55,  3.58it/s] 76%|  | 608/805 [04:35<01:21,  2.41it/s] 76%|  | 609/805 [04:35<01:13,  2.67it/s] 76%|  | 610/805 [04:35<01:07,  2.89it/s] 76%|  | 611/805 [04:35<01:03,  3.07it/s] 76%|  | 612/805 [04:36<01:00,  3.21it/s] 76%|  | 613/805 [04:36<00:58,  3.31it/s] 76%|  | 614/805 [04:36<00:56,  3.39it/s] 76%|  | 615/805 [04:37<00:55,  3.45it/s] 77%|  | 616/805 [04:37<00:54,  3.49it/s] 77%|  | 617/805 [04:37<00:53,  3.53it/s] 77%|  | 618/805 [04:38<01:08,  2.75it/s] 77%|  | 619/805 [04:38<01:02,  2.96it/s] 77%|  | 620/805 [04:38<00:59,  3.12it/s] 77%|  | 621/805 [04:38<00:56,  3.25it/s] 77%|  | 622/805 [04:39<00:54,  3.34it/s] 77%|  | 623/805 [04:39<00:53,  3.41it/s] 78%|  | 624/805 [04:39<00:52,  3.46it/s] 78%|  | 625/805 [04:40<00:51,  3.51it/s] 78%|  | 626/805 [04:40<00:50,  3.54it/s] 78%|  | 627/805 [04:40<00:50,  3.56it/s] 78%|  | 628/805 [04:40<00:54,  3.26it/s] 78%|  | 629/805 [04:41<00:52,  3.36it/s] 78%|  | 630/805 [04:41<00:59,  2.95it/s] 78%|  | 631/805 [04:41<00:55,  3.11it/s] 79%|  | 632/805 [04:42<00:53,  3.24it/s] 79%|  | 633/805 [04:42<00:51,  3.34it/s] 79%|  | 634/805 [04:42<00:50,  3.41it/s] 79%|  | 635/805 [04:43<00:49,  3.46it/s] 79%|  | 636/805 [04:43<00:48,  3.50it/s] 79%|  | 637/805 [04:43<00:54,  3.09it/s] 79%|  | 638/805 [04:44<00:56,  2.98it/s] 79%|  | 639/805 [04:44<00:52,  3.14it/s] 80%|  | 640/805 [04:44<00:50,  3.27it/s] 80%|  | 641/805 [04:44<00:48,  3.35it/s] 80%|  | 642/805 [04:45<00:47,  3.42it/s] 80%|  | 643/805 [04:45<00:46,  3.47it/s] 80%|  | 644/805 [04:45<00:45,  3.56it/s][INFO|trainer.py:2140] 2023-08-28 17:34:57,646 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:34:57,646 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:34:57,646 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 11.0645, 'eval_samples_per_second': 314.247, 'eval_steps_per_second': 39.315, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.1940993788819876e-05, 'epoch': 3.11}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.30it/s][A
  3%|         | 12/435 [00:00<00:08, 49.11it/s][A
  4%|         | 17/435 [00:00<00:08, 47.45it/s][A
  5%|         | 22/435 [00:00<00:21, 18.97it/s][A
  6%|         | 27/435 [00:00<00:17, 23.74it/s][A
  7%|         | 32/435 [00:01<00:14, 28.15it/s][A
  9%|         | 37/435 [00:01<00:12, 32.00it/s][A
 10%|         | 42/435 [00:01<00:11, 35.27it/s][A
 11%|         | 47/435 [00:01<00:10, 37.77it/s][A
 12%|        | 52/435 [00:01<00:09, 39.80it/s][A
 13%|        | 57/435 [00:01<00:09, 41.20it/s][A
 14%|        | 62/435 [00:01<00:08, 41.93it/s][A
 15%|        | 67/435 [00:01<00:08, 42.63it/s][A
 17%|        | 72/435 [00:01<00:08, 43.24it/s][A
 18%|        | 77/435 [00:02<00:08, 43.73it/s][A
 19%|        | 82/435 [00:02<00:07, 44.13it/s][A
 20%|        | 87/435 [00:02<00:07, 44.47it/s][A
 21%|        | 92/435 [00:02<00:07, 44.64it/s][A
 22%|       | 97/435 [00:02<00:07, 44.80it/s][A
 23%|       | 102/435 [00:02<00:07, 44.77it/s][A
 25%|       | 107/435 [00:02<00:07, 44.61it/s][A
 26%|       | 112/435 [00:02<00:07, 44.52it/s][A
 27%|       | 117/435 [00:03<00:07, 44.58it/s][A
 28%|       | 122/435 [00:03<00:07, 44.62it/s][A
 29%|       | 127/435 [00:03<00:06, 44.83it/s][A
 30%|       | 132/435 [00:03<00:06, 44.83it/s][A
 31%|      | 137/435 [00:03<00:10, 28.73it/s][A
 33%|      | 142/435 [00:03<00:09, 32.25it/s][A
 34%|      | 147/435 [00:03<00:08, 35.29it/s][A
 35%|      | 152/435 [00:03<00:07, 37.76it/s][A
 36%|      | 157/435 [00:04<00:07, 39.70it/s][A
 37%|      | 162/435 [00:04<00:06, 41.22it/s][A
 38%|      | 167/435 [00:04<00:06, 42.28it/s][A
 40%|      | 172/435 [00:04<00:06, 43.04it/s][A
 41%|      | 177/435 [00:04<00:05, 43.15it/s][A
 42%|     | 182/435 [00:04<00:05, 43.56it/s][A
 43%|     | 187/435 [00:04<00:05, 43.94it/s][A
 44%|     | 192/435 [00:04<00:05, 44.25it/s][A
 45%|     | 197/435 [00:04<00:05, 44.52it/s][A
 46%|     | 202/435 [00:05<00:05, 44.71it/s][A
 48%|     | 207/435 [00:05<00:05, 44.80it/s][A
 49%|     | 212/435 [00:05<00:04, 44.94it/s][A
 50%|     | 217/435 [00:05<00:04, 44.85it/s][A
 51%|     | 222/435 [00:05<00:04, 44.63it/s][A
 52%|    | 227/435 [00:05<00:04, 44.55it/s][A
 53%|    | 232/435 [00:05<00:04, 44.58it/s][A
 54%|    | 237/435 [00:05<00:04, 44.77it/s][A
 56%|    | 242/435 [00:05<00:04, 44.83it/s][A
 57%|    | 247/435 [00:06<00:04, 44.92it/s][A
 58%|    | 252/435 [00:06<00:04, 44.95it/s][A
 59%|    | 257/435 [00:06<00:03, 45.00it/s][A
 60%|    | 262/435 [00:06<00:03, 44.89it/s][A
 61%|   | 267/435 [00:06<00:05, 33.59it/s][A
 63%|   | 272/435 [00:06<00:04, 36.44it/s][A
 64%|   | 277/435 [00:06<00:04, 38.70it/s][A
 65%|   | 282/435 [00:07<00:03, 40.42it/s][A
 66%|   | 287/435 [00:07<00:03, 41.73it/s][A
 67%|   | 292/435 [00:07<00:03, 42.71it/s][A
 68%|   | 297/435 [00:07<00:03, 43.45it/s][A
 69%|   | 302/435 [00:07<00:03, 43.79it/s][A
 71%|   | 307/435 [00:07<00:02, 43.73it/s][A
 72%|  | 312/435 [00:07<00:02, 43.92it/s][A
 73%|  | 317/435 [00:07<00:02, 44.17it/s][A
 74%|  | 322/435 [00:07<00:02, 44.40it/s][A
 75%|  | 327/435 [00:08<00:02, 44.66it/s][A
 76%|  | 332/435 [00:08<00:02, 44.82it/s][A
 77%|  | 337/435 [00:08<00:02, 44.97it/s][A
 79%|  | 342/435 [00:08<00:02, 45.00it/s][A
 80%|  | 347/435 [00:08<00:01, 44.85it/s][A
 81%|  | 352/435 [00:08<00:01, 44.64it/s][A
 82%| | 357/435 [00:08<00:01, 44.62it/s][A
 83%| | 362/435 [00:08<00:01, 44.54it/s][A
 84%| | 367/435 [00:08<00:01, 44.65it/s][A
 86%| | 372/435 [00:09<00:01, 44.71it/s][A
 87%| | 377/435 [00:09<00:01, 44.87it/s][A
 88%| | 382/435 [00:09<00:01, 44.96it/s][A
 89%| | 387/435 [00:09<00:01, 44.96it/s][A
 90%| | 392/435 [00:09<00:00, 44.88it/s][A
 91%|| 397/435 [00:10<00:01, 20.10it/s][A
 92%|| 402/435 [00:10<00:01, 24.10it/s][A
 94%|| 407/435 [00:10<00:00, 28.04it/s][A
 95%|| 412/435 [00:10<00:00, 31.65it/s][A
 96%|| 417/435 [00:10<00:00, 34.78it/s][A
 97%|| 422/435 [00:10<00:00, 37.33it/s][A
 98%|| 427/435 [00:10<00:00, 39.35it/s][A
 99%|| 432/435 [00:10<00:00, 40.56it/s][A
                                                 [A                                                 
100%|| 435/435 [00:10<00:00, 40.56it/s][A 80%|  | 644/805 [04:56<00:45,  3.56it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:35:10,171 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-644
[INFO|configuration_utils.py:351] 2023-08-28 17:35:10,667 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-644/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:35:24,987 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-644/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:35:25,216 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-644/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:35:25,319 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-644/special_tokens_map.json
 80%|  | 645/805 [05:17<25:32,  9.58s/it] 80%|  | 646/805 [05:17<17:59,  6.79s/it] 80%|  | 647/805 [05:17<12:44,  4.84s/it] 80%|  | 648/805 [05:17<09:04,  3.47s/it] 81%|  | 649/805 [05:18<06:31,  2.51s/it] 81%|  | 650/805 [05:18<04:45,  1.84s/it] 81%|  | 651/805 [05:18<03:31,  1.37s/it] 81%|  | 652/805 [05:19<02:39,  1.04s/it] 81%|  | 653/805 [05:19<02:17,  1.11it/s] 81%|  | 654/805 [05:19<01:48,  1.39it/s] 81%| | 655/805 [05:20<02:00,  1.25it/s] 81%| | 656/805 [05:21<01:36,  1.55it/s] 82%| | 657/805 [05:21<01:19,  1.87it/s] 82%| | 658/805 [05:21<01:07,  2.19it/s] 82%| | 659/805 [05:21<00:58,  2.48it/s] 82%| | 660/805 [05:22<00:53,  2.74it/s] 82%| | 661/805 [05:22<00:48,  2.95it/s] 82%| | 662/805 [05:22<00:45,  3.12it/s] 82%| | 663/805 [05:23<00:43,  3.25it/s] 82%| | 664/805 [05:23<01:02,  2.27it/s] 83%| | 665/805 [05:24<00:54,  2.55it/s] 83%| | 666/805 [05:24<00:49,  2.80it/s] 83%| | 667/805 [05:24<00:45,  3.00it/s] 83%| | 668/805 [05:24<00:43,  3.16it/s] 83%| | 669/805 [05:25<00:41,  3.28it/s] 83%| | 670/805 [05:25<00:40,  3.36it/s] 83%| | 671/805 [05:25<00:39,  3.43it/s] 83%| | 672/805 [05:26<00:38,  3.48it/s] 84%| | 673/805 [05:26<00:37,  3.52it/s] 84%| | 674/805 [05:27<01:07,  1.93it/s] 84%| | 675/805 [05:27<00:57,  2.25it/s] 84%| | 676/805 [05:27<00:50,  2.53it/s] 84%| | 677/805 [05:28<00:45,  2.78it/s] 84%| | 678/805 [05:28<00:42,  2.99it/s] 84%| | 679/805 [05:28<00:40,  3.15it/s] 84%| | 680/805 [05:29<00:38,  3.27it/s] 85%| | 681/805 [05:29<00:36,  3.36it/s] 85%| | 682/805 [05:29<00:37,  3.27it/s] 85%| | 683/805 [05:29<00:36,  3.36it/s] 85%| | 684/805 [05:30<00:35,  3.43it/s] 85%| | 685/805 [05:30<00:34,  3.48it/s] 85%| | 686/805 [05:30<00:33,  3.51it/s] 85%| | 687/805 [05:31<00:33,  3.54it/s] 85%| | 688/805 [05:31<00:32,  3.56it/s] 86%| | 689/805 [05:31<00:32,  3.57it/s] 86%| | 690/805 [05:31<00:32,  3.58it/s] 86%| | 691/805 [05:32<00:31,  3.58it/s] 86%| | 692/805 [05:32<00:31,  3.59it/s] 86%| | 693/805 [05:33<00:42,  2.65it/s] 86%| | 694/805 [05:33<00:38,  2.88it/s] 86%| | 695/805 [05:33<00:35,  3.06it/s] 86%| | 696/805 [05:33<00:34,  3.21it/s] 87%| | 697/805 [05:34<00:32,  3.31it/s] 87%| | 698/805 [05:34<00:31,  3.39it/s] 87%| | 699/805 [05:34<00:30,  3.45it/s] 87%| | 700/805 [05:35<00:30,  3.49it/s] 87%| | 701/805 [05:35<00:29,  3.52it/s] 87%| | 702/805 [05:35<00:29,  3.54it/s] 87%| | 703/805 [05:36<00:34,  2.95it/s] 87%| | 704/805 [05:36<00:32,  3.12it/s] 88%| | 705/805 [05:36<00:30,  3.25it/s] 88%| | 706/805 [05:36<00:29,  3.34it/s] 88%| | 707/805 [05:37<00:28,  3.41it/s] 88%| | 708/805 [05:37<00:28,  3.46it/s] 88%| | 709/805 [05:37<00:27,  3.50it/s] 88%| | 710/805 [05:37<00:26,  3.53it/s] 88%| | 711/805 [05:38<00:26,  3.55it/s] 88%| | 712/805 [05:38<00:26,  3.56it/s] 89%| | 713/805 [05:38<00:25,  3.58it/s] 89%| | 714/805 [05:39<00:34,  2.65it/s] 89%| | 715/805 [05:39<00:31,  2.88it/s] 89%| | 716/805 [05:39<00:29,  3.06it/s] 89%| | 717/805 [05:40<00:27,  3.21it/s] 89%| | 718/805 [05:40<00:26,  3.32it/s] 89%| | 719/805 [05:40<00:25,  3.40it/s] 89%| | 720/805 [05:41<00:24,  3.45it/s] 90%| | 721/805 [05:41<00:24,  3.49it/s] 90%| | 722/805 [05:41<00:27,  3.05it/s] 90%| | 723/805 [05:42<00:25,  3.18it/s] 90%| | 724/805 [05:42<00:25,  3.14it/s] 90%| | 725/805 [05:42<00:30,  2.66it/s] 90%| | 726/805 [05:43<00:27,  2.88it/s] 90%| | 727/805 [05:43<00:25,  3.07it/s] 90%| | 728/805 [05:43<00:23,  3.21it/s] 91%| | 729/805 [05:44<00:22,  3.31it/s] 91%| | 730/805 [05:44<00:22,  3.39it/s] 91%| | 731/805 [05:44<00:21,  3.45it/s] 91%| | 732/805 [05:44<00:20,  3.49it/s] 91%| | 733/805 [05:45<00:20,  3.53it/s] 91%| | 734/805 [05:45<00:20,  3.54it/s] 91%|| 735/805 [05:45<00:24,  2.82it/s] 91%|| 736/805 [05:46<00:22,  3.02it/s] 92%|| 737/805 [05:46<00:21,  3.17it/s] 92%|| 738/805 [05:46<00:20,  3.29it/s] 92%|| 739/805 [05:47<00:19,  3.38it/s] 92%|| 740/805 [05:47<00:18,  3.44it/s] 92%|| 741/805 [05:47<00:18,  3.49it/s] 92%|| 742/805 [05:47<00:18,  3.34it/s] 92%|| 743/805 [05:48<00:18,  3.41it/s] 92%|| 744/805 [05:48<00:17,  3.47it/s] 93%|| 745/805 [05:48<00:17,  3.51it/s] 93%|| 746/805 [05:49<00:16,  3.53it/s] 93%|| 747/805 [05:49<00:16,  3.55it/s] 93%|| 748/805 [05:49<00:16,  3.56it/s] 93%|| 749/805 [05:49<00:15,  3.57it/s] 93%|| 750/805 [05:50<00:15,  3.57it/s] 93%|| 751/805 [05:50<00:15,  3.58it/s] 93%|| 752/805 [05:50<00:14,  3.56it/s] 94%|| 753/805 [05:51<00:16,  3.09it/s] 94%|| 754/805 [05:51<00:15,  3.21it/s] 94%|| 755/805 [05:51<00:15,  3.31it/s] 94%|| 756/805 [05:51<00:14,  3.37it/s] 94%|| 757/805 [05:52<00:14,  3.42it/s] 94%|| 758/805 [05:52<00:13,  3.46it/s] 94%|| 759/805 [05:52<00:13,  3.48it/s] 94%|| 760/805 [05:53<00:15,  2.91it/s] 95%|| 761/805 [05:53<00:14,  3.07it/s] 95%|| 762/805 [05:53<00:13,  3.20it/s] 95%|| 763/805 [05:54<00:12,  3.29it/s] 95%|| 764/805 [05:54<00:12,  3.36it/s] 95%|| 765/805 [05:54<00:11,  3.41it/s] 95%|| 766/805 [05:55<00:11,  3.45it/s] 95%|| 767/805 [05:55<00:10,  3.48it/s] 95%|| 768/805 [05:55<00:10,  3.50it/s] 96%|| 769/805 [05:55<00:10,  3.51it/s] 96%|| 770/805 [05:56<00:13,  2.57it/s] 96%|| 771/805 [05:56<00:12,  2.80it/s] 96%|| 772/805 [05:57<00:11,  2.99it/s] 96%|| 773/805 [05:57<00:10,  3.13it/s] 96%|| 774/805 [05:57<00:09,  3.25it/s] 96%|| 775/805 [05:57<00:08,  3.35it/s] 96%|| 776/805 [05:58<00:08,  3.42it/s] 97%|| 777/805 [05:58<00:08,  3.47it/s] 97%|| 778/805 [05:58<00:07,  3.50it/s] 97%|| 779/805 [05:59<00:07,  3.53it/s] 97%|| 780/805 [05:59<00:08,  3.08it/s] 97%|| 781/805 [05:59<00:07,  3.22it/s] 97%|| 782/805 [06:00<00:08,  2.61it/s] 97%|| 783/805 [06:00<00:07,  2.83it/s] 97%|| 784/805 [06:01<00:09,  2.11it/s] 98%|| 785/805 [06:01<00:08,  2.40it/s] 98%|| 786/805 [06:01<00:07,  2.67it/s] 98%|| 787/805 [06:02<00:06,  2.89it/s] 98%|| 788/805 [06:02<00:06,  2.72it/s] 98%|| 789/805 [06:02<00:05,  2.94it/s] 98%|| 790/805 [06:03<00:04,  3.11it/s] 98%|| 791/805 [06:03<00:04,  3.24it/s] 98%|| 792/805 [06:03<00:03,  3.34it/s] 99%|| 793/805 [06:03<00:03,  3.42it/s] 99%|| 794/805 [06:04<00:04,  2.60it/s] 99%|| 795/805 [06:04<00:03,  2.82it/s] 99%|| 796/805 [06:05<00:02,  3.02it/s] 99%|| 797/805 [06:05<00:02,  3.17it/s] 99%|| 798/805 [06:05<00:02,  2.81it/s] 99%|| 799/805 [06:06<00:01,  3.00it/s] 99%|| 800/805 [06:06<00:01,  3.16it/s]100%|| 801/805 [06:06<00:01,  3.28it/s]100%|| 802/805 [06:06<00:00,  3.37it/s]100%|| 803/805 [06:07<00:00,  3.44it/s]100%|| 804/805 [06:07<00:00,  3.48it/s]100%|| 805/805 [06:07<00:00,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 17:36:19,619 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:36:19,619 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:36:19,620 >>   Batch size = 8
{'eval_loss': 0.9974275827407837, 'eval_runtime': 10.8971, 'eval_samples_per_second': 319.076, 'eval_steps_per_second': 39.919, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.61it/s][A
  3%|         | 12/435 [00:00<00:08, 49.19it/s][A
  4%|         | 17/435 [00:00<00:08, 47.41it/s][A
  5%|         | 22/435 [00:00<00:08, 46.35it/s][A
  6%|         | 27/435 [00:00<00:08, 45.86it/s][A
  7%|         | 32/435 [00:01<00:08, 45.24it/s][A
  9%|         | 37/435 [00:01<00:23, 16.71it/s][A
 10%|         | 42/435 [00:01<00:18, 20.82it/s][A
 11%|         | 47/435 [00:01<00:15, 25.02it/s][A
 12%|        | 52/435 [00:01<00:13, 28.99it/s][A
 13%|        | 57/435 [00:01<00:11, 32.52it/s][A
 14%|        | 62/435 [00:01<00:10, 35.56it/s][A
 15%|        | 67/435 [00:02<00:09, 38.01it/s][A
 17%|        | 72/435 [00:02<00:09, 39.85it/s][A
 18%|        | 77/435 [00:02<00:08, 40.83it/s][A
 19%|        | 82/435 [00:02<00:08, 41.90it/s][A
 20%|        | 87/435 [00:02<00:08, 42.74it/s][A
 21%|        | 92/435 [00:02<00:07, 43.38it/s][A
 22%|       | 97/435 [00:02<00:07, 43.89it/s][A
 23%|       | 102/435 [00:02<00:07, 44.26it/s][A
 25%|       | 107/435 [00:02<00:07, 44.50it/s][A
 26%|       | 112/435 [00:03<00:07, 44.67it/s][A
 27%|       | 117/435 [00:03<00:07, 44.59it/s][A
 28%|       | 122/435 [00:03<00:07, 44.46it/s][A
 29%|       | 127/435 [00:03<00:06, 44.38it/s][A
 30%|       | 132/435 [00:03<00:06, 44.49it/s][A
 31%|      | 137/435 [00:03<00:06, 44.56it/s][A
 33%|      | 142/435 [00:03<00:06, 44.75it/s][A
 34%|      | 147/435 [00:03<00:07, 36.50it/s][A
 35%|      | 152/435 [00:04<00:07, 38.74it/s][A
 36%|      | 157/435 [00:04<00:06, 40.50it/s][A
 37%|      | 162/435 [00:04<00:06, 41.79it/s][A
 38%|      | 167/435 [00:04<00:06, 42.75it/s][A
 40%|      | 172/435 [00:04<00:06, 43.21it/s][A
 41%|      | 177/435 [00:04<00:05, 43.87it/s][A
 42%|     | 182/435 [00:04<00:05, 44.16it/s][A
 43%|     | 187/435 [00:04<00:05, 43.99it/s][A
 44%|     | 192/435 [00:04<00:05, 44.13it/s][A
 45%|     | 197/435 [00:05<00:05, 44.30it/s][A
 46%|     | 202/435 [00:05<00:05, 44.53it/s][A
 48%|     | 207/435 [00:05<00:05, 44.70it/s][A
 49%|     | 212/435 [00:05<00:04, 44.83it/s][A
 50%|     | 217/435 [00:05<00:04, 44.95it/s][A
 51%|     | 222/435 [00:05<00:04, 44.89it/s][A
 52%|    | 227/435 [00:05<00:04, 44.80it/s][A
 53%|    | 232/435 [00:05<00:04, 44.52it/s][A
 54%|    | 237/435 [00:05<00:04, 44.51it/s][A
 56%|    | 242/435 [00:06<00:04, 44.52it/s][A
 57%|    | 247/435 [00:06<00:04, 44.62it/s][A
 58%|    | 252/435 [00:06<00:04, 44.80it/s][A
 59%|    | 257/435 [00:06<00:03, 44.89it/s][A
 60%|    | 262/435 [00:06<00:03, 44.96it/s][A
 61%|   | 267/435 [00:06<00:03, 44.97it/s][A
 63%|   | 272/435 [00:06<00:03, 44.82it/s][A
 64%|   | 277/435 [00:07<00:03, 44.77it/s][A
 65%|   | 282/435 [00:07<00:05, 26.34it/s][A
 66%|   | 287/435 [00:07<00:04, 30.11it/s][A
 67%|   | 292/435 [00:07<00:04, 33.47it/s][A
 68%|   | 297/435 [00:07<00:03, 36.29it/s][A
 69%|   | 302/435 [00:07<00:03, 38.58it/s][A
 71%|   | 307/435 [00:07<00:03, 40.30it/s][A
 72%|  | 312/435 [00:07<00:03, 34.75it/s][A
 73%|  | 318/435 [00:08<00:03, 38.74it/s][A
 74%|  | 323/435 [00:08<00:02, 40.42it/s][A
 75%|  | 328/435 [00:08<00:02, 41.70it/s][A
 77%|  | 333/435 [00:08<00:02, 42.68it/s][A
 78%|  | 338/435 [00:08<00:02, 43.31it/s][A
 79%|  | 343/435 [00:08<00:02, 43.83it/s][A
 80%|  | 348/435 [00:08<00:01, 44.21it/s][A
 81%|  | 353/435 [00:08<00:01, 44.30it/s][A
 82%| | 358/435 [00:08<00:01, 44.10it/s][A
 83%| | 363/435 [00:09<00:01, 44.23it/s][A
 85%| | 368/435 [00:09<00:01, 44.43it/s][A
 86%| | 373/435 [00:09<00:01, 44.66it/s][A
 87%| | 378/435 [00:09<00:01, 44.77it/s][A
 88%| | 383/435 [00:09<00:01, 44.91it/s][A
 89%| | 388/435 [00:09<00:01, 44.94it/s][A
 90%| | 393/435 [00:09<00:00, 44.93it/s][A
 91%|| 398/435 [00:09<00:00, 44.82it/s][A
 93%|| 403/435 [00:10<00:00, 36.15it/s][A
 94%|| 408/435 [00:10<00:00, 38.49it/s][A
 95%|| 413/435 [00:10<00:00, 40.33it/s][A
 96%|| 418/435 [00:10<00:00, 41.67it/s][A
 97%|| 423/435 [00:10<00:00, 42.65it/s][A
 98%|| 428/435 [00:10<00:00, 43.36it/s][A
100%|| 433/435 [00:10<00:00, 43.89it/s][A
                                                 [A                                                 
100%|| 435/435 [00:10<00:00, 43.89it/s][A100%|| 805/805 [06:18<00:00,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:36:31,292 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-805
[INFO|configuration_utils.py:351] 2023-08-28 17:36:31,756 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-805/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:36:42,875 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-805/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:36:44,488 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-805/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:36:44,999 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-805/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:36:52,811 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:36:52,873 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-161 (score: 0.9974275827407837).
                                                 100%|| 805/805 [07:06<00:00,  3.57it/s]100%|| 805/805 [07:06<00:00,  1.89it/s]
[INFO|trainer.py:1894] 2023-08-28 17:37:18,840 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 17:37:19,050 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:37:27,074 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:37:28,793 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:37:29,326 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:37:30,810 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:31,001 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:31,001 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:31,001 >>   train_runtime            = 0:07:06.81
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:31,001 >>   train_samples            =      10300
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:31,001 >>   train_samples_per_second =     120.66
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:31,001 >>   train_steps_per_second   =      1.886
{'eval_loss': 0.9974275827407837, 'eval_runtime': 10.7695, 'eval_samples_per_second': 322.857, 'eval_steps_per_second': 40.392, 'epoch': 5.0}
{'train_runtime': 426.8181, 'train_samples_per_second': 120.66, 'train_steps_per_second': 1.886, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 17:37:32 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:37:32,329 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:37:32,329 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:37:32,329 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 55.55it/s]  3%|         | 12/435 [00:00<00:08, 49.40it/s]  4%|         | 17/435 [00:00<00:08, 47.45it/s]  5%|         | 22/435 [00:00<00:10, 38.95it/s]  6%|         | 28/435 [00:00<00:09, 42.54it/s]  8%|         | 33/435 [00:00<00:09, 43.52it/s]  9%|         | 38/435 [00:00<00:09, 44.05it/s] 10%|         | 43/435 [00:00<00:08, 44.56it/s] 11%|         | 48/435 [00:01<00:08, 44.91it/s] 12%|        | 53/435 [00:01<00:08, 45.13it/s] 13%|        | 58/435 [00:01<00:08, 45.28it/s] 14%|        | 63/435 [00:01<00:08, 45.22it/s] 16%|        | 68/435 [00:01<00:08, 44.95it/s] 17%|        | 73/435 [00:01<00:08, 42.14it/s] 18%|        | 78/435 [00:01<00:08, 43.12it/s] 19%|        | 83/435 [00:01<00:08, 43.77it/s] 20%|        | 88/435 [00:01<00:07, 44.33it/s] 21%|       | 93/435 [00:02<00:07, 44.70it/s] 23%|       | 98/435 [00:02<00:07, 44.94it/s] 24%|       | 103/435 [00:02<00:07, 45.06it/s] 25%|       | 108/435 [00:02<00:07, 45.19it/s] 26%|       | 113/435 [00:02<00:07, 44.88it/s] 27%|       | 118/435 [00:02<00:07, 44.96it/s] 28%|       | 123/435 [00:02<00:06, 44.97it/s] 29%|       | 128/435 [00:02<00:06, 45.20it/s] 31%|       | 133/435 [00:03<00:10, 28.28it/s] 32%|      | 139/435 [00:03<00:08, 33.17it/s] 33%|      | 144/435 [00:03<00:08, 35.99it/s] 34%|      | 149/435 [00:03<00:07, 38.24it/s] 35%|      | 154/435 [00:03<00:06, 40.25it/s] 37%|      | 159/435 [00:03<00:06, 41.60it/s] 38%|      | 164/435 [00:03<00:06, 42.70it/s] 39%|      | 169/435 [00:03<00:06, 43.44it/s] 40%|      | 174/435 [00:04<00:05, 43.95it/s] 41%|      | 179/435 [00:04<00:05, 44.03it/s] 42%|     | 184/435 [00:04<00:05, 44.32it/s] 43%|     | 189/435 [00:04<00:05, 44.67it/s] 45%|     | 194/435 [00:04<00:05, 44.97it/s] 46%|     | 199/435 [00:04<00:08, 29.24it/s] 47%|     | 204/435 [00:04<00:07, 32.74it/s] 48%|     | 209/435 [00:05<00:06, 35.76it/s] 49%|     | 214/435 [00:05<00:05, 38.16it/s] 50%|     | 219/435 [00:05<00:05, 40.06it/s] 51%|    | 224/435 [00:05<00:05, 41.51it/s] 53%|    | 229/435 [00:05<00:04, 42.58it/s] 54%|    | 234/435 [00:05<00:04, 43.25it/s] 55%|    | 239/435 [00:05<00:04, 43.45it/s] 56%|    | 244/435 [00:05<00:04, 43.70it/s] 57%|    | 249/435 [00:05<00:04, 44.13it/s] 58%|    | 254/435 [00:06<00:04, 44.54it/s] 60%|    | 259/435 [00:06<00:03, 44.80it/s] 61%|    | 264/435 [00:06<00:03, 45.01it/s] 62%|   | 269/435 [00:06<00:03, 45.17it/s] 63%|   | 274/435 [00:06<00:03, 45.27it/s] 64%|   | 279/435 [00:06<00:03, 45.25it/s] 65%|   | 284/435 [00:06<00:03, 45.10it/s] 66%|   | 289/435 [00:06<00:03, 44.92it/s] 68%|   | 294/435 [00:06<00:03, 45.03it/s] 69%|   | 299/435 [00:07<00:03, 44.96it/s] 70%|   | 304/435 [00:07<00:02, 45.08it/s] 71%|   | 309/435 [00:07<00:02, 45.29it/s] 72%|  | 314/435 [00:07<00:02, 45.29it/s] 73%|  | 319/435 [00:07<00:02, 45.28it/s] 74%|  | 324/435 [00:07<00:02, 45.32it/s] 76%|  | 329/435 [00:07<00:03, 30.61it/s] 77%|  | 334/435 [00:08<00:02, 34.01it/s] 78%|  | 339/435 [00:08<00:02, 36.83it/s] 79%|  | 344/435 [00:08<00:02, 39.10it/s] 80%|  | 349/435 [00:08<00:02, 40.86it/s] 81%| | 354/435 [00:08<00:01, 42.20it/s] 83%| | 359/435 [00:08<00:01, 43.08it/s] 84%| | 364/435 [00:08<00:01, 43.76it/s] 85%| | 369/435 [00:08<00:01, 43.86it/s] 86%| | 374/435 [00:08<00:01, 43.89it/s] 87%| | 379/435 [00:09<00:01, 44.13it/s] 88%| | 384/435 [00:09<00:01, 44.49it/s] 89%| | 389/435 [00:09<00:01, 44.81it/s] 91%| | 394/435 [00:09<00:00, 44.93it/s] 92%|| 399/435 [00:09<00:00, 45.04it/s] 93%|| 404/435 [00:09<00:00, 45.21it/s] 94%|| 409/435 [00:09<00:00, 45.30it/s] 95%|| 414/435 [00:09<00:00, 45.15it/s] 96%|| 419/435 [00:09<00:00, 44.93it/s] 97%|| 424/435 [00:10<00:00, 44.96it/s] 99%|| 429/435 [00:10<00:00, 45.08it/s]100%|| 434/435 [00:10<00:00, 45.14it/s]100%|| 435/435 [00:10<00:00, 42.44it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:37:42,596 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:42,596 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:42,596 >>   eval_loss               =     0.9974
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:42,596 >>   eval_runtime            = 0:00:10.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:42,596 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:42,596 >>   eval_samples_per_second =    338.649
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:42,597 >>   eval_steps_per_second   =     42.368
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:37:42,597 >>   perplexity              =     2.7113
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:18,134 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:18,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:18,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:18,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:18,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:38:18,898 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:38:18,899 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:38:19,827 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:38:21,217 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:38:21,469 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:24,073 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:24,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:24,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:24,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:24,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:38:25,139 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:38:25,179 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:38:25,785 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:38:26,088 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:38:26,088 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-644
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-483
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-322
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-805
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/generator/iter5/model/checkpoint-161
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.73it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.65it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:10,  1.60it/s]Extractor Predicting: 19it [00:11,  1.54it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:12,  1.62it/s]Extractor Predicting: 22it [00:13,  1.49it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.46it/s]Extractor Predicting: 28it [00:17,  1.50it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.44it/s]Extractor Predicting: 33it [00:20,  1.47it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:22,  1.67it/s]Extractor Predicting: 37it [00:23,  1.45it/s]Extractor Predicting: 38it [00:23,  1.48it/s]Extractor Predicting: 39it [00:24,  1.55it/s]Extractor Predicting: 40it [00:25,  1.59it/s]Extractor Predicting: 41it [00:25,  1.66it/s]Extractor Predicting: 42it [00:26,  1.32it/s]Extractor Predicting: 43it [00:27,  1.40it/s]Extractor Predicting: 44it [00:27,  1.47it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:30,  1.36it/s]Extractor Predicting: 48it [00:30,  1.45it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.66it/s]Extractor Predicting: 52it [00:33,  1.55it/s]Extractor Predicting: 53it [00:33,  1.58it/s]Extractor Predicting: 54it [00:34,  1.64it/s]Extractor Predicting: 55it [00:34,  1.64it/s]Extractor Predicting: 56it [00:35,  1.64it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:36,  1.58it/s]Extractor Predicting: 59it [00:37,  1.66it/s]Extractor Predicting: 60it [00:38,  1.46it/s]Extractor Predicting: 61it [00:38,  1.53it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:39,  1.64it/s]Extractor Predicting: 64it [00:40,  1.62it/s]Extractor Predicting: 65it [00:41,  1.66it/s]Extractor Predicting: 66it [00:41,  1.66it/s]Extractor Predicting: 67it [00:42,  1.68it/s]Extractor Predicting: 68it [00:42,  1.71it/s]Extractor Predicting: 69it [00:43,  1.73it/s]Extractor Predicting: 70it [00:44,  1.64it/s]Extractor Predicting: 71it [00:44,  1.66it/s]Extractor Predicting: 72it [00:45,  1.70it/s]Extractor Predicting: 73it [00:45,  1.71it/s]Extractor Predicting: 74it [00:46,  1.74it/s]Extractor Predicting: 75it [00:46,  1.78it/s]Extractor Predicting: 76it [00:48,  1.17it/s]Extractor Predicting: 77it [00:49,  1.27it/s]Extractor Predicting: 78it [00:49,  1.38it/s]Extractor Predicting: 79it [00:50,  1.45it/s]Extractor Predicting: 80it [00:51,  1.45it/s]Extractor Predicting: 81it [00:51,  1.49it/s]Extractor Predicting: 82it [00:52,  1.57it/s]Extractor Predicting: 83it [00:52,  1.62it/s]Extractor Predicting: 84it [00:53,  1.66it/s]Extractor Predicting: 85it [00:54,  1.29it/s]Extractor Predicting: 86it [00:55,  1.40it/s]Extractor Predicting: 87it [00:55,  1.47it/s]Extractor Predicting: 88it [00:56,  1.53it/s]Extractor Predicting: 89it [00:56,  1.61it/s]Extractor Predicting: 90it [00:57,  1.53it/s]Extractor Predicting: 91it [00:58,  1.63it/s]Extractor Predicting: 92it [00:58,  1.71it/s]Extractor Predicting: 93it [00:59,  1.75it/s]Extractor Predicting: 94it [00:59,  1.73it/s]Extractor Predicting: 95it [01:00,  1.79it/s]Extractor Predicting: 96it [01:01,  1.53it/s]Extractor Predicting: 97it [01:01,  1.59it/s]Extractor Predicting: 98it [01:02,  1.65it/s]Extractor Predicting: 99it [01:02,  1.65it/s]Extractor Predicting: 100it [01:03,  1.65it/s]Extractor Predicting: 101it [01:04,  1.31it/s]Extractor Predicting: 102it [01:05,  1.47it/s]Extractor Predicting: 103it [01:05,  1.58it/s]Extractor Predicting: 104it [01:06,  1.64it/s]Extractor Predicting: 105it [01:06,  1.69it/s]Extractor Predicting: 106it [01:07,  1.50it/s]Extractor Predicting: 107it [01:08,  1.59it/s]Extractor Predicting: 108it [01:08,  1.63it/s]Extractor Predicting: 109it [01:09,  1.68it/s]Extractor Predicting: 110it [01:09,  1.56it/s]Extractor Predicting: 111it [01:10,  1.63it/s]Extractor Predicting: 112it [01:11,  1.67it/s]Extractor Predicting: 113it [01:11,  1.66it/s]Extractor Predicting: 114it [01:12,  1.74it/s]Extractor Predicting: 115it [01:12,  1.76it/s]Extractor Predicting: 116it [01:13,  1.78it/s]Extractor Predicting: 117it [01:13,  1.81it/s]Extractor Predicting: 118it [01:14,  1.79it/s]Extractor Predicting: 119it [01:15,  1.16it/s]Extractor Predicting: 120it [01:16,  1.27it/s]Extractor Predicting: 121it [01:17,  1.37it/s]Extractor Predicting: 122it [01:17,  1.46it/s]Extractor Predicting: 123it [01:18,  1.47it/s]Extractor Predicting: 124it [01:19,  1.52it/s]Extractor Predicting: 125it [01:19,  1.58it/s]Extractor Predicting: 126it [01:20,  1.64it/s]Extractor Predicting: 127it [01:20,  1.65it/s]Extractor Predicting: 128it [01:21,  1.63it/s]Extractor Predicting: 129it [01:21,  1.66it/s]Extractor Predicting: 130it [01:22,  1.66it/s]Extractor Predicting: 131it [01:23,  1.68it/s]Extractor Predicting: 132it [01:23,  1.66it/s]Extractor Predicting: 133it [01:24,  1.60it/s]Extractor Predicting: 134it [01:24,  1.67it/s]Extractor Predicting: 135it [01:25,  1.69it/s]Extractor Predicting: 136it [01:26,  1.67it/s]Extractor Predicting: 137it [01:26,  1.72it/s]Extractor Predicting: 138it [01:27,  1.71it/s]Extractor Predicting: 139it [01:28,  1.33it/s]Extractor Predicting: 140it [01:29,  1.43it/s]Extractor Predicting: 141it [01:29,  1.52it/s]Extractor Predicting: 142it [01:30,  1.57it/s]Extractor Predicting: 143it [01:30,  1.64it/s]Extractor Predicting: 144it [01:31,  1.42it/s]Extractor Predicting: 144it [01:31,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:25,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:25,115 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:25,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:25,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:25,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:40:26,496 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:40:26,497 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:40:27,804 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:40:29,028 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:40:29,334 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:33,918 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:34,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:34,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:34,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:40:34,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:40:35,719 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:40:35,720 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:40:36,384 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:40:36,748 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:40:36,748 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.78it/s]Extractor Predicting: 9it [00:05,  1.74it/s]Extractor Predicting: 10it [00:05,  1.78it/s]Extractor Predicting: 11it [00:06,  1.53it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.70it/s]Extractor Predicting: 15it [00:08,  1.74it/s]Extractor Predicting: 16it [00:09,  1.45it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:12,  1.70it/s]Extractor Predicting: 21it [00:13,  1.41it/s]Extractor Predicting: 22it [00:13,  1.41it/s]Extractor Predicting: 23it [00:15,  1.01it/s]Extractor Predicting: 24it [00:16,  1.16it/s]Extractor Predicting: 25it [00:16,  1.29it/s]Extractor Predicting: 26it [00:17,  1.42it/s]Extractor Predicting: 27it [00:17,  1.35it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.55it/s]Extractor Predicting: 30it [00:19,  1.64it/s]Extractor Predicting: 31it [00:20,  1.67it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.58it/s]Extractor Predicting: 34it [00:22,  1.62it/s]Extractor Predicting: 35it [00:22,  1.66it/s]Extractor Predicting: 36it [00:23,  1.71it/s]Extractor Predicting: 37it [00:24,  1.27it/s]Extractor Predicting: 38it [00:25,  1.41it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:26,  1.57it/s]Extractor Predicting: 41it [00:26,  1.65it/s]Extractor Predicting: 42it [00:27,  1.46it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:29,  1.62it/s]Extractor Predicting: 46it [00:29,  1.64it/s]Extractor Predicting: 47it [00:30,  1.58it/s]Extractor Predicting: 48it [00:31,  1.62it/s]Extractor Predicting: 49it [00:31,  1.69it/s]Extractor Predicting: 50it [00:32,  1.71it/s]Extractor Predicting: 51it [00:32,  1.71it/s]Extractor Predicting: 52it [00:33,  1.70it/s]Extractor Predicting: 53it [00:34,  1.52it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:35,  1.65it/s]Extractor Predicting: 56it [00:35,  1.68it/s]Extractor Predicting: 57it [00:36,  1.69it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:38,  1.52it/s]Extractor Predicting: 60it [00:38,  1.60it/s]Extractor Predicting: 61it [00:39,  1.63it/s]Extractor Predicting: 62it [00:39,  1.66it/s]Extractor Predicting: 63it [00:40,  1.57it/s]Extractor Predicting: 64it [00:41,  1.61it/s]Extractor Predicting: 65it [00:41,  1.70it/s]Extractor Predicting: 66it [00:42,  1.68it/s]Extractor Predicting: 67it [00:42,  1.68it/s]Extractor Predicting: 68it [00:43,  1.53it/s]Extractor Predicting: 69it [00:44,  1.59it/s]Extractor Predicting: 70it [00:44,  1.59it/s]Extractor Predicting: 71it [00:45,  1.63it/s]Extractor Predicting: 72it [00:45,  1.64it/s]Extractor Predicting: 73it [00:46,  1.64it/s]Extractor Predicting: 74it [00:47,  1.68it/s]Extractor Predicting: 75it [00:47,  1.68it/s]Extractor Predicting: 76it [00:48,  1.66it/s]Extractor Predicting: 77it [00:48,  1.69it/s]Extractor Predicting: 78it [00:49,  1.73it/s]Extractor Predicting: 79it [00:50,  1.73it/s]Extractor Predicting: 80it [00:50,  1.80it/s]Extractor Predicting: 81it [00:51,  1.81it/s]Extractor Predicting: 82it [00:51,  1.72it/s]Extractor Predicting: 83it [00:52,  1.74it/s]Extractor Predicting: 84it [00:52,  1.74it/s]Extractor Predicting: 85it [00:53,  1.72it/s]Extractor Predicting: 86it [00:54,  1.68it/s]Extractor Predicting: 87it [00:54,  1.49it/s]Extractor Predicting: 88it [00:55,  1.57it/s]Extractor Predicting: 89it [00:56,  1.59it/s]Extractor Predicting: 90it [00:56,  1.59it/s]Extractor Predicting: 91it [00:57,  1.61it/s]Extractor Predicting: 92it [00:58,  1.23it/s]Extractor Predicting: 93it [00:59,  1.35it/s]Extractor Predicting: 94it [00:59,  1.46it/s]Extractor Predicting: 95it [01:00,  1.52it/s]Extractor Predicting: 96it [01:00,  1.58it/s]Extractor Predicting: 97it [01:01,  1.33it/s]Extractor Predicting: 98it [01:02,  1.40it/s]Extractor Predicting: 99it [01:03,  1.47it/s]Extractor Predicting: 100it [01:03,  1.56it/s]Extractor Predicting: 101it [01:04,  1.60it/s]Extractor Predicting: 102it [01:05,  1.39it/s]Extractor Predicting: 103it [01:05,  1.46it/s]Extractor Predicting: 104it [01:06,  1.51it/s]Extractor Predicting: 105it [01:07,  1.58it/s]Extractor Predicting: 106it [01:07,  1.61it/s]Extractor Predicting: 107it [01:08,  1.48it/s]Extractor Predicting: 108it [01:08,  1.56it/s]Extractor Predicting: 109it [01:09,  1.59it/s]Extractor Predicting: 110it [01:10,  1.61it/s]Extractor Predicting: 111it [01:10,  1.64it/s]Extractor Predicting: 112it [01:11,  1.43it/s]Extractor Predicting: 113it [01:12,  1.50it/s]Extractor Predicting: 114it [01:12,  1.58it/s]Extractor Predicting: 115it [01:13,  1.61it/s]Extractor Predicting: 116it [01:14,  1.59it/s]Extractor Predicting: 117it [01:14,  1.47it/s]Extractor Predicting: 118it [01:15,  1.55it/s]Extractor Predicting: 119it [01:16,  1.47it/s]Extractor Predicting: 120it [01:16,  1.53it/s]Extractor Predicting: 121it [01:17,  1.58it/s]Extractor Predicting: 122it [01:17,  1.65it/s]Extractor Predicting: 123it [01:18,  1.67it/s]Extractor Predicting: 124it [01:19,  1.61it/s]Extractor Predicting: 125it [01:19,  1.64it/s]Extractor Predicting: 126it [01:20,  1.73it/s]Extractor Predicting: 127it [01:20,  1.75it/s]Extractor Predicting: 128it [01:21,  1.73it/s]Extractor Predicting: 129it [01:21,  1.78it/s]Extractor Predicting: 130it [01:22,  1.70it/s]Extractor Predicting: 131it [01:23,  1.71it/s]Extractor Predicting: 132it [01:23,  1.74it/s]Extractor Predicting: 133it [01:24,  1.80it/s]Extractor Predicting: 134it [01:24,  1.83it/s]Extractor Predicting: 135it [01:25,  1.76it/s]Extractor Predicting: 136it [01:25,  1.69it/s]Extractor Predicting: 137it [01:26,  1.73it/s]Extractor Predicting: 138it [01:27,  1.79it/s]Extractor Predicting: 139it [01:27,  1.79it/s]Extractor Predicting: 140it [01:28,  1.78it/s]Extractor Predicting: 141it [01:28,  1.77it/s]Extractor Predicting: 142it [01:29,  1.66it/s]Extractor Predicting: 143it [01:30,  1.67it/s]Extractor Predicting: 144it [01:30,  1.67it/s]Extractor Predicting: 145it [01:31,  1.69it/s]Extractor Predicting: 146it [01:31,  1.73it/s]Extractor Predicting: 147it [01:32,  1.78it/s]Extractor Predicting: 148it [01:32,  1.80it/s]Extractor Predicting: 149it [01:33,  1.83it/s]Extractor Predicting: 150it [01:33,  1.87it/s]Extractor Predicting: 151it [01:34,  1.87it/s]Extractor Predicting: 152it [01:34,  1.88it/s]Extractor Predicting: 153it [01:35,  1.82it/s]Extractor Predicting: 154it [01:35,  1.88it/s]Extractor Predicting: 155it [01:36,  1.86it/s]Extractor Predicting: 156it [01:37,  1.63it/s]Extractor Predicting: 157it [01:37,  1.73it/s]Extractor Predicting: 158it [01:38,  1.77it/s]Extractor Predicting: 159it [01:38,  1.78it/s]Extractor Predicting: 160it [01:39,  1.88it/s]Extractor Predicting: 161it [01:39,  1.86it/s]Extractor Predicting: 162it [01:40,  1.84it/s]Extractor Predicting: 163it [01:41,  1.85it/s]Extractor Predicting: 164it [01:41,  1.87it/s]Extractor Predicting: 165it [01:42,  1.75it/s]Extractor Predicting: 166it [01:42,  1.82it/s]Extractor Predicting: 167it [01:43,  1.85it/s]Extractor Predicting: 168it [01:43,  1.84it/s]Extractor Predicting: 169it [01:44,  1.90it/s]Extractor Predicting: 170it [01:44,  1.89it/s]Extractor Predicting: 171it [01:45,  1.82it/s]Extractor Predicting: 172it [01:45,  1.85it/s]Extractor Predicting: 173it [01:46,  1.87it/s]Extractor Predicting: 174it [01:47,  1.79it/s]Extractor Predicting: 175it [01:47,  1.79it/s]Extractor Predicting: 176it [01:48,  1.77it/s]Extractor Predicting: 177it [01:48,  1.75it/s]Extractor Predicting: 178it [01:49,  1.70it/s]Extractor Predicting: 179it [01:50,  1.50it/s]Extractor Predicting: 180it [01:50,  1.58it/s]Extractor Predicting: 181it [01:51,  1.60it/s]Extractor Predicting: 182it [01:52,  1.63it/s]Extractor Predicting: 183it [01:52,  1.68it/s]Extractor Predicting: 184it [01:53,  1.53it/s]Extractor Predicting: 185it [01:53,  1.59it/s]Extractor Predicting: 186it [01:54,  1.64it/s]Extractor Predicting: 187it [01:55,  1.66it/s]Extractor Predicting: 188it [01:55,  1.65it/s]Extractor Predicting: 189it [01:56,  1.45it/s]Extractor Predicting: 190it [01:57,  1.50it/s]Extractor Predicting: 191it [01:57,  1.53it/s]Extractor Predicting: 192it [01:58,  1.57it/s]Extractor Predicting: 193it [01:59,  1.59it/s]Extractor Predicting: 194it [01:59,  1.43it/s]Extractor Predicting: 195it [02:00,  1.51it/s]Extractor Predicting: 196it [02:00,  1.59it/s]Extractor Predicting: 197it [02:01,  1.64it/s]Extractor Predicting: 198it [02:02,  1.66it/s]Extractor Predicting: 199it [02:02,  1.59it/s]Extractor Predicting: 200it [02:03,  1.59it/s]Extractor Predicting: 201it [02:04,  1.61it/s]Extractor Predicting: 202it [02:04,  1.59it/s]Extractor Predicting: 203it [02:05,  1.58it/s]Extractor Predicting: 204it [02:06,  1.50it/s]Extractor Predicting: 205it [02:06,  1.53it/s]Extractor Predicting: 206it [02:07,  1.56it/s]Extractor Predicting: 207it [02:07,  1.60it/s]Extractor Predicting: 208it [02:08,  1.59it/s]Extractor Predicting: 209it [02:09,  1.52it/s]Extractor Predicting: 210it [02:09,  1.54it/s]Extractor Predicting: 211it [02:10,  1.55it/s]Extractor Predicting: 212it [02:11,  1.59it/s]Extractor Predicting: 213it [02:11,  1.59it/s]Extractor Predicting: 214it [02:12,  1.57it/s]Extractor Predicting: 215it [02:13,  1.56it/s]Extractor Predicting: 216it [02:13,  1.58it/s]Extractor Predicting: 217it [02:14,  1.60it/s]Extractor Predicting: 218it [02:14,  1.60it/s]Extractor Predicting: 219it [02:15,  1.55it/s]Extractor Predicting: 220it [02:16,  1.53it/s]Extractor Predicting: 221it [02:16,  1.52it/s]Extractor Predicting: 222it [02:17,  1.53it/s]Extractor Predicting: 223it [02:18,  1.58it/s]Extractor Predicting: 224it [02:18,  1.59it/s]Extractor Predicting: 225it [02:19,  1.59it/s]Extractor Predicting: 226it [02:20,  1.57it/s]Extractor Predicting: 227it [02:20,  1.53it/s]Extractor Predicting: 228it [02:21,  1.55it/s]Extractor Predicting: 229it [02:22,  1.58it/s]Extractor Predicting: 230it [02:22,  1.62it/s]Extractor Predicting: 231it [02:23,  1.67it/s]Extractor Predicting: 232it [02:23,  1.53it/s]Extractor Predicting: 233it [02:24,  1.59it/s]Extractor Predicting: 234it [02:25,  1.59it/s]Extractor Predicting: 235it [02:25,  1.65it/s]Extractor Predicting: 236it [02:26,  1.67it/s]Extractor Predicting: 237it [02:26,  1.57it/s]Extractor Predicting: 238it [02:27,  1.62it/s]Extractor Predicting: 239it [02:28,  1.62it/s]Extractor Predicting: 240it [02:28,  1.69it/s]Extractor Predicting: 241it [02:29,  1.72it/s]Extractor Predicting: 242it [02:29,  1.68it/s]Extractor Predicting: 243it [02:30,  1.70it/s]Extractor Predicting: 244it [02:30,  1.78it/s]Extractor Predicting: 245it [02:31,  1.75it/s]Extractor Predicting: 246it [02:32,  1.73it/s]Extractor Predicting: 247it [02:32,  1.71it/s]Extractor Predicting: 248it [02:33,  1.66it/s]Extractor Predicting: 249it [02:33,  1.70it/s]Extractor Predicting: 250it [02:34,  1.70it/s]Extractor Predicting: 251it [02:35,  1.75it/s]Extractor Predicting: 252it [02:35,  1.79it/s]Extractor Predicting: 253it [02:36,  1.77it/s]Extractor Predicting: 254it [02:36,  1.70it/s]Extractor Predicting: 255it [02:37,  1.72it/s]Extractor Predicting: 256it [02:37,  1.71it/s]Extractor Predicting: 257it [02:38,  1.66it/s]Extractor Predicting: 258it [02:39,  1.67it/s]Extractor Predicting: 259it [02:39,  1.66it/s]Extractor Predicting: 260it [02:40,  1.65it/s]Extractor Predicting: 261it [02:41,  1.67it/s]Extractor Predicting: 262it [02:41,  1.69it/s]Extractor Predicting: 263it [02:42,  1.72it/s]Extractor Predicting: 264it [02:42,  1.70it/s]Extractor Predicting: 265it [02:43,  1.54it/s]Extractor Predicting: 266it [02:44,  1.56it/s]Extractor Predicting: 267it [02:44,  1.59it/s]Extractor Predicting: 268it [02:45,  1.60it/s]Extractor Predicting: 269it [02:45,  1.63it/s]Extractor Predicting: 270it [02:47,  1.29it/s]Extractor Predicting: 271it [02:47,  1.40it/s]Extractor Predicting: 272it [02:48,  1.50it/s]Extractor Predicting: 273it [02:48,  1.51it/s]Extractor Predicting: 274it [02:49,  1.54it/s]Extractor Predicting: 275it [02:50,  1.58it/s]Extractor Predicting: 276it [02:50,  1.60it/s]Extractor Predicting: 277it [02:51,  1.57it/s]Extractor Predicting: 278it [02:52,  1.41it/s]Extractor Predicting: 279it [02:52,  1.47it/s]Extractor Predicting: 280it [02:53,  1.53it/s]Extractor Predicting: 281it [02:54,  1.59it/s]Extractor Predicting: 282it [02:54,  1.49it/s]Extractor Predicting: 283it [02:55,  1.51it/s]Extractor Predicting: 284it [02:55,  1.61it/s]Extractor Predicting: 285it [02:56,  1.61it/s]Extractor Predicting: 286it [02:57,  1.66it/s]Extractor Predicting: 287it [02:57,  1.58it/s]Extractor Predicting: 288it [02:58,  1.60it/s]Extractor Predicting: 289it [02:59,  1.61it/s]Extractor Predicting: 290it [02:59,  1.62it/s]Extractor Predicting: 291it [03:00,  1.58it/s]Extractor Predicting: 292it [03:01,  1.54it/s]Extractor Predicting: 293it [03:01,  1.59it/s]Extractor Predicting: 294it [03:02,  1.58it/s]Extractor Predicting: 295it [03:02,  1.61it/s]Extractor Predicting: 296it [03:03,  1.60it/s]Extractor Predicting: 297it [03:04,  1.57it/s]Extractor Predicting: 298it [03:04,  1.57it/s]Extractor Predicting: 299it [03:05,  1.57it/s]Extractor Predicting: 300it [03:06,  1.61it/s]Extractor Predicting: 301it [03:06,  1.60it/s]Extractor Predicting: 302it [03:07,  1.56it/s]Extractor Predicting: 303it [03:07,  1.55it/s]Extractor Predicting: 304it [03:08,  1.56it/s]Extractor Predicting: 305it [03:09,  1.57it/s]Extractor Predicting: 306it [03:09,  1.63it/s]Extractor Predicting: 307it [03:10,  1.50it/s]Extractor Predicting: 308it [03:11,  1.56it/s]Extractor Predicting: 309it [03:11,  1.58it/s]Extractor Predicting: 310it [03:12,  1.62it/s]Extractor Predicting: 311it [03:12,  1.62it/s]Extractor Predicting: 312it [03:14,  1.28it/s]Extractor Predicting: 313it [03:14,  1.41it/s]Extractor Predicting: 314it [03:15,  1.51it/s]Extractor Predicting: 315it [03:15,  1.59it/s]Extractor Predicting: 316it [03:16,  1.60it/s]Extractor Predicting: 317it [03:17,  1.52it/s]Extractor Predicting: 318it [03:17,  1.56it/s]Extractor Predicting: 319it [03:18,  1.59it/s]Extractor Predicting: 320it [03:18,  1.60it/s]Extractor Predicting: 321it [03:19,  1.63it/s]Extractor Predicting: 322it [03:20,  1.48it/s]Extractor Predicting: 323it [03:21,  1.49it/s]Extractor Predicting: 324it [03:21,  1.53it/s]Extractor Predicting: 325it [03:22,  1.56it/s]Extractor Predicting: 326it [03:22,  1.60it/s]Extractor Predicting: 327it [03:23,  1.54it/s]Extractor Predicting: 328it [03:24,  1.57it/s]Extractor Predicting: 329it [03:24,  1.62it/s]Extractor Predicting: 330it [03:25,  1.64it/s]Extractor Predicting: 331it [03:25,  1.64it/s]Extractor Predicting: 332it [03:26,  1.58it/s]Extractor Predicting: 333it [03:27,  1.64it/s]Extractor Predicting: 334it [03:27,  1.65it/s]Extractor Predicting: 335it [03:28,  1.68it/s]Extractor Predicting: 336it [03:28,  1.66it/s]Extractor Predicting: 337it [03:29,  1.62it/s]Extractor Predicting: 338it [03:30,  1.65it/s]Extractor Predicting: 339it [03:30,  1.72it/s]Extractor Predicting: 340it [03:31,  1.74it/s]Extractor Predicting: 341it [03:31,  1.74it/s]Extractor Predicting: 342it [03:32,  1.72it/s]Extractor Predicting: 343it [03:33,  1.68it/s]Extractor Predicting: 344it [03:33,  1.72it/s]Extractor Predicting: 345it [03:34,  1.68it/s]Extractor Predicting: 346it [03:34,  1.69it/s]Extractor Predicting: 347it [03:35,  1.70it/s]Extractor Predicting: 348it [03:36,  1.71it/s]Extractor Predicting: 349it [03:36,  1.71it/s]Extractor Predicting: 350it [03:37,  1.73it/s]Extractor Predicting: 351it [03:37,  1.74it/s]Extractor Predicting: 352it [03:38,  1.71it/s]Extractor Predicting: 353it [03:38,  1.73it/s]Extractor Predicting: 354it [03:39,  1.77it/s]Extractor Predicting: 355it [03:40,  1.66it/s]Extractor Predicting: 356it [03:40,  1.67it/s]Extractor Predicting: 357it [03:41,  1.69it/s]Extractor Predicting: 358it [03:41,  1.72it/s]Extractor Predicting: 359it [03:42,  1.71it/s]Extractor Predicting: 360it [03:43,  1.27it/s]Extractor Predicting: 361it [03:44,  1.39it/s]Extractor Predicting: 362it [03:44,  1.49it/s]Extractor Predicting: 363it [03:45,  1.58it/s]Extractor Predicting: 364it [03:45,  1.65it/s]Extractor Predicting: 365it [03:46,  1.71it/s]Extractor Predicting: 366it [03:47,  1.71it/s]Extractor Predicting: 367it [03:47,  1.68it/s]Extractor Predicting: 368it [03:48,  1.58it/s]Extractor Predicting: 369it [03:48,  1.63it/s]Extractor Predicting: 370it [03:49,  1.71it/s]Extractor Predicting: 371it [03:49,  1.74it/s]Extractor Predicting: 372it [03:50,  1.76it/s]Extractor Predicting: 373it [03:51,  1.50it/s]Extractor Predicting: 374it [03:52,  1.53it/s]Extractor Predicting: 375it [03:52,  1.59it/s]Extractor Predicting: 376it [03:53,  1.65it/s]Extractor Predicting: 377it [03:53,  1.70it/s]Extractor Predicting: 378it [03:54,  1.75it/s]Extractor Predicting: 379it [03:54,  1.73it/s]Extractor Predicting: 380it [03:55,  1.41it/s]Extractor Predicting: 381it [03:56,  1.51it/s]Extractor Predicting: 382it [03:57,  1.57it/s]Extractor Predicting: 383it [03:57,  1.66it/s]Extractor Predicting: 384it [03:58,  1.73it/s]Extractor Predicting: 385it [03:59,  1.45it/s]Extractor Predicting: 386it [03:59,  1.54it/s]Extractor Predicting: 387it [04:00,  1.61it/s]Extractor Predicting: 388it [04:00,  1.64it/s]Extractor Predicting: 389it [04:01,  1.69it/s]Extractor Predicting: 390it [04:02,  1.38it/s]Extractor Predicting: 391it [04:02,  1.46it/s]Extractor Predicting: 392it [04:03,  1.55it/s]Extractor Predicting: 393it [04:03,  1.63it/s]Extractor Predicting: 394it [04:04,  1.60it/s]Extractor Predicting: 395it [04:05,  1.51it/s]Extractor Predicting: 396it [04:06,  1.53it/s]Extractor Predicting: 397it [04:06,  1.54it/s]Extractor Predicting: 398it [04:07,  1.56it/s]Extractor Predicting: 399it [04:07,  1.57it/s]Extractor Predicting: 400it [04:08,  1.55it/s]Extractor Predicting: 401it [04:09,  1.55it/s]Extractor Predicting: 402it [04:09,  1.54it/s]Extractor Predicting: 403it [04:10,  1.57it/s]Extractor Predicting: 404it [04:11,  1.52it/s]Extractor Predicting: 405it [04:12,  1.36it/s]Extractor Predicting: 406it [04:12,  1.42it/s]Extractor Predicting: 407it [04:13,  1.45it/s]Extractor Predicting: 408it [04:13,  1.51it/s]Extractor Predicting: 409it [04:14,  1.55it/s]Extractor Predicting: 410it [04:15,  1.45it/s]Extractor Predicting: 411it [04:16,  1.50it/s]Extractor Predicting: 412it [04:16,  1.53it/s]Extractor Predicting: 413it [04:17,  1.58it/s]Extractor Predicting: 414it [04:17,  1.61it/s]Extractor Predicting: 415it [04:18,  1.67it/s]Extractor Predicting: 416it [04:19,  1.57it/s]Extractor Predicting: 417it [04:19,  1.60it/s]Extractor Predicting: 418it [04:20,  1.61it/s]Extractor Predicting: 419it [04:20,  1.57it/s]Extractor Predicting: 420it [04:21,  1.58it/s]Extractor Predicting: 421it [04:22,  1.58it/s]Extractor Predicting: 421it [04:22,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:31,365 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:31,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:31,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:31,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:31,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:45:31,802 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:45:31,803 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:45:32,130 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:45:33,195 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:45:33,195 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:35,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:35,746 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:35,746 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:35,746 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:35,746 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:45:36,599 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:45:36,600 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:45:36,893 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:45:37,051 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:45:37,051 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:04,  1.25it/s]Extractor Predicting: 7it [00:05,  1.33it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:05,  1.90it/s]Extractor Predicting: 9it [00:05,  1.56it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_1', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:19<06:03, 19.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:36<05:25, 18.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:53<04:59, 17.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:10<04:38, 17.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:31<04:38, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:47<04:07, 17.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:04<03:46, 17.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:22<03:31, 17.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:37<03:07, 17.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:56<02:54, 17.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:13<02:36, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:29<02:15, 16.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:46<02:00, 17.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [04:01<01:38, 16.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:18<01:23, 16.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:35<01:06, 16.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:51<00:49, 16.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:09<00:34, 17.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:26<00:16, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:42<00:00, 16.83s/it]Generating: 100%|| 20/20 [05:42<00:00, 17.15s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 441, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 512, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7379807692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 234, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 625, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7512019230769231, 'errors': {'', "('Hans', 'head of government', '', 'He was succeeded by his brother , Dr . Hans .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : military branch .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('British ships', 'military branch', '', 'The Battle of Bataillon was the battle of the Battle of the Bastille , where British ships sank at least 1,400 French ships .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 181, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 274, 'raw': 384}
{'target': 600, 'success': 295, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 339, 'raw': 480}
{'target': 600, 'success': 361, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 428, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 602, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.6967592592592593, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('European Championships', 'winner', '', 'The previous year , he won the European Championships , and finished runner in 5th place in the category of medals .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : crosses . Context : Later in the year ( 11411231 ) he married Brigadier John B. Stoughton , sister of King James VI , the King of England . Head Entity : Robert Stoughton , Tail Entity : John B . Stoughton .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : crosses .', 'success_rate': 0.77, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', "('FreeBSD', 'operating system', '', 'The operating system is based on FreeBSD 2.1 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 519, 'raw': 704}
{'target': 600, 'success': 544, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : participant .', 'success_rate': 0.7415865384615384, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : position played on team / speciality . Context : Later in 2008 , he played in the United States national team squad for the 2002 FIFA World Cup and 2010 FIFA World Cup games . Head Entity : Walter , Tail Entity : United States national team .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 99, 'raw': 160}
{'target': 600, 'success': 119, 'raw': 192}
{'target': 600, 'success': 141, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 183, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 224, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 266, 'raw': 416}
{'target': 600, 'success': 287, 'raw': 448}
{'target': 600, 'success': 312, 'raw': 480}
{'target': 600, 'success': 330, 'raw': 512}
{'target': 600, 'success': 348, 'raw': 544}
{'target': 600, 'success': 367, 'raw': 576}
{'target': 600, 'success': 386, 'raw': 608}
{'target': 600, 'success': 404, 'raw': 640}
{'target': 600, 'success': 421, 'raw': 672}
{'target': 600, 'success': 448, 'raw': 704}
{'target': 600, 'success': 465, 'raw': 736}
{'target': 600, 'success': 482, 'raw': 768}
{'target': 600, 'success': 499, 'raw': 800}
{'target': 600, 'success': 525, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 574, 'raw': 896}
{'target': 600, 'success': 594, 'raw': 928}
{'target': 600, 'success': 611, 'raw': 960}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.6364583333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : publisher . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : season 2 , Tail Entity : the Walking Dead .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8288043478260869, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 17454
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17554, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:19, 19.45s/it]Extractor Estimating: 2it [00:21,  9.13s/it]Extractor Estimating: 3it [00:22,  5.26s/it]Extractor Estimating: 4it [00:22,  3.44s/it]Extractor Estimating: 5it [00:23,  2.45s/it]Extractor Estimating: 6it [00:23,  1.84s/it]Extractor Estimating: 7it [00:24,  1.42s/it]Extractor Estimating: 8it [00:25,  1.18s/it]Extractor Estimating: 9it [00:25,  1.02it/s]Extractor Estimating: 10it [00:26,  1.08it/s]Extractor Estimating: 11it [00:27,  1.22it/s]Extractor Estimating: 12it [00:27,  1.27it/s]Extractor Estimating: 13it [00:28,  1.36it/s]Extractor Estimating: 14it [00:29,  1.44it/s]Extractor Estimating: 15it [00:30,  1.24it/s]Extractor Estimating: 16it [00:30,  1.35it/s]Extractor Estimating: 17it [00:31,  1.41it/s]Extractor Estimating: 18it [00:31,  1.47it/s]Extractor Estimating: 19it [00:32,  1.51it/s]Extractor Estimating: 20it [00:33,  1.61it/s]Extractor Estimating: 21it [00:33,  1.67it/s]Extractor Estimating: 22it [00:34,  1.29it/s]Extractor Estimating: 23it [00:35,  1.39it/s]Extractor Estimating: 24it [00:36,  1.45it/s]Extractor Estimating: 25it [00:36,  1.53it/s]Extractor Estimating: 26it [00:37,  1.52it/s]Extractor Estimating: 27it [00:37,  1.53it/s]Extractor Estimating: 28it [00:39,  1.20it/s]Extractor Estimating: 29it [00:41,  1.21s/it]Extractor Estimating: 30it [00:41,  1.05s/it]Extractor Estimating: 31it [00:42,  1.07it/s]Extractor Estimating: 32it [00:43,  1.09it/s]Extractor Estimating: 33it [00:44,  1.21it/s]Extractor Estimating: 34it [00:44,  1.26it/s]Extractor Estimating: 35it [00:45,  1.33it/s]Extractor Estimating: 36it [00:46,  1.40it/s]Extractor Estimating: 37it [00:46,  1.32it/s]Extractor Estimating: 38it [00:47,  1.40it/s]Extractor Estimating: 39it [00:48,  1.46it/s]Extractor Estimating: 40it [00:48,  1.48it/s]Extractor Estimating: 41it [00:49,  1.51it/s]Extractor Estimating: 42it [00:50,  1.42it/s]Extractor Estimating: 43it [00:50,  1.42it/s]Extractor Estimating: 44it [00:51,  1.47it/s]Extractor Estimating: 45it [00:52,  1.50it/s]Extractor Estimating: 46it [00:52,  1.51it/s]Extractor Estimating: 47it [00:53,  1.44it/s]Extractor Estimating: 48it [00:54,  1.45it/s]Extractor Estimating: 49it [00:54,  1.52it/s]Extractor Estimating: 50it [00:55,  1.56it/s]Extractor Estimating: 51it [00:56,  1.60it/s]Extractor Estimating: 52it [00:57,  1.41it/s]Extractor Estimating: 53it [00:58,  1.17it/s]Extractor Estimating: 54it [00:58,  1.29it/s]Extractor Estimating: 55it [00:59,  1.43it/s]Extractor Estimating: 56it [01:00,  1.30it/s]Extractor Estimating: 57it [01:00,  1.41it/s]Extractor Estimating: 58it [01:01,  1.48it/s]Extractor Estimating: 59it [01:02,  1.51it/s]Extractor Estimating: 60it [01:02,  1.59it/s]Extractor Estimating: 61it [01:04,  1.14it/s]Extractor Estimating: 62it [01:04,  1.26it/s]Extractor Estimating: 63it [01:05,  1.33it/s]Extractor Estimating: 64it [01:05,  1.40it/s]Extractor Estimating: 65it [01:06,  1.47it/s]Extractor Estimating: 66it [01:07,  1.47it/s]Extractor Estimating: 67it [01:07,  1.49it/s]Extractor Estimating: 68it [01:08,  1.56it/s]Extractor Estimating: 69it [01:09,  1.54it/s]Extractor Estimating: 70it [01:09,  1.61it/s]Extractor Estimating: 71it [01:10,  1.45it/s]Extractor Estimating: 72it [01:11,  1.51it/s]Extractor Estimating: 73it [01:11,  1.48it/s]Extractor Estimating: 74it [01:12,  1.51it/s]Extractor Estimating: 75it [01:13,  1.54it/s]Extractor Estimating: 76it [01:13,  1.47it/s]Extractor Estimating: 77it [01:14,  1.47it/s]Extractor Estimating: 78it [01:15,  1.52it/s]Extractor Estimating: 79it [01:15,  1.52it/s]Extractor Estimating: 80it [01:16,  1.51it/s]Extractor Estimating: 81it [01:17,  1.46it/s]Extractor Estimating: 82it [01:17,  1.51it/s]Extractor Estimating: 83it [01:18,  1.53it/s]Extractor Estimating: 84it [01:19,  1.52it/s]Extractor Estimating: 85it [01:19,  1.54it/s]Extractor Estimating: 86it [01:20,  1.49it/s]Extractor Estimating: 87it [01:21,  1.51it/s]Extractor Estimating: 88it [01:21,  1.54it/s]Extractor Estimating: 89it [01:22,  1.61it/s]Extractor Estimating: 90it [01:22,  1.60it/s]Extractor Estimating: 91it [01:23,  1.43it/s]Extractor Estimating: 92it [01:24,  1.48it/s]Extractor Estimating: 93it [01:24,  1.53it/s]Extractor Estimating: 94it [01:25,  1.60it/s]Extractor Estimating: 95it [01:26,  1.46it/s]Extractor Estimating: 96it [01:27,  1.24it/s]Extractor Estimating: 97it [01:28,  1.32it/s]Extractor Estimating: 98it [01:28,  1.40it/s]Extractor Estimating: 99it [01:29,  1.44it/s]Extractor Estimating: 100it [01:29,  1.49it/s]Extractor Estimating: 101it [01:30,  1.36it/s]Extractor Estimating: 102it [01:31,  1.43it/s]Extractor Estimating: 103it [01:32,  1.45it/s]Extractor Estimating: 104it [01:32,  1.51it/s]Extractor Estimating: 105it [01:33,  1.56it/s]Extractor Estimating: 106it [01:34,  1.13it/s]Extractor Estimating: 107it [01:35,  1.25it/s]Extractor Estimating: 108it [01:35,  1.36it/s]Extractor Estimating: 109it [01:36,  1.49it/s]Extractor Estimating: 110it [01:37,  1.50it/s]Extractor Estimating: 111it [01:38,  1.11it/s]Extractor Estimating: 112it [01:39,  1.22it/s]Extractor Estimating: 113it [01:39,  1.34it/s]Extractor Estimating: 114it [01:40,  1.41it/s]Extractor Estimating: 115it [01:41,  1.12it/s]Extractor Estimating: 116it [01:42,  1.25it/s]Extractor Estimating: 117it [01:42,  1.32it/s]Extractor Estimating: 118it [01:43,  1.37it/s]Extractor Estimating: 119it [01:44,  1.34it/s]Extractor Estimating: 120it [01:44,  1.45it/s]Extractor Estimating: 121it [01:45,  1.49it/s]Extractor Estimating: 122it [01:46,  1.52it/s]Extractor Estimating: 123it [01:46,  1.55it/s]Extractor Estimating: 124it [01:47,  1.33it/s]Extractor Estimating: 125it [01:48,  1.43it/s]Extractor Estimating: 126it [01:49,  1.49it/s]Extractor Estimating: 127it [01:49,  1.50it/s]Extractor Estimating: 128it [01:50,  1.52it/s]Extractor Estimating: 129it [01:51,  1.20it/s]Extractor Estimating: 130it [01:52,  1.00it/s]Extractor Estimating: 131it [01:53,  1.13it/s]Extractor Estimating: 132it [01:54,  1.16it/s]Extractor Estimating: 133it [01:55,  1.22it/s]Extractor Estimating: 134it [01:55,  1.29it/s]Extractor Estimating: 135it [01:56,  1.34it/s]Extractor Estimating: 136it [01:57,  1.40it/s]Extractor Estimating: 137it [01:57,  1.31it/s]Extractor Estimating: 138it [01:58,  1.40it/s]Extractor Estimating: 139it [01:59,  1.44it/s]Extractor Estimating: 140it [01:59,  1.42it/s]Extractor Estimating: 141it [02:00,  1.47it/s]Extractor Estimating: 142it [02:01,  1.18it/s]Extractor Estimating: 143it [02:02,  1.23it/s]Extractor Estimating: 144it [02:03,  1.32it/s]Extractor Estimating: 145it [02:03,  1.42it/s]Extractor Estimating: 146it [02:05,  1.15it/s]Extractor Estimating: 147it [02:05,  1.20it/s]Extractor Estimating: 148it [02:06,  1.28it/s]Extractor Estimating: 149it [02:07,  1.36it/s]Extractor Estimating: 150it [02:07,  1.41it/s]Extractor Estimating: 151it [02:08,  1.27it/s]Extractor Estimating: 152it [02:09,  1.34it/s]Extractor Estimating: 153it [02:09,  1.44it/s]Extractor Estimating: 154it [02:10,  1.45it/s]Extractor Estimating: 155it [02:11,  1.52it/s]Extractor Estimating: 156it [02:12,  1.15it/s]Extractor Estimating: 157it [02:13,  1.24it/s]Extractor Estimating: 158it [02:13,  1.37it/s]Extractor Estimating: 159it [02:14,  1.42it/s]Extractor Estimating: 160it [02:15,  1.33it/s]Extractor Estimating: 161it [02:15,  1.43it/s]Extractor Estimating: 162it [02:16,  1.55it/s]Extractor Estimating: 163it [02:16,  1.63it/s]Extractor Estimating: 164it [02:17,  1.64it/s]Extractor Estimating: 165it [02:18,  1.36it/s]Extractor Estimating: 166it [02:19,  1.44it/s]Extractor Estimating: 167it [02:19,  1.50it/s]Extractor Estimating: 168it [02:20,  1.55it/s]Extractor Estimating: 169it [02:20,  1.63it/s]Extractor Estimating: 170it [02:21,  1.39it/s]Extractor Estimating: 171it [02:22,  1.44it/s]Extractor Estimating: 172it [02:23,  1.50it/s]Extractor Estimating: 173it [02:23,  1.56it/s]Extractor Estimating: 174it [02:24,  1.56it/s]Extractor Estimating: 175it [02:25,  1.38it/s]Extractor Estimating: 176it [02:26,  1.31it/s]Extractor Estimating: 177it [02:26,  1.36it/s]Extractor Estimating: 178it [02:27,  1.37it/s]Extractor Estimating: 179it [02:28,  1.35it/s]Extractor Estimating: 180it [02:28,  1.44it/s]Extractor Estimating: 181it [02:29,  1.50it/s]Extractor Estimating: 182it [02:29,  1.54it/s]Extractor Estimating: 183it [02:30,  1.52it/s]Extractor Estimating: 184it [02:31,  1.20it/s]Extractor Estimating: 185it [02:32,  1.25it/s]Extractor Estimating: 186it [02:33,  1.29it/s]Extractor Estimating: 187it [02:34,  1.35it/s]Extractor Estimating: 188it [02:34,  1.38it/s]Extractor Estimating: 189it [02:35,  1.38it/s]Extractor Estimating: 190it [02:36,  1.39it/s]Extractor Estimating: 191it [02:36,  1.47it/s]Extractor Estimating: 192it [02:37,  1.43it/s]Extractor Estimating: 193it [02:38,  1.40it/s]Extractor Estimating: 194it [02:38,  1.48it/s]Extractor Estimating: 195it [02:40,  1.05it/s]Extractor Estimating: 196it [02:41,  1.13it/s]Extractor Estimating: 197it [02:41,  1.23it/s]Extractor Estimating: 198it [02:42,  1.32it/s]Extractor Estimating: 199it [02:43,  1.23it/s]Extractor Estimating: 200it [02:43,  1.34it/s]Extractor Estimating: 201it [02:44,  1.41it/s]Extractor Estimating: 202it [02:45,  1.43it/s]Extractor Estimating: 203it [02:45,  1.42it/s]Extractor Estimating: 204it [02:46,  1.35it/s]Extractor Estimating: 205it [02:47,  1.41it/s]Extractor Estimating: 206it [02:48,  1.46it/s]Extractor Estimating: 207it [02:48,  1.52it/s]Extractor Estimating: 208it [02:49,  1.56it/s]Extractor Estimating: 209it [02:50,  1.30it/s]Extractor Estimating: 210it [02:51,  1.32it/s]Extractor Estimating: 211it [02:51,  1.42it/s]Extractor Estimating: 212it [02:52,  1.46it/s]Extractor Estimating: 213it [02:52,  1.42it/s]Extractor Estimating: 214it [02:53,  1.41it/s]Extractor Estimating: 215it [02:54,  1.45it/s]Extractor Estimating: 216it [02:55,  1.45it/s]Extractor Estimating: 217it [02:55,  1.48it/s]Extractor Estimating: 218it [02:56,  1.45it/s]Extractor Estimating: 219it [02:57,  1.42it/s]Extractor Estimating: 220it [02:57,  1.47it/s]Extractor Estimating: 221it [02:58,  1.48it/s]Extractor Estimating: 222it [02:59,  1.47it/s]Extractor Estimating: 223it [02:59,  1.43it/s]Extractor Estimating: 224it [03:00,  1.48it/s]Extractor Estimating: 225it [03:01,  1.51it/s]Extractor Estimating: 226it [03:01,  1.48it/s]Extractor Estimating: 227it [03:02,  1.47it/s]Extractor Estimating: 228it [03:03,  1.42it/s]Extractor Estimating: 229it [03:03,  1.46it/s]Extractor Estimating: 230it [03:04,  1.46it/s]Extractor Estimating: 231it [03:05,  1.47it/s]Extractor Estimating: 232it [03:05,  1.51it/s]Extractor Estimating: 233it [03:07,  1.06it/s]Extractor Estimating: 234it [03:09,  1.24s/it]Extractor Estimating: 235it [03:10,  1.29s/it]Extractor Estimating: 236it [03:11,  1.10s/it]Extractor Estimating: 237it [03:12,  1.05it/s]Extractor Estimating: 238it [03:12,  1.16it/s]Extractor Estimating: 239it [03:14,  1.00it/s]Extractor Estimating: 240it [03:14,  1.09it/s]Extractor Estimating: 241it [03:15,  1.21it/s]Extractor Estimating: 242it [03:15,  1.33it/s]Extractor Estimating: 243it [03:16,  1.29it/s]Extractor Estimating: 244it [03:17,  1.36it/s]Extractor Estimating: 245it [03:18,  1.41it/s]Extractor Estimating: 246it [03:18,  1.43it/s]Extractor Estimating: 247it [03:19,  1.49it/s]Extractor Estimating: 248it [03:20,  1.22it/s]Extractor Estimating: 249it [03:21,  1.30it/s]Extractor Estimating: 250it [03:21,  1.34it/s]Extractor Estimating: 251it [03:22,  1.29it/s]Extractor Estimating: 252it [03:24,  1.04it/s]Extractor Estimating: 253it [03:24,  1.14it/s]Extractor Estimating: 254it [03:25,  1.22it/s]Extractor Estimating: 255it [03:26,  1.30it/s]Extractor Estimating: 256it [03:27,  1.20it/s]Extractor Estimating: 257it [03:27,  1.25it/s]Extractor Estimating: 258it [03:28,  1.30it/s]Extractor Estimating: 259it [03:29,  1.34it/s]Extractor Estimating: 260it [03:29,  1.33it/s]Extractor Estimating: 261it [03:30,  1.36it/s]Extractor Estimating: 262it [03:31,  1.39it/s]Extractor Estimating: 263it [03:32,  1.39it/s]Extractor Estimating: 264it [03:32,  1.44it/s]Extractor Estimating: 265it [03:33,  1.41it/s]Extractor Estimating: 266it [03:34,  1.47it/s]Extractor Estimating: 267it [03:34,  1.49it/s]Extractor Estimating: 268it [03:35,  1.49it/s]Extractor Estimating: 269it [03:36,  1.51it/s]Extractor Estimating: 270it [03:36,  1.40it/s]Extractor Estimating: 271it [03:37,  1.28it/s]Extractor Estimating: 272it [03:38,  1.33it/s]Extractor Estimating: 273it [03:39,  1.41it/s]Extractor Estimating: 274it [03:39,  1.41it/s]Extractor Estimating: 275it [03:40,  1.40it/s]Extractor Estimating: 276it [03:41,  1.48it/s]Extractor Estimating: 277it [03:41,  1.52it/s]Extractor Estimating: 278it [03:42,  1.54it/s]Extractor Estimating: 279it [03:43,  1.55it/s]Extractor Estimating: 280it [03:43,  1.52it/s]Extractor Estimating: 281it [03:44,  1.56it/s]Extractor Estimating: 282it [03:44,  1.57it/s]Extractor Estimating: 283it [03:45,  1.61it/s]Extractor Estimating: 284it [03:46,  1.66it/s]Extractor Estimating: 285it [03:47,  1.37it/s]Extractor Estimating: 286it [03:47,  1.44it/s]Extractor Estimating: 287it [03:48,  1.45it/s]Extractor Estimating: 288it [03:48,  1.50it/s]Extractor Estimating: 289it [03:49,  1.53it/s]Extractor Estimating: 290it [03:50,  1.50it/s]Extractor Estimating: 291it [03:50,  1.58it/s]Extractor Estimating: 292it [03:51,  1.59it/s]Extractor Estimating: 293it [03:52,  1.58it/s]Extractor Estimating: 294it [03:52,  1.58it/s]Extractor Estimating: 295it [03:53,  1.42it/s]Extractor Estimating: 296it [03:54,  1.51it/s]Extractor Estimating: 297it [03:54,  1.51it/s]Extractor Estimating: 298it [03:55,  1.56it/s]Extractor Estimating: 299it [03:56,  1.58it/s]Extractor Estimating: 300it [03:56,  1.47it/s]Extractor Estimating: 301it [03:57,  1.53it/s]Extractor Estimating: 302it [03:58,  1.48it/s]Extractor Estimating: 303it [03:58,  1.49it/s]Extractor Estimating: 304it [03:59,  1.53it/s]Extractor Estimating: 305it [04:00,  1.53it/s]Extractor Estimating: 306it [04:00,  1.51it/s]Extractor Estimating: 307it [04:01,  1.49it/s]Extractor Estimating: 308it [04:02,  1.49it/s]Extractor Estimating: 309it [04:02,  1.54it/s]Extractor Estimating: 310it [04:03,  1.39it/s]Extractor Estimating: 311it [04:04,  1.47it/s]Extractor Estimating: 312it [04:04,  1.50it/s]Extractor Estimating: 313it [04:05,  1.54it/s]Extractor Estimating: 314it [04:06,  1.57it/s]Extractor Estimating: 315it [04:07,  1.11it/s]Extractor Estimating: 316it [04:08,  1.24it/s]Extractor Estimating: 317it [04:08,  1.33it/s]Extractor Estimating: 318it [04:09,  1.38it/s]Extractor Estimating: 319it [04:10,  1.43it/s]Extractor Estimating: 320it [04:10,  1.53it/s]Extractor Estimating: 321it [04:11,  1.22it/s]Extractor Estimating: 322it [04:12,  1.34it/s]Extractor Estimating: 323it [04:13,  1.43it/s]Extractor Estimating: 324it [04:13,  1.45it/s]Extractor Estimating: 325it [04:14,  1.34it/s]Extractor Estimating: 326it [04:15,  1.31it/s]Extractor Estimating: 327it [04:15,  1.43it/s]Extractor Estimating: 328it [04:16,  1.54it/s]Extractor Estimating: 329it [04:17,  1.51it/s]Extractor Estimating: 330it [04:18,  1.30it/s]Extractor Estimating: 331it [04:18,  1.31it/s]Extractor Estimating: 332it [04:19,  1.38it/s]Extractor Estimating: 333it [04:20,  1.49it/s]Extractor Estimating: 334it [04:20,  1.56it/s]Extractor Estimating: 335it [04:21,  1.56it/s]Extractor Estimating: 336it [04:21,  1.64it/s]Extractor Estimating: 337it [04:22,  1.66it/s]Extractor Estimating: 338it [04:23,  1.67it/s]Extractor Estimating: 339it [04:23,  1.60it/s]Extractor Estimating: 340it [04:25,  1.09it/s]Extractor Estimating: 341it [04:25,  1.22it/s]Extractor Estimating: 342it [04:26,  1.37it/s]Extractor Estimating: 343it [04:27,  1.43it/s]Extractor Estimating: 344it [04:27,  1.36it/s]Extractor Estimating: 345it [04:28,  1.44it/s]Extractor Estimating: 346it [04:29,  1.48it/s]Extractor Estimating: 347it [04:29,  1.55it/s]Extractor Estimating: 348it [04:30,  1.58it/s]Extractor Estimating: 349it [04:31,  1.48it/s]Extractor Estimating: 350it [04:31,  1.57it/s]Extractor Estimating: 351it [04:32,  1.60it/s]Extractor Estimating: 352it [04:32,  1.60it/s]Extractor Estimating: 353it [04:33,  1.61it/s]Extractor Estimating: 354it [04:34,  1.53it/s]Extractor Estimating: 355it [04:34,  1.56it/s]Extractor Estimating: 356it [04:35,  1.61it/s]Extractor Estimating: 357it [04:35,  1.64it/s]Extractor Estimating: 358it [04:36,  1.62it/s]Extractor Estimating: 359it [04:37,  1.59it/s]Extractor Estimating: 360it [04:37,  1.60it/s]Extractor Estimating: 361it [04:38,  1.63it/s]Extractor Estimating: 362it [04:38,  1.70it/s]Extractor Estimating: 363it [04:39,  1.73it/s]Extractor Estimating: 364it [04:40,  1.78it/s]Extractor Estimating: 365it [04:40,  1.76it/s]Extractor Estimating: 366it [04:41,  1.68it/s]Extractor Estimating: 367it [04:42,  1.46it/s]Extractor Estimating: 368it [04:42,  1.46it/s]Extractor Estimating: 369it [04:43,  1.53it/s]Extractor Estimating: 370it [04:44,  1.59it/s]Extractor Estimating: 371it [04:44,  1.64it/s]Extractor Estimating: 372it [04:45,  1.57it/s]Extractor Estimating: 373it [04:45,  1.63it/s]Extractor Estimating: 374it [04:46,  1.65it/s]Extractor Estimating: 375it [04:47,  1.65it/s]Extractor Estimating: 376it [04:47,  1.67it/s]Extractor Estimating: 377it [04:48,  1.63it/s]Extractor Estimating: 378it [04:48,  1.69it/s]Extractor Estimating: 379it [04:49,  1.70it/s]Extractor Estimating: 380it [04:49,  1.71it/s]Extractor Estimating: 381it [04:50,  1.68it/s]Extractor Estimating: 382it [04:51,  1.68it/s]Extractor Estimating: 383it [04:51,  1.60it/s]Extractor Estimating: 384it [04:52,  1.58it/s]Extractor Estimating: 385it [04:53,  1.60it/s]Extractor Estimating: 386it [04:53,  1.61it/s]Extractor Estimating: 387it [04:54,  1.66it/s]Extractor Estimating: 388it [04:55,  1.38it/s]Extractor Estimating: 389it [04:55,  1.49it/s]Extractor Estimating: 390it [04:56,  1.48it/s]Extractor Estimating: 391it [04:57,  1.54it/s]Extractor Estimating: 392it [04:57,  1.58it/s]Extractor Estimating: 393it [04:58,  1.43it/s]Extractor Estimating: 394it [04:59,  1.51it/s]Extractor Estimating: 395it [04:59,  1.56it/s]Extractor Estimating: 396it [05:00,  1.60it/s]Extractor Estimating: 397it [05:00,  1.60it/s]Extractor Estimating: 398it [05:01,  1.48it/s]Extractor Estimating: 399it [05:02,  1.53it/s]Extractor Estimating: 400it [05:02,  1.55it/s]Extractor Estimating: 401it [05:03,  1.56it/s]Extractor Estimating: 402it [05:04,  1.52it/s]Extractor Estimating: 403it [05:05,  1.46it/s]Extractor Estimating: 404it [05:05,  1.49it/s]Extractor Estimating: 405it [05:06,  1.53it/s]Extractor Estimating: 406it [05:06,  1.54it/s]Extractor Estimating: 407it [05:07,  1.55it/s]Extractor Estimating: 408it [05:08,  1.47it/s]Extractor Estimating: 409it [05:08,  1.57it/s]Extractor Estimating: 410it [05:09,  1.35it/s]Extractor Estimating: 411it [05:10,  1.42it/s]Extractor Estimating: 412it [05:11,  1.43it/s]Extractor Estimating: 413it [05:11,  1.56it/s]Extractor Estimating: 414it [05:12,  1.58it/s]Extractor Estimating: 415it [05:13,  1.34it/s]Extractor Estimating: 416it [05:13,  1.40it/s]Extractor Estimating: 417it [05:14,  1.46it/s]Extractor Estimating: 418it [05:15,  1.54it/s]Extractor Estimating: 419it [05:15,  1.43it/s]Extractor Estimating: 420it [05:16,  1.38it/s]Extractor Estimating: 421it [05:17,  1.47it/s]Extractor Estimating: 422it [05:17,  1.50it/s]Extractor Estimating: 423it [05:18,  1.51it/s]Extractor Estimating: 424it [05:19,  1.55it/s]Extractor Estimating: 425it [05:19,  1.49it/s]Extractor Estimating: 426it [05:20,  1.55it/s]Extractor Estimating: 427it [05:21,  1.54it/s]Extractor Estimating: 428it [05:21,  1.56it/s]Extractor Estimating: 429it [05:22,  1.59it/s]Extractor Estimating: 430it [05:23,  1.32it/s]Extractor Estimating: 431it [05:24,  1.40it/s]Extractor Estimating: 432it [05:24,  1.51it/s]Extractor Estimating: 433it [05:25,  1.57it/s]Extractor Estimating: 434it [05:25,  1.57it/s]Extractor Estimating: 435it [05:26,  1.54it/s]Extractor Estimating: 436it [05:27,  1.56it/s]Extractor Estimating: 437it [05:27,  1.60it/s]Extractor Estimating: 438it [05:28,  1.60it/s]Extractor Estimating: 439it [05:28,  1.61it/s]Extractor Estimating: 440it [05:30,  1.18it/s]Extractor Estimating: 441it [05:30,  1.32it/s]Extractor Estimating: 442it [05:31,  1.38it/s]Extractor Estimating: 443it [05:32,  1.45it/s]Extractor Estimating: 444it [05:33,  1.18it/s]Extractor Estimating: 445it [05:33,  1.31it/s]Extractor Estimating: 446it [05:34,  1.40it/s]Extractor Estimating: 447it [05:35,  1.49it/s]Extractor Estimating: 448it [05:35,  1.51it/s]Extractor Estimating: 449it [05:36,  1.50it/s]Extractor Estimating: 450it [05:37,  1.49it/s]Extractor Estimating: 451it [05:37,  1.52it/s]Extractor Estimating: 452it [05:38,  1.47it/s]Extractor Estimating: 453it [05:39,  1.36it/s]Extractor Estimating: 454it [05:40,  1.31it/s]Extractor Estimating: 455it [05:40,  1.39it/s]Extractor Estimating: 456it [05:41,  1.47it/s]Extractor Estimating: 457it [05:41,  1.48it/s]Extractor Estimating: 458it [05:42,  1.49it/s]Extractor Estimating: 459it [05:43,  1.34it/s]Extractor Estimating: 460it [05:44,  1.37it/s]Extractor Estimating: 461it [05:44,  1.43it/s]Extractor Estimating: 462it [05:45,  1.44it/s]Extractor Estimating: 463it [05:46,  1.47it/s]Extractor Estimating: 464it [05:47,  1.35it/s]Extractor Estimating: 465it [05:47,  1.36it/s]Extractor Estimating: 466it [05:48,  1.39it/s]Extractor Estimating: 467it [05:49,  1.41it/s]Extractor Estimating: 468it [05:49,  1.42it/s]Extractor Estimating: 469it [05:50,  1.37it/s]Extractor Estimating: 470it [05:51,  1.44it/s]Extractor Estimating: 471it [05:51,  1.44it/s]Extractor Estimating: 472it [05:52,  1.43it/s]Extractor Estimating: 473it [05:53,  1.44it/s]Extractor Estimating: 474it [05:54,  1.43it/s]Extractor Estimating: 475it [05:54,  1.48it/s]Extractor Estimating: 476it [05:55,  1.52it/s]Extractor Estimating: 477it [05:55,  1.56it/s]Extractor Estimating: 478it [05:56,  1.55it/s]Extractor Estimating: 479it [05:57,  1.59it/s]Extractor Estimating: 480it [05:57,  1.61it/s]Extractor Estimating: 481it [05:58,  1.65it/s]Extractor Estimating: 482it [05:58,  1.64it/s]Extractor Estimating: 483it [05:59,  1.59it/s]Extractor Estimating: 484it [06:00,  1.53it/s]Extractor Estimating: 485it [06:00,  1.58it/s]Extractor Estimating: 486it [06:01,  1.52it/s]Extractor Estimating: 487it [06:02,  1.56it/s]Extractor Estimating: 488it [06:02,  1.62it/s]Extractor Estimating: 489it [06:03,  1.54it/s]Extractor Estimating: 490it [06:04,  1.54it/s]Extractor Estimating: 491it [06:04,  1.61it/s]Extractor Estimating: 492it [06:05,  1.57it/s]Extractor Estimating: 493it [06:06,  1.57it/s]Extractor Estimating: 494it [06:06,  1.48it/s]Extractor Estimating: 495it [06:07,  1.56it/s]Extractor Estimating: 496it [06:08,  1.55it/s]Extractor Estimating: 497it [06:08,  1.59it/s]Extractor Estimating: 498it [06:09,  1.59it/s]Extractor Estimating: 499it [06:10,  1.33it/s]Extractor Estimating: 500it [06:10,  1.45it/s]Extractor Estimating: 500it [06:10,  1.35it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9934 mean pseudo reward: 0.916518326605689
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 30983
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31083, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_15_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31083, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.246, loss:1072.2823
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.973, loss:1049.9722
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.052, loss:998.9546
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.966, loss:1042.0491
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 86, avg_time 0.975, loss:974.9484
>> valid entity prec:0.5697, rec:0.4994, f1:0.5322
>> valid relation prec:0.0337, rec:0.0095, f1:0.0148
>> valid relation with NER prec:0.0337, rec:0.0095, f1:0.0148
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 186, avg_time 2.295, loss:977.3806
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 286, avg_time 0.984, loss:979.0209
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 386, avg_time 0.958, loss:969.5898
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 72, avg_time 0.979, loss:936.0026
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 172, avg_time 0.985, loss:948.4454
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5920, rec:0.5515, f1:0.5710
>> valid relation prec:0.0561, rec:0.0216, f1:0.0312
>> valid relation with NER prec:0.0561, rec:0.0216, f1:0.0312
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 272, avg_time 2.265, loss:966.3780
g_step 1200, step 372, avg_time 0.969, loss:975.3686
g_step 1300, step 58, avg_time 0.968, loss:926.4202
g_step 1400, step 158, avg_time 0.967, loss:893.3245
g_step 1500, step 258, avg_time 0.977, loss:877.5386
>> valid entity prec:0.5613, rec:0.6077, f1:0.5836
>> valid relation prec:0.1126, rec:0.0587, f1:0.0771
>> valid relation with NER prec:0.1126, rec:0.0587, f1:0.0771
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 358, avg_time 2.246, loss:924.8894
g_step 1700, step 44, avg_time 0.983, loss:866.9241
g_step 1800, step 144, avg_time 0.991, loss:827.4906
g_step 1900, step 244, avg_time 0.972, loss:861.0248
g_step 2000, step 344, avg_time 0.970, loss:894.9333
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5104, rec:0.5539, f1:0.5312
>> valid relation prec:0.0528, rec:0.0247, f1:0.0337
>> valid relation with NER prec:0.0528, rec:0.0247, f1:0.0337
g_step 2100, step 30, avg_time 2.207, loss:870.1582
g_step 2200, step 130, avg_time 0.980, loss:781.5284
g_step 2300, step 230, avg_time 0.977, loss:818.3188
g_step 2400, step 330, avg_time 0.974, loss:849.8660
g_step 2500, step 16, avg_time 0.985, loss:812.9641
>> valid entity prec:0.5356, rec:0.5699, f1:0.5522
>> valid relation prec:0.0965, rec:0.0466, f1:0.0628
>> valid relation with NER prec:0.0965, rec:0.0466, f1:0.0628
g_step 2600, step 116, avg_time 2.219, loss:739.9207
g_step 2700, step 216, avg_time 0.970, loss:789.5988
g_step 2800, step 316, avg_time 0.985, loss:787.2245
g_step 2900, step 2, avg_time 0.980, loss:789.3428
g_step 3000, step 102, avg_time 0.987, loss:734.1795
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5519, rec:0.5414, f1:0.5466
>> valid relation prec:0.0882, rec:0.0411, f1:0.0561
>> valid relation with NER prec:0.0882, rec:0.0411, f1:0.0561
g_step 3100, step 202, avg_time 2.206, loss:729.8624
g_step 3200, step 302, avg_time 0.970, loss:736.2036
g_step 3300, step 402, avg_time 0.962, loss:763.5894
g_step 3400, step 88, avg_time 0.967, loss:682.5043
g_step 3500, step 188, avg_time 0.963, loss:725.9387
>> valid entity prec:0.5952, rec:0.5306, f1:0.5611
>> valid relation prec:0.1198, rec:0.0460, f1:0.0665
>> valid relation with NER prec:0.1198, rec:0.0460, f1:0.0665
g_step 3600, step 288, avg_time 2.180, loss:707.8260
g_step 3700, step 388, avg_time 0.965, loss:702.3439
g_step 3800, step 74, avg_time 0.970, loss:679.9310
g_step 3900, step 174, avg_time 0.974, loss:665.5636
g_step 4000, step 274, avg_time 0.987, loss:696.9766
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5854, rec:0.5048, f1:0.5421
>> valid relation prec:0.0740, rec:0.0325, f1:0.0452
>> valid relation with NER prec:0.0740, rec:0.0325, f1:0.0452
g_step 4100, step 374, avg_time 2.199, loss:670.6087
g_step 4200, step 60, avg_time 0.973, loss:653.1556
g_step 4300, step 160, avg_time 0.990, loss:632.7446
g_step 4400, step 260, avg_time 0.967, loss:635.1386
g_step 4500, step 360, avg_time 0.972, loss:687.5004
>> valid entity prec:0.5386, rec:0.5295, f1:0.5340
>> valid relation prec:0.0695, rec:0.0403, f1:0.0510
>> valid relation with NER prec:0.0695, rec:0.0403, f1:0.0510
g_step 4600, step 46, avg_time 2.214, loss:608.3842
g_step 4700, step 146, avg_time 0.981, loss:617.4387
g_step 4800, step 246, avg_time 0.980, loss:610.5809
g_step 4900, step 346, avg_time 0.973, loss:616.2383
g_step 5000, step 32, avg_time 0.977, loss:603.4299
learning rate was adjusted to 0.0008
>> valid entity prec:0.5473, rec:0.4965, f1:0.5207
>> valid relation prec:0.0875, rec:0.0394, f1:0.0543
>> valid relation with NER prec:0.0875, rec:0.0394, f1:0.0543
g_step 5100, step 132, avg_time 2.204, loss:578.6849
g_step 5200, step 232, avg_time 0.990, loss:590.5033
g_step 5300, step 332, avg_time 0.975, loss:608.4300
g_step 5400, step 18, avg_time 0.969, loss:595.8226
g_step 5500, step 118, avg_time 0.967, loss:551.5657
>> valid entity prec:0.5138, rec:0.5227, f1:0.5182
>> valid relation prec:0.0708, rec:0.0397, f1:0.0509
>> valid relation with NER prec:0.0708, rec:0.0397, f1:0.0509
g_step 5600, step 218, avg_time 2.198, loss:570.0926
g_step 5700, step 318, avg_time 0.987, loss:562.9528
g_step 5800, step 4, avg_time 0.976, loss:582.6724
g_step 5900, step 104, avg_time 0.975, loss:508.3032
g_step 6000, step 204, avg_time 0.967, loss:553.6887
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5557, rec:0.5046, f1:0.5289
>> valid relation prec:0.0757, rec:0.0368, f1:0.0495
>> valid relation with NER prec:0.0757, rec:0.0368, f1:0.0495
g_step 6100, step 304, avg_time 2.219, loss:559.1475
g_step 6200, step 404, avg_time 0.977, loss:568.3417
g_step 6300, step 90, avg_time 0.964, loss:502.0332
g_step 6400, step 190, avg_time 0.972, loss:515.0257
g_step 6500, step 290, avg_time 0.980, loss:538.1304
>> valid entity prec:0.5446, rec:0.5400, f1:0.5423
>> valid relation prec:0.1066, rec:0.0624, f1:0.0787
>> valid relation with NER prec:0.1066, rec:0.0624, f1:0.0787
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6600, step 390, avg_time 2.213, loss:541.9595
g_step 6700, step 76, avg_time 0.977, loss:504.5501
g_step 6800, step 176, avg_time 0.982, loss:496.2877
g_step 6900, step 276, avg_time 0.983, loss:521.0170
g_step 7000, step 376, avg_time 0.978, loss:520.3943
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5444, rec:0.5211, f1:0.5325
>> valid relation prec:0.0739, rec:0.0397, f1:0.0516
>> valid relation with NER prec:0.0739, rec:0.0397, f1:0.0516
g_step 7100, step 62, avg_time 2.212, loss:470.0291
g_step 7200, step 162, avg_time 0.975, loss:490.3160
g_step 7300, step 262, avg_time 0.979, loss:475.3446
g_step 7400, step 362, avg_time 0.971, loss:496.2478
g_step 7500, step 48, avg_time 0.970, loss:488.7657
>> valid entity prec:0.4953, rec:0.5019, f1:0.4986
>> valid relation prec:0.0860, rec:0.0477, f1:0.0614
>> valid relation with NER prec:0.0860, rec:0.0477, f1:0.0614
g_step 7600, step 148, avg_time 2.205, loss:456.9405
g_step 7700, step 248, avg_time 0.990, loss:480.8590
g_step 7800, step 348, avg_time 0.969, loss:464.2567
g_step 7900, step 34, avg_time 0.980, loss:458.4334
g_step 8000, step 134, avg_time 0.969, loss:438.5896
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5338, rec:0.5608, f1:0.5470
>> valid relation prec:0.0864, rec:0.0512, f1:0.0643
>> valid relation with NER prec:0.0864, rec:0.0512, f1:0.0643
g_step 8100, step 234, avg_time 2.201, loss:439.1216
g_step 8200, step 334, avg_time 0.976, loss:489.6797
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:18:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:18:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-18-12_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:18:15 - WARNING - datasets.builder -   Using custom data configuration default-c2ef05a536031ae5
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c2ef05a536031ae5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:18:27,740 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:18:28,394 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:18:28,394 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:18:28,395 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:18:28,824 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:18:28,933 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:18:28,933 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:18:28,933 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:18:28,933 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:18:28,933 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:18:28,933 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:18:29,673 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:18:33,136 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:18:33,245 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c2ef05a536031ae5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 21:18:33 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14aec318bef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:04,  1.85ba/s] 20%|        | 2/10 [00:00<00:02,  2.86ba/s] 30%|       | 3/10 [00:00<00:02,  3.42ba/s] 40%|      | 4/10 [00:01<00:01,  3.75ba/s] 50%|     | 5/10 [00:01<00:01,  3.98ba/s] 60%|    | 6/10 [00:01<00:00,  4.13ba/s] 70%|   | 7/10 [00:01<00:00,  4.23ba/s] 80%|  | 8/10 [00:02<00:00,  4.30ba/s] 90%| | 9/10 [00:02<00:00,  4.34ba/s]100%|| 10/10 [00:02<00:00,  4.37ba/s]100%|| 10/10 [00:02<00:00,  3.91ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.43ba/s] 50%|     | 2/4 [00:00<00:00,  3.96ba/s] 75%|  | 3/4 [00:00<00:00,  4.17ba/s]100%|| 4/4 [00:00<00:00,  5.31ba/s]100%|| 4/4 [00:00<00:00,  4.70ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  5.51ba/s] 30%|       | 3/10 [00:00<00:00,  8.55ba/s] 50%|     | 5/10 [00:00<00:00,  9.51ba/s] 70%|   | 7/10 [00:00<00:00, 10.01ba/s] 90%| | 9/10 [00:00<00:00, 10.16ba/s]100%|| 10/10 [00:01<00:00,  9.70ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:02,  1.27ba/s] 50%|     | 2/4 [00:00<00:00,  2.53ba/s] 75%|  | 3/4 [00:01<00:00,  3.79ba/s]100%|| 4/4 [00:01<00:00,  3.76ba/s]
[INFO|trainer.py:414] 2023-08-28 21:18:43,434 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:18:43,616 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:18:43,616 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-28 21:18:43,617 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:18:43,617 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:18:43,617 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:18:43,617 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:18:43,617 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:56,  3.29it/s]  0%|          | 2/780 [00:01<13:14,  1.02s/it]  0%|          | 3/780 [00:02<08:54,  1.45it/s]  1%|          | 4/780 [00:02<06:53,  1.88it/s]  1%|          | 5/780 [00:02<05:45,  2.24it/s]  1%|          | 6/780 [00:03<09:16,  1.39it/s]  1%|          | 7/780 [00:04<07:28,  1.72it/s]  1%|          | 8/780 [00:04<06:17,  2.05it/s]  1%|          | 9/780 [00:04<05:29,  2.34it/s]  1%|         | 10/780 [00:05<04:57,  2.59it/s]  1%|         | 11/780 [00:05<04:35,  2.79it/s]  2%|         | 12/780 [00:05<04:19,  2.96it/s]  2%|         | 13/780 [00:06<04:09,  3.08it/s]  2%|         | 14/780 [00:06<04:01,  3.17it/s]  2%|         | 15/780 [00:06<03:56,  3.24it/s]  2%|         | 16/780 [00:07<04:31,  2.82it/s]  2%|         | 17/780 [00:07<04:16,  2.97it/s]  2%|         | 18/780 [00:07<04:06,  3.09it/s]  2%|         | 19/780 [00:07<03:59,  3.18it/s]  3%|         | 20/780 [00:08<03:54,  3.24it/s]  3%|         | 21/780 [00:08<03:51,  3.28it/s]  3%|         | 22/780 [00:08<03:48,  3.32it/s]  3%|         | 23/780 [00:09<03:46,  3.35it/s]  3%|         | 24/780 [00:09<03:45,  3.36it/s]  3%|         | 25/780 [00:09<03:44,  3.37it/s]  3%|         | 26/780 [00:10<04:26,  2.83it/s]  3%|         | 27/780 [00:10<04:12,  2.98it/s]  4%|         | 28/780 [00:10<04:02,  3.10it/s]  4%|         | 29/780 [00:11<03:56,  3.18it/s]  4%|         | 30/780 [00:11<03:51,  3.24it/s]  4%|         | 31/780 [00:11<03:47,  3.29it/s]  4%|         | 32/780 [00:11<03:45,  3.32it/s]  4%|         | 33/780 [00:12<03:43,  3.34it/s]  4%|         | 34/780 [00:12<03:42,  3.36it/s]  4%|         | 35/780 [00:12<03:41,  3.37it/s]  5%|         | 36/780 [00:13<03:55,  3.16it/s]  5%|         | 37/780 [00:13<03:50,  3.23it/s]  5%|         | 38/780 [00:13<03:46,  3.28it/s]  5%|         | 39/780 [00:14<03:43,  3.31it/s]  5%|         | 40/780 [00:14<03:51,  3.20it/s]  5%|         | 41/780 [00:14<03:46,  3.26it/s]  5%|         | 42/780 [00:15<03:43,  3.30it/s]  6%|         | 43/780 [00:15<03:41,  3.33it/s]  6%|         | 44/780 [00:15<03:39,  3.35it/s]  6%|         | 45/780 [00:15<03:38,  3.36it/s]  6%|         | 46/780 [00:16<04:09,  2.94it/s]  6%|         | 47/780 [00:16<03:59,  3.06it/s]  6%|         | 48/780 [00:16<03:52,  3.15it/s]  6%|         | 49/780 [00:17<03:46,  3.22it/s]  6%|         | 50/780 [00:17<03:43,  3.27it/s]  7%|         | 51/780 [00:17<03:40,  3.31it/s]  7%|         | 52/780 [00:18<03:38,  3.33it/s]  7%|         | 53/780 [00:18<03:36,  3.35it/s]  7%|         | 54/780 [00:18<03:35,  3.36it/s]  7%|         | 55/780 [00:18<03:34,  3.37it/s]  7%|         | 56/780 [00:19<04:05,  2.95it/s]  7%|         | 57/780 [00:19<03:55,  3.07it/s]  7%|         | 58/780 [00:20<03:48,  3.16it/s]  8%|         | 59/780 [00:20<03:42,  3.23it/s]  8%|         | 60/780 [00:20<03:38,  3.30it/s]  8%|         | 61/780 [00:20<03:35,  3.34it/s]  8%|         | 62/780 [00:21<03:32,  3.37it/s]  8%|         | 63/780 [00:21<03:31,  3.39it/s]  8%|         | 64/780 [00:21<03:29,  3.41it/s]  8%|         | 65/780 [00:22<03:29,  3.42it/s]  8%|         | 66/780 [00:22<04:14,  2.81it/s]  9%|         | 67/780 [00:22<03:59,  2.97it/s]  9%|         | 68/780 [00:23<03:49,  3.10it/s]  9%|         | 69/780 [00:23<03:42,  3.20it/s]  9%|         | 70/780 [00:23<03:37,  3.27it/s]  9%|         | 71/780 [00:24<03:33,  3.32it/s]  9%|         | 72/780 [00:24<03:30,  3.36it/s]  9%|         | 73/780 [00:24<03:28,  3.38it/s]  9%|         | 74/780 [00:24<03:27,  3.40it/s] 10%|         | 75/780 [00:25<03:26,  3.41it/s] 10%|         | 76/780 [00:25<04:01,  2.91it/s] 10%|         | 77/780 [00:25<03:50,  3.06it/s] 10%|         | 78/780 [00:26<03:42,  3.16it/s] 10%|         | 79/780 [00:26<03:36,  3.24it/s] 10%|         | 80/780 [00:26<03:32,  3.30it/s] 10%|         | 81/780 [00:27<03:29,  3.34it/s] 11%|         | 82/780 [00:27<03:26,  3.37it/s] 11%|         | 83/780 [00:27<03:25,  3.40it/s] 11%|         | 84/780 [00:27<03:24,  3.41it/s] 11%|         | 85/780 [00:28<03:23,  3.42it/s] 11%|         | 86/780 [00:28<03:36,  3.21it/s] 11%|         | 87/780 [00:28<03:31,  3.28it/s] 11%|        | 88/780 [00:29<03:28,  3.33it/s] 11%|        | 89/780 [00:29<03:25,  3.36it/s] 12%|        | 90/780 [00:29<03:23,  3.38it/s] 12%|        | 91/780 [00:30<03:22,  3.40it/s] 12%|        | 92/780 [00:30<03:21,  3.41it/s] 12%|        | 93/780 [00:30<03:20,  3.42it/s] 12%|        | 94/780 [00:30<03:20,  3.43it/s] 12%|        | 95/780 [00:31<03:19,  3.43it/s] 12%|        | 96/780 [00:31<03:19,  3.44it/s] 12%|        | 97/780 [00:31<03:52,  2.94it/s] 13%|        | 98/780 [00:32<03:42,  3.07it/s] 13%|        | 99/780 [00:32<03:34,  3.17it/s] 13%|        | 100/780 [00:32<03:29,  3.25it/s] 13%|        | 101/780 [00:33<03:25,  3.31it/s] 13%|        | 102/780 [00:33<03:22,  3.35it/s] 13%|        | 103/780 [00:33<03:20,  3.37it/s] 13%|        | 104/780 [00:33<03:18,  3.40it/s] 13%|        | 105/780 [00:34<03:17,  3.41it/s] 14%|        | 106/780 [00:34<03:47,  2.96it/s] 14%|        | 107/780 [00:35<03:37,  3.09it/s] 14%|        | 108/780 [00:35<03:30,  3.19it/s] 14%|        | 109/780 [00:35<03:25,  3.26it/s] 14%|        | 110/780 [00:35<03:22,  3.31it/s] 14%|        | 111/780 [00:36<03:19,  3.35it/s] 14%|        | 112/780 [00:36<03:17,  3.38it/s] 14%|        | 113/780 [00:36<03:16,  3.40it/s] 15%|        | 114/780 [00:37<03:15,  3.41it/s] 15%|        | 115/780 [00:37<03:14,  3.42it/s] 15%|        | 116/780 [00:37<03:30,  3.16it/s] 15%|        | 117/780 [00:37<03:24,  3.24it/s] 15%|        | 118/780 [00:38<03:20,  3.30it/s] 15%|        | 119/780 [00:38<03:18,  3.34it/s] 15%|        | 120/780 [00:38<03:16,  3.37it/s] 16%|        | 121/780 [00:39<03:14,  3.39it/s] 16%|        | 122/780 [00:39<03:13,  3.40it/s] 16%|        | 123/780 [00:39<03:12,  3.41it/s] 16%|        | 124/780 [00:40<03:11,  3.42it/s] 16%|        | 125/780 [00:40<03:11,  3.43it/s] 16%|        | 126/780 [00:40<03:10,  3.43it/s] 16%|        | 127/780 [00:41<04:09,  2.62it/s] 16%|        | 128/780 [00:41<03:51,  2.82it/s] 17%|        | 129/780 [00:41<03:38,  2.98it/s] 17%|        | 130/780 [00:42<03:29,  3.11it/s] 17%|        | 131/780 [00:42<03:22,  3.20it/s] 17%|        | 132/780 [00:42<03:18,  3.27it/s] 17%|        | 133/780 [00:42<03:14,  3.32it/s] 17%|        | 134/780 [00:43<03:12,  3.36it/s] 17%|        | 135/780 [00:43<03:10,  3.39it/s] 17%|        | 136/780 [00:43<03:09,  3.40it/s] 18%|        | 137/780 [00:44<04:52,  2.20it/s] 18%|        | 138/780 [00:44<04:20,  2.47it/s] 18%|        | 139/780 [00:45<03:57,  2.70it/s] 18%|        | 140/780 [00:45<03:42,  2.88it/s] 18%|        | 141/780 [00:45<03:30,  3.03it/s] 18%|        | 142/780 [00:46<03:22,  3.14it/s] 18%|        | 143/780 [00:46<03:17,  3.23it/s] 18%|        | 144/780 [00:46<03:13,  3.29it/s] 19%|        | 145/780 [00:46<03:10,  3.33it/s] 19%|        | 146/780 [00:47<03:47,  2.79it/s] 19%|        | 147/780 [00:47<03:34,  2.96it/s] 19%|        | 148/780 [00:48<03:24,  3.09it/s] 19%|        | 149/780 [00:48<03:17,  3.19it/s] 19%|        | 150/780 [00:48<03:13,  3.26it/s] 19%|        | 151/780 [00:48<03:09,  3.31it/s] 19%|        | 152/780 [00:49<03:07,  3.35it/s] 20%|        | 153/780 [00:49<03:05,  3.37it/s] 20%|        | 154/780 [00:49<03:04,  3.39it/s] 20%|        | 155/780 [00:50<03:03,  3.41it/s] 20%|        | 156/780 [00:50<03:13,  3.22it/s][INFO|trainer.py:2140] 2023-08-28 21:19:34,091 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:19:34,091 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:19:34,091 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.05it/s][A
  3%|         | 12/435 [00:00<00:23, 17.93it/s][A
  4%|         | 17/435 [00:00<00:17, 24.34it/s][A
  5%|         | 22/435 [00:00<00:14, 29.36it/s][A
  6%|         | 27/435 [00:00<00:12, 33.43it/s][A
  7%|         | 32/435 [00:01<00:11, 36.52it/s][A
  9%|         | 37/435 [00:01<00:10, 38.87it/s][A
 10%|         | 42/435 [00:01<00:09, 40.66it/s][A
 11%|         | 47/435 [00:01<00:09, 41.88it/s][A
 12%|        | 52/435 [00:01<00:08, 42.74it/s][A
 13%|        | 57/435 [00:01<00:08, 42.96it/s][A
 14%|        | 62/435 [00:01<00:08, 43.46it/s][A
 15%|        | 67/435 [00:01<00:08, 43.80it/s][A
 17%|        | 72/435 [00:01<00:08, 44.18it/s][A
 18%|        | 77/435 [00:02<00:08, 44.45it/s][A
 19%|        | 82/435 [00:02<00:07, 44.64it/s][A
 20%|        | 87/435 [00:02<00:07, 44.82it/s][A
 21%|        | 92/435 [00:02<00:07, 44.78it/s][A
 22%|       | 97/435 [00:02<00:07, 44.67it/s][A
 23%|       | 102/435 [00:02<00:07, 44.48it/s][A
 25%|       | 107/435 [00:02<00:10, 30.62it/s][A
 26%|       | 112/435 [00:02<00:09, 33.83it/s][A
 27%|       | 117/435 [00:03<00:08, 36.62it/s][A
 28%|       | 122/435 [00:03<00:08, 38.83it/s][A
 29%|       | 127/435 [00:03<00:07, 40.50it/s][A
 30%|       | 132/435 [00:03<00:07, 41.77it/s][A
 31%|      | 137/435 [00:03<00:06, 42.60it/s][A
 33%|      | 142/435 [00:03<00:06, 43.26it/s][A
 34%|      | 147/435 [00:03<00:06, 43.44it/s][A
 35%|      | 152/435 [00:03<00:06, 43.70it/s][A
 36%|      | 157/435 [00:03<00:06, 44.13it/s][A
 37%|      | 162/435 [00:04<00:06, 44.29it/s][A
 38%|      | 167/435 [00:04<00:06, 44.63it/s][A
 40%|      | 172/435 [00:04<00:05, 44.71it/s][A
 41%|      | 177/435 [00:04<00:05, 44.72it/s][A
 42%|     | 182/435 [00:04<00:05, 44.76it/s][A
 43%|     | 187/435 [00:04<00:05, 44.68it/s][A
 44%|     | 192/435 [00:04<00:05, 44.59it/s][A
 45%|     | 197/435 [00:04<00:05, 44.43it/s][A
 46%|     | 202/435 [00:05<00:05, 44.63it/s][A
 48%|     | 207/435 [00:05<00:05, 44.63it/s][A
 49%|     | 212/435 [00:05<00:04, 44.78it/s][A
 50%|     | 217/435 [00:05<00:04, 44.80it/s][A
 51%|     | 222/435 [00:05<00:04, 44.75it/s][A
 52%|    | 227/435 [00:05<00:04, 44.82it/s][A
 53%|    | 232/435 [00:05<00:04, 44.71it/s][A
 54%|    | 237/435 [00:05<00:04, 40.44it/s][A
 56%|    | 242/435 [00:05<00:04, 41.84it/s][A
 57%|    | 247/435 [00:06<00:04, 42.79it/s][A
 58%|    | 252/435 [00:06<00:04, 43.45it/s][A
 59%|    | 257/435 [00:06<00:04, 43.98it/s][A
 60%|    | 262/435 [00:06<00:03, 44.24it/s][A
 61%|   | 267/435 [00:06<00:03, 44.38it/s][A
 63%|   | 272/435 [00:06<00:03, 44.56it/s][A
 64%|   | 277/435 [00:06<00:03, 44.32it/s][A
 65%|   | 282/435 [00:06<00:03, 44.18it/s][A
 66%|   | 287/435 [00:06<00:03, 44.31it/s][A
 67%|   | 292/435 [00:07<00:03, 44.48it/s][A
 68%|   | 297/435 [00:07<00:03, 44.75it/s][A
 69%|   | 302/435 [00:07<00:02, 44.85it/s][A
 71%|   | 307/435 [00:07<00:02, 44.95it/s][A
 72%|  | 312/435 [00:07<00:02, 44.78it/s][A
 73%|  | 317/435 [00:07<00:02, 44.76it/s][A
 74%|  | 322/435 [00:07<00:02, 44.62it/s][A
 75%|  | 327/435 [00:07<00:02, 44.45it/s][A
 76%|  | 332/435 [00:07<00:02, 44.41it/s][A
 77%|  | 337/435 [00:08<00:02, 44.58it/s][A
 79%|  | 342/435 [00:08<00:02, 44.66it/s][A
 80%|  | 347/435 [00:08<00:01, 44.83it/s][A
 81%|  | 352/435 [00:08<00:01, 44.86it/s][A
 82%| | 357/435 [00:08<00:01, 44.87it/s][A
 83%| | 362/435 [00:08<00:01, 44.94it/s][A
 84%| | 367/435 [00:08<00:01, 44.60it/s][A
 86%| | 372/435 [00:08<00:01, 37.40it/s][A
 87%| | 377/435 [00:09<00:01, 39.41it/s][A
 88%| | 382/435 [00:09<00:01, 40.98it/s][A
 89%| | 387/435 [00:09<00:01, 41.90it/s][A
 90%| | 392/435 [00:09<00:01, 42.80it/s][A
 91%|| 397/435 [00:09<00:00, 43.44it/s][A
 92%|| 402/435 [00:09<00:00, 43.88it/s][A
 94%|| 407/435 [00:09<00:00, 44.15it/s][A
 95%|| 412/435 [00:09<00:00, 43.90it/s][A
 96%|| 417/435 [00:09<00:00, 44.13it/s][A
 97%|| 422/435 [00:10<00:00, 44.32it/s][A
 98%|| 427/435 [00:10<00:00, 44.46it/s][A
 99%|| 432/435 [00:10<00:00, 44.70it/s][A                                                 
                                                 [A 20%|        | 156/780 [01:00<03:13,  3.22it/s]
100%|| 435/435 [00:10<00:00, 44.70it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:19:45,887 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 21:19:47,268 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:19:56,570 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:19:56,731 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:19:56,853 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:34<2:19:30, 13.43s/it] 20%|        | 158/780 [01:34<1:38:28,  9.50s/it] 20%|        | 159/780 [01:35<1:09:44,  6.74s/it] 21%|        | 160/780 [01:35<49:38,  4.80s/it]   21%|        | 161/780 [01:35<35:36,  3.45s/it] 21%|        | 162/780 [01:35<25:47,  2.50s/it] 21%|        | 163/780 [01:36<18:56,  1.84s/it] 21%|        | 164/780 [01:36<14:08,  1.38s/it] 21%|        | 165/780 [01:36<10:47,  1.05s/it] 21%|       | 166/780 [01:37<08:26,  1.21it/s] 21%|       | 167/780 [01:37<06:47,  1.50it/s] 22%|       | 168/780 [01:37<05:38,  1.81it/s] 22%|       | 169/780 [01:38<04:49,  2.11it/s] 22%|       | 170/780 [01:38<04:15,  2.39it/s] 22%|       | 171/780 [01:38<03:51,  2.63it/s] 22%|       | 172/780 [01:38<03:34,  2.83it/s] 22%|       | 173/780 [01:39<04:25,  2.28it/s] 22%|       | 174/780 [01:39<03:58,  2.54it/s] 22%|       | 175/780 [01:40<03:39,  2.76it/s] 23%|       | 176/780 [01:40<03:25,  2.93it/s] 23%|       | 177/780 [01:40<03:16,  3.07it/s] 23%|       | 178/780 [01:40<03:09,  3.18it/s] 23%|       | 179/780 [01:41<03:04,  3.25it/s] 23%|       | 180/780 [01:41<03:01,  3.31it/s] 23%|       | 181/780 [01:41<02:58,  3.35it/s] 23%|       | 182/780 [01:42<02:56,  3.38it/s] 23%|       | 183/780 [01:42<03:03,  3.26it/s] 24%|       | 184/780 [01:42<02:59,  3.32it/s] 24%|       | 185/780 [01:43<02:57,  3.36it/s] 24%|       | 186/780 [01:43<02:55,  3.39it/s] 24%|       | 187/780 [01:43<02:54,  3.41it/s] 24%|       | 188/780 [01:43<02:53,  3.42it/s] 24%|       | 189/780 [01:44<02:52,  3.43it/s] 24%|       | 190/780 [01:44<02:51,  3.44it/s] 24%|       | 191/780 [01:44<02:51,  3.44it/s] 25%|       | 192/780 [01:45<02:50,  3.44it/s] 25%|       | 193/780 [01:45<02:50,  3.44it/s] 25%|       | 194/780 [01:45<02:59,  3.26it/s] 25%|       | 195/780 [01:46<02:56,  3.32it/s] 25%|       | 196/780 [01:46<02:53,  3.36it/s] 25%|       | 197/780 [01:46<02:52,  3.39it/s] 25%|       | 198/780 [01:46<02:50,  3.41it/s] 26%|       | 199/780 [01:47<02:49,  3.42it/s] 26%|       | 200/780 [01:47<02:49,  3.43it/s] 26%|       | 201/780 [01:47<02:48,  3.44it/s] 26%|       | 202/780 [01:48<02:47,  3.44it/s] 26%|       | 203/780 [01:48<02:47,  3.45it/s] 26%|       | 204/780 [01:48<02:46,  3.45it/s] 26%|       | 205/780 [01:49<03:21,  2.85it/s] 26%|       | 206/780 [01:49<03:10,  3.01it/s] 27%|       | 207/780 [01:49<03:02,  3.13it/s] 27%|       | 208/780 [01:49<02:57,  3.22it/s] 27%|       | 209/780 [01:50<02:53,  3.29it/s] 27%|       | 210/780 [01:50<02:51,  3.33it/s] 27%|       | 211/780 [01:51<03:18,  2.86it/s] 27%|       | 212/780 [01:51<03:08,  3.02it/s] 27%|       | 213/780 [01:51<03:00,  3.13it/s] 27%|       | 214/780 [01:51<02:55,  3.22it/s] 28%|       | 215/780 [01:52<03:06,  3.03it/s] 28%|       | 216/780 [01:52<02:59,  3.15it/s] 28%|       | 217/780 [01:52<02:54,  3.23it/s] 28%|       | 218/780 [01:53<02:50,  3.29it/s] 28%|       | 219/780 [01:53<02:48,  3.34it/s] 28%|       | 220/780 [01:53<02:46,  3.37it/s] 28%|       | 221/780 [01:54<02:44,  3.39it/s] 28%|       | 222/780 [01:54<02:43,  3.41it/s] 29%|       | 223/780 [01:54<02:42,  3.42it/s] 29%|       | 224/780 [01:54<02:42,  3.43it/s] 29%|       | 225/780 [01:55<02:41,  3.43it/s] 29%|       | 226/780 [01:55<03:04,  3.00it/s] 29%|       | 227/780 [01:55<02:57,  3.12it/s] 29%|       | 228/780 [01:56<02:51,  3.21it/s] 29%|       | 229/780 [01:56<02:48,  3.28it/s] 29%|       | 230/780 [01:56<02:45,  3.33it/s] 30%|       | 231/780 [01:57<02:43,  3.36it/s] 30%|       | 232/780 [01:57<02:41,  3.39it/s] 30%|       | 233/780 [01:57<02:40,  3.40it/s] 30%|       | 234/780 [01:57<02:39,  3.42it/s] 30%|       | 235/780 [01:58<02:39,  3.43it/s] 30%|       | 236/780 [01:58<02:42,  3.35it/s] 30%|       | 237/780 [01:58<02:40,  3.38it/s] 31%|       | 238/780 [01:59<02:39,  3.40it/s] 31%|       | 239/780 [01:59<02:38,  3.41it/s] 31%|       | 240/780 [01:59<02:37,  3.42it/s] 31%|       | 241/780 [01:59<02:37,  3.43it/s] 31%|       | 242/780 [02:00<02:36,  3.43it/s] 31%|       | 243/780 [02:00<02:36,  3.43it/s] 31%|      | 244/780 [02:00<02:36,  3.43it/s] 31%|      | 245/780 [02:01<02:58,  3.00it/s] 32%|      | 246/780 [02:01<03:09,  2.81it/s] 32%|      | 247/780 [02:02<03:07,  2.84it/s] 32%|      | 248/780 [02:03<05:01,  1.76it/s] 32%|      | 249/780 [02:03<04:31,  1.95it/s] 32%|      | 250/780 [02:03<04:15,  2.07it/s] 32%|      | 251/780 [02:04<03:57,  2.23it/s] 32%|      | 252/780 [02:04<03:31,  2.50it/s] 32%|      | 253/780 [02:05<04:30,  1.95it/s] 33%|      | 254/780 [02:05<03:54,  2.24it/s] 33%|      | 255/780 [02:05<03:29,  2.50it/s] 33%|      | 256/780 [02:06<03:12,  2.73it/s] 33%|      | 257/780 [02:06<02:59,  2.91it/s] 33%|      | 258/780 [02:07<03:38,  2.39it/s] 33%|      | 259/780 [02:07<03:18,  2.63it/s] 33%|      | 260/780 [02:07<03:03,  2.83it/s] 33%|      | 261/780 [02:07<02:53,  2.99it/s] 34%|      | 262/780 [02:08<02:46,  3.11it/s] 34%|      | 263/780 [02:08<02:41,  3.21it/s] 34%|      | 264/780 [02:08<02:37,  3.27it/s] 34%|      | 265/780 [02:09<02:34,  3.33it/s] 34%|      | 266/780 [02:09<02:33,  3.36it/s] 34%|      | 267/780 [02:09<02:31,  3.39it/s] 34%|      | 268/780 [02:09<02:31,  3.38it/s] 34%|      | 269/780 [02:10<02:30,  3.40it/s] 35%|      | 270/780 [02:10<02:29,  3.41it/s] 35%|      | 271/780 [02:10<02:28,  3.42it/s] 35%|      | 272/780 [02:11<02:28,  3.43it/s] 35%|      | 273/780 [02:11<02:27,  3.43it/s] 35%|      | 274/780 [02:11<02:27,  3.44it/s] 35%|      | 275/780 [02:12<02:26,  3.44it/s] 35%|      | 276/780 [02:12<02:26,  3.44it/s] 36%|      | 277/780 [02:12<02:26,  3.44it/s] 36%|      | 278/780 [02:12<02:25,  3.44it/s] 36%|      | 279/780 [02:13<02:36,  3.21it/s] 36%|      | 280/780 [02:13<02:32,  3.27it/s] 36%|      | 281/780 [02:13<02:30,  3.32it/s] 36%|      | 282/780 [02:14<02:28,  3.36it/s] 36%|      | 283/780 [02:14<02:27,  3.38it/s] 36%|      | 284/780 [02:14<02:25,  3.40it/s] 37%|      | 285/780 [02:15<02:25,  3.41it/s] 37%|      | 286/780 [02:15<02:24,  3.42it/s] 37%|      | 287/780 [02:15<02:23,  3.43it/s] 37%|      | 288/780 [02:15<02:23,  3.43it/s] 37%|      | 289/780 [02:16<02:23,  3.43it/s] 37%|      | 290/780 [02:16<02:27,  3.32it/s] 37%|      | 291/780 [02:16<02:25,  3.36it/s] 37%|      | 292/780 [02:17<02:24,  3.38it/s] 38%|      | 293/780 [02:17<02:23,  3.40it/s] 38%|      | 294/780 [02:17<02:22,  3.41it/s] 38%|      | 295/780 [02:17<02:21,  3.42it/s] 38%|      | 296/780 [02:18<02:21,  3.43it/s] 38%|      | 297/780 [02:18<02:20,  3.43it/s] 38%|      | 298/780 [02:18<02:20,  3.44it/s] 38%|      | 299/780 [02:19<02:19,  3.44it/s] 38%|      | 300/780 [02:19<02:19,  3.44it/s] 39%|      | 301/780 [02:19<03:00,  2.65it/s] 39%|      | 302/780 [02:20<02:47,  2.85it/s] 39%|      | 303/780 [02:20<02:38,  3.01it/s] 39%|      | 304/780 [02:20<02:32,  3.12it/s] 39%|      | 305/780 [02:21<02:27,  3.22it/s] 39%|      | 306/780 [02:21<02:24,  3.28it/s] 39%|      | 307/780 [02:21<02:22,  3.33it/s] 39%|      | 308/780 [02:22<02:20,  3.36it/s] 40%|      | 309/780 [02:22<02:19,  3.39it/s] 40%|      | 310/780 [02:22<02:18,  3.40it/s] 40%|      | 311/780 [02:22<02:33,  3.05it/s] 40%|      | 312/780 [02:23<02:28,  3.16it/s][INFO|trainer.py:2140] 2023-08-28 21:21:06,946 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:21:06,946 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:21:06,946 >>   Batch size = 8
{'eval_loss': 0.9284214973449707, 'eval_runtime': 10.3708, 'eval_samples_per_second': 335.268, 'eval_steps_per_second': 41.945, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.80it/s][A
  3%|         | 12/435 [00:00<00:08, 48.94it/s][A
  4%|         | 17/435 [00:00<00:08, 47.20it/s][A
  5%|         | 22/435 [00:00<00:08, 46.15it/s][A
  6%|         | 27/435 [00:00<00:08, 45.61it/s][A
  7%|         | 32/435 [00:00<00:08, 45.26it/s][A
  9%|         | 37/435 [00:00<00:08, 45.08it/s][A
 10%|         | 42/435 [00:00<00:08, 44.79it/s][A
 11%|         | 47/435 [00:01<00:08, 44.78it/s][A
 12%|        | 52/435 [00:01<00:08, 44.89it/s][A
 13%|        | 57/435 [00:01<00:08, 44.91it/s][A
 14%|        | 62/435 [00:01<00:08, 44.93it/s][A
 15%|        | 67/435 [00:01<00:08, 44.86it/s][A
 17%|        | 72/435 [00:01<00:08, 44.76it/s][A
 18%|        | 77/435 [00:01<00:07, 44.81it/s][A
 19%|        | 82/435 [00:01<00:07, 44.73it/s][A
 20%|        | 87/435 [00:01<00:07, 44.63it/s][A
 21%|        | 92/435 [00:02<00:07, 44.74it/s][A
 22%|       | 97/435 [00:02<00:07, 44.74it/s][A
 23%|       | 102/435 [00:02<00:07, 44.75it/s][A
 25%|       | 107/435 [00:02<00:08, 39.75it/s][A
 26%|       | 112/435 [00:02<00:07, 41.15it/s][A
 27%|       | 117/435 [00:02<00:07, 42.32it/s][A
 28%|       | 122/435 [00:02<00:07, 43.01it/s][A
 29%|       | 127/435 [00:02<00:07, 43.68it/s][A
 30%|       | 132/435 [00:02<00:06, 44.01it/s][A
 31%|      | 137/435 [00:03<00:06, 44.36it/s][A
 33%|      | 142/435 [00:03<00:06, 44.49it/s][A
 34%|      | 147/435 [00:03<00:06, 44.19it/s][A
 35%|      | 152/435 [00:03<00:06, 44.20it/s][A
 36%|      | 157/435 [00:03<00:06, 44.42it/s][A
 37%|      | 162/435 [00:03<00:06, 44.59it/s][A
 38%|      | 167/435 [00:03<00:05, 44.69it/s][A
 40%|      | 172/435 [00:03<00:05, 44.83it/s][A
 41%|      | 177/435 [00:03<00:05, 44.85it/s][A
 42%|     | 182/435 [00:04<00:05, 44.88it/s][A
 43%|     | 187/435 [00:04<00:05, 44.85it/s][A
 44%|     | 192/435 [00:04<00:05, 44.67it/s][A
 45%|     | 197/435 [00:04<00:05, 44.53it/s][A
 46%|     | 202/435 [00:04<00:05, 44.65it/s][A
 48%|     | 207/435 [00:04<00:05, 44.70it/s][A
 49%|     | 212/435 [00:04<00:04, 44.82it/s][A
 50%|     | 217/435 [00:04<00:04, 44.80it/s][A
 51%|     | 222/435 [00:04<00:04, 44.87it/s][A
 52%|    | 227/435 [00:05<00:04, 44.85it/s][A
 53%|    | 232/435 [00:05<00:04, 44.85it/s][A
 54%|    | 237/435 [00:05<00:04, 44.83it/s][A
 56%|    | 242/435 [00:05<00:04, 42.94it/s][A
 57%|    | 247/435 [00:05<00:04, 43.55it/s][A
 58%|    | 252/435 [00:05<00:04, 43.90it/s][A
 59%|    | 257/435 [00:05<00:04, 44.28it/s][A
 60%|    | 262/435 [00:05<00:03, 44.42it/s][A
 61%|   | 267/435 [00:05<00:03, 44.55it/s][A
 63%|   | 272/435 [00:06<00:03, 44.34it/s][A
 64%|   | 277/435 [00:06<00:03, 44.73it/s][A
 65%|   | 282/435 [00:06<00:03, 44.59it/s][A
 66%|   | 287/435 [00:06<00:03, 44.61it/s][A
 67%|   | 292/435 [00:06<00:03, 44.69it/s][A
 68%|   | 297/435 [00:06<00:03, 44.76it/s][A
 69%|   | 302/435 [00:06<00:02, 44.77it/s][A
 71%|   | 307/435 [00:06<00:02, 44.84it/s][A
 72%|  | 312/435 [00:07<00:02, 44.83it/s][A
 73%|  | 317/435 [00:07<00:02, 44.85it/s][A
 74%|  | 322/435 [00:07<00:02, 44.79it/s][A
 75%|  | 327/435 [00:07<00:02, 44.68it/s][A
 76%|  | 332/435 [00:07<00:02, 44.65it/s][A
 77%|  | 337/435 [00:07<00:02, 44.59it/s][A
 79%|  | 342/435 [00:07<00:02, 44.59it/s][A
 80%|  | 347/435 [00:07<00:01, 44.60it/s][A
 81%|  | 352/435 [00:07<00:01, 44.66it/s][A
 82%| | 357/435 [00:08<00:01, 44.73it/s][A
 83%| | 362/435 [00:08<00:01, 44.72it/s][A
 84%| | 367/435 [00:08<00:01, 44.61it/s][A
 86%| | 372/435 [00:08<00:01, 44.63it/s][A
 87%| | 377/435 [00:08<00:01, 40.90it/s][A
 88%| | 382/435 [00:08<00:01, 42.09it/s][A
 89%| | 387/435 [00:08<00:01, 42.95it/s][A
 90%| | 392/435 [00:08<00:00, 43.51it/s][A
 91%|| 397/435 [00:08<00:00, 44.07it/s][A
 92%|| 402/435 [00:09<00:00, 44.41it/s][A
 94%|| 407/435 [00:09<00:00, 44.51it/s][A
 95%|| 412/435 [00:09<00:00, 44.56it/s][A
 96%|| 417/435 [00:09<00:00, 44.33it/s][A
 97%|| 422/435 [00:09<00:00, 44.40it/s][A
 98%|| 427/435 [00:09<00:00, 44.54it/s][A
 99%|| 432/435 [00:09<00:00, 44.54it/s][A                                                 
                                                 [A 40%|      | 312/780 [02:33<02:28,  3.16it/s]
100%|| 435/435 [00:09<00:00, 44.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:21:17,841 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 21:21:18,910 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:21:25,667 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:21:26,019 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:21:26,151 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [03:00<1:28:59, 11.43s/it] 40%|      | 314/780 [03:01<1:03:08,  8.13s/it] 40%|      | 315/780 [03:01<45:07,  5.82s/it]   41%|      | 316/780 [03:01<32:18,  4.18s/it] 41%|      | 317/780 [03:02<23:17,  3.02s/it] 41%|      | 318/780 [03:02<17:08,  2.23s/it] 41%|      | 319/780 [03:02<12:41,  1.65s/it] 41%|      | 320/780 [03:03<09:32,  1.24s/it] 41%|      | 321/780 [03:03<07:29,  1.02it/s] 41%|     | 322/780 [03:04<06:20,  1.20it/s] 41%|     | 323/780 [03:04<05:23,  1.41it/s] 42%|     | 324/780 [03:04<04:25,  1.71it/s] 42%|     | 325/780 [03:05<03:45,  2.01it/s] 42%|     | 326/780 [03:05<03:17,  2.30it/s] 42%|     | 327/780 [03:05<02:58,  2.54it/s] 42%|     | 328/780 [03:05<02:44,  2.75it/s] 42%|     | 329/780 [03:06<02:34,  2.92it/s] 42%|     | 330/780 [03:06<02:27,  3.05it/s] 42%|     | 331/780 [03:06<02:22,  3.14it/s] 43%|     | 332/780 [03:07<02:19,  3.22it/s] 43%|     | 333/780 [03:07<02:20,  3.17it/s] 43%|     | 334/780 [03:07<02:17,  3.24it/s] 43%|     | 335/780 [03:07<02:15,  3.28it/s] 43%|     | 336/780 [03:08<02:13,  3.32it/s] 43%|     | 337/780 [03:08<02:12,  3.34it/s] 43%|     | 338/780 [03:08<02:11,  3.36it/s] 43%|     | 339/780 [03:09<02:10,  3.37it/s] 44%|     | 340/780 [03:09<02:10,  3.38it/s] 44%|     | 341/780 [03:09<02:09,  3.39it/s] 44%|     | 342/780 [03:10<02:09,  3.39it/s] 44%|     | 343/780 [03:10<02:08,  3.39it/s] 44%|     | 344/780 [03:10<02:08,  3.39it/s] 44%|     | 345/780 [03:10<02:08,  3.39it/s] 44%|     | 346/780 [03:11<02:17,  3.15it/s] 44%|     | 347/780 [03:11<02:14,  3.22it/s] 45%|     | 348/780 [03:11<02:11,  3.27it/s] 45%|     | 349/780 [03:12<02:10,  3.31it/s] 45%|     | 350/780 [03:12<02:08,  3.34it/s] 45%|     | 351/780 [03:12<02:07,  3.35it/s] 45%|     | 352/780 [03:13<02:07,  3.37it/s] 45%|     | 353/780 [03:13<02:06,  3.38it/s] 45%|     | 354/780 [03:13<02:05,  3.38it/s] 46%|     | 355/780 [03:13<02:05,  3.39it/s] 46%|     | 356/780 [03:14<02:04,  3.40it/s] 46%|     | 357/780 [03:14<02:03,  3.41it/s] 46%|     | 358/780 [03:14<02:03,  3.42it/s] 46%|     | 359/780 [03:15<02:02,  3.43it/s] 46%|     | 360/780 [03:15<02:02,  3.43it/s] 46%|     | 361/780 [03:15<02:01,  3.44it/s] 46%|     | 362/780 [03:15<02:01,  3.44it/s] 47%|     | 363/780 [03:16<02:01,  3.44it/s] 47%|     | 364/780 [03:16<02:00,  3.44it/s] 47%|     | 365/780 [03:16<02:00,  3.44it/s] 47%|     | 366/780 [03:17<02:00,  3.44it/s] 47%|     | 367/780 [03:17<02:00,  3.42it/s] 47%|     | 368/780 [03:17<02:00,  3.43it/s] 47%|     | 369/780 [03:18<01:59,  3.43it/s] 47%|     | 370/780 [03:18<01:59,  3.44it/s] 48%|     | 371/780 [03:18<01:58,  3.44it/s] 48%|     | 372/780 [03:18<01:58,  3.44it/s] 48%|     | 373/780 [03:19<01:58,  3.44it/s] 48%|     | 374/780 [03:19<01:58,  3.44it/s] 48%|     | 375/780 [03:19<01:57,  3.44it/s] 48%|     | 376/780 [03:20<01:57,  3.44it/s] 48%|     | 377/780 [03:20<01:56,  3.45it/s] 48%|     | 378/780 [03:20<02:02,  3.29it/s] 49%|     | 379/780 [03:20<02:00,  3.33it/s] 49%|     | 380/780 [03:21<01:58,  3.37it/s] 49%|     | 381/780 [03:21<01:57,  3.39it/s] 49%|     | 382/780 [03:21<01:56,  3.41it/s] 49%|     | 383/780 [03:22<01:56,  3.42it/s] 49%|     | 384/780 [03:22<01:55,  3.43it/s] 49%|     | 385/780 [03:22<01:55,  3.43it/s] 49%|     | 386/780 [03:22<01:54,  3.44it/s] 50%|     | 387/780 [03:23<01:54,  3.44it/s] 50%|     | 388/780 [03:23<01:53,  3.44it/s] 50%|     | 389/780 [03:23<01:58,  3.31it/s] 50%|     | 390/780 [03:24<01:56,  3.35it/s] 50%|     | 391/780 [03:24<01:55,  3.38it/s] 50%|     | 392/780 [03:24<01:54,  3.40it/s] 50%|     | 393/780 [03:25<01:53,  3.41it/s] 51%|     | 394/780 [03:25<01:52,  3.42it/s] 51%|     | 395/780 [03:25<01:52,  3.43it/s] 51%|     | 396/780 [03:25<01:51,  3.43it/s] 51%|     | 397/780 [03:26<01:51,  3.43it/s] 51%|     | 398/780 [03:26<01:51,  3.44it/s] 51%|     | 399/780 [03:26<01:50,  3.44it/s] 51%|    | 400/780 [03:27<01:55,  3.30it/s] 51%|    | 401/780 [03:27<01:53,  3.34it/s] 52%|    | 402/780 [03:27<01:52,  3.37it/s] 52%|    | 403/780 [03:28<01:51,  3.39it/s] 52%|    | 404/780 [03:28<01:50,  3.41it/s] 52%|    | 405/780 [03:28<01:49,  3.42it/s] 52%|    | 406/780 [03:28<01:49,  3.42it/s] 52%|    | 407/780 [03:29<01:48,  3.43it/s] 52%|    | 408/780 [03:29<01:48,  3.43it/s] 52%|    | 409/780 [03:29<01:48,  3.42it/s] 53%|    | 410/780 [03:30<01:47,  3.43it/s] 53%|    | 411/780 [03:30<02:33,  2.40it/s] 53%|    | 412/780 [03:31<02:19,  2.64it/s] 53%|    | 413/780 [03:31<02:09,  2.84it/s] 53%|    | 414/780 [03:31<02:02,  3.00it/s] 53%|    | 415/780 [03:31<01:56,  3.12it/s] 53%|    | 416/780 [03:32<01:53,  3.21it/s] 53%|    | 417/780 [03:32<01:50,  3.28it/s] 54%|    | 418/780 [03:32<01:48,  3.33it/s] 54%|    | 419/780 [03:33<01:47,  3.36it/s] 54%|    | 420/780 [03:33<02:02,  2.93it/s] 54%|    | 421/780 [03:33<01:56,  3.07it/s] 54%|    | 422/780 [03:34<01:52,  3.17it/s] 54%|    | 423/780 [03:34<01:49,  3.25it/s] 54%|    | 424/780 [03:34<01:47,  3.31it/s] 54%|    | 425/780 [03:34<01:45,  3.35it/s] 55%|    | 426/780 [03:35<01:44,  3.38it/s] 55%|    | 427/780 [03:35<01:43,  3.40it/s] 55%|    | 428/780 [03:35<01:43,  3.41it/s] 55%|    | 429/780 [03:36<01:42,  3.42it/s] 55%|    | 430/780 [03:36<01:45,  3.32it/s] 55%|    | 431/780 [03:36<01:44,  3.35it/s] 55%|    | 432/780 [03:37<01:42,  3.38it/s] 56%|    | 433/780 [03:37<01:42,  3.40it/s] 56%|    | 434/780 [03:37<01:41,  3.41it/s] 56%|    | 435/780 [03:37<01:40,  3.42it/s] 56%|    | 436/780 [03:38<01:40,  3.42it/s] 56%|    | 437/780 [03:38<01:40,  3.43it/s] 56%|    | 438/780 [03:38<01:39,  3.43it/s] 56%|    | 439/780 [03:39<01:54,  2.98it/s] 56%|    | 440/780 [03:39<01:49,  3.11it/s] 57%|    | 441/780 [03:39<01:46,  3.20it/s] 57%|    | 442/780 [03:40<01:43,  3.27it/s] 57%|    | 443/780 [03:40<01:41,  3.32it/s] 57%|    | 444/780 [03:40<01:40,  3.36it/s] 57%|    | 445/780 [03:40<01:39,  3.38it/s] 57%|    | 446/780 [03:41<01:38,  3.40it/s] 57%|    | 447/780 [03:41<01:37,  3.41it/s] 57%|    | 448/780 [03:41<01:37,  3.42it/s] 58%|    | 449/780 [03:42<01:46,  3.11it/s] 58%|    | 450/780 [03:42<01:42,  3.21it/s] 58%|    | 451/780 [03:42<01:40,  3.27it/s] 58%|    | 452/780 [03:43<01:38,  3.32it/s] 58%|    | 453/780 [03:43<01:37,  3.36it/s] 58%|    | 454/780 [03:43<01:36,  3.38it/s] 58%|    | 455/780 [03:43<01:35,  3.40it/s] 58%|    | 456/780 [03:44<01:34,  3.41it/s] 59%|    | 457/780 [03:44<01:34,  3.42it/s] 59%|    | 458/780 [03:44<01:33,  3.43it/s] 59%|    | 459/780 [03:45<01:34,  3.41it/s] 59%|    | 460/780 [03:45<01:33,  3.42it/s] 59%|    | 461/780 [03:45<01:33,  3.43it/s] 59%|    | 462/780 [03:46<01:32,  3.43it/s] 59%|    | 463/780 [03:46<01:32,  3.43it/s] 59%|    | 464/780 [03:46<01:31,  3.44it/s] 60%|    | 465/780 [03:46<01:31,  3.43it/s] 60%|    | 466/780 [03:47<01:31,  3.44it/s] 60%|    | 467/780 [03:47<01:31,  3.44it/s] 60%|    | 468/780 [03:47<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 21:22:31,411 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:22:31,411 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:22:31,411 >>   Batch size = 8
{'eval_loss': 0.9316580295562744, 'eval_runtime': 9.8386, 'eval_samples_per_second': 353.404, 'eval_steps_per_second': 44.214, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.11it/s][A
  3%|         | 12/435 [00:00<00:09, 44.96it/s][A
  4%|         | 17/435 [00:00<00:09, 44.98it/s][A
  5%|         | 22/435 [00:00<00:09, 44.90it/s][A
  6%|         | 27/435 [00:00<00:09, 44.79it/s][A
  7%|         | 32/435 [00:00<00:09, 44.69it/s][A
  9%|         | 37/435 [00:00<00:08, 44.51it/s][A
 10%|         | 42/435 [00:00<00:08, 44.61it/s][A
 11%|         | 47/435 [00:01<00:08, 44.67it/s][A
 12%|        | 52/435 [00:01<00:08, 44.70it/s][A
 13%|        | 57/435 [00:01<00:08, 44.83it/s][A
 14%|        | 62/435 [00:01<00:08, 44.91it/s][A
 15%|        | 67/435 [00:01<00:08, 44.96it/s][A
 17%|        | 72/435 [00:01<00:08, 44.81it/s][A
 18%|        | 77/435 [00:01<00:07, 44.78it/s][A
 19%|        | 82/435 [00:01<00:07, 44.63it/s][A
 20%|        | 87/435 [00:01<00:07, 44.64it/s][A
 21%|        | 92/435 [00:02<00:07, 44.71it/s][A
 22%|       | 97/435 [00:02<00:07, 44.77it/s][A
 23%|       | 102/435 [00:02<00:07, 44.78it/s][A
 25%|       | 107/435 [00:02<00:07, 44.88it/s][A
 26%|       | 112/435 [00:02<00:07, 44.81it/s][A
 27%|       | 117/435 [00:02<00:07, 44.85it/s][A
 28%|       | 122/435 [00:02<00:06, 44.72it/s][A
 29%|       | 127/435 [00:02<00:06, 44.77it/s][A
 30%|       | 132/435 [00:02<00:06, 44.61it/s][A
 31%|      | 137/435 [00:03<00:06, 44.54it/s][A
 33%|      | 142/435 [00:03<00:06, 44.71it/s][A
 34%|      | 147/435 [00:03<00:06, 44.78it/s][A
 35%|      | 152/435 [00:03<00:06, 44.64it/s][A
 36%|      | 157/435 [00:03<00:06, 44.67it/s][A
 37%|      | 162/435 [00:03<00:06, 44.61it/s][A
 38%|      | 167/435 [00:03<00:06, 44.62it/s][A
 40%|      | 172/435 [00:03<00:05, 44.56it/s][A
 41%|      | 177/435 [00:03<00:05, 44.69it/s][A
 42%|     | 182/435 [00:04<00:05, 44.69it/s][A
 43%|     | 187/435 [00:04<00:05, 44.77it/s][A
 44%|     | 192/435 [00:04<00:05, 44.81it/s][A
 45%|     | 197/435 [00:04<00:05, 44.80it/s][A
 46%|     | 202/435 [00:04<00:05, 44.77it/s][A
 48%|     | 207/435 [00:04<00:05, 44.72it/s][A
 49%|     | 212/435 [00:04<00:04, 44.73it/s][A
 50%|     | 217/435 [00:04<00:04, 44.65it/s][A
 51%|     | 222/435 [00:04<00:04, 44.71it/s][A
 52%|    | 227/435 [00:05<00:04, 44.74it/s][A
 53%|    | 232/435 [00:05<00:04, 44.81it/s][A
 54%|    | 237/435 [00:05<00:04, 44.78it/s][A
 56%|    | 242/435 [00:05<00:04, 44.83it/s][A
 57%|    | 247/435 [00:05<00:04, 44.76it/s][A
 58%|    | 252/435 [00:05<00:04, 44.79it/s][A
 59%|    | 257/435 [00:05<00:03, 44.67it/s][A
 60%|    | 262/435 [00:05<00:03, 44.75it/s][A
 61%|   | 267/435 [00:05<00:03, 44.72it/s][A
 63%|   | 272/435 [00:06<00:03, 44.83it/s][A
 64%|   | 277/435 [00:06<00:03, 44.89it/s][A
 65%|   | 282/435 [00:06<00:04, 34.97it/s][A
 66%|   | 287/435 [00:06<00:03, 37.51it/s][A
 67%|   | 292/435 [00:06<00:03, 39.47it/s][A
 68%|   | 297/435 [00:06<00:03, 40.90it/s][A
 69%|   | 302/435 [00:06<00:03, 42.12it/s][A
 71%|   | 307/435 [00:06<00:02, 42.94it/s][A
 72%|  | 312/435 [00:07<00:02, 43.56it/s][A
 73%|  | 317/435 [00:07<00:02, 43.90it/s][A
 74%|  | 322/435 [00:07<00:02, 43.85it/s][A
 75%|  | 327/435 [00:07<00:02, 43.97it/s][A
 76%|  | 332/435 [00:07<00:02, 44.22it/s][A
 77%|  | 337/435 [00:07<00:02, 44.49it/s][A
 79%|  | 342/435 [00:07<00:02, 44.48it/s][A
 80%|  | 347/435 [00:07<00:01, 44.55it/s][A
 81%|  | 352/435 [00:07<00:01, 44.77it/s][A
 82%| | 357/435 [00:08<00:01, 44.76it/s][A
 83%| | 362/435 [00:08<00:01, 44.77it/s][A
 84%| | 367/435 [00:08<00:01, 44.48it/s][A
 86%| | 372/435 [00:08<00:01, 44.46it/s][A
 87%| | 377/435 [00:08<00:01, 44.52it/s][A
 88%| | 382/435 [00:08<00:01, 44.58it/s][A
 89%| | 387/435 [00:08<00:01, 44.73it/s][A
 90%| | 392/435 [00:08<00:00, 44.83it/s][A
 91%|| 397/435 [00:08<00:00, 44.88it/s][A
 92%|| 402/435 [00:09<00:00, 44.90it/s][A
 94%|| 407/435 [00:09<00:00, 44.81it/s][A
 95%|| 412/435 [00:09<00:00, 44.10it/s][A
 96%|| 417/435 [00:09<00:00, 44.29it/s][A
 97%|| 422/435 [00:09<00:00, 44.24it/s][A
 98%|| 427/435 [00:09<00:00, 44.49it/s][A
 99%|| 432/435 [00:09<00:00, 44.61it/s][A                                                 
                                                 [A 60%|    | 468/780 [03:57<01:30,  3.44it/s]
100%|| 435/435 [00:09<00:00, 44.61it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:22:41,590 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:22:41,693 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:22:50,188 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:22:51,105 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:22:51,600 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [04:29<1:06:10, 12.77s/it] 60%|    | 470/780 [04:30<46:49,  9.06s/it]   60%|    | 471/780 [04:30<33:07,  6.43s/it] 61%|    | 472/780 [04:30<23:34,  4.59s/it] 61%|    | 473/780 [04:30<16:53,  3.30s/it] 61%|    | 474/780 [04:31<12:14,  2.40s/it] 61%|    | 475/780 [04:31<08:59,  1.77s/it] 61%|    | 476/780 [04:31<06:42,  1.33s/it] 61%|    | 477/780 [04:32<05:07,  1.02s/it] 61%|   | 478/780 [04:32<04:01,  1.25it/s] 61%|   | 479/780 [04:32<03:14,  1.54it/s] 62%|   | 480/780 [04:33<03:30,  1.42it/s] 62%|   | 481/780 [04:33<02:53,  1.72it/s] 62%|   | 482/780 [04:34<02:27,  2.02it/s] 62%|   | 483/780 [04:34<02:08,  2.30it/s] 62%|   | 484/780 [04:34<01:55,  2.55it/s] 62%|   | 485/780 [04:34<01:46,  2.76it/s] 62%|   | 486/780 [04:35<01:40,  2.93it/s] 62%|   | 487/780 [04:35<01:35,  3.06it/s] 63%|   | 488/780 [04:35<01:32,  3.15it/s] 63%|   | 489/780 [04:36<01:43,  2.82it/s] 63%|   | 490/780 [04:36<01:37,  2.98it/s] 63%|   | 491/780 [04:36<01:33,  3.10it/s] 63%|   | 492/780 [04:37<01:30,  3.18it/s] 63%|   | 493/780 [04:37<01:28,  3.24it/s] 63%|   | 494/780 [04:37<01:26,  3.29it/s] 63%|   | 495/780 [04:38<01:25,  3.33it/s] 64%|   | 496/780 [04:38<01:24,  3.35it/s] 64%|   | 497/780 [04:38<01:24,  3.37it/s] 64%|   | 498/780 [04:38<01:23,  3.38it/s] 64%|   | 499/780 [04:39<01:35,  2.96it/s] 64%|   | 500/780 [04:39<01:30,  3.08it/s]                                                  64%|   | 500/780 [04:39<01:30,  3.08it/s] 64%|   | 501/780 [04:39<01:28,  3.17it/s] 64%|   | 502/780 [04:40<01:25,  3.24it/s] 64%|   | 503/780 [04:40<01:27,  3.18it/s] 65%|   | 504/780 [04:40<01:25,  3.25it/s] 65%|   | 505/780 [04:41<01:23,  3.29it/s] 65%|   | 506/780 [04:41<01:22,  3.32it/s] 65%|   | 507/780 [04:41<01:21,  3.35it/s] 65%|   | 508/780 [04:42<01:20,  3.36it/s] 65%|   | 509/780 [04:42<01:20,  3.37it/s] 65%|   | 510/780 [04:42<01:19,  3.38it/s] 66%|   | 511/780 [04:42<01:19,  3.39it/s] 66%|   | 512/780 [04:43<01:18,  3.39it/s] 66%|   | 513/780 [04:43<01:32,  2.90it/s] 66%|   | 514/780 [04:44<01:27,  3.03it/s] 66%|   | 515/780 [04:44<01:24,  3.14it/s] 66%|   | 516/780 [04:44<01:22,  3.21it/s] 66%|   | 517/780 [04:44<01:20,  3.27it/s] 66%|   | 518/780 [04:45<01:19,  3.31it/s] 67%|   | 519/780 [04:45<01:18,  3.34it/s] 67%|   | 520/780 [04:45<01:17,  3.36it/s] 67%|   | 521/780 [04:46<01:16,  3.37it/s] 67%|   | 522/780 [04:46<01:16,  3.38it/s] 67%|   | 523/780 [04:46<01:24,  3.03it/s] 67%|   | 524/780 [04:47<01:21,  3.13it/s] 67%|   | 525/780 [04:47<01:19,  3.21it/s] 67%|   | 526/780 [04:47<01:17,  3.26it/s] 68%|   | 527/780 [04:47<01:16,  3.31it/s] 68%|   | 528/780 [04:48<01:15,  3.33it/s] 68%|   | 529/780 [04:48<01:14,  3.35it/s] 68%|   | 530/780 [04:48<01:14,  3.37it/s] 68%|   | 531/780 [04:49<01:13,  3.38it/s] 68%|   | 532/780 [04:49<01:13,  3.39it/s] 68%|   | 533/780 [04:49<01:22,  3.01it/s] 68%|   | 534/780 [04:50<01:19,  3.11it/s] 69%|   | 535/780 [04:50<01:16,  3.20it/s] 69%|   | 536/780 [04:54<05:35,  1.38s/it] 69%|   | 537/780 [04:55<05:04,  1.26s/it] 69%|   | 538/780 [04:55<03:53,  1.04it/s] 69%|   | 539/780 [04:55<03:03,  1.31it/s] 69%|   | 540/780 [04:56<02:28,  1.61it/s] 69%|   | 541/780 [04:56<02:04,  1.92it/s] 69%|   | 542/780 [04:56<01:47,  2.22it/s] 70%|   | 543/780 [04:56<01:35,  2.48it/s] 70%|   | 544/780 [04:57<01:26,  2.71it/s] 70%|   | 545/780 [04:57<01:38,  2.39it/s] 70%|   | 546/780 [04:58<01:28,  2.64it/s] 70%|   | 547/780 [04:58<01:22,  2.84it/s] 70%|   | 548/780 [04:58<01:17,  3.00it/s] 70%|   | 549/780 [04:58<01:14,  3.12it/s] 71%|   | 550/780 [04:59<01:11,  3.21it/s] 71%|   | 551/780 [04:59<01:09,  3.28it/s] 71%|   | 552/780 [04:59<01:08,  3.33it/s] 71%|   | 553/780 [05:00<01:07,  3.36it/s] 71%|   | 554/780 [05:00<01:06,  3.39it/s] 71%|   | 555/780 [05:01<01:32,  2.45it/s] 71%|  | 556/780 [05:01<01:23,  2.68it/s] 71%|  | 557/780 [05:01<01:21,  2.72it/s] 72%|  | 558/780 [05:02<01:21,  2.73it/s] 72%|  | 559/780 [05:02<01:25,  2.59it/s] 72%|  | 560/780 [05:03<01:48,  2.03it/s] 72%|  | 561/780 [05:03<01:39,  2.21it/s] 72%|  | 562/780 [05:04<02:17,  1.59it/s] 72%|  | 563/780 [05:04<01:54,  1.89it/s] 72%|  | 564/780 [05:05<01:38,  2.19it/s] 72%|  | 565/780 [05:05<01:27,  2.46it/s] 73%|  | 566/780 [05:05<01:19,  2.69it/s] 73%|  | 567/780 [05:06<01:13,  2.88it/s] 73%|  | 568/780 [05:06<01:09,  3.03it/s] 73%|  | 569/780 [05:06<01:07,  3.14it/s] 73%|  | 570/780 [05:07<01:10,  2.97it/s] 73%|  | 571/780 [05:07<01:07,  3.10it/s] 73%|  | 572/780 [05:07<01:05,  3.20it/s] 73%|  | 573/780 [05:07<01:03,  3.27it/s] 74%|  | 574/780 [05:08<01:02,  3.32it/s] 74%|  | 575/780 [05:08<01:01,  3.36it/s] 74%|  | 576/780 [05:08<01:00,  3.39it/s] 74%|  | 577/780 [05:09<00:59,  3.40it/s] 74%|  | 578/780 [05:09<00:59,  3.42it/s] 74%|  | 579/780 [05:09<00:58,  3.43it/s] 74%|  | 580/780 [05:09<00:58,  3.44it/s] 74%|  | 581/780 [05:10<01:10,  2.81it/s] 75%|  | 582/780 [05:10<01:06,  2.97it/s] 75%|  | 583/780 [05:11<01:03,  3.10it/s] 75%|  | 584/780 [05:11<01:01,  3.20it/s] 75%|  | 585/780 [05:11<00:59,  3.27it/s] 75%|  | 586/780 [05:11<00:58,  3.32it/s] 75%|  | 587/780 [05:12<00:57,  3.36it/s] 75%|  | 588/780 [05:12<00:56,  3.38it/s] 76%|  | 589/780 [05:12<00:56,  3.41it/s] 76%|  | 590/780 [05:13<00:55,  3.41it/s] 76%|  | 591/780 [05:13<01:00,  3.13it/s] 76%|  | 592/780 [05:13<00:58,  3.22it/s] 76%|  | 593/780 [05:14<00:56,  3.29it/s] 76%|  | 594/780 [05:14<00:55,  3.33it/s] 76%|  | 595/780 [05:14<00:54,  3.37it/s] 76%|  | 596/780 [05:14<00:54,  3.39it/s] 77%|  | 597/780 [05:15<00:53,  3.41it/s] 77%|  | 598/780 [05:15<00:53,  3.42it/s] 77%|  | 599/780 [05:15<00:52,  3.43it/s] 77%|  | 600/780 [05:16<00:52,  3.44it/s] 77%|  | 601/780 [05:16<00:52,  3.43it/s] 77%|  | 602/780 [05:16<01:05,  2.73it/s] 77%|  | 603/780 [05:17<01:00,  2.91it/s] 77%|  | 604/780 [05:17<00:57,  3.06it/s] 78%|  | 605/780 [05:17<00:55,  3.16it/s] 78%|  | 606/780 [05:18<00:53,  3.24it/s] 78%|  | 607/780 [05:18<00:52,  3.30it/s] 78%|  | 608/780 [05:18<00:51,  3.34it/s] 78%|  | 609/780 [05:18<00:50,  3.37it/s] 78%|  | 610/780 [05:19<00:50,  3.40it/s] 78%|  | 611/780 [05:19<00:49,  3.41it/s] 78%|  | 612/780 [05:20<00:59,  2.83it/s] 79%|  | 613/780 [05:20<00:55,  2.99it/s] 79%|  | 614/780 [05:20<00:53,  3.11it/s] 79%|  | 615/780 [05:20<00:51,  3.20it/s] 79%|  | 616/780 [05:21<00:50,  3.28it/s] 79%|  | 617/780 [05:22<01:25,  1.92it/s] 79%|  | 618/780 [05:22<01:13,  2.21it/s] 79%|  | 619/780 [05:22<01:04,  2.48it/s] 79%|  | 620/780 [05:23<00:59,  2.71it/s] 80%|  | 621/780 [05:23<00:55,  2.89it/s] 80%|  | 622/780 [05:23<00:51,  3.04it/s] 80%|  | 623/780 [05:23<00:49,  3.15it/s] 80%|  | 624/780 [05:24<00:48,  3.23it/s][INFO|trainer.py:2140] 2023-08-28 21:24:08,043 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:24:08,043 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:24:08,043 >>   Batch size = 8
{'eval_loss': 0.930887758731842, 'eval_runtime': 9.8727, 'eval_samples_per_second': 352.182, 'eval_steps_per_second': 44.061, 'epoch': 3.0}
{'loss': 0.8322, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.62it/s][A
  3%|         | 12/435 [00:00<00:08, 48.69it/s][A
  4%|         | 17/435 [00:00<00:08, 46.93it/s][A
  5%|         | 22/435 [00:00<00:08, 46.16it/s][A
  6%|         | 27/435 [00:00<00:08, 45.62it/s][A
  7%|         | 32/435 [00:00<00:08, 45.47it/s][A
  9%|         | 37/435 [00:00<00:08, 45.29it/s][A
 10%|         | 42/435 [00:00<00:08, 44.81it/s][A
 11%|         | 47/435 [00:01<00:08, 44.66it/s][A
 12%|        | 52/435 [00:01<00:08, 44.73it/s][A
 13%|        | 57/435 [00:01<00:08, 44.87it/s][A
 14%|        | 62/435 [00:01<00:08, 44.88it/s][A
 15%|        | 67/435 [00:01<00:08, 44.99it/s][A
 17%|        | 72/435 [00:01<00:08, 44.98it/s][A
 18%|        | 77/435 [00:01<00:07, 45.00it/s][A
 19%|        | 82/435 [00:01<00:07, 44.80it/s][A
 20%|        | 87/435 [00:01<00:07, 44.63it/s][A
 21%|        | 92/435 [00:02<00:07, 44.66it/s][A
 22%|       | 97/435 [00:02<00:07, 44.70it/s][A
 23%|       | 102/435 [00:02<00:07, 44.69it/s][A
 25%|       | 107/435 [00:02<00:07, 44.84it/s][A
 26%|       | 112/435 [00:02<00:07, 44.90it/s][A
 27%|       | 117/435 [00:02<00:07, 44.95it/s][A
 28%|       | 122/435 [00:02<00:06, 44.97it/s][A
 29%|       | 127/435 [00:03<00:06, 44.98it/s][A
 30%|       | 132/435 [00:03<00:11, 26.62it/s][A
 31%|      | 137/435 [00:03<00:09, 30.37it/s][A
 33%|      | 142/435 [00:03<00:08, 33.68it/s][A
 34%|      | 147/435 [00:03<00:07, 36.42it/s][A
 35%|      | 152/435 [00:03<00:07, 38.69it/s][A
 36%|      | 157/435 [00:03<00:06, 40.44it/s][A
 37%|      | 162/435 [00:03<00:06, 41.74it/s][A
 38%|      | 167/435 [00:03<00:06, 42.63it/s][A
 40%|      | 172/435 [00:04<00:06, 42.95it/s][A
 41%|      | 177/435 [00:04<00:05, 43.36it/s][A
 42%|     | 182/435 [00:04<00:05, 43.80it/s][A
 43%|     | 187/435 [00:04<00:05, 44.11it/s][A
 44%|     | 192/435 [00:04<00:05, 44.43it/s][A
 45%|     | 197/435 [00:04<00:05, 44.58it/s][A
 46%|     | 202/435 [00:04<00:05, 44.75it/s][A
 48%|     | 207/435 [00:04<00:05, 44.88it/s][A
 49%|     | 212/435 [00:04<00:04, 44.81it/s][A
 50%|     | 217/435 [00:05<00:04, 44.74it/s][A
 51%|     | 222/435 [00:05<00:04, 44.55it/s][A
 52%|    | 227/435 [00:05<00:04, 44.61it/s][A
 53%|    | 232/435 [00:05<00:04, 44.68it/s][A
 54%|    | 237/435 [00:05<00:04, 44.77it/s][A
 56%|    | 242/435 [00:05<00:04, 44.92it/s][A
 57%|    | 247/435 [00:05<00:04, 44.96it/s][A
 58%|    | 252/435 [00:06<00:04, 44.92it/s][A
 59%|    | 257/435 [00:06<00:07, 25.36it/s][A
 60%|    | 262/435 [00:06<00:05, 29.18it/s][A
 61%|   | 267/435 [00:06<00:05, 32.64it/s][A
 63%|   | 272/435 [00:06<00:04, 35.57it/s][A
 64%|   | 277/435 [00:06<00:04, 38.02it/s][A
 65%|   | 282/435 [00:06<00:03, 39.92it/s][A
 66%|   | 287/435 [00:06<00:03, 41.35it/s][A
 67%|   | 292/435 [00:07<00:03, 42.26it/s][A
 68%|   | 297/435 [00:07<00:03, 42.78it/s][A
 69%|   | 302/435 [00:07<00:03, 43.28it/s][A
 71%|   | 307/435 [00:07<00:02, 43.72it/s][A
 72%|  | 312/435 [00:07<00:02, 44.07it/s][A
 73%|  | 317/435 [00:07<00:02, 44.44it/s][A
 74%|  | 322/435 [00:07<00:02, 44.59it/s][A
 75%|  | 327/435 [00:07<00:02, 44.73it/s][A
 76%|  | 332/435 [00:07<00:02, 44.83it/s][A
 77%|  | 337/435 [00:08<00:02, 44.74it/s][A
 79%|  | 342/435 [00:08<00:02, 44.62it/s][A
 80%|  | 347/435 [00:08<00:01, 44.52it/s][A
 81%|  | 352/435 [00:08<00:01, 44.66it/s][A
 82%| | 357/435 [00:08<00:01, 44.65it/s][A
 83%| | 362/435 [00:08<00:01, 44.84it/s][A
 84%| | 367/435 [00:08<00:01, 44.92it/s][A
 86%| | 372/435 [00:08<00:01, 45.01it/s][A
 87%| | 377/435 [00:09<00:01, 44.90it/s][A
 88%| | 382/435 [00:09<00:02, 22.84it/s][A
 89%| | 387/435 [00:09<00:01, 26.82it/s][A
 90%| | 392/435 [00:09<00:01, 30.53it/s][A
 91%|| 397/435 [00:09<00:01, 33.83it/s][A
 92%|| 402/435 [00:09<00:00, 36.57it/s][A
 94%|| 407/435 [00:09<00:00, 38.73it/s][A
 95%|| 412/435 [00:10<00:00, 40.41it/s][A
 96%|| 417/435 [00:10<00:00, 41.71it/s][A
 97%|| 422/435 [00:10<00:00, 42.25it/s][A
 98%|| 427/435 [00:10<00:00, 42.90it/s][A
 99%|| 432/435 [00:10<00:00, 43.44it/s][A                                                 
                                                 [A 80%|  | 624/780 [05:35<00:48,  3.23it/s]
100%|| 435/435 [00:10<00:00, 43.44it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:24:20,553 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 21:24:22,874 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:24:40,734 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:24:41,417 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:24:41,590 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [06:23<46:45, 18.10s/it] 80%|  | 626/780 [06:24<32:45, 12.76s/it] 80%|  | 627/780 [06:24<23:00,  9.02s/it] 81%|  | 628/780 [06:24<16:13,  6.40s/it] 81%|  | 629/780 [06:25<11:30,  4.57s/it] 81%|  | 630/780 [06:25<08:13,  3.29s/it] 81%|  | 631/780 [06:25<05:55,  2.39s/it] 81%|  | 632/780 [06:26<04:24,  1.79s/it] 81%|  | 633/780 [06:26<03:16,  1.34s/it] 81%| | 634/780 [06:26<02:29,  1.03s/it] 81%| | 635/780 [06:26<01:56,  1.24it/s] 82%| | 636/780 [06:27<01:33,  1.53it/s] 82%| | 637/780 [06:27<01:17,  1.84it/s] 82%| | 638/780 [06:27<01:06,  2.14it/s] 82%| | 639/780 [06:28<00:58,  2.42it/s] 82%| | 640/780 [06:28<00:52,  2.66it/s] 82%| | 641/780 [06:28<00:48,  2.86it/s] 82%| | 642/780 [06:28<00:49,  2.81it/s] 82%| | 643/780 [06:29<00:45,  2.98it/s] 83%| | 644/780 [06:29<00:43,  3.11it/s] 83%| | 645/780 [06:29<00:42,  3.20it/s] 83%| | 646/780 [06:30<00:40,  3.27it/s] 83%| | 647/780 [06:30<00:39,  3.33it/s] 83%| | 648/780 [06:30<00:39,  3.37it/s] 83%| | 649/780 [06:31<00:38,  3.39it/s] 83%| | 650/780 [06:31<00:38,  3.41it/s] 83%| | 651/780 [06:31<00:37,  3.43it/s] 84%| | 652/780 [06:31<00:37,  3.43it/s] 84%| | 653/780 [06:32<00:38,  3.28it/s] 84%| | 654/780 [06:32<00:37,  3.33it/s] 84%| | 655/780 [06:32<00:37,  3.37it/s] 84%| | 656/780 [06:33<00:36,  3.40it/s] 84%| | 657/780 [06:33<00:36,  3.41it/s] 84%| | 658/780 [06:33<00:35,  3.42it/s] 84%| | 659/780 [06:33<00:35,  3.43it/s] 85%| | 660/780 [06:34<00:34,  3.44it/s] 85%| | 661/780 [06:34<00:34,  3.45it/s] 85%| | 662/780 [06:34<00:34,  3.45it/s] 85%| | 663/780 [06:35<00:33,  3.45it/s] 85%| | 664/780 [06:35<00:40,  2.88it/s] 85%| | 665/780 [06:35<00:37,  3.03it/s] 85%| | 666/780 [06:36<00:36,  3.15it/s] 86%| | 667/780 [06:36<00:34,  3.24it/s] 86%| | 668/780 [06:36<00:33,  3.30it/s] 86%| | 669/780 [06:37<00:33,  3.34it/s] 86%| | 670/780 [06:37<00:32,  3.37it/s] 86%| | 671/780 [06:37<00:32,  3.40it/s] 86%| | 672/780 [06:37<00:31,  3.41it/s] 86%| | 673/780 [06:38<00:31,  3.42it/s] 86%| | 674/780 [06:38<00:32,  3.24it/s] 87%| | 675/780 [06:38<00:31,  3.30it/s] 87%| | 676/780 [06:39<00:31,  3.35it/s] 87%| | 677/780 [06:39<00:30,  3.38it/s] 87%| | 678/780 [06:39<00:29,  3.40it/s] 87%| | 679/780 [06:39<00:29,  3.42it/s] 87%| | 680/780 [06:40<00:29,  3.43it/s] 87%| | 681/780 [06:40<00:28,  3.44it/s] 87%| | 682/780 [06:40<00:28,  3.44it/s] 88%| | 683/780 [06:41<00:28,  3.44it/s] 88%| | 684/780 [06:41<00:27,  3.45it/s] 88%| | 685/780 [06:41<00:28,  3.30it/s] 88%| | 686/780 [06:42<00:28,  3.34it/s] 88%| | 687/780 [06:42<00:27,  3.38it/s] 88%| | 688/780 [06:42<00:27,  3.40it/s] 88%| | 689/780 [06:42<00:26,  3.41it/s] 88%| | 690/780 [06:43<00:26,  3.42it/s] 89%| | 691/780 [06:43<00:25,  3.43it/s] 89%| | 692/780 [06:43<00:25,  3.43it/s] 89%| | 693/780 [06:44<00:25,  3.44it/s] 89%| | 694/780 [06:44<00:24,  3.44it/s] 89%| | 695/780 [06:44<00:24,  3.45it/s] 89%| | 696/780 [06:45<00:38,  2.20it/s] 89%| | 697/780 [06:45<00:33,  2.47it/s] 89%| | 698/780 [06:46<00:30,  2.70it/s] 90%| | 699/780 [06:46<00:28,  2.89it/s] 90%| | 700/780 [06:46<00:26,  3.04it/s] 90%| | 701/780 [06:46<00:25,  3.15it/s] 90%| | 702/780 [06:47<00:24,  3.24it/s] 90%| | 703/780 [06:47<00:23,  3.29it/s] 90%| | 704/780 [06:47<00:22,  3.34it/s] 90%| | 705/780 [06:48<00:25,  2.97it/s] 91%| | 706/780 [06:48<00:23,  3.10it/s] 91%| | 707/780 [06:48<00:22,  3.20it/s] 91%| | 708/780 [06:49<00:22,  3.27it/s] 91%| | 709/780 [06:49<00:21,  3.32it/s] 91%| | 710/780 [06:49<00:20,  3.36it/s] 91%| | 711/780 [06:49<00:20,  3.39it/s] 91%|| 712/780 [06:50<00:19,  3.40it/s] 91%|| 713/780 [06:50<00:19,  3.42it/s] 92%|| 714/780 [06:50<00:19,  3.42it/s] 92%|| 715/780 [06:51<00:30,  2.17it/s] 92%|| 716/780 [06:52<00:26,  2.44it/s] 92%|| 717/780 [06:52<00:23,  2.67it/s] 92%|| 718/780 [06:52<00:21,  2.87it/s] 92%|| 719/780 [06:52<00:20,  3.02it/s] 92%|| 720/780 [06:53<00:19,  3.13it/s] 92%|| 721/780 [06:54<00:35,  1.68it/s] 93%|| 722/780 [06:54<00:29,  1.98it/s] 93%|| 723/780 [06:54<00:25,  2.27it/s] 93%|| 724/780 [06:55<00:22,  2.53it/s] 93%|| 725/780 [06:55<00:19,  2.75it/s] 93%|| 726/780 [06:55<00:18,  2.93it/s] 93%|| 727/780 [06:56<00:17,  3.07it/s] 93%|| 728/780 [06:56<00:18,  2.86it/s] 93%|| 729/780 [06:56<00:16,  3.01it/s] 94%|| 730/780 [06:57<00:15,  3.13it/s] 94%|| 731/780 [06:57<00:15,  3.22it/s] 94%|| 732/780 [06:57<00:14,  3.28it/s] 94%|| 733/780 [06:58<00:14,  3.33it/s] 94%|| 734/780 [06:58<00:13,  3.36it/s] 94%|| 735/780 [06:58<00:13,  3.38it/s] 94%|| 736/780 [06:58<00:12,  3.40it/s] 94%|| 737/780 [06:59<00:12,  3.42it/s] 95%|| 738/780 [06:59<00:15,  2.79it/s] 95%|| 739/780 [06:59<00:13,  2.96it/s] 95%|| 740/780 [07:00<00:12,  3.09it/s] 95%|| 741/780 [07:00<00:12,  3.19it/s] 95%|| 742/780 [07:00<00:11,  3.27it/s] 95%|| 743/780 [07:01<00:11,  3.32it/s] 95%|| 744/780 [07:01<00:10,  3.36it/s] 96%|| 745/780 [07:01<00:10,  3.38it/s] 96%|| 746/780 [07:02<00:11,  3.04it/s] 96%|| 747/780 [07:03<00:17,  1.87it/s] 96%|| 748/780 [07:04<00:24,  1.30it/s] 96%|| 749/780 [07:05<00:22,  1.35it/s] 96%|| 750/780 [07:05<00:18,  1.65it/s] 96%|| 751/780 [07:05<00:14,  1.96it/s] 96%|| 752/780 [07:05<00:12,  2.25it/s] 97%|| 753/780 [07:06<00:10,  2.51it/s] 97%|| 754/780 [07:06<00:10,  2.41it/s] 97%|| 755/780 [07:07<00:09,  2.65it/s] 97%|| 756/780 [07:07<00:08,  2.85it/s] 97%|| 757/780 [07:07<00:07,  2.99it/s] 97%|| 758/780 [07:07<00:07,  3.12it/s] 97%|| 759/780 [07:08<00:06,  3.21it/s] 97%|| 760/780 [07:08<00:06,  3.28it/s] 98%|| 761/780 [07:08<00:05,  3.33it/s] 98%|| 762/780 [07:09<00:05,  3.36it/s] 98%|| 763/780 [07:09<00:05,  3.39it/s] 98%|| 764/780 [07:09<00:05,  2.72it/s] 98%|| 765/780 [07:10<00:05,  2.90it/s] 98%|| 766/780 [07:10<00:04,  3.05it/s] 98%|| 767/780 [07:10<00:04,  3.16it/s] 98%|| 768/780 [07:11<00:03,  3.24it/s] 99%|| 769/780 [07:11<00:03,  3.30it/s] 99%|| 770/780 [07:11<00:02,  3.34it/s] 99%|| 771/780 [07:11<00:02,  3.37it/s] 99%|| 772/780 [07:12<00:02,  3.39it/s] 99%|| 773/780 [07:12<00:02,  3.41it/s] 99%|| 774/780 [07:12<00:01,  3.17it/s] 99%|| 775/780 [07:13<00:01,  3.25it/s] 99%|| 776/780 [07:13<00:01,  3.31it/s]100%|| 777/780 [07:13<00:00,  3.35it/s]100%|| 778/780 [07:14<00:00,  3.38it/s]100%|| 779/780 [07:14<00:00,  3.40it/s]100%|| 780/780 [07:14<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:25:58,234 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:25:58,234 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:25:58,234 >>   Batch size = 8
{'eval_loss': 0.931060791015625, 'eval_runtime': 10.6209, 'eval_samples_per_second': 327.372, 'eval_steps_per_second': 40.957, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.98it/s][A
  3%|         | 12/435 [00:00<00:08, 49.10it/s][A
  4%|         | 17/435 [00:00<00:08, 47.36it/s][A
  5%|         | 22/435 [00:00<00:08, 46.40it/s][A
  6%|         | 27/435 [00:00<00:08, 45.75it/s][A
  7%|         | 32/435 [00:01<00:15, 25.92it/s][A
  8%|         | 36/435 [00:01<00:21, 18.83it/s][A
  9%|         | 41/435 [00:01<00:16, 23.27it/s][A
 11%|         | 46/435 [00:01<00:14, 27.54it/s][A
 12%|        | 51/435 [00:01<00:12, 31.35it/s][A
 13%|        | 56/435 [00:01<00:10, 34.64it/s][A
 14%|        | 61/435 [00:01<00:10, 37.30it/s][A
 15%|        | 66/435 [00:01<00:09, 39.33it/s][A
 16%|        | 71/435 [00:02<00:08, 40.90it/s][A
 17%|        | 76/435 [00:02<00:08, 41.85it/s][A
 19%|        | 81/435 [00:02<00:08, 42.58it/s][A
 20%|        | 86/435 [00:02<00:08, 43.16it/s][A
 21%|        | 91/435 [00:02<00:07, 43.73it/s][A
 22%|       | 96/435 [00:02<00:07, 44.19it/s][A
 23%|       | 101/435 [00:02<00:07, 44.47it/s][A
 24%|       | 106/435 [00:02<00:07, 44.53it/s][A
 26%|       | 111/435 [00:02<00:07, 44.76it/s][A
 27%|       | 116/435 [00:03<00:07, 44.78it/s][A
 28%|       | 121/435 [00:03<00:07, 44.67it/s][A
 29%|       | 126/435 [00:03<00:06, 44.58it/s][A
 30%|       | 131/435 [00:03<00:06, 44.55it/s][A
 31%|      | 136/435 [00:03<00:06, 44.74it/s][A
 32%|      | 141/435 [00:03<00:06, 44.80it/s][A
 34%|      | 146/435 [00:03<00:06, 44.95it/s][A
 35%|      | 151/435 [00:03<00:06, 44.97it/s][A
 36%|      | 156/435 [00:04<00:06, 44.96it/s][A
 37%|      | 161/435 [00:04<00:09, 28.35it/s][A
 38%|      | 166/435 [00:04<00:08, 31.99it/s][A
 39%|      | 171/435 [00:04<00:07, 35.09it/s][A
 40%|      | 176/435 [00:04<00:06, 37.59it/s][A
 42%|     | 181/435 [00:04<00:06, 39.60it/s][A
 43%|     | 186/435 [00:04<00:06, 41.17it/s][A
 44%|     | 191/435 [00:04<00:05, 42.38it/s][A
 45%|     | 196/435 [00:05<00:05, 43.10it/s][A
 46%|     | 201/435 [00:05<00:05, 43.23it/s][A
 47%|     | 206/435 [00:05<00:05, 43.27it/s][A
 49%|     | 211/435 [00:05<00:05, 43.76it/s][A
 50%|     | 216/435 [00:05<00:04, 44.13it/s][A
 51%|     | 221/435 [00:05<00:04, 44.36it/s][A
 52%|    | 226/435 [00:05<00:04, 44.60it/s][A
 53%|    | 231/435 [00:05<00:04, 44.76it/s][A
 54%|    | 236/435 [00:05<00:04, 44.89it/s][A
 55%|    | 241/435 [00:06<00:04, 44.87it/s][A
 57%|    | 246/435 [00:06<00:04, 44.82it/s][A
 58%|    | 251/435 [00:06<00:04, 44.67it/s][A
 59%|    | 256/435 [00:06<00:04, 44.67it/s][A
 60%|    | 261/435 [00:06<00:03, 44.78it/s][A
 61%|    | 266/435 [00:06<00:03, 44.76it/s][A
 62%|   | 271/435 [00:06<00:03, 44.80it/s][A
 63%|   | 276/435 [00:06<00:03, 44.94it/s][A
 65%|   | 281/435 [00:07<00:03, 45.00it/s][A
 66%|   | 286/435 [00:07<00:06, 23.28it/s][A
 67%|   | 291/435 [00:07<00:05, 27.21it/s][A
 68%|   | 296/435 [00:07<00:04, 30.91it/s][A
 69%|   | 301/435 [00:07<00:03, 34.18it/s][A
 70%|   | 306/435 [00:07<00:03, 36.81it/s][A
 71%|  | 311/435 [00:07<00:03, 38.92it/s][A
 73%|  | 316/435 [00:08<00:02, 40.60it/s][A
 74%|  | 321/435 [00:08<00:02, 41.86it/s][A
 75%|  | 326/435 [00:08<00:02, 42.38it/s][A
 76%|  | 331/435 [00:08<00:02, 42.96it/s][A
 77%|  | 336/435 [00:08<00:02, 43.58it/s][A
 78%|  | 341/435 [00:08<00:02, 43.95it/s][A
 80%|  | 346/435 [00:08<00:02, 44.32it/s][A
 81%|  | 351/435 [00:08<00:01, 44.48it/s][A
 82%| | 356/435 [00:08<00:01, 44.64it/s][A
 83%| | 361/435 [00:09<00:01, 44.71it/s][A
 84%| | 366/435 [00:09<00:01, 44.69it/s][A
 85%| | 371/435 [00:09<00:01, 44.59it/s][A
 86%| | 376/435 [00:10<00:03, 17.30it/s][A
 88%| | 381/435 [00:10<00:02, 21.22it/s][A
 89%| | 386/435 [00:10<00:01, 25.23it/s][A
 90%| | 391/435 [00:10<00:01, 29.08it/s][A
 91%| | 396/435 [00:10<00:01, 32.57it/s][A
 92%|| 401/435 [00:10<00:00, 35.56it/s][A
 93%|| 406/435 [00:10<00:00, 37.98it/s][A
 94%|| 411/435 [00:10<00:00, 39.69it/s][A
 96%|| 416/435 [00:10<00:00, 40.82it/s][A
 97%|| 421/435 [00:11<00:00, 41.86it/s][A
 98%|| 426/435 [00:11<00:00, 42.81it/s][A
 99%|| 431/435 [00:11<00:00, 43.28it/s][A                                                 
                                                 [A100%|| 780/780 [07:25<00:00,  3.41it/s]
100%|| 435/435 [00:11<00:00, 43.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:26:10,375 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 21:26:10,941 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:26:29,178 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:26:29,851 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:26:30,474 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:26:56,786 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:26:56,852 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-156 (score: 0.9284214973449707).
                                                 100%|| 780/780 [08:47<00:00,  3.41it/s]100%|| 780/780 [08:47<00:00,  1.48it/s]
[INFO|trainer.py:1894] 2023-08-28 21:27:31,782 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 21:27:32,296 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:28:00,723 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:28:02,541 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:28:03,027 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:28:08,773 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:08,871 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:08,871 >>   train_loss               =     0.8182
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:08,872 >>   train_runtime            = 0:08:46.70
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:08,872 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:08,872 >>   train_samples_per_second =     94.931
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:08,872 >>   train_steps_per_second   =      1.481
{'eval_loss': 0.9332853555679321, 'eval_runtime': 11.3625, 'eval_samples_per_second': 306.008, 'eval_steps_per_second': 38.284, 'epoch': 5.0}
{'train_runtime': 526.7004, 'train_samples_per_second': 94.931, 'train_steps_per_second': 1.481, 'train_loss': 0.8181562961676182, 'epoch': 5.0}
08/28/2023 21:28:11 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:28:11,216 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:28:11,216 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:28:11,216 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:08, 50.38it/s]  3%|         | 12/435 [00:00<00:09, 46.96it/s]  4%|         | 17/435 [00:00<00:09, 46.33it/s]  5%|         | 22/435 [00:00<00:08, 45.89it/s]  6%|         | 27/435 [00:00<00:08, 45.85it/s]  7%|         | 32/435 [00:01<00:25, 15.69it/s]  9%|         | 37/435 [00:01<00:20, 19.87it/s] 10%|         | 42/435 [00:01<00:16, 24.13it/s] 11%|         | 47/435 [00:01<00:13, 28.21it/s] 12%|        | 52/435 [00:01<00:12, 31.88it/s] 13%|        | 57/435 [00:01<00:10, 35.08it/s] 14%|        | 62/435 [00:01<00:09, 37.75it/s] 15%|        | 67/435 [00:02<00:09, 39.72it/s] 17%|        | 72/435 [00:02<00:08, 40.96it/s] 18%|        | 77/435 [00:02<00:08, 42.02it/s] 19%|        | 82/435 [00:02<00:08, 42.96it/s] 20%|        | 87/435 [00:02<00:07, 43.73it/s] 21%|        | 92/435 [00:02<00:07, 44.30it/s] 22%|       | 97/435 [00:02<00:07, 44.64it/s] 23%|       | 102/435 [00:02<00:07, 44.91it/s] 25%|       | 107/435 [00:02<00:07, 45.07it/s] 26%|       | 112/435 [00:03<00:07, 45.02it/s] 27%|       | 117/435 [00:03<00:07, 44.76it/s] 28%|       | 122/435 [00:03<00:06, 44.84it/s] 29%|       | 127/435 [00:03<00:06, 44.96it/s] 30%|       | 132/435 [00:03<00:06, 45.03it/s] 31%|      | 137/435 [00:03<00:06, 45.19it/s] 33%|      | 142/435 [00:03<00:07, 39.13it/s] 34%|      | 147/435 [00:03<00:07, 40.85it/s] 35%|      | 152/435 [00:04<00:06, 42.11it/s] 36%|      | 157/435 [00:04<00:06, 42.94it/s] 37%|      | 162/435 [00:04<00:06, 43.64it/s] 38%|      | 167/435 [00:04<00:07, 36.28it/s] 40%|      | 172/435 [00:04<00:06, 38.63it/s] 41%|      | 177/435 [00:04<00:06, 40.49it/s] 42%|     | 182/435 [00:04<00:06, 41.99it/s] 43%|     | 187/435 [00:04<00:05, 42.94it/s] 44%|     | 192/435 [00:04<00:05, 43.70it/s] 45%|     | 197/435 [00:05<00:05, 44.20it/s] 46%|     | 202/435 [00:05<00:05, 44.60it/s] 48%|     | 207/435 [00:05<00:05, 44.42it/s] 49%|     | 212/435 [00:05<00:05, 44.29it/s] 50%|     | 217/435 [00:05<00:04, 44.41it/s] 51%|     | 222/435 [00:05<00:04, 44.66it/s] 52%|    | 227/435 [00:05<00:04, 44.59it/s] 53%|    | 232/435 [00:05<00:04, 45.14it/s] 54%|    | 237/435 [00:05<00:04, 45.23it/s] 56%|    | 242/435 [00:06<00:04, 45.36it/s] 57%|    | 247/435 [00:06<00:04, 45.31it/s] 58%|    | 252/435 [00:06<00:04, 45.15it/s] 59%|    | 257/435 [00:06<00:03, 44.92it/s] 60%|    | 262/435 [00:06<00:03, 44.86it/s] 61%|   | 267/435 [00:06<00:03, 44.92it/s] 63%|   | 272/435 [00:06<00:03, 44.97it/s] 64%|   | 277/435 [00:06<00:03, 45.15it/s] 65%|   | 282/435 [00:06<00:03, 45.24it/s] 66%|   | 287/435 [00:07<00:03, 45.27it/s] 67%|   | 292/435 [00:07<00:03, 45.26it/s] 68%|   | 297/435 [00:07<00:03, 45.15it/s] 69%|   | 302/435 [00:07<00:04, 31.48it/s] 71%|   | 307/435 [00:07<00:03, 34.61it/s] 72%|  | 312/435 [00:07<00:03, 37.29it/s] 73%|  | 317/435 [00:07<00:02, 39.47it/s] 74%|  | 322/435 [00:08<00:02, 41.09it/s] 75%|  | 327/435 [00:08<00:02, 42.40it/s] 76%|  | 332/435 [00:08<00:02, 43.33it/s] 77%|  | 337/435 [00:08<00:02, 43.89it/s] 79%|  | 342/435 [00:08<00:02, 43.92it/s] 80%|  | 347/435 [00:08<00:02, 43.99it/s] 81%|  | 352/435 [00:08<00:01, 44.20it/s] 82%| | 357/435 [00:08<00:01, 44.47it/s] 83%| | 362/435 [00:08<00:01, 44.68it/s] 84%| | 367/435 [00:09<00:01, 44.93it/s] 86%| | 372/435 [00:09<00:01, 44.98it/s] 87%| | 377/435 [00:09<00:01, 45.15it/s] 88%| | 382/435 [00:09<00:01, 45.22it/s] 89%| | 387/435 [00:09<00:01, 45.18it/s] 90%| | 392/435 [00:09<00:00, 44.97it/s] 91%|| 397/435 [00:09<00:00, 44.87it/s] 92%|| 402/435 [00:09<00:00, 44.98it/s] 94%|| 407/435 [00:09<00:00, 45.01it/s] 95%|| 412/435 [00:10<00:00, 45.15it/s] 96%|| 417/435 [00:10<00:00, 45.05it/s] 97%|| 422/435 [00:10<00:00, 45.13it/s] 98%|| 427/435 [00:10<00:00, 45.18it/s] 99%|| 432/435 [00:10<00:00, 23.05it/s]100%|| 435/435 [00:10<00:00, 39.89it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:28:22,157 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:22,157 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:22,157 >>   eval_loss               =     0.9284
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:22,157 >>   eval_runtime            = 0:00:10.94
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:22,157 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:22,157 >>   eval_samples_per_second =    317.803
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:22,157 >>   eval_steps_per_second   =      39.76
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:28:22,157 >>   perplexity              =     2.5305
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:47,891 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:47,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:47,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:47,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:47,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:28:49,243 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:28:49,322 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:28:50,202 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:28:51,454 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:28:51,454 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:54,888 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:55,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:55,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:55,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:55,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:28:56,196 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:28:56,198 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:28:56,835 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:28:57,296 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:28:57,296 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.37it/s]Extractor Predicting: 11it [00:07,  1.42it/s]Extractor Predicting: 12it [00:07,  1.45it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.36it/s]Extractor Predicting: 16it [00:10,  1.42it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.52it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.24it/s]Extractor Predicting: 21it [00:14,  1.33it/s]Extractor Predicting: 22it [00:15,  1.41it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:16,  1.32it/s]Extractor Predicting: 25it [00:17,  1.37it/s]Extractor Predicting: 26it [00:17,  1.41it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:19,  1.48it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:21,  1.50it/s]Extractor Predicting: 32it [00:21,  1.52it/s]Extractor Predicting: 33it [00:22,  1.50it/s]Extractor Predicting: 34it [00:23,  1.20it/s]Extractor Predicting: 35it [00:24,  1.31it/s]Extractor Predicting: 36it [00:24,  1.41it/s]Extractor Predicting: 37it [00:25,  1.46it/s]Extractor Predicting: 38it [00:26,  1.38it/s]Extractor Predicting: 39it [00:26,  1.45it/s]Extractor Predicting: 40it [00:27,  1.36it/s]Extractor Predicting: 41it [00:28,  1.45it/s]Extractor Predicting: 42it [00:29,  1.49it/s]Extractor Predicting: 43it [00:29,  1.50it/s]Extractor Predicting: 44it [00:30,  1.48it/s]Extractor Predicting: 45it [00:31,  1.49it/s]Extractor Predicting: 46it [00:31,  1.53it/s]Extractor Predicting: 47it [00:32,  1.56it/s]Extractor Predicting: 48it [00:32,  1.57it/s]Extractor Predicting: 49it [00:33,  1.37it/s]Extractor Predicting: 50it [00:34,  1.47it/s]Extractor Predicting: 51it [00:35,  1.51it/s]Extractor Predicting: 52it [00:35,  1.50it/s]Extractor Predicting: 53it [00:36,  1.51it/s]Extractor Predicting: 54it [00:37,  1.17it/s]Extractor Predicting: 55it [00:38,  1.26it/s]Extractor Predicting: 56it [00:38,  1.33it/s]Extractor Predicting: 57it [00:39,  1.39it/s]Extractor Predicting: 58it [00:40,  1.40it/s]Extractor Predicting: 59it [00:40,  1.48it/s]Extractor Predicting: 60it [00:41,  1.52it/s]Extractor Predicting: 61it [00:42,  1.55it/s]Extractor Predicting: 62it [00:42,  1.60it/s]Extractor Predicting: 63it [00:43,  1.30it/s]Extractor Predicting: 64it [00:44,  1.38it/s]Extractor Predicting: 65it [00:45,  1.45it/s]Extractor Predicting: 66it [00:45,  1.47it/s]Extractor Predicting: 67it [00:46,  1.51it/s]Extractor Predicting: 68it [00:47,  1.23it/s]Extractor Predicting: 69it [00:48,  1.32it/s]Extractor Predicting: 70it [00:48,  1.39it/s]Extractor Predicting: 71it [00:49,  1.44it/s]Extractor Predicting: 72it [00:50,  1.41it/s]Extractor Predicting: 73it [00:50,  1.46it/s]Extractor Predicting: 74it [00:51,  1.52it/s]Extractor Predicting: 75it [00:51,  1.58it/s]Extractor Predicting: 76it [00:52,  1.58it/s]Extractor Predicting: 77it [00:53,  1.20it/s]Extractor Predicting: 78it [00:54,  1.29it/s]Extractor Predicting: 79it [00:55,  1.36it/s]Extractor Predicting: 80it [00:55,  1.41it/s]Extractor Predicting: 81it [00:56,  1.32it/s]Extractor Predicting: 82it [00:57,  1.40it/s]Extractor Predicting: 83it [00:58,  1.12it/s]Extractor Predicting: 84it [00:59,  1.23it/s]Extractor Predicting: 85it [00:59,  1.32it/s]Extractor Predicting: 86it [01:00,  1.39it/s]Extractor Predicting: 87it [01:01,  1.30it/s]Extractor Predicting: 88it [01:01,  1.37it/s]Extractor Predicting: 89it [01:02,  1.45it/s]Extractor Predicting: 90it [01:03,  1.49it/s]Extractor Predicting: 91it [01:03,  1.56it/s]Extractor Predicting: 92it [01:04,  1.41it/s]Extractor Predicting: 93it [01:05,  1.49it/s]Extractor Predicting: 94it [01:05,  1.51it/s]Extractor Predicting: 95it [01:06,  1.58it/s]Extractor Predicting: 96it [01:07,  1.60it/s]Extractor Predicting: 97it [01:08,  1.36it/s]Extractor Predicting: 98it [01:08,  1.44it/s]Extractor Predicting: 99it [01:09,  1.46it/s]Extractor Predicting: 100it [01:09,  1.48it/s]Extractor Predicting: 101it [01:10,  1.50it/s]Extractor Predicting: 102it [01:11,  1.56it/s]Extractor Predicting: 103it [01:11,  1.61it/s]Extractor Predicting: 104it [01:12,  1.62it/s]Extractor Predicting: 105it [01:12,  1.62it/s]Extractor Predicting: 106it [01:13,  1.63it/s]Extractor Predicting: 107it [01:14,  1.65it/s]Extractor Predicting: 108it [01:14,  1.53it/s]Extractor Predicting: 109it [01:15,  1.56it/s]Extractor Predicting: 110it [01:16,  1.59it/s]Extractor Predicting: 111it [01:16,  1.61it/s]Extractor Predicting: 112it [01:17,  1.62it/s]Extractor Predicting: 113it [01:18,  1.46it/s]Extractor Predicting: 114it [01:18,  1.55it/s]Extractor Predicting: 115it [01:19,  1.58it/s]Extractor Predicting: 116it [01:19,  1.61it/s]Extractor Predicting: 117it [01:20,  1.64it/s]Extractor Predicting: 118it [01:21,  1.59it/s]Extractor Predicting: 119it [01:21,  1.62it/s]Extractor Predicting: 120it [01:22,  1.60it/s]Extractor Predicting: 121it [01:23,  1.58it/s]Extractor Predicting: 122it [01:23,  1.60it/s]Extractor Predicting: 123it [01:24,  1.50it/s]Extractor Predicting: 124it [01:25,  1.51it/s]Extractor Predicting: 125it [01:25,  1.55it/s]Extractor Predicting: 126it [01:26,  1.58it/s]Extractor Predicting: 127it [01:26,  1.58it/s]Extractor Predicting: 128it [01:27,  1.55it/s]Extractor Predicting: 129it [01:28,  1.57it/s]Extractor Predicting: 130it [01:28,  1.56it/s]Extractor Predicting: 131it [01:29,  1.58it/s]Extractor Predicting: 132it [01:30,  1.56it/s]Extractor Predicting: 133it [01:30,  1.60it/s]Extractor Predicting: 134it [01:31,  1.63it/s]Extractor Predicting: 135it [01:32,  1.51it/s]Extractor Predicting: 136it [01:32,  1.40it/s]Extractor Predicting: 137it [01:33,  1.47it/s]Extractor Predicting: 138it [01:34,  1.50it/s]Extractor Predicting: 139it [01:34,  1.51it/s]Extractor Predicting: 140it [01:35,  1.43it/s]Extractor Predicting: 141it [01:36,  1.50it/s]Extractor Predicting: 142it [01:36,  1.51it/s]Extractor Predicting: 143it [01:37,  1.56it/s]Extractor Predicting: 144it [01:38,  1.62it/s]Extractor Predicting: 144it [01:38,  1.47it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:18,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:18,554 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:18,555 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:18,555 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:18,555 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:31:19,329 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:31:19,330 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:31:20,008 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:31:21,066 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:31:21,066 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:27,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:27,423 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:27,424 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:27,424 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:27,424 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:31:28,155 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:31:28,156 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:31:29,356 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:31:29,524 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:31:29,524 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4249512670565302,
  "recall": 0.0626977279263733,
  "score": 0.10927318295739348,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.61it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:17,  1.64it/s]Extractor Predicting: 30it [00:18,  1.49it/s]Extractor Predicting: 31it [00:19,  1.52it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:22,  1.26it/s]Extractor Predicting: 36it [00:22,  1.36it/s]Extractor Predicting: 37it [00:23,  1.46it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:24,  1.58it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.46it/s]Extractor Predicting: 42it [00:26,  1.54it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:27,  1.57it/s]Extractor Predicting: 45it [00:28,  1.46it/s]Extractor Predicting: 46it [00:29,  1.49it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:31,  1.61it/s]Extractor Predicting: 50it [00:31,  1.53it/s]Extractor Predicting: 51it [00:32,  1.55it/s]Extractor Predicting: 52it [00:33,  1.55it/s]Extractor Predicting: 53it [00:33,  1.59it/s]Extractor Predicting: 54it [00:34,  1.57it/s]Extractor Predicting: 55it [00:35,  1.60it/s]Extractor Predicting: 56it [00:35,  1.60it/s]Extractor Predicting: 57it [00:36,  1.60it/s]Extractor Predicting: 58it [00:36,  1.58it/s]Extractor Predicting: 59it [00:37,  1.56it/s]Extractor Predicting: 60it [00:38,  1.57it/s]Extractor Predicting: 61it [00:38,  1.56it/s]Extractor Predicting: 62it [00:39,  1.58it/s]Extractor Predicting: 63it [00:40,  1.60it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:41,  1.57it/s]Extractor Predicting: 66it [00:42,  1.56it/s]Extractor Predicting: 67it [00:42,  1.57it/s]Extractor Predicting: 68it [00:43,  1.57it/s]Extractor Predicting: 69it [00:43,  1.58it/s]Extractor Predicting: 70it [00:44,  1.56it/s]Extractor Predicting: 71it [00:45,  1.58it/s]Extractor Predicting: 72it [00:45,  1.56it/s]Extractor Predicting: 73it [00:46,  1.54it/s]Extractor Predicting: 74it [00:47,  1.57it/s]Extractor Predicting: 75it [00:47,  1.57it/s]Extractor Predicting: 76it [00:48,  1.60it/s]Extractor Predicting: 77it [00:49,  1.53it/s]Extractor Predicting: 78it [00:49,  1.57it/s]Extractor Predicting: 79it [00:50,  1.59it/s]Extractor Predicting: 80it [00:50,  1.65it/s]Extractor Predicting: 81it [00:51,  1.66it/s]Extractor Predicting: 82it [00:52,  1.57it/s]Extractor Predicting: 83it [00:52,  1.59it/s]Extractor Predicting: 84it [00:53,  1.60it/s]Extractor Predicting: 85it [00:54,  1.59it/s]Extractor Predicting: 86it [00:54,  1.54it/s]Extractor Predicting: 87it [00:55,  1.19it/s]Extractor Predicting: 88it [00:56,  1.30it/s]Extractor Predicting: 89it [00:57,  1.36it/s]Extractor Predicting: 90it [00:57,  1.39it/s]Extractor Predicting: 91it [00:58,  1.27it/s]Extractor Predicting: 92it [00:59,  1.34it/s]Extractor Predicting: 93it [01:00,  1.41it/s]Extractor Predicting: 94it [01:00,  1.47it/s]Extractor Predicting: 95it [01:01,  1.50it/s]Extractor Predicting: 96it [01:02,  1.33it/s]Extractor Predicting: 97it [01:03,  1.37it/s]Extractor Predicting: 98it [01:03,  1.40it/s]Extractor Predicting: 99it [01:04,  1.44it/s]Extractor Predicting: 100it [01:04,  1.49it/s]Extractor Predicting: 101it [01:06,  1.09it/s]Extractor Predicting: 102it [01:07,  1.21it/s]Extractor Predicting: 103it [01:07,  1.28it/s]Extractor Predicting: 104it [01:08,  1.34it/s]Extractor Predicting: 105it [01:09,  1.33it/s]Extractor Predicting: 106it [01:09,  1.39it/s]Extractor Predicting: 107it [01:10,  1.44it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:11,  1.50it/s]Extractor Predicting: 110it [01:12,  1.37it/s]Extractor Predicting: 111it [01:13,  1.43it/s]Extractor Predicting: 112it [01:13,  1.46it/s]Extractor Predicting: 113it [01:14,  1.50it/s]Extractor Predicting: 114it [01:15,  1.54it/s]Extractor Predicting: 115it [01:15,  1.55it/s]Extractor Predicting: 116it [01:16,  1.51it/s]Extractor Predicting: 117it [01:17,  1.24it/s]Extractor Predicting: 118it [01:18,  1.33it/s]Extractor Predicting: 119it [01:18,  1.38it/s]Extractor Predicting: 120it [01:19,  1.43it/s]Extractor Predicting: 121it [01:20,  1.32it/s]Extractor Predicting: 122it [01:21,  1.41it/s]Extractor Predicting: 123it [01:21,  1.46it/s]Extractor Predicting: 124it [01:22,  1.50it/s]Extractor Predicting: 125it [01:22,  1.52it/s]Extractor Predicting: 126it [01:23,  1.58it/s]Extractor Predicting: 127it [01:24,  1.60it/s]Extractor Predicting: 128it [01:24,  1.58it/s]Extractor Predicting: 129it [01:25,  1.64it/s]Extractor Predicting: 130it [01:25,  1.63it/s]Extractor Predicting: 131it [01:27,  1.27it/s]Extractor Predicting: 132it [01:27,  1.37it/s]Extractor Predicting: 133it [01:28,  1.48it/s]Extractor Predicting: 134it [01:28,  1.54it/s]Extractor Predicting: 135it [01:29,  1.54it/s]Extractor Predicting: 136it [01:30,  1.51it/s]Extractor Predicting: 137it [01:30,  1.56it/s]Extractor Predicting: 138it [01:31,  1.63it/s]Extractor Predicting: 139it [01:31,  1.64it/s]Extractor Predicting: 140it [01:32,  1.63it/s]Extractor Predicting: 141it [01:33,  1.55it/s]Extractor Predicting: 142it [01:33,  1.59it/s]Extractor Predicting: 143it [01:34,  1.58it/s]Extractor Predicting: 144it [01:35,  1.58it/s]Extractor Predicting: 145it [01:35,  1.59it/s]Extractor Predicting: 146it [01:36,  1.36it/s]Extractor Predicting: 147it [01:37,  1.48it/s]Extractor Predicting: 148it [01:37,  1.54it/s]Extractor Predicting: 149it [01:38,  1.60it/s]Extractor Predicting: 150it [01:39,  1.66it/s]Extractor Predicting: 151it [01:39,  1.58it/s]Extractor Predicting: 152it [01:40,  1.63it/s]Extractor Predicting: 153it [01:40,  1.65it/s]Extractor Predicting: 154it [01:41,  1.70it/s]Extractor Predicting: 155it [01:42,  1.70it/s]Extractor Predicting: 156it [01:42,  1.67it/s]Extractor Predicting: 157it [01:43,  1.73it/s]Extractor Predicting: 158it [01:43,  1.73it/s]Extractor Predicting: 159it [01:44,  1.80it/s]Extractor Predicting: 160it [01:44,  1.86it/s]Extractor Predicting: 161it [01:45,  1.79it/s]Extractor Predicting: 162it [01:45,  1.75it/s]Extractor Predicting: 163it [01:46,  1.73it/s]Extractor Predicting: 164it [01:47,  1.67it/s]Extractor Predicting: 165it [01:47,  1.72it/s]Extractor Predicting: 166it [01:48,  1.75it/s]Extractor Predicting: 167it [01:48,  1.75it/s]Extractor Predicting: 168it [01:49,  1.73it/s]Extractor Predicting: 169it [01:49,  1.77it/s]Extractor Predicting: 170it [01:50,  1.66it/s]Extractor Predicting: 171it [01:51,  1.72it/s]Extractor Predicting: 172it [01:51,  1.74it/s]Extractor Predicting: 173it [01:52,  1.54it/s]Extractor Predicting: 174it [01:53,  1.53it/s]Extractor Predicting: 175it [01:53,  1.50it/s]Extractor Predicting: 176it [01:54,  1.52it/s]Extractor Predicting: 177it [01:55,  1.54it/s]Extractor Predicting: 178it [01:55,  1.53it/s]Extractor Predicting: 179it [01:56,  1.53it/s]Extractor Predicting: 180it [01:57,  1.54it/s]Extractor Predicting: 181it [01:57,  1.54it/s]Extractor Predicting: 182it [01:58,  1.55it/s]Extractor Predicting: 183it [01:59,  1.58it/s]Extractor Predicting: 184it [01:59,  1.58it/s]Extractor Predicting: 185it [02:00,  1.57it/s]Extractor Predicting: 186it [02:00,  1.60it/s]Extractor Predicting: 187it [02:01,  1.59it/s]Extractor Predicting: 188it [02:02,  1.57it/s]Extractor Predicting: 189it [02:02,  1.58it/s]Extractor Predicting: 190it [02:03,  1.54it/s]Extractor Predicting: 191it [02:04,  1.52it/s]Extractor Predicting: 192it [02:04,  1.53it/s]Extractor Predicting: 193it [02:05,  1.52it/s]Extractor Predicting: 194it [02:06,  1.52it/s]Extractor Predicting: 195it [02:07,  1.19it/s]Extractor Predicting: 196it [02:08,  1.30it/s]Extractor Predicting: 197it [02:08,  1.39it/s]Extractor Predicting: 198it [02:09,  1.44it/s]Extractor Predicting: 199it [02:09,  1.45it/s]Extractor Predicting: 200it [02:10,  1.46it/s]Extractor Predicting: 201it [02:11,  1.48it/s]Extractor Predicting: 202it [02:11,  1.47it/s]Extractor Predicting: 203it [02:12,  1.47it/s]Extractor Predicting: 204it [02:13,  1.28it/s]Extractor Predicting: 205it [02:14,  1.34it/s]Extractor Predicting: 206it [02:15,  1.39it/s]Extractor Predicting: 207it [02:15,  1.43it/s]Extractor Predicting: 208it [02:16,  1.45it/s]Extractor Predicting: 209it [02:17,  1.42it/s]Extractor Predicting: 210it [02:17,  1.44it/s]Extractor Predicting: 211it [02:18,  1.32it/s]Extractor Predicting: 212it [02:19,  1.39it/s]Extractor Predicting: 213it [02:19,  1.42it/s]Extractor Predicting: 214it [02:20,  1.46it/s]Extractor Predicting: 215it [02:21,  1.45it/s]Extractor Predicting: 216it [02:22,  1.20it/s]Extractor Predicting: 217it [02:23,  1.28it/s]Extractor Predicting: 218it [02:23,  1.34it/s]Extractor Predicting: 219it [02:24,  1.37it/s]Extractor Predicting: 220it [02:25,  1.35it/s]Extractor Predicting: 221it [02:25,  1.37it/s]Extractor Predicting: 222it [02:26,  1.39it/s]Extractor Predicting: 223it [02:27,  1.44it/s]Extractor Predicting: 224it [02:27,  1.47it/s]Extractor Predicting: 225it [02:28,  1.38it/s]Extractor Predicting: 226it [02:29,  1.39it/s]Extractor Predicting: 227it [02:30,  1.43it/s]Extractor Predicting: 228it [02:30,  1.46it/s]Extractor Predicting: 229it [02:31,  1.49it/s]Extractor Predicting: 230it [02:32,  1.37it/s]Extractor Predicting: 231it [02:32,  1.45it/s]Extractor Predicting: 232it [02:33,  1.53it/s]Extractor Predicting: 233it [02:34,  1.57it/s]Extractor Predicting: 234it [02:34,  1.54it/s]Extractor Predicting: 235it [02:35,  1.54it/s]Extractor Predicting: 236it [02:35,  1.56it/s]Extractor Predicting: 237it [02:36,  1.57it/s]Extractor Predicting: 238it [02:37,  1.59it/s]Extractor Predicting: 239it [02:37,  1.57it/s]Extractor Predicting: 240it [02:38,  1.33it/s]Extractor Predicting: 241it [02:39,  1.41it/s]Extractor Predicting: 242it [02:40,  1.50it/s]Extractor Predicting: 243it [02:40,  1.54it/s]Extractor Predicting: 244it [02:41,  1.62it/s]Extractor Predicting: 245it [02:41,  1.54it/s]Extractor Predicting: 246it [02:42,  1.54it/s]Extractor Predicting: 247it [02:43,  1.55it/s]Extractor Predicting: 248it [02:43,  1.55it/s]Extractor Predicting: 249it [02:44,  1.59it/s]Extractor Predicting: 250it [02:45,  1.43it/s]Extractor Predicting: 251it [02:45,  1.50it/s]Extractor Predicting: 252it [02:46,  1.57it/s]Extractor Predicting: 253it [02:47,  1.58it/s]Extractor Predicting: 254it [02:47,  1.56it/s]Extractor Predicting: 255it [02:48,  1.58it/s]Extractor Predicting: 256it [02:49,  1.54it/s]Extractor Predicting: 257it [02:49,  1.52it/s]Extractor Predicting: 258it [02:50,  1.54it/s]Extractor Predicting: 259it [02:51,  1.56it/s]Extractor Predicting: 260it [02:51,  1.55it/s]Extractor Predicting: 261it [02:52,  1.48it/s]Extractor Predicting: 262it [02:53,  1.51it/s]Extractor Predicting: 263it [02:53,  1.55it/s]Extractor Predicting: 264it [02:54,  1.55it/s]Extractor Predicting: 265it [02:54,  1.56it/s]Extractor Predicting: 266it [02:55,  1.49it/s]Extractor Predicting: 267it [02:56,  1.51it/s]Extractor Predicting: 268it [02:56,  1.51it/s]Extractor Predicting: 269it [02:57,  1.53it/s]Extractor Predicting: 270it [02:58,  1.54it/s]Extractor Predicting: 271it [02:59,  1.39it/s]Extractor Predicting: 272it [02:59,  1.46it/s]Extractor Predicting: 273it [03:00,  1.45it/s]Extractor Predicting: 274it [03:01,  1.46it/s]Extractor Predicting: 275it [03:01,  1.49it/s]Extractor Predicting: 276it [03:02,  1.47it/s]Extractor Predicting: 277it [03:03,  1.50it/s]Extractor Predicting: 278it [03:03,  1.35it/s]Extractor Predicting: 279it [03:04,  1.39it/s]Extractor Predicting: 280it [03:05,  1.44it/s]Extractor Predicting: 281it [03:06,  1.27it/s]Extractor Predicting: 282it [03:06,  1.33it/s]Extractor Predicting: 283it [03:07,  1.36it/s]Extractor Predicting: 284it [03:08,  1.46it/s]Extractor Predicting: 285it [03:08,  1.47it/s]Extractor Predicting: 286it [03:09,  1.45it/s]Extractor Predicting: 287it [03:10,  1.49it/s]Extractor Predicting: 288it [03:10,  1.51it/s]Extractor Predicting: 289it [03:11,  1.52it/s]Extractor Predicting: 290it [03:12,  1.53it/s]Extractor Predicting: 291it [03:13,  1.34it/s]Extractor Predicting: 292it [03:13,  1.38it/s]Extractor Predicting: 293it [03:14,  1.45it/s]Extractor Predicting: 294it [03:15,  1.45it/s]Extractor Predicting: 295it [03:15,  1.49it/s]Extractor Predicting: 296it [03:16,  1.32it/s]Extractor Predicting: 297it [03:17,  1.39it/s]Extractor Predicting: 298it [03:17,  1.42it/s]Extractor Predicting: 299it [03:18,  1.43it/s]Extractor Predicting: 300it [03:19,  1.48it/s]Extractor Predicting: 301it [03:20,  1.45it/s]Extractor Predicting: 302it [03:20,  1.50it/s]Extractor Predicting: 303it [03:21,  1.48it/s]Extractor Predicting: 304it [03:22,  1.48it/s]Extractor Predicting: 305it [03:22,  1.49it/s]Extractor Predicting: 306it [03:23,  1.48it/s]Extractor Predicting: 307it [03:24,  1.49it/s]Extractor Predicting: 308it [03:24,  1.52it/s]Extractor Predicting: 309it [03:25,  1.53it/s]Extractor Predicting: 310it [03:25,  1.56it/s]Extractor Predicting: 311it [03:26,  1.51it/s]Extractor Predicting: 312it [03:27,  1.57it/s]Extractor Predicting: 313it [03:27,  1.62it/s]Extractor Predicting: 314it [03:28,  1.64it/s]Extractor Predicting: 315it [03:28,  1.67it/s]Extractor Predicting: 316it [03:29,  1.55it/s]Extractor Predicting: 317it [03:30,  1.55it/s]Extractor Predicting: 318it [03:30,  1.55it/s]Extractor Predicting: 319it [03:31,  1.56it/s]Extractor Predicting: 320it [03:32,  1.55it/s]Extractor Predicting: 321it [03:33,  1.48it/s]Extractor Predicting: 322it [03:33,  1.51it/s]Extractor Predicting: 323it [03:34,  1.49it/s]Extractor Predicting: 324it [03:34,  1.50it/s]Extractor Predicting: 325it [03:35,  1.51it/s]Extractor Predicting: 326it [03:36,  1.45it/s]Extractor Predicting: 327it [03:36,  1.51it/s]Extractor Predicting: 328it [03:37,  1.52it/s]Extractor Predicting: 329it [03:38,  1.55it/s]Extractor Predicting: 330it [03:38,  1.56it/s]Extractor Predicting: 331it [03:39,  1.49it/s]Extractor Predicting: 332it [03:40,  1.50it/s]Extractor Predicting: 333it [03:40,  1.55it/s]Extractor Predicting: 334it [03:41,  1.56it/s]Extractor Predicting: 335it [03:42,  1.59it/s]Extractor Predicting: 336it [03:42,  1.53it/s]Extractor Predicting: 337it [03:43,  1.55it/s]Extractor Predicting: 338it [03:44,  1.57it/s]Extractor Predicting: 339it [03:44,  1.62it/s]Extractor Predicting: 340it [03:45,  1.63it/s]Extractor Predicting: 341it [03:46,  1.35it/s]Extractor Predicting: 342it [03:46,  1.41it/s]Extractor Predicting: 343it [03:47,  1.32it/s]Extractor Predicting: 344it [03:48,  1.42it/s]Extractor Predicting: 345it [03:49,  1.44it/s]Extractor Predicting: 346it [03:49,  1.48it/s]Extractor Predicting: 347it [03:50,  1.41it/s]Extractor Predicting: 348it [03:51,  1.46it/s]Extractor Predicting: 349it [03:51,  1.52it/s]Extractor Predicting: 350it [03:52,  1.55it/s]Extractor Predicting: 351it [03:52,  1.59it/s]Extractor Predicting: 352it [03:53,  1.33it/s]Extractor Predicting: 353it [03:54,  1.41it/s]Extractor Predicting: 354it [03:55,  1.49it/s]Extractor Predicting: 355it [03:55,  1.56it/s]Extractor Predicting: 356it [03:56,  1.56it/s]Extractor Predicting: 357it [03:57,  1.50it/s]Extractor Predicting: 358it [03:57,  1.54it/s]Extractor Predicting: 359it [03:58,  1.55it/s]Extractor Predicting: 360it [03:58,  1.58it/s]Extractor Predicting: 361it [03:59,  1.60it/s]Extractor Predicting: 362it [04:00,  1.27it/s]Extractor Predicting: 363it [04:01,  1.37it/s]Extractor Predicting: 364it [04:01,  1.45it/s]Extractor Predicting: 365it [04:02,  1.52it/s]Extractor Predicting: 366it [04:03,  1.54it/s]Extractor Predicting: 367it [04:03,  1.49it/s]Extractor Predicting: 368it [04:04,  1.51it/s]Extractor Predicting: 369it [04:05,  1.53it/s]Extractor Predicting: 370it [04:05,  1.60it/s]Extractor Predicting: 371it [04:06,  1.62it/s]Extractor Predicting: 372it [04:07,  1.32it/s]Extractor Predicting: 373it [04:07,  1.38it/s]Extractor Predicting: 374it [04:08,  1.45it/s]Extractor Predicting: 375it [04:09,  1.51it/s]Extractor Predicting: 376it [04:09,  1.55it/s]Extractor Predicting: 377it [04:10,  1.42it/s]Extractor Predicting: 378it [04:11,  1.51it/s]Extractor Predicting: 379it [04:11,  1.51it/s]Extractor Predicting: 380it [04:12,  1.54it/s]Extractor Predicting: 381it [04:13,  1.39it/s]Extractor Predicting: 382it [04:14,  1.38it/s]Extractor Predicting: 383it [04:14,  1.47it/s]Extractor Predicting: 384it [04:15,  1.55it/s]Extractor Predicting: 385it [04:15,  1.55it/s]Extractor Predicting: 386it [04:16,  1.58it/s]Extractor Predicting: 387it [04:17,  1.49it/s]Extractor Predicting: 388it [04:17,  1.52it/s]Extractor Predicting: 389it [04:18,  1.54it/s]Extractor Predicting: 390it [04:19,  1.56it/s]Extractor Predicting: 391it [04:19,  1.56it/s]Extractor Predicting: 392it [04:20,  1.58it/s]Extractor Predicting: 393it [04:20,  1.60it/s]Extractor Predicting: 394it [04:21,  1.54it/s]Extractor Predicting: 395it [04:22,  1.50it/s]Extractor Predicting: 396it [04:23,  1.50it/s]Extractor Predicting: 397it [04:23,  1.49it/s]Extractor Predicting: 398it [04:24,  1.46it/s]Extractor Predicting: 399it [04:25,  1.47it/s]Extractor Predicting: 400it [04:25,  1.52it/s]Extractor Predicting: 401it [04:26,  1.51it/s]Extractor Predicting: 402it [04:27,  1.49it/s]Extractor Predicting: 403it [04:27,  1.48it/s]Extractor Predicting: 404it [04:28,  1.45it/s]Extractor Predicting: 405it [04:29,  1.45it/s]Extractor Predicting: 406it [04:29,  1.46it/s]Extractor Predicting: 407it [04:30,  1.45it/s]Extractor Predicting: 408it [04:31,  1.41it/s]Extractor Predicting: 409it [04:31,  1.45it/s]Extractor Predicting: 410it [04:32,  1.44it/s]Extractor Predicting: 411it [04:33,  1.46it/s]Extractor Predicting: 412it [04:33,  1.48it/s]Extractor Predicting: 413it [04:34,  1.46it/s]Extractor Predicting: 414it [04:35,  1.50it/s]Extractor Predicting: 415it [04:35,  1.55it/s]Extractor Predicting: 416it [04:36,  1.55it/s]Extractor Predicting: 417it [04:37,  1.56it/s]Extractor Predicting: 418it [04:37,  1.52it/s]Extractor Predicting: 419it [04:38,  1.48it/s]Extractor Predicting: 420it [04:39,  1.49it/s]Extractor Predicting: 421it [04:39,  1.59it/s]Extractor Predicting: 421it [04:39,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:52,706 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:52,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:52,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:52,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:52,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:36:53,753 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:36:53,754 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:36:55,057 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:36:56,340 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:36:56,466 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:03,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:03,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:03,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:03,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:03,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:37:03,889 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:37:03,890 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:04,827 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:05,000 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:05,000 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.25154894671623296,
  "recall": 0.040217929668152554,
  "score": 0.06934836450593561,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.29it/s]Extractor Predicting: 7it [00:05,  1.34it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.30it/s]Extractor Predicting: 9it [00:06,  1.35it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:37:15,368 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:37:15,370 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:37:15,422 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:37:15,423 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:37:15,447 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:37:51,074 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:37:51,113 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:37:52,112 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:37:52,113 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:37:52,377 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:52,442 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:52,442 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:52,442 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:52,442 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:52,442 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:52,443 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.35,
  "recall": 0.01728395061728395,
  "score": 0.03294117647058824,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:37:52,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:53,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:54,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:54,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:55,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:56,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:56,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:57,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:58,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:58,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:59,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:00,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:00,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:01,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:01,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:02,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:03,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:03,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:04,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:05,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:05,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:06,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:07,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:07,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:15<04:55, 15.58s/it][WARNING|generation_utils.py:914] 2023-08-28 21:38:08,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:09,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:09,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:10,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:11,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:12,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:12,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:13,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:14,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:14,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:15,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:15,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:16,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:17,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:18,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:19,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:19,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:20,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:21,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:21,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:22,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:23,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:24,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:24,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:32<04:57, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-28 21:38:25,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:26,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:26,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:27,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:28,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:28,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:29,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:30,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:30,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:31,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:32,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:32,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:33,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:34,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:34,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:35,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:36,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:36,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:37,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:38,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:38,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:39,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:39,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:47<04:26, 15.68s/it][WARNING|generation_utils.py:914] 2023-08-28 21:38:40,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:41,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:41,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:42,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:43,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:43,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:44,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:45,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:45,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:46,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:47,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:47,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:48,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:49,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:50,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:51,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:51,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:52,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:53,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:54,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:54,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:55,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:03<04:12, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-28 21:38:56,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:56,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:57,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:58,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:58,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:59,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:00,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:00,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:01,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:01,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:02,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:03,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:03,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:04,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:05,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:05,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:06,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:06,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:07,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:08,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:08,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:09,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:10,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:10,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:11,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:12,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:19<04:01, 16.07s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:12,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:13,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:14,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:14,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:15,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:16,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:16,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:17,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:18,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:18,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:19,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:20,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:20,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:21,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:22,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:23,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:23,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:24,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:25,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:25,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:26,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:27,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:34<03:39, 15.71s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:27,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:28,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:29,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:29,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:30,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:31,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:32,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:32,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:33,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:34,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:35,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:35,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:36,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:37,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:37,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:38,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:39,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:40,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:40,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:41,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:42,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:42,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:43,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:51<03:27, 15.98s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:44,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:44,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:45,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:46,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:47,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:47,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:48,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:49,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:50,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:51,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:52,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:52,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:53,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:54,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:54,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:55,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:56,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:57,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:57,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:58,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:59,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:00,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:00,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:01,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:02,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:02,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:10<03:24, 17.04s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:03,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:04,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:05,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:05,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:06,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:07,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:08,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:08,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:09,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:10,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:11,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:11,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:12,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:13,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:13,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:14,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:15,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:15,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:16,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:17,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:18,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:19,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:26<03:04, 16.74s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:20,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:20,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:21,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:22,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:23,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:23,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:24,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:25,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:25,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:26,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:27,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:28,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:28,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:29,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:30,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:31,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:31,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:32,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:33,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:33,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:34,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:35,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:36,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:43<02:48, 16.83s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:36,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:37,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:38,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:39,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:39,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:40,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:41,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:42,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:42,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:43,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:44,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:45,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:46,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:47,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:48,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:48,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:49,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:50,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:51,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:52,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:52,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:53,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:01<02:32, 16.99s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:54,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:54,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:55,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:56,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:56,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:57,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:58,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:59,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:59,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:00,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:01,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:01,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:02,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:03,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:03,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:04,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:05,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:06,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:06,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:07,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:08,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:15<02:10, 16.28s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:08,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:09,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:10,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:11,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:11,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:12,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:12,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:13,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:14,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:15,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:15,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:16,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:17,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:18,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:18,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:19,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:20,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:21,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:21,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:22,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:22,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:23,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:24,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:32<01:53, 16.27s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:25,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:25,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:26,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:26,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:27,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:28,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:28,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:29,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:30,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:30,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:31,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:33,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:34,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:34,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:35,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:35,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:36,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:36,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:37,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:45<01:32, 15.46s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:38,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:39,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:39,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:40,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:42,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:44,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:45,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:45,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:46,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:47,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:47,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:49,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:50,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:50,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:51,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:51,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:53,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:01<01:17, 15.42s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:53,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:55,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:55,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:56,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:57,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:00,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:02,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:05,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:06,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:08,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:16<01:01, 15.36s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:09,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:09,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:10,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:11,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:11,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:15,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:15,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:17,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:17,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:18,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:20,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:20,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:21,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:21,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:23,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:30<00:45, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:23,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:24,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:26,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:26,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:29,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:29,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:32,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:34,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:35,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:35,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:36,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:38,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:40,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [04:48<00:31, 15.90s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:41,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:43,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:45,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:46,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:46,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:47,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:49,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:50,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:51,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:52,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:52,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:53,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:55,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:56,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:04<00:15, 15.92s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:57,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:00,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:02,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:03,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:05,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:06,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:08,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:09,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:09,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:10,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:12,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:13,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:21<00:00, 16.22s/it]Generating: 100%|| 20/20 [05:21<00:00, 16.07s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:32,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:32,650 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:32,650 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:32,650 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:32,650 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:43:33,967 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:43:33,968 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:43:34,958 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:43:36,195 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:43:36,529 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:41,022 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:41,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:41,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:41,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:41,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:43:42,558 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:43:42,560 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:43:43,783 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:43:44,244 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:43:44,404 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 581, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7903645833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.7877604166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : head of government .', 'success_rate': 0.8355978260869565, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : military branch .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 572, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7415865384615384, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : characters .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : crosses . Context : Later in the year ( 114346 ) , he married Brigitte of Breslau , daughter of Alexander II of Austria , the King of Austria . Head Entity : Brigitte of Breslau , Tail Entity : cross .\n']
['Relation : crosses . Context : Later in the year ( 114346 ) , he married Brigitte of Breslau , daughter of Alexander II of Austria , the King of Austria . Head Entity : Brigitte of Breslau , Tail Entity : cross .\n', 'Relation : crosses . Context : After the death of his father and successor , Emperor Sigismund II was transferred to the throne to form one of the two regents of the Imperial Military . Head Entity : emperor Sigismund II , Tail Entity : regents .\n']
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 404, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 520, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 612, 'raw': 832}
{'prompt': 'Relation : crosses .', 'success_rate': 0.7355769230769231, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.859375, 'errors': {'', '(\'" "\', \'distributed by\', \'\', \'The first of its types of " " , the first of its types of " " is , " . "\')', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : Following his leadership in the 1972 elections under the leadership of Vice President of Pakistan Shah Binali , Binali was elected as Prime Minister of Pakistan under the leadership of Chief Minister of India Bharti Naidu . Head Entity : Governor of Pakistan Shah Binali , Tail Entity : Field of Work .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8355978260869565, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8892045454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : participant .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : participating team . Context : On 31 March 2014 , the Armenian national squad played a friendly at Dagan Avituri Stadium in Baku in Azerbaijan Football League 2014 , against Azerbaijan Football League Armenia , Azerbaijan National Team . Head Entity : Azerbaijani national squad , Tail Entity : Azerbaijan Football League .\n']
['Relation : participating team . Context : On 31 March 2014 , the Armenian national squad played a friendly at Dagan Avituri Stadium in Baku in Azerbaijan Football League 2014 , against Azerbaijan Football League Armenia , Azerbaijan National Team . Head Entity : Azerbaijani national squad , Tail Entity : Azerbaijan Football League .\n', 'Relation : participating team . Context : After the 2008 Summer Olympics , the team moved to the 2011 Summer Olympics as an alternative option to the team . Head Entity : 2012 Summer Olympics , Tail Entity : 2011 Summer Olympics .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8152173913043478, 'errors': {'', "('Microsoft', 'platform', '', 'On November 8 2014 , Microsoft announced new support for .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 135, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 227, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 296, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 337, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 433, 'raw': 608}
{'target': 600, 'success': 457, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 503, 'raw': 704}
{'target': 600, 'success': 527, 'raw': 736}
{'target': 600, 'success': 551, 'raw': 768}
{'target': 600, 'success': 575, 'raw': 800}
{'target': 600, 'success': 601, 'raw': 832}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.7223557692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : publisher . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : season 2 , Tail Entity : the Walking Dead .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : publisher .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : spouse . Context : Later in Life , he married his third wife , actress Patricia Leavitt of " Rosemary " , who later married her second husband , the actor Harry Pym of " The Fifth Estate " . Head Entity : Patricia Leavitt , Tail Entity : Patricia Leavitt of " The Fifth Estate .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8464673913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 16137
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16237, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.60it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:02,  1.50it/s]Extractor Estimating: 4it [00:02,  1.50it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.52it/s]Extractor Estimating: 7it [00:04,  1.56it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.62it/s]Extractor Estimating: 10it [00:06,  1.67it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:07,  1.52it/s]Extractor Estimating: 13it [00:08,  1.54it/s]Extractor Estimating: 14it [00:08,  1.60it/s]Extractor Estimating: 15it [00:09,  1.60it/s]Extractor Estimating: 16it [00:10,  1.62it/s]Extractor Estimating: 17it [00:10,  1.56it/s]Extractor Estimating: 18it [00:11,  1.64it/s]Extractor Estimating: 19it [00:12,  1.62it/s]Extractor Estimating: 20it [00:12,  1.56it/s]Extractor Estimating: 21it [00:13,  1.60it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.62it/s]Extractor Estimating: 24it [00:15,  1.64it/s]Extractor Estimating: 25it [00:15,  1.61it/s]Extractor Estimating: 26it [00:16,  1.54it/s]Extractor Estimating: 27it [00:17,  1.52it/s]Extractor Estimating: 28it [00:18,  1.41it/s]Extractor Estimating: 29it [00:18,  1.39it/s]Extractor Estimating: 30it [00:19,  1.39it/s]Extractor Estimating: 31it [00:20,  1.16it/s]Extractor Estimating: 32it [00:21,  1.25it/s]Extractor Estimating: 33it [00:22,  1.30it/s]Extractor Estimating: 34it [00:22,  1.32it/s]Extractor Estimating: 35it [00:23,  1.28it/s]Extractor Estimating: 36it [00:24,  1.35it/s]Extractor Estimating: 37it [00:24,  1.38it/s]Extractor Estimating: 38it [00:26,  1.18it/s]Extractor Estimating: 39it [00:26,  1.26it/s]Extractor Estimating: 40it [00:27,  1.29it/s]Extractor Estimating: 41it [00:28,  1.30it/s]Extractor Estimating: 42it [00:29,  1.14it/s]Extractor Estimating: 43it [00:30,  1.20it/s]Extractor Estimating: 44it [00:31,  1.10it/s]Extractor Estimating: 45it [00:31,  1.17it/s]Extractor Estimating: 46it [00:32,  1.24it/s]Extractor Estimating: 47it [00:33,  1.31it/s]Extractor Estimating: 48it [00:34,  1.05it/s]Extractor Estimating: 49it [00:35,  1.12it/s]Extractor Estimating: 50it [00:36,  1.20it/s]Extractor Estimating: 51it [00:36,  1.32it/s]Extractor Estimating: 52it [00:37,  1.41it/s]Extractor Estimating: 53it [00:37,  1.41it/s]Extractor Estimating: 54it [00:38,  1.46it/s]Extractor Estimating: 55it [00:39,  1.47it/s]Extractor Estimating: 56it [00:39,  1.53it/s]Extractor Estimating: 57it [00:40,  1.60it/s]Extractor Estimating: 58it [00:41,  1.50it/s]Extractor Estimating: 59it [00:41,  1.52it/s]Extractor Estimating: 60it [00:42,  1.56it/s]Extractor Estimating: 61it [00:43,  1.61it/s]Extractor Estimating: 62it [00:43,  1.59it/s]Extractor Estimating: 63it [00:44,  1.57it/s]Extractor Estimating: 64it [00:44,  1.55it/s]Extractor Estimating: 65it [00:45,  1.63it/s]Extractor Estimating: 66it [00:46,  1.51it/s]Extractor Estimating: 67it [00:46,  1.52it/s]Extractor Estimating: 68it [00:47,  1.52it/s]Extractor Estimating: 69it [00:48,  1.52it/s]Extractor Estimating: 70it [00:48,  1.59it/s]Extractor Estimating: 71it [00:49,  1.53it/s]Extractor Estimating: 72it [00:50,  1.60it/s]Extractor Estimating: 73it [00:50,  1.67it/s]Extractor Estimating: 74it [00:51,  1.69it/s]Extractor Estimating: 75it [00:51,  1.77it/s]Extractor Estimating: 76it [00:52,  1.69it/s]Extractor Estimating: 77it [00:53,  1.60it/s]Extractor Estimating: 78it [00:53,  1.61it/s]Extractor Estimating: 79it [00:54,  1.59it/s]Extractor Estimating: 80it [00:55,  1.51it/s]Extractor Estimating: 81it [00:55,  1.55it/s]Extractor Estimating: 82it [00:56,  1.52it/s]Extractor Estimating: 83it [00:57,  1.51it/s]Extractor Estimating: 84it [00:57,  1.50it/s]Extractor Estimating: 85it [00:58,  1.52it/s]Extractor Estimating: 86it [00:59,  1.51it/s]Extractor Estimating: 87it [00:59,  1.48it/s]Extractor Estimating: 88it [01:00,  1.48it/s]Extractor Estimating: 89it [01:00,  1.54it/s]Extractor Estimating: 90it [01:01,  1.53it/s]Extractor Estimating: 91it [01:02,  1.53it/s]Extractor Estimating: 92it [01:02,  1.53it/s]Extractor Estimating: 93it [01:03,  1.54it/s]Extractor Estimating: 94it [01:04,  1.58it/s]Extractor Estimating: 95it [01:04,  1.52it/s]Extractor Estimating: 96it [01:05,  1.49it/s]Extractor Estimating: 97it [01:06,  1.53it/s]Extractor Estimating: 98it [01:06,  1.51it/s]Extractor Estimating: 99it [01:07,  1.50it/s]Extractor Estimating: 100it [01:08,  1.51it/s]Extractor Estimating: 101it [01:08,  1.49it/s]Extractor Estimating: 102it [01:09,  1.57it/s]Extractor Estimating: 103it [01:10,  1.52it/s]Extractor Estimating: 104it [01:10,  1.54it/s]Extractor Estimating: 105it [01:11,  1.50it/s]Extractor Estimating: 106it [01:12,  1.56it/s]Extractor Estimating: 107it [01:13,  1.38it/s]Extractor Estimating: 108it [01:13,  1.45it/s]Extractor Estimating: 109it [01:14,  1.51it/s]Extractor Estimating: 110it [01:14,  1.52it/s]Extractor Estimating: 111it [01:15,  1.48it/s]Extractor Estimating: 112it [01:16,  1.54it/s]Extractor Estimating: 113it [01:16,  1.56it/s]Extractor Estimating: 114it [01:17,  1.58it/s]Extractor Estimating: 115it [01:18,  1.52it/s]Extractor Estimating: 116it [01:18,  1.57it/s]Extractor Estimating: 117it [01:19,  1.56it/s]Extractor Estimating: 118it [01:19,  1.59it/s]Extractor Estimating: 119it [01:20,  1.65it/s]Extractor Estimating: 120it [01:21,  1.44it/s]Extractor Estimating: 121it [01:22,  1.48it/s]Extractor Estimating: 122it [01:22,  1.52it/s]Extractor Estimating: 123it [01:23,  1.33it/s]Extractor Estimating: 124it [01:24,  1.41it/s]Extractor Estimating: 125it [01:24,  1.43it/s]Extractor Estimating: 126it [01:25,  1.42it/s]Extractor Estimating: 127it [01:26,  1.39it/s]Extractor Estimating: 128it [01:27,  1.28it/s]Extractor Estimating: 129it [01:27,  1.37it/s]Extractor Estimating: 130it [01:28,  1.26it/s]Extractor Estimating: 131it [01:29,  1.32it/s]Extractor Estimating: 132it [01:30,  1.35it/s]Extractor Estimating: 133it [01:30,  1.42it/s]Extractor Estimating: 134it [01:31,  1.41it/s]Extractor Estimating: 135it [01:32,  1.42it/s]Extractor Estimating: 136it [01:32,  1.48it/s]Extractor Estimating: 137it [01:33,  1.49it/s]Extractor Estimating: 138it [01:34,  1.34it/s]Extractor Estimating: 139it [01:35,  1.35it/s]Extractor Estimating: 140it [01:35,  1.41it/s]Extractor Estimating: 141it [01:36,  1.41it/s]Extractor Estimating: 142it [01:37,  1.38it/s]Extractor Estimating: 143it [01:37,  1.41it/s]Extractor Estimating: 144it [01:38,  1.46it/s]Extractor Estimating: 145it [01:39,  1.45it/s]Extractor Estimating: 146it [01:39,  1.46it/s]Extractor Estimating: 147it [01:40,  1.43it/s]Extractor Estimating: 148it [01:41,  1.44it/s]Extractor Estimating: 149it [01:42,  1.43it/s]Extractor Estimating: 150it [01:42,  1.39it/s]Extractor Estimating: 151it [01:43,  1.46it/s]Extractor Estimating: 152it [01:44,  1.48it/s]Extractor Estimating: 153it [01:44,  1.59it/s]Extractor Estimating: 154it [01:45,  1.61it/s]Extractor Estimating: 155it [01:45,  1.61it/s]Extractor Estimating: 156it [01:46,  1.62it/s]Extractor Estimating: 157it [01:47,  1.60it/s]Extractor Estimating: 158it [01:47,  1.44it/s]Extractor Estimating: 159it [01:48,  1.51it/s]Extractor Estimating: 160it [01:49,  1.54it/s]Extractor Estimating: 161it [01:49,  1.49it/s]Extractor Estimating: 162it [01:50,  1.57it/s]Extractor Estimating: 163it [01:51,  1.59it/s]Extractor Estimating: 164it [01:51,  1.64it/s]Extractor Estimating: 165it [01:52,  1.71it/s]Extractor Estimating: 166it [01:52,  1.63it/s]Extractor Estimating: 167it [01:53,  1.71it/s]Extractor Estimating: 168it [01:53,  1.67it/s]Extractor Estimating: 169it [01:54,  1.64it/s]Extractor Estimating: 170it [01:55,  1.60it/s]Extractor Estimating: 171it [01:55,  1.57it/s]Extractor Estimating: 172it [01:56,  1.59it/s]Extractor Estimating: 173it [01:57,  1.63it/s]Extractor Estimating: 174it [01:57,  1.62it/s]Extractor Estimating: 175it [01:58,  1.46it/s]Extractor Estimating: 176it [01:59,  1.51it/s]Extractor Estimating: 177it [01:59,  1.48it/s]Extractor Estimating: 178it [02:00,  1.43it/s]Extractor Estimating: 179it [02:01,  1.47it/s]Extractor Estimating: 180it [02:01,  1.51it/s]Extractor Estimating: 181it [02:02,  1.53it/s]Extractor Estimating: 182it [02:03,  1.52it/s]Extractor Estimating: 183it [02:04,  1.44it/s]Extractor Estimating: 184it [02:04,  1.40it/s]Extractor Estimating: 185it [02:05,  1.40it/s]Extractor Estimating: 186it [02:06,  1.43it/s]Extractor Estimating: 187it [02:06,  1.45it/s]Extractor Estimating: 188it [02:07,  1.48it/s]Extractor Estimating: 189it [02:08,  1.47it/s]Extractor Estimating: 190it [02:09,  1.33it/s]Extractor Estimating: 191it [02:09,  1.36it/s]Extractor Estimating: 192it [02:10,  1.43it/s]Extractor Estimating: 193it [02:11,  1.35it/s]Extractor Estimating: 194it [02:11,  1.38it/s]Extractor Estimating: 195it [02:12,  1.39it/s]Extractor Estimating: 196it [02:13,  1.44it/s]Extractor Estimating: 197it [02:14,  1.41it/s]Extractor Estimating: 198it [02:14,  1.39it/s]Extractor Estimating: 199it [02:15,  1.43it/s]Extractor Estimating: 200it [02:16,  1.42it/s]Extractor Estimating: 201it [02:16,  1.46it/s]Extractor Estimating: 202it [02:17,  1.45it/s]Extractor Estimating: 203it [02:18,  1.51it/s]Extractor Estimating: 204it [02:18,  1.54it/s]Extractor Estimating: 205it [02:19,  1.29it/s]Extractor Estimating: 206it [02:20,  1.36it/s]Extractor Estimating: 207it [02:21,  1.38it/s]Extractor Estimating: 208it [02:21,  1.43it/s]Extractor Estimating: 209it [02:22,  1.49it/s]Extractor Estimating: 210it [02:22,  1.50it/s]Extractor Estimating: 211it [02:23,  1.50it/s]Extractor Estimating: 212it [02:24,  1.45it/s]Extractor Estimating: 213it [02:25,  1.32it/s]Extractor Estimating: 214it [02:26,  1.35it/s]Extractor Estimating: 215it [02:26,  1.27it/s]Extractor Estimating: 216it [02:27,  1.30it/s]Extractor Estimating: 217it [02:28,  1.35it/s]Extractor Estimating: 218it [02:28,  1.39it/s]Extractor Estimating: 219it [02:29,  1.44it/s]Extractor Estimating: 220it [02:30,  1.38it/s]Extractor Estimating: 221it [02:31,  1.36it/s]Extractor Estimating: 222it [02:32,  1.13it/s]Extractor Estimating: 223it [02:33,  1.22it/s]Extractor Estimating: 224it [02:33,  1.27it/s]Extractor Estimating: 225it [02:34,  1.32it/s]Extractor Estimating: 226it [02:35,  1.31it/s]Extractor Estimating: 227it [02:35,  1.34it/s]Extractor Estimating: 228it [02:36,  1.41it/s]Extractor Estimating: 229it [02:37,  1.45it/s]Extractor Estimating: 230it [02:37,  1.53it/s]Extractor Estimating: 231it [02:38,  1.59it/s]Extractor Estimating: 232it [02:38,  1.64it/s]Extractor Estimating: 233it [02:39,  1.55it/s]Extractor Estimating: 234it [02:40,  1.33it/s]Extractor Estimating: 235it [02:41,  1.39it/s]Extractor Estimating: 236it [02:41,  1.41it/s]Extractor Estimating: 237it [02:42,  1.44it/s]Extractor Estimating: 238it [02:43,  1.49it/s]Extractor Estimating: 239it [02:44,  1.36it/s]Extractor Estimating: 240it [02:44,  1.38it/s]Extractor Estimating: 241it [02:45,  1.44it/s]Extractor Estimating: 242it [02:46,  1.44it/s]Extractor Estimating: 243it [02:46,  1.48it/s]Extractor Estimating: 244it [02:47,  1.33it/s]Extractor Estimating: 245it [02:48,  1.40it/s]Extractor Estimating: 246it [02:48,  1.47it/s]Extractor Estimating: 247it [02:49,  1.32it/s]Extractor Estimating: 248it [02:50,  1.35it/s]Extractor Estimating: 249it [02:51,  1.41it/s]Extractor Estimating: 250it [02:51,  1.48it/s]Extractor Estimating: 251it [02:52,  1.43it/s]Extractor Estimating: 252it [02:53,  1.44it/s]Extractor Estimating: 253it [02:54,  1.35it/s]Extractor Estimating: 254it [02:54,  1.42it/s]Extractor Estimating: 255it [02:55,  1.39it/s]Extractor Estimating: 256it [02:56,  1.42it/s]Extractor Estimating: 257it [02:56,  1.45it/s]Extractor Estimating: 258it [02:58,  1.11it/s]Extractor Estimating: 259it [02:58,  1.20it/s]Extractor Estimating: 260it [02:59,  1.29it/s]Extractor Estimating: 261it [03:00,  1.36it/s]Extractor Estimating: 262it [03:00,  1.34it/s]Extractor Estimating: 263it [03:01,  1.36it/s]Extractor Estimating: 264it [03:02,  1.40it/s]Extractor Estimating: 265it [03:02,  1.43it/s]Extractor Estimating: 266it [03:03,  1.44it/s]Extractor Estimating: 267it [03:04,  1.43it/s]Extractor Estimating: 268it [03:05,  1.43it/s]Extractor Estimating: 269it [03:05,  1.39it/s]Extractor Estimating: 270it [03:06,  1.32it/s]Extractor Estimating: 271it [03:07,  1.38it/s]Extractor Estimating: 272it [03:07,  1.44it/s]Extractor Estimating: 273it [03:08,  1.48it/s]Extractor Estimating: 274it [03:09,  1.43it/s]Extractor Estimating: 275it [03:10,  1.42it/s]Extractor Estimating: 276it [03:10,  1.46it/s]Extractor Estimating: 277it [03:11,  1.50it/s]Extractor Estimating: 278it [03:12,  1.35it/s]Extractor Estimating: 279it [03:12,  1.40it/s]Extractor Estimating: 280it [03:13,  1.47it/s]Extractor Estimating: 281it [03:14,  1.52it/s]Extractor Estimating: 282it [03:14,  1.53it/s]Extractor Estimating: 283it [03:15,  1.59it/s]Extractor Estimating: 284it [03:15,  1.60it/s]Extractor Estimating: 285it [03:16,  1.60it/s]Extractor Estimating: 286it [03:17,  1.55it/s]Extractor Estimating: 287it [03:17,  1.56it/s]Extractor Estimating: 288it [03:18,  1.52it/s]Extractor Estimating: 289it [03:19,  1.54it/s]Extractor Estimating: 290it [03:19,  1.51it/s]Extractor Estimating: 291it [03:20,  1.55it/s]Extractor Estimating: 292it [03:21,  1.55it/s]Extractor Estimating: 293it [03:21,  1.56it/s]Extractor Estimating: 294it [03:22,  1.39it/s]Extractor Estimating: 295it [03:23,  1.43it/s]Extractor Estimating: 296it [03:24,  1.39it/s]Extractor Estimating: 297it [03:24,  1.47it/s]Extractor Estimating: 298it [03:25,  1.51it/s]Extractor Estimating: 299it [03:25,  1.50it/s]Extractor Estimating: 300it [03:26,  1.51it/s]Extractor Estimating: 301it [03:27,  1.50it/s]Extractor Estimating: 302it [03:28,  1.42it/s]Extractor Estimating: 303it [03:28,  1.41it/s]Extractor Estimating: 304it [03:29,  1.31it/s]Extractor Estimating: 305it [03:30,  1.37it/s]Extractor Estimating: 306it [03:31,  1.39it/s]Extractor Estimating: 307it [03:31,  1.44it/s]Extractor Estimating: 308it [03:32,  1.45it/s]Extractor Estimating: 309it [03:33,  1.38it/s]Extractor Estimating: 310it [03:33,  1.44it/s]Extractor Estimating: 311it [03:34,  1.45it/s]Extractor Estimating: 312it [03:35,  1.32it/s]Extractor Estimating: 313it [03:35,  1.42it/s]Extractor Estimating: 314it [03:36,  1.47it/s]Extractor Estimating: 315it [03:37,  1.51it/s]Extractor Estimating: 316it [03:38,  1.36it/s]Extractor Estimating: 317it [03:38,  1.43it/s]Extractor Estimating: 318it [03:39,  1.46it/s]Extractor Estimating: 319it [03:39,  1.51it/s]Extractor Estimating: 320it [03:40,  1.46it/s]Extractor Estimating: 321it [03:41,  1.43it/s]Extractor Estimating: 322it [03:42,  1.48it/s]Extractor Estimating: 323it [03:42,  1.44it/s]Extractor Estimating: 324it [03:43,  1.46it/s]Extractor Estimating: 325it [03:44,  1.47it/s]Extractor Estimating: 326it [03:44,  1.58it/s]Extractor Estimating: 327it [03:45,  1.57it/s]Extractor Estimating: 328it [03:46,  1.51it/s]Extractor Estimating: 329it [03:46,  1.58it/s]Extractor Estimating: 330it [03:47,  1.66it/s]Extractor Estimating: 331it [03:47,  1.70it/s]Extractor Estimating: 332it [03:48,  1.64it/s]Extractor Estimating: 333it [03:49,  1.60it/s]Extractor Estimating: 334it [03:49,  1.63it/s]Extractor Estimating: 335it [03:50,  1.64it/s]Extractor Estimating: 336it [03:51,  1.45it/s]Extractor Estimating: 337it [03:51,  1.51it/s]Extractor Estimating: 338it [03:52,  1.59it/s]Extractor Estimating: 339it [03:52,  1.60it/s]Extractor Estimating: 340it [03:53,  1.65it/s]Extractor Estimating: 341it [03:54,  1.37it/s]Extractor Estimating: 342it [03:55,  1.46it/s]Extractor Estimating: 343it [03:55,  1.52it/s]Extractor Estimating: 344it [03:56,  1.54it/s]Extractor Estimating: 345it [03:56,  1.59it/s]Extractor Estimating: 346it [03:57,  1.54it/s]Extractor Estimating: 347it [03:58,  1.61it/s]Extractor Estimating: 348it [03:58,  1.63it/s]Extractor Estimating: 349it [03:59,  1.68it/s]Extractor Estimating: 350it [03:59,  1.69it/s]Extractor Estimating: 351it [04:00,  1.72it/s]Extractor Estimating: 352it [04:01,  1.64it/s]Extractor Estimating: 353it [04:01,  1.65it/s]Extractor Estimating: 354it [04:02,  1.66it/s]Extractor Estimating: 355it [04:02,  1.64it/s]Extractor Estimating: 356it [04:03,  1.62it/s]Extractor Estimating: 357it [04:04,  1.60it/s]Extractor Estimating: 358it [04:04,  1.64it/s]Extractor Estimating: 359it [04:05,  1.65it/s]Extractor Estimating: 360it [04:05,  1.61it/s]Extractor Estimating: 361it [04:06,  1.64it/s]Extractor Estimating: 362it [04:07,  1.43it/s]Extractor Estimating: 363it [04:08,  1.45it/s]Extractor Estimating: 364it [04:08,  1.52it/s]Extractor Estimating: 365it [04:09,  1.59it/s]Extractor Estimating: 366it [04:09,  1.60it/s]Extractor Estimating: 367it [04:10,  1.38it/s]Extractor Estimating: 368it [04:11,  1.46it/s]Extractor Estimating: 369it [04:11,  1.56it/s]Extractor Estimating: 370it [04:12,  1.63it/s]Extractor Estimating: 371it [04:13,  1.67it/s]Extractor Estimating: 372it [04:13,  1.63it/s]Extractor Estimating: 373it [04:14,  1.69it/s]Extractor Estimating: 374it [04:14,  1.65it/s]Extractor Estimating: 375it [04:15,  1.62it/s]Extractor Estimating: 376it [04:16,  1.55it/s]Extractor Estimating: 377it [04:16,  1.59it/s]Extractor Estimating: 378it [04:17,  1.55it/s]Extractor Estimating: 379it [04:18,  1.39it/s]Extractor Estimating: 380it [04:19,  1.48it/s]Extractor Estimating: 381it [04:19,  1.44it/s]Extractor Estimating: 382it [04:20,  1.50it/s]Extractor Estimating: 383it [04:21,  1.48it/s]Extractor Estimating: 384it [04:21,  1.47it/s]Extractor Estimating: 385it [04:22,  1.54it/s]Extractor Estimating: 386it [04:22,  1.56it/s]Extractor Estimating: 387it [04:23,  1.59it/s]Extractor Estimating: 388it [04:24,  1.61it/s]Extractor Estimating: 389it [04:24,  1.58it/s]Extractor Estimating: 390it [04:25,  1.60it/s]Extractor Estimating: 391it [04:26,  1.61it/s]Extractor Estimating: 392it [04:26,  1.60it/s]Extractor Estimating: 393it [04:27,  1.63it/s]Extractor Estimating: 394it [04:27,  1.68it/s]Extractor Estimating: 395it [04:28,  1.72it/s]Extractor Estimating: 396it [04:28,  1.73it/s]Extractor Estimating: 397it [04:29,  1.76it/s]Extractor Estimating: 398it [04:30,  1.58it/s]Extractor Estimating: 399it [04:30,  1.62it/s]Extractor Estimating: 400it [04:31,  1.65it/s]Extractor Estimating: 401it [04:32,  1.58it/s]Extractor Estimating: 402it [04:32,  1.57it/s]Extractor Estimating: 403it [04:33,  1.58it/s]Extractor Estimating: 404it [04:33,  1.58it/s]Extractor Estimating: 405it [04:34,  1.61it/s]Extractor Estimating: 406it [04:35,  1.46it/s]Extractor Estimating: 407it [04:36,  1.51it/s]Extractor Estimating: 408it [04:36,  1.55it/s]Extractor Estimating: 409it [04:37,  1.52it/s]Extractor Estimating: 410it [04:37,  1.58it/s]Extractor Estimating: 411it [04:38,  1.55it/s]Extractor Estimating: 412it [04:39,  1.59it/s]Extractor Estimating: 413it [04:39,  1.60it/s]Extractor Estimating: 414it [04:40,  1.54it/s]Extractor Estimating: 415it [04:41,  1.55it/s]Extractor Estimating: 416it [04:41,  1.62it/s]Extractor Estimating: 417it [04:42,  1.66it/s]Extractor Estimating: 418it [04:42,  1.64it/s]Extractor Estimating: 419it [04:43,  1.64it/s]Extractor Estimating: 420it [04:44,  1.61it/s]Extractor Estimating: 421it [04:44,  1.67it/s]Extractor Estimating: 422it [04:45,  1.65it/s]Extractor Estimating: 423it [04:46,  1.50it/s]Extractor Estimating: 424it [04:46,  1.53it/s]Extractor Estimating: 425it [04:47,  1.54it/s]Extractor Estimating: 426it [04:47,  1.55it/s]Extractor Estimating: 427it [04:48,  1.54it/s]Extractor Estimating: 428it [04:49,  1.63it/s]Extractor Estimating: 429it [04:49,  1.67it/s]Extractor Estimating: 430it [04:50,  1.70it/s]Extractor Estimating: 431it [04:51,  1.52it/s]Extractor Estimating: 432it [04:51,  1.53it/s]Extractor Estimating: 433it [04:52,  1.54it/s]Extractor Estimating: 434it [04:53,  1.47it/s]Extractor Estimating: 435it [04:53,  1.54it/s]Extractor Estimating: 436it [04:54,  1.55it/s]Extractor Estimating: 437it [04:55,  1.55it/s]Extractor Estimating: 438it [04:55,  1.38it/s]Extractor Estimating: 439it [04:56,  1.44it/s]Extractor Estimating: 440it [04:57,  1.51it/s]Extractor Estimating: 441it [04:57,  1.57it/s]Extractor Estimating: 442it [04:58,  1.56it/s]Extractor Estimating: 443it [04:58,  1.63it/s]Extractor Estimating: 444it [04:59,  1.65it/s]Extractor Estimating: 445it [05:00,  1.69it/s]Extractor Estimating: 446it [05:00,  1.66it/s]Extractor Estimating: 447it [05:01,  1.59it/s]Extractor Estimating: 448it [05:01,  1.62it/s]Extractor Estimating: 449it [05:02,  1.66it/s]Extractor Estimating: 450it [05:03,  1.64it/s]Extractor Estimating: 451it [05:03,  1.59it/s]Extractor Estimating: 452it [05:04,  1.58it/s]Extractor Estimating: 453it [05:05,  1.53it/s]Extractor Estimating: 454it [05:05,  1.50it/s]Extractor Estimating: 455it [05:06,  1.30it/s]Extractor Estimating: 456it [05:07,  1.38it/s]Extractor Estimating: 457it [05:08,  1.44it/s]Extractor Estimating: 458it [05:08,  1.47it/s]Extractor Estimating: 459it [05:09,  1.50it/s]Extractor Estimating: 460it [05:10,  1.53it/s]Extractor Estimating: 461it [05:10,  1.54it/s]Extractor Estimating: 462it [05:11,  1.30it/s]Extractor Estimating: 463it [05:12,  1.36it/s]Extractor Estimating: 464it [05:13,  1.36it/s]Extractor Estimating: 465it [05:13,  1.37it/s]Extractor Estimating: 466it [05:14,  1.42it/s]Extractor Estimating: 467it [05:15,  1.39it/s]Extractor Estimating: 468it [05:15,  1.47it/s]Extractor Estimating: 469it [05:16,  1.53it/s]Extractor Estimating: 470it [05:17,  1.41it/s]Extractor Estimating: 471it [05:17,  1.42it/s]Extractor Estimating: 472it [05:18,  1.42it/s]Extractor Estimating: 473it [05:19,  1.27it/s]Extractor Estimating: 474it [05:20,  1.30it/s]Extractor Estimating: 475it [05:21,  1.33it/s]Extractor Estimating: 476it [05:21,  1.28it/s]Extractor Estimating: 477it [05:22,  1.39it/s]Extractor Estimating: 478it [05:23,  1.41it/s]Extractor Estimating: 479it [05:23,  1.43it/s]Extractor Estimating: 480it [05:24,  1.50it/s]Extractor Estimating: 481it [05:25,  1.51it/s]Extractor Estimating: 482it [05:25,  1.51it/s]Extractor Estimating: 483it [05:26,  1.52it/s]Extractor Estimating: 484it [05:27,  1.37it/s]Extractor Estimating: 485it [05:27,  1.42it/s]Extractor Estimating: 486it [05:28,  1.44it/s]Extractor Estimating: 487it [05:29,  1.20it/s]Extractor Estimating: 488it [05:30,  1.25it/s]Extractor Estimating: 489it [05:31,  1.34it/s]Extractor Estimating: 490it [05:31,  1.44it/s]Extractor Estimating: 491it [05:32,  1.44it/s]Extractor Estimating: 492it [05:33,  1.49it/s]Extractor Estimating: 493it [05:33,  1.46it/s]Extractor Estimating: 494it [05:34,  1.52it/s]Extractor Estimating: 495it [05:34,  1.57it/s]Extractor Estimating: 496it [05:35,  1.48it/s]Extractor Estimating: 497it [05:36,  1.46it/s]Extractor Estimating: 498it [05:37,  1.48it/s]Extractor Estimating: 499it [05:37,  1.35it/s]Extractor Estimating: 500it [05:38,  1.42it/s]Extractor Estimating: 500it [05:38,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:03,877 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:03,960 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:03,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:03,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:03,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:50:05,513 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:50:05,514 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:50:06,259 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:50:07,814 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:50:07,875 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:12,350 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:12,452 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:12,452 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:12,453 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:12,453 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:50:14,191 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:50:14,192 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:50:15,482 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:50:15,911 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:50:15,911 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:42:03,168 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:42:03,362 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9967 mean pseudo reward: 0.9200980841638025
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 28665
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28765, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28765, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.990, loss:867.4183
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.997, loss:803.3614
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.994, loss:808.4104
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.993, loss:815.2957
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.011, loss:760.1643
>> valid entity prec:0.5899, rec:0.5378, f1:0.5626
>> valid relation prec:0.1970, rec:0.0719, f1:0.1054
>> valid relation with NER prec:0.1970, rec:0.0719, f1:0.1054
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.263, loss:792.4325
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.995, loss:816.8762
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 0.996, loss:832.8717
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.986, loss:815.7775
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.988, loss:794.1291
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5455, rec:0.5647, f1:0.5549
>> valid relation prec:0.1622, rec:0.0733, f1:0.1010
>> valid relation with NER prec:0.1622, rec:0.0733, f1:0.1010
g_step 1100, step 268, avg_time 2.235, loss:831.7344
g_step 1200, step 368, avg_time 1.012, loss:813.7895
g_step 1300, step 52, avg_time 1.003, loss:762.0609
g_step 1400, step 152, avg_time 0.992, loss:771.3885
g_step 1500, step 252, avg_time 0.994, loss:771.2020
>> valid entity prec:0.5578, rec:0.5661, f1:0.5619
>> valid relation prec:0.2279, rec:0.1099, f1:0.1483
>> valid relation with NER prec:0.2279, rec:0.1099, f1:0.1483
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.265, loss:796.5661
g_step 1700, step 36, avg_time 0.993, loss:758.3160
g_step 1800, step 136, avg_time 1.022, loss:719.4505
g_step 1900, step 236, avg_time 0.989, loss:725.2261
g_step 2000, step 336, avg_time 1.000, loss:750.6554
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5096, rec:0.5973, f1:0.5500
>> valid relation prec:0.1166, rec:0.0661, f1:0.0844
>> valid relation with NER prec:0.1166, rec:0.0661, f1:0.0844
g_step 2100, step 20, avg_time 2.228, loss:722.5498
g_step 2200, step 120, avg_time 0.995, loss:693.0448
g_step 2300, step 220, avg_time 1.006, loss:712.2002
g_step 2400, step 320, avg_time 0.990, loss:723.2035
g_step 2500, step 4, avg_time 0.999, loss:679.6520
>> valid entity prec:0.5880, rec:0.5462, f1:0.5663
>> valid relation prec:0.1669, rec:0.0894, f1:0.1165
>> valid relation with NER prec:0.1669, rec:0.0894, f1:0.1165
new max entity f1 on valid!
g_step 2600, step 104, avg_time 2.233, loss:646.2590
g_step 2700, step 204, avg_time 0.992, loss:690.2903
g_step 2800, step 304, avg_time 1.001, loss:667.1777
g_step 2900, step 404, avg_time 0.998, loss:673.5069
g_step 3000, step 88, avg_time 1.000, loss:622.9421
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5557, rec:0.6281, f1:0.5897
>> valid relation prec:0.1440, rec:0.0972, f1:0.1161
>> valid relation with NER prec:0.1440, rec:0.0972, f1:0.1161
new max entity f1 on valid!
g_step 3100, step 188, avg_time 2.236, loss:650.5749
g_step 3200, step 288, avg_time 1.003, loss:639.4251
g_step 3300, step 388, avg_time 0.991, loss:649.7417
g_step 3400, step 72, avg_time 0.995, loss:583.4129
g_step 3500, step 172, avg_time 0.996, loss:596.1709
>> valid entity prec:0.6212, rec:0.5025, f1:0.5556
>> valid relation prec:0.2069, rec:0.0846, f1:0.1200
>> valid relation with NER prec:0.2069, rec:0.0846, f1:0.1200
g_step 3600, step 272, avg_time 2.221, loss:609.3279
g_step 3700, step 372, avg_time 1.000, loss:636.0082
g_step 3800, step 56, avg_time 0.990, loss:594.5721
g_step 3900, step 156, avg_time 1.001, loss:556.5277
g_step 4000, step 256, avg_time 0.990, loss:602.1996
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5595, rec:0.5344, f1:0.5466
>> valid relation prec:0.1323, rec:0.0693, f1:0.0910
>> valid relation with NER prec:0.1323, rec:0.0693, f1:0.0910
g_step 4100, step 356, avg_time 2.219, loss:592.4003
g_step 4200, step 40, avg_time 0.992, loss:582.8642
g_step 4300, step 140, avg_time 0.996, loss:544.3181
g_step 4400, step 240, avg_time 1.015, loss:562.2744
g_step 4500, step 340, avg_time 0.987, loss:578.7670
>> valid entity prec:0.5863, rec:0.5358, f1:0.5599
>> valid relation prec:0.1855, rec:0.0917, f1:0.1228
>> valid relation with NER prec:0.1855, rec:0.0917, f1:0.1228
g_step 4600, step 24, avg_time 2.221, loss:563.9506
g_step 4700, step 124, avg_time 0.986, loss:516.7191
g_step 4800, step 224, avg_time 0.993, loss:551.1530
g_step 4900, step 324, avg_time 0.992, loss:540.3811
g_step 5000, step 8, avg_time 1.006, loss:535.4311
learning rate was adjusted to 0.0008
>> valid entity prec:0.5904, rec:0.5266, f1:0.5567
>> valid relation prec:0.1777, rec:0.0831, f1:0.1133
>> valid relation with NER prec:0.1777, rec:0.0831, f1:0.1133
g_step 5100, step 108, avg_time 2.234, loss:497.4295
g_step 5200, step 208, avg_time 1.009, loss:519.6820
g_step 5300, step 308, avg_time 1.000, loss:528.4505
g_step 5400, step 408, avg_time 0.980, loss:541.0767
g_step 5500, step 92, avg_time 0.989, loss:483.6785
>> valid entity prec:0.5633, rec:0.5284, f1:0.5453
>> valid relation prec:0.1291, rec:0.0607, f1:0.0826
>> valid relation with NER prec:0.1291, rec:0.0607, f1:0.0826
g_step 5600, step 192, avg_time 2.250, loss:512.0877
g_step 5700, step 292, avg_time 0.983, loss:495.3898
g_step 5800, step 392, avg_time 1.009, loss:520.6196
g_step 5900, step 76, avg_time 1.001, loss:468.7080
g_step 6000, step 176, avg_time 1.006, loss:490.4421
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5707, rec:0.5123, f1:0.5399
>> valid relation prec:0.1275, rec:0.0659, f1:0.0869
>> valid relation with NER prec:0.1275, rec:0.0659, f1:0.0869
g_step 6100, step 276, avg_time 2.236, loss:477.1873
g_step 6200, step 376, avg_time 1.002, loss:484.6580
g_step 6300, step 60, avg_time 0.993, loss:470.1713
g_step 6400, step 160, avg_time 0.997, loss:439.1618
g_step 6500, step 260, avg_time 1.006, loss:443.1979
>> valid entity prec:0.5692, rec:0.5286, f1:0.5481
>> valid relation prec:0.1339, rec:0.0693, f1:0.0913
>> valid relation with NER prec:0.1339, rec:0.0693, f1:0.0913
g_step 6600, step 360, avg_time 2.239, loss:475.7773
g_step 6700, step 44, avg_time 1.002, loss:475.0686
g_step 6800, step 144, avg_time 1.002, loss:431.8109
g_step 6900, step 244, avg_time 0.997, loss:451.3600
g_step 7000, step 344, avg_time 1.003, loss:461.4852
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5681, rec:0.5114, f1:0.5383
>> valid relation prec:0.1129, rec:0.0624, f1:0.0804
>> valid relation with NER prec:0.1129, rec:0.0624, f1:0.0804
g_step 7100, step 28, avg_time 2.230, loss:448.3230
g_step 7200, step 128, avg_time 0.996, loss:414.9895
g_step 7300, step 228, avg_time 1.003, loss:418.6753
g_step 7400, step 328, avg_time 1.008, loss:442.7224
g_step 7500, step 12, avg_time 0.997, loss:440.7530
>> valid entity prec:0.5585, rec:0.5270, f1:0.5423
>> valid relation prec:0.1094, rec:0.0552, f1:0.0734
>> valid relation with NER prec:0.1094, rec:0.0552, f1:0.0734
g_step 7600, step 112, avg_time 2.228, loss:384.7937
g_step 7700, step 212, avg_time 0.999, loss:406.7717
g_step 7800, step 312, avg_time 1.006, loss:419.3744
g_step 7900, step 412, avg_time 1.009, loss:436.8888
g_step 8000, step 96, avg_time 1.000, loss:386.1906
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5570, rec:0.5391, f1:0.5479
>> valid relation prec:0.1370, rec:0.0848, f1:0.1048
>> valid relation with NER prec:0.1370, rec:0.0848, f1:0.1048
g_step 8100, step 196, avg_time 2.234, loss:401.0569
g_step 8200, step 296, avg_time 0.998, loss:412.4337
g_step 8300, step 396, avg_time 0.997, loss:413.0734
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:42:03 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:42:03 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-42-03_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:42:04 - WARNING - datasets.builder -   Using custom data configuration default-44330725636ea168
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-44330725636ea168/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:42:05,641 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:42:05,642 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:42:05,643 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:42:05,644 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:42:05,732 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:05,778 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:05,778 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:05,778 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:05,778 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:05,778 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:05,778 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:42:06,094 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:42:09,195 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:42:09,199 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-44330725636ea168/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.94ba/s] 18%|        | 2/11 [00:00<00:02,  3.77ba/s] 27%|       | 3/11 [00:00<00:01,  4.05ba/s] 36%|      | 4/11 [00:00<00:01,  4.24ba/s] 45%|     | 5/11 [00:01<00:01,  4.30ba/s] 55%|    | 6/11 [00:01<00:01,  3.59ba/s] 64%|   | 7/11 [00:01<00:01,  3.84ba/s] 73%|  | 8/11 [00:02<00:00,  4.03ba/s] 82%| | 9/11 [00:02<00:00,  4.16ba/s] 91%| | 10/11 [00:02<00:00,  4.26ba/s]100%|| 11/11 [00:02<00:00,  4.41ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.75ba/s] 50%|     | 2/4 [00:00<00:00,  4.09ba/s] 75%|  | 3/4 [00:00<00:00,  4.15ba/s]100%|| 4/4 [00:00<00:00,  5.26ba/s]100%|| 4/4 [00:00<00:00,  4.74ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  9.12ba/s] 27%|       | 3/11 [00:00<00:00, 10.33ba/s] 45%|     | 5/11 [00:00<00:00, 10.59ba/s] 64%|   | 7/11 [00:00<00:00, 10.53ba/s] 82%| | 9/11 [00:00<00:00, 10.52ba/s]100%|| 11/11 [00:00<00:00, 11.46ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  7.96ba/s] 75%|  | 3/4 [00:00<00:00,  9.86ba/s]100%|| 4/4 [00:00<00:00, 11.21ba/s]
[INFO|trainer.py:414] 2023-08-29 00:42:14,338 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:42:14,358 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:42:14,358 >>   Num examples = 10010
[INFO|trainer.py:1149] 2023-08-29 00:42:14,358 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:42:14,358 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:42:14,358 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:42:14,358 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:42:14,358 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:55,  3.31it/s]  0%|          | 2/780 [00:00<03:50,  3.38it/s]  0%|          | 3/780 [00:00<03:48,  3.40it/s]  1%|          | 4/780 [00:01<03:47,  3.41it/s]  1%|          | 5/780 [00:01<03:46,  3.42it/s]  1%|          | 6/780 [00:01<03:46,  3.42it/s]  1%|          | 7/780 [00:02<03:46,  3.42it/s]  1%|          | 8/780 [00:02<03:46,  3.41it/s]  1%|          | 9/780 [00:02<03:45,  3.42it/s]  1%|         | 10/780 [00:02<03:44,  3.42it/s]  1%|         | 11/780 [00:03<03:44,  3.43it/s]  2%|         | 12/780 [00:03<03:44,  3.43it/s]  2%|         | 13/780 [00:03<03:43,  3.43it/s]  2%|         | 14/780 [00:04<03:43,  3.43it/s]  2%|         | 15/780 [00:04<03:42,  3.43it/s]  2%|         | 16/780 [00:04<03:42,  3.44it/s]  2%|         | 17/780 [00:04<03:41,  3.44it/s]  2%|         | 18/780 [00:05<03:41,  3.44it/s]  2%|         | 19/780 [00:05<03:41,  3.43it/s]  3%|         | 20/780 [00:05<03:41,  3.44it/s]  3%|         | 21/780 [00:06<03:41,  3.43it/s]  3%|         | 22/780 [00:06<03:41,  3.43it/s]  3%|         | 23/780 [00:06<03:40,  3.43it/s]  3%|         | 24/780 [00:07<03:40,  3.43it/s]  3%|         | 25/780 [00:07<03:41,  3.41it/s]  3%|         | 26/780 [00:07<03:40,  3.42it/s]  3%|         | 27/780 [00:07<03:39,  3.42it/s]  4%|         | 28/780 [00:08<03:39,  3.43it/s]  4%|         | 29/780 [00:08<03:39,  3.42it/s]  4%|         | 30/780 [00:08<03:38,  3.43it/s]  4%|         | 31/780 [00:09<03:38,  3.43it/s]  4%|         | 32/780 [00:09<03:37,  3.43it/s]  4%|         | 33/780 [00:09<03:37,  3.43it/s]  4%|         | 34/780 [00:09<03:37,  3.43it/s]  4%|         | 35/780 [00:10<03:37,  3.43it/s]  5%|         | 36/780 [00:10<03:36,  3.43it/s]  5%|         | 37/780 [00:10<03:36,  3.43it/s]  5%|         | 38/780 [00:11<03:36,  3.43it/s]  5%|         | 39/780 [00:11<03:36,  3.43it/s]  5%|         | 40/780 [00:11<03:35,  3.43it/s]  5%|         | 41/780 [00:11<03:35,  3.43it/s]  5%|         | 42/780 [00:12<03:35,  3.43it/s]  6%|         | 43/780 [00:12<03:36,  3.41it/s]  6%|         | 44/780 [00:12<03:35,  3.42it/s]  6%|         | 45/780 [00:13<03:34,  3.42it/s]  6%|         | 46/780 [00:13<03:34,  3.43it/s]  6%|         | 47/780 [00:13<03:33,  3.43it/s]  6%|         | 48/780 [00:14<03:33,  3.43it/s]  6%|         | 49/780 [00:14<03:33,  3.43it/s]  6%|         | 50/780 [00:14<03:32,  3.43it/s]  7%|         | 51/780 [00:14<03:32,  3.43it/s]  7%|         | 52/780 [00:15<03:31,  3.43it/s]  7%|         | 53/780 [00:15<03:31,  3.43it/s]  7%|         | 54/780 [00:15<03:42,  3.26it/s]  7%|         | 55/780 [00:16<03:39,  3.31it/s]  7%|         | 56/780 [00:16<03:36,  3.35it/s]  7%|         | 57/780 [00:16<03:34,  3.37it/s]  7%|         | 58/780 [00:16<03:32,  3.39it/s]  8%|         | 59/780 [00:17<03:31,  3.40it/s]  8%|         | 60/780 [00:17<03:30,  3.41it/s]  8%|         | 61/780 [00:17<03:30,  3.42it/s]  8%|         | 62/780 [00:18<03:29,  3.42it/s]  8%|         | 63/780 [00:18<03:29,  3.42it/s]  8%|         | 64/780 [00:18<03:29,  3.43it/s]  8%|         | 65/780 [00:19<03:42,  3.21it/s]  8%|         | 66/780 [00:19<03:38,  3.27it/s]  9%|         | 67/780 [00:19<03:35,  3.32it/s]  9%|         | 68/780 [00:19<03:32,  3.35it/s]  9%|         | 69/780 [00:20<03:30,  3.37it/s]  9%|         | 70/780 [00:20<03:29,  3.39it/s]  9%|         | 71/780 [00:20<03:28,  3.40it/s]  9%|         | 72/780 [00:21<03:27,  3.41it/s]  9%|         | 73/780 [00:21<03:26,  3.42it/s]  9%|         | 74/780 [00:21<03:26,  3.42it/s] 10%|         | 75/780 [00:21<03:25,  3.42it/s] 10%|         | 76/780 [00:22<03:36,  3.25it/s] 10%|         | 77/780 [00:22<03:33,  3.30it/s] 10%|         | 78/780 [00:22<03:30,  3.33it/s] 10%|         | 79/780 [00:23<03:28,  3.36it/s] 10%|         | 80/780 [00:23<03:27,  3.38it/s] 10%|         | 81/780 [00:23<03:26,  3.39it/s] 11%|         | 82/780 [00:24<03:25,  3.40it/s] 11%|         | 83/780 [00:24<03:24,  3.41it/s] 11%|         | 84/780 [00:24<03:23,  3.42it/s] 11%|         | 85/780 [00:24<03:23,  3.42it/s] 11%|         | 86/780 [00:25<03:23,  3.42it/s] 11%|         | 87/780 [00:25<03:22,  3.42it/s] 11%|        | 88/780 [00:25<03:22,  3.42it/s] 11%|        | 89/780 [00:26<03:21,  3.42it/s] 12%|        | 90/780 [00:26<03:21,  3.43it/s] 12%|        | 91/780 [00:26<03:21,  3.42it/s] 12%|        | 92/780 [00:27<03:20,  3.42it/s] 12%|        | 93/780 [00:27<03:20,  3.42it/s] 12%|        | 94/780 [00:27<03:20,  3.42it/s] 12%|        | 95/780 [00:27<03:23,  3.36it/s] 12%|        | 96/780 [00:28<03:22,  3.38it/s] 12%|        | 97/780 [00:28<03:21,  3.39it/s] 13%|        | 98/780 [00:28<03:20,  3.40it/s] 13%|        | 99/780 [00:29<03:19,  3.41it/s] 13%|        | 100/780 [00:29<03:18,  3.42it/s] 13%|        | 101/780 [00:29<03:18,  3.42it/s] 13%|        | 102/780 [00:29<03:17,  3.42it/s] 13%|        | 103/780 [00:30<03:17,  3.43it/s] 13%|        | 104/780 [00:30<03:17,  3.43it/s] 13%|        | 105/780 [00:30<03:17,  3.42it/s] 14%|        | 106/780 [00:31<03:16,  3.42it/s] 14%|        | 107/780 [00:31<03:16,  3.42it/s] 14%|        | 108/780 [00:31<03:16,  3.42it/s] 14%|        | 109/780 [00:31<03:15,  3.43it/s] 14%|        | 110/780 [00:32<03:15,  3.43it/s] 14%|        | 111/780 [00:32<03:15,  3.43it/s] 14%|        | 112/780 [00:32<03:14,  3.43it/s] 14%|        | 113/780 [00:33<03:18,  3.36it/s] 15%|        | 114/780 [00:33<03:17,  3.38it/s] 15%|        | 115/780 [00:33<03:16,  3.39it/s] 15%|        | 116/780 [00:34<03:15,  3.40it/s] 15%|        | 117/780 [00:34<03:14,  3.41it/s] 15%|        | 118/780 [00:34<03:13,  3.41it/s] 15%|        | 119/780 [00:34<03:13,  3.42it/s] 15%|        | 120/780 [00:35<03:12,  3.42it/s] 16%|        | 121/780 [00:35<03:12,  3.42it/s] 16%|        | 122/780 [00:35<03:12,  3.42it/s] 16%|        | 123/780 [00:36<03:11,  3.43it/s] 16%|        | 124/780 [00:36<03:11,  3.43it/s] 16%|        | 125/780 [00:36<03:11,  3.43it/s] 16%|        | 126/780 [00:36<03:10,  3.43it/s] 16%|        | 127/780 [00:37<03:10,  3.43it/s] 16%|        | 128/780 [00:37<03:10,  3.43it/s] 17%|        | 129/780 [00:37<03:09,  3.43it/s] 17%|        | 130/780 [00:38<03:18,  3.28it/s] 17%|        | 131/780 [00:38<03:15,  3.32it/s] 17%|        | 132/780 [00:38<03:13,  3.35it/s] 17%|        | 133/780 [00:39<03:11,  3.37it/s] 17%|        | 134/780 [00:39<03:10,  3.39it/s] 17%|        | 135/780 [00:39<03:09,  3.40it/s] 17%|        | 136/780 [00:39<03:09,  3.41it/s] 18%|        | 137/780 [00:40<03:08,  3.41it/s] 18%|        | 138/780 [00:40<03:08,  3.41it/s] 18%|        | 139/780 [00:40<03:07,  3.42it/s] 18%|        | 140/780 [00:41<03:07,  3.42it/s] 18%|        | 141/780 [00:41<03:06,  3.42it/s] 18%|        | 142/780 [00:41<03:06,  3.42it/s] 18%|        | 143/780 [00:41<03:06,  3.42it/s] 18%|        | 144/780 [00:42<03:05,  3.43it/s] 19%|        | 145/780 [00:42<03:05,  3.43it/s] 19%|        | 146/780 [00:42<03:04,  3.43it/s] 19%|        | 147/780 [00:43<03:15,  3.23it/s] 19%|        | 148/780 [00:43<03:12,  3.29it/s] 19%|        | 149/780 [00:43<03:09,  3.33it/s] 19%|        | 150/780 [00:44<03:07,  3.36it/s] 19%|        | 151/780 [00:44<03:06,  3.38it/s] 19%|        | 152/780 [00:44<03:05,  3.39it/s] 20%|        | 153/780 [00:44<03:04,  3.40it/s] 20%|        | 154/780 [00:45<03:03,  3.41it/s] 20%|        | 155/780 [00:45<03:02,  3.42it/s] 20%|        | 156/780 [00:45<03:02,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 00:43:00,306 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:43:00,306 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:43:00,306 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.84it/s][A
  3%|         | 12/435 [00:00<00:08, 48.80it/s][A
  4%|         | 17/435 [00:00<00:08, 46.90it/s][A
  5%|         | 22/435 [00:00<00:08, 46.00it/s][A
  6%|         | 27/435 [00:00<00:08, 45.36it/s][A
  7%|         | 32/435 [00:00<00:08, 45.12it/s][A
  9%|         | 37/435 [00:00<00:08, 44.97it/s][A
 10%|         | 42/435 [00:00<00:09, 42.49it/s][A
 11%|         | 47/435 [00:01<00:08, 43.48it/s][A
 12%|        | 52/435 [00:01<00:08, 43.83it/s][A
 13%|        | 57/435 [00:01<00:08, 44.25it/s][A
 14%|        | 62/435 [00:01<00:08, 44.42it/s][A
 15%|        | 67/435 [00:01<00:08, 44.53it/s][A
 17%|        | 72/435 [00:01<00:08, 44.67it/s][A
 18%|        | 77/435 [00:01<00:08, 44.73it/s][A
 19%|        | 82/435 [00:01<00:07, 44.54it/s][A
 20%|        | 87/435 [00:01<00:07, 44.39it/s][A
 21%|        | 92/435 [00:02<00:07, 44.58it/s][A
 22%|       | 97/435 [00:02<00:08, 41.98it/s][A
 23%|       | 102/435 [00:02<00:07, 42.86it/s][A
 25%|       | 107/435 [00:02<00:07, 43.48it/s][A
 26%|       | 112/435 [00:02<00:07, 43.90it/s][A
 27%|       | 117/435 [00:02<00:07, 44.24it/s][A
 28%|       | 122/435 [00:02<00:07, 44.48it/s][A
 29%|       | 127/435 [00:02<00:06, 44.52it/s][A
 30%|       | 132/435 [00:03<00:11, 27.01it/s][A
 31%|      | 137/435 [00:03<00:09, 30.67it/s][A
 33%|      | 142/435 [00:03<00:08, 33.92it/s][A
 34%|      | 147/435 [00:03<00:07, 36.67it/s][A
 35%|      | 152/435 [00:03<00:07, 38.78it/s][A
 36%|      | 157/435 [00:03<00:06, 40.50it/s][A
 37%|      | 162/435 [00:03<00:06, 41.76it/s][A
 38%|      | 167/435 [00:03<00:06, 42.70it/s][A
 40%|      | 172/435 [00:04<00:06, 42.89it/s][A
 41%|      | 177/435 [00:04<00:05, 43.25it/s][A
 42%|     | 182/435 [00:04<00:05, 43.58it/s][A
 43%|     | 187/435 [00:04<00:05, 44.05it/s][A
 44%|     | 192/435 [00:04<00:05, 44.32it/s][A
 45%|     | 197/435 [00:04<00:05, 44.49it/s][A
 46%|     | 202/435 [00:04<00:05, 44.66it/s][A
 48%|     | 207/435 [00:04<00:05, 44.77it/s][A
 49%|     | 212/435 [00:04<00:04, 44.77it/s][A
 50%|     | 217/435 [00:05<00:04, 44.60it/s][A
 51%|     | 222/435 [00:05<00:04, 44.57it/s][A
 52%|    | 227/435 [00:05<00:04, 44.56it/s][A
 53%|    | 232/435 [00:05<00:04, 44.64it/s][A
 54%|    | 237/435 [00:05<00:04, 44.70it/s][A
 56%|    | 242/435 [00:05<00:04, 44.76it/s][A
 57%|    | 247/435 [00:05<00:04, 44.76it/s][A
 58%|    | 252/435 [00:05<00:04, 44.79it/s][A
 59%|    | 257/435 [00:05<00:03, 44.87it/s][A
 60%|    | 262/435 [00:06<00:03, 44.71it/s][A
 61%|   | 267/435 [00:06<00:03, 44.63it/s][A
 63%|   | 272/435 [00:06<00:03, 44.57it/s][A
 64%|   | 277/435 [00:06<00:03, 44.70it/s][A
 65%|   | 282/435 [00:06<00:03, 44.78it/s][A
 66%|   | 287/435 [00:06<00:03, 44.72it/s][A
 67%|   | 292/435 [00:06<00:03, 44.78it/s][A
 68%|   | 297/435 [00:06<00:03, 44.76it/s][A
 69%|   | 302/435 [00:07<00:02, 44.71it/s][A
 71%|   | 307/435 [00:07<00:02, 44.61it/s][A
 72%|  | 312/435 [00:07<00:02, 44.52it/s][A
 73%|  | 317/435 [00:07<00:02, 43.68it/s][A
 74%|  | 322/435 [00:07<00:02, 44.12it/s][A
 75%|  | 327/435 [00:07<00:02, 44.33it/s][A
 76%|  | 332/435 [00:07<00:02, 44.51it/s][A
 77%|  | 337/435 [00:07<00:02, 44.61it/s][A
 79%|  | 342/435 [00:07<00:02, 44.55it/s][A
 80%|  | 347/435 [00:08<00:01, 44.59it/s][A
 81%|  | 352/435 [00:08<00:01, 44.48it/s][A
 82%| | 357/435 [00:08<00:01, 44.41it/s][A
 83%| | 362/435 [00:08<00:01, 44.60it/s][A
 84%| | 367/435 [00:08<00:01, 44.67it/s][A
 86%| | 372/435 [00:08<00:01, 44.78it/s][A
 87%| | 377/435 [00:08<00:01, 44.69it/s][A
 88%| | 382/435 [00:08<00:01, 44.69it/s][A
 89%| | 387/435 [00:08<00:01, 44.69it/s][A
 90%| | 392/435 [00:09<00:00, 44.65it/s][A
 91%|| 397/435 [00:09<00:00, 44.53it/s][A
 92%|| 402/435 [00:09<00:00, 44.44it/s][A
 94%|| 407/435 [00:09<00:00, 39.50it/s][A
 95%|| 412/435 [00:09<00:00, 41.02it/s][A
 96%|| 417/435 [00:09<00:00, 42.11it/s][A
 97%|| 422/435 [00:09<00:00, 42.94it/s][A
 98%|| 427/435 [00:09<00:00, 43.47it/s][A
 99%|| 432/435 [00:09<00:00, 43.93it/s][A
                                                 [A                                                 
100%|| 435/435 [00:10<00:00, 43.93it/s][A 20%|        | 156/780 [00:56<03:02,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:43:10,603 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 00:43:10,758 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:43:13,640 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:43:13,763 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:43:13,820 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:06<1:06:05,  6.36s/it] 20%|        | 158/780 [01:06<47:10,  4.55s/it]   20%|        | 159/780 [01:06<33:53,  3.27s/it] 21%|        | 160/780 [01:07<24:36,  2.38s/it] 21%|        | 161/780 [01:07<18:06,  1.76s/it] 21%|        | 162/780 [01:07<13:34,  1.32s/it] 21%|        | 163/780 [01:08<10:23,  1.01s/it] 21%|        | 164/780 [01:08<08:10,  1.26it/s] 21%|        | 165/780 [01:08<06:37,  1.55it/s] 21%|       | 166/780 [01:09<05:32,  1.85it/s] 21%|       | 167/780 [01:09<04:47,  2.14it/s] 22%|       | 168/780 [01:09<04:14,  2.40it/s] 22%|       | 169/780 [01:09<03:57,  2.58it/s] 22%|       | 170/780 [01:10<03:39,  2.78it/s] 22%|       | 171/780 [01:10<03:27,  2.93it/s] 22%|       | 172/780 [01:10<03:19,  3.05it/s] 22%|       | 173/780 [01:11<03:13,  3.14it/s] 22%|       | 174/780 [01:11<03:08,  3.21it/s] 22%|       | 175/780 [01:11<03:05,  3.26it/s] 23%|       | 176/780 [01:12<03:03,  3.30it/s] 23%|       | 177/780 [01:12<03:01,  3.32it/s] 23%|       | 178/780 [01:12<03:00,  3.34it/s] 23%|       | 179/780 [01:12<02:59,  3.35it/s] 23%|       | 180/780 [01:13<03:03,  3.28it/s] 23%|       | 181/780 [01:13<03:01,  3.31it/s] 23%|       | 182/780 [01:13<02:59,  3.33it/s] 23%|       | 183/780 [01:14<02:58,  3.34it/s] 24%|       | 184/780 [01:14<02:57,  3.35it/s] 24%|       | 185/780 [01:14<02:57,  3.36it/s] 24%|       | 186/780 [01:15<02:56,  3.37it/s] 24%|       | 187/780 [01:15<02:55,  3.39it/s] 24%|       | 188/780 [01:15<02:54,  3.40it/s] 24%|       | 189/780 [01:15<02:53,  3.41it/s] 24%|       | 190/780 [01:16<02:52,  3.41it/s] 24%|       | 191/780 [01:16<02:58,  3.30it/s] 25%|       | 192/780 [01:16<02:56,  3.34it/s] 25%|       | 193/780 [01:17<02:54,  3.36it/s] 25%|       | 194/780 [01:17<02:53,  3.38it/s] 25%|       | 195/780 [01:17<02:52,  3.40it/s] 25%|       | 196/780 [01:17<02:51,  3.41it/s] 25%|       | 197/780 [01:18<02:50,  3.41it/s] 25%|       | 198/780 [01:18<02:50,  3.42it/s] 26%|       | 199/780 [01:18<02:49,  3.42it/s] 26%|       | 200/780 [01:19<02:49,  3.42it/s] 26%|       | 201/780 [01:19<02:49,  3.43it/s] 26%|       | 202/780 [01:19<02:56,  3.27it/s] 26%|       | 203/780 [01:20<02:54,  3.31it/s] 26%|       | 204/780 [01:20<02:52,  3.34it/s] 26%|       | 205/780 [01:20<02:50,  3.37it/s] 26%|       | 206/780 [01:20<02:49,  3.38it/s] 27%|       | 207/780 [01:21<02:48,  3.40it/s] 27%|       | 208/780 [01:21<02:47,  3.41it/s] 27%|       | 209/780 [01:21<02:47,  3.41it/s] 27%|       | 210/780 [01:22<02:46,  3.42it/s] 27%|       | 211/780 [01:22<02:46,  3.42it/s] 27%|       | 212/780 [01:22<02:45,  3.42it/s] 27%|       | 213/780 [01:23<02:58,  3.18it/s] 27%|       | 214/780 [01:23<02:54,  3.25it/s] 28%|       | 215/780 [01:23<02:51,  3.30it/s] 28%|       | 216/780 [01:23<02:48,  3.34it/s] 28%|       | 217/780 [01:24<02:47,  3.36it/s] 28%|       | 218/780 [01:24<02:46,  3.38it/s] 28%|       | 219/780 [01:24<02:45,  3.39it/s] 28%|       | 220/780 [01:25<02:44,  3.40it/s] 28%|       | 221/780 [01:25<02:43,  3.41it/s] 28%|       | 222/780 [01:25<02:43,  3.41it/s] 29%|       | 223/780 [01:25<02:42,  3.42it/s] 29%|       | 224/780 [01:26<02:45,  3.36it/s] 29%|       | 225/780 [01:26<02:44,  3.37it/s] 29%|       | 226/780 [01:26<02:43,  3.39it/s] 29%|       | 227/780 [01:27<02:42,  3.40it/s] 29%|       | 228/780 [01:27<02:41,  3.41it/s] 29%|       | 229/780 [01:27<02:41,  3.42it/s] 29%|       | 230/780 [01:28<02:41,  3.42it/s] 30%|       | 231/780 [01:28<02:40,  3.42it/s] 30%|       | 232/780 [01:28<02:40,  3.42it/s] 30%|       | 233/780 [01:28<02:40,  3.42it/s] 30%|       | 234/780 [01:29<02:39,  3.42it/s] 30%|       | 235/780 [01:29<02:43,  3.33it/s] 30%|       | 236/780 [01:29<02:41,  3.36it/s] 30%|       | 237/780 [01:30<02:40,  3.38it/s] 31%|       | 238/780 [01:30<02:39,  3.39it/s] 31%|       | 239/780 [01:30<02:39,  3.40it/s] 31%|       | 240/780 [01:30<02:38,  3.41it/s] 31%|       | 241/780 [01:31<02:38,  3.41it/s] 31%|       | 242/780 [01:31<02:37,  3.42it/s] 31%|       | 243/780 [01:31<02:36,  3.42it/s] 31%|      | 244/780 [01:32<02:36,  3.43it/s] 31%|      | 245/780 [01:32<02:36,  3.42it/s] 32%|      | 246/780 [01:32<02:43,  3.27it/s] 32%|      | 247/780 [01:33<02:40,  3.32it/s] 32%|      | 248/780 [01:33<02:38,  3.35it/s] 32%|      | 249/780 [01:33<02:37,  3.37it/s] 32%|      | 250/780 [01:33<02:36,  3.39it/s] 32%|      | 251/780 [01:34<02:35,  3.40it/s] 32%|      | 252/780 [01:34<02:35,  3.40it/s] 32%|      | 253/780 [01:34<02:34,  3.41it/s] 33%|      | 254/780 [01:35<02:33,  3.42it/s] 33%|      | 255/780 [01:35<02:33,  3.42it/s] 33%|      | 256/780 [01:35<02:33,  3.42it/s] 33%|      | 257/780 [01:36<02:38,  3.30it/s] 33%|      | 258/780 [01:36<02:36,  3.34it/s] 33%|      | 259/780 [01:36<02:35,  3.36it/s] 33%|      | 260/780 [01:36<02:33,  3.38it/s] 33%|      | 261/780 [01:37<02:33,  3.39it/s] 34%|      | 262/780 [01:37<02:32,  3.40it/s] 34%|      | 263/780 [01:37<02:31,  3.41it/s] 34%|      | 264/780 [01:38<02:31,  3.41it/s] 34%|      | 265/780 [01:38<02:30,  3.42it/s] 34%|      | 266/780 [01:38<02:30,  3.42it/s] 34%|      | 267/780 [01:38<02:29,  3.42it/s] 34%|      | 268/780 [01:39<02:35,  3.29it/s] 34%|      | 269/780 [01:39<02:33,  3.33it/s] 35%|      | 270/780 [01:39<02:31,  3.36it/s] 35%|      | 271/780 [01:40<02:30,  3.37it/s] 35%|      | 272/780 [01:40<02:29,  3.39it/s] 35%|      | 273/780 [01:40<02:29,  3.40it/s] 35%|      | 274/780 [01:41<02:28,  3.40it/s] 35%|      | 275/780 [01:41<02:28,  3.41it/s] 35%|      | 276/780 [01:41<02:27,  3.41it/s] 36%|      | 277/780 [01:41<02:27,  3.41it/s] 36%|      | 278/780 [01:42<02:26,  3.42it/s] 36%|      | 279/780 [01:42<02:32,  3.28it/s] 36%|      | 280/780 [01:42<02:30,  3.33it/s] 36%|      | 281/780 [01:43<02:28,  3.35it/s] 36%|      | 282/780 [01:43<02:27,  3.37it/s] 36%|      | 283/780 [01:43<02:26,  3.39it/s] 36%|      | 284/780 [01:44<02:25,  3.40it/s] 37%|      | 285/780 [01:44<02:25,  3.41it/s] 37%|      | 286/780 [01:44<02:24,  3.41it/s] 37%|      | 287/780 [01:44<02:24,  3.41it/s] 37%|      | 288/780 [01:45<02:23,  3.42it/s] 37%|      | 289/780 [01:45<02:23,  3.42it/s] 37%|      | 290/780 [01:45<02:26,  3.36it/s] 37%|      | 291/780 [01:46<02:24,  3.37it/s] 37%|      | 292/780 [01:46<02:24,  3.39it/s] 38%|      | 293/780 [01:46<02:23,  3.40it/s] 38%|      | 294/780 [01:46<02:24,  3.35it/s] 38%|      | 295/780 [01:47<02:23,  3.38it/s] 38%|      | 296/780 [01:47<02:22,  3.39it/s] 38%|      | 297/780 [01:47<02:22,  3.40it/s] 38%|      | 298/780 [01:48<02:21,  3.41it/s] 38%|      | 299/780 [01:48<02:20,  3.41it/s] 38%|      | 300/780 [01:48<02:20,  3.42it/s] 39%|      | 301/780 [01:49<02:50,  2.82it/s] 39%|      | 302/780 [01:49<02:40,  2.97it/s] 39%|      | 303/780 [01:49<02:34,  3.09it/s] 39%|      | 304/780 [01:50<02:29,  3.19it/s] 39%|      | 305/780 [01:50<02:25,  3.26it/s] 39%|      | 306/780 [01:50<02:23,  3.30it/s] 39%|      | 307/780 [01:50<02:21,  3.34it/s] 39%|      | 308/780 [01:51<02:20,  3.37it/s] 40%|      | 309/780 [01:51<02:19,  3.38it/s] 40%|      | 310/780 [01:51<02:18,  3.40it/s] 40%|      | 311/780 [01:52<02:22,  3.29it/s] 40%|      | 312/780 [01:52<02:20,  3.33it/s][INFO|trainer.py:2140] 2023-08-29 00:44:06,931 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:44:06,931 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:44:06,931 >>   Batch size = 8
{'eval_loss': 0.9278207421302795, 'eval_runtime': 10.0562, 'eval_samples_per_second': 345.756, 'eval_steps_per_second': 43.257, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.19it/s][A
  3%|         | 12/435 [00:00<00:08, 48.70it/s][A
  4%|         | 17/435 [00:00<00:08, 46.99it/s][A
  5%|         | 22/435 [00:00<00:08, 46.17it/s][A
  6%|         | 27/435 [00:00<00:08, 45.55it/s][A
  7%|         | 32/435 [00:00<00:08, 45.17it/s][A
  9%|         | 37/435 [00:00<00:08, 44.96it/s][A
 10%|         | 42/435 [00:00<00:08, 44.70it/s][A
 11%|         | 47/435 [00:01<00:08, 44.72it/s][A
 12%|        | 52/435 [00:01<00:08, 44.75it/s][A
 13%|        | 57/435 [00:01<00:08, 44.85it/s][A
 14%|        | 62/435 [00:01<00:08, 44.93it/s][A
 15%|        | 67/435 [00:01<00:08, 44.88it/s][A
 17%|        | 72/435 [00:01<00:08, 44.78it/s][A
 18%|        | 77/435 [00:01<00:07, 44.80it/s][A
 19%|        | 82/435 [00:01<00:07, 44.67it/s][A
 20%|        | 87/435 [00:01<00:07, 44.58it/s][A
 21%|        | 92/435 [00:02<00:07, 44.53it/s][A
 22%|       | 97/435 [00:02<00:07, 44.67it/s][A
 23%|       | 102/435 [00:02<00:07, 44.67it/s][A
 25%|       | 107/435 [00:02<00:07, 43.92it/s][A
 26%|       | 112/435 [00:02<00:07, 44.19it/s][A
 27%|       | 117/435 [00:02<00:07, 44.38it/s][A
 28%|       | 122/435 [00:02<00:07, 44.41it/s][A
 29%|       | 127/435 [00:02<00:06, 44.30it/s][A
 30%|       | 132/435 [00:02<00:06, 44.56it/s][A
 31%|      | 137/435 [00:03<00:06, 44.50it/s][A
 33%|      | 142/435 [00:03<00:06, 44.60it/s][A
 34%|      | 147/435 [00:03<00:06, 44.62it/s][A
 35%|      | 152/435 [00:03<00:06, 44.64it/s][A
 36%|      | 157/435 [00:03<00:06, 44.70it/s][A
 37%|      | 162/435 [00:03<00:06, 44.69it/s][A
 38%|      | 167/435 [00:03<00:06, 44.55it/s][A
 40%|      | 172/435 [00:03<00:05, 44.59it/s][A
 41%|      | 177/435 [00:03<00:05, 44.52it/s][A
 42%|     | 182/435 [00:04<00:05, 44.61it/s][A
 43%|     | 187/435 [00:04<00:05, 44.59it/s][A
 44%|     | 192/435 [00:04<00:05, 44.71it/s][A
 45%|     | 197/435 [00:04<00:05, 44.78it/s][A
 46%|     | 202/435 [00:04<00:05, 44.75it/s][A
 48%|     | 207/435 [00:04<00:05, 44.69it/s][A
 49%|     | 212/435 [00:04<00:04, 44.72it/s][A
 50%|     | 217/435 [00:04<00:04, 44.62it/s][A
 51%|     | 222/435 [00:04<00:04, 44.41it/s][A
 52%|    | 227/435 [00:05<00:04, 44.58it/s][A
 53%|    | 232/435 [00:05<00:04, 44.68it/s][A
 54%|    | 237/435 [00:05<00:04, 44.67it/s][A
 56%|    | 242/435 [00:05<00:04, 44.78it/s][A
 57%|    | 247/435 [00:05<00:04, 44.77it/s][A
 58%|    | 252/435 [00:05<00:04, 44.79it/s][A
 59%|    | 257/435 [00:05<00:03, 44.78it/s][A
 60%|    | 262/435 [00:05<00:03, 44.73it/s][A
 61%|   | 267/435 [00:05<00:03, 44.62it/s][A
 63%|   | 272/435 [00:06<00:03, 44.66it/s][A
 64%|   | 277/435 [00:06<00:03, 44.61it/s][A
 65%|   | 282/435 [00:06<00:03, 44.74it/s][A
 66%|   | 287/435 [00:06<00:03, 44.71it/s][A
 67%|   | 292/435 [00:06<00:03, 44.60it/s][A
 68%|   | 297/435 [00:06<00:03, 44.72it/s][A
 69%|   | 302/435 [00:06<00:02, 44.67it/s][A
 71%|   | 307/435 [00:06<00:02, 44.54it/s][A
 72%|  | 312/435 [00:06<00:02, 44.50it/s][A
 73%|  | 317/435 [00:07<00:02, 44.51it/s][A
 74%|  | 322/435 [00:07<00:02, 43.20it/s][A
 75%|  | 327/435 [00:07<00:02, 43.69it/s][A
 76%|  | 332/435 [00:07<00:02, 44.15it/s][A
 77%|  | 337/435 [00:07<00:02, 44.29it/s][A
 79%|  | 342/435 [00:07<00:02, 44.54it/s][A
 80%|  | 347/435 [00:07<00:01, 44.62it/s][A
 81%|  | 352/435 [00:07<00:01, 44.59it/s][A
 82%| | 357/435 [00:07<00:01, 44.60it/s][A
 83%| | 362/435 [00:08<00:01, 44.38it/s][A
 84%| | 367/435 [00:08<00:01, 44.25it/s][A
 86%| | 372/435 [00:08<00:01, 44.68it/s][A
 87%| | 377/435 [00:08<00:01, 44.72it/s][A
 88%| | 382/435 [00:08<00:01, 44.81it/s][A
 89%| | 387/435 [00:08<00:01, 44.78it/s][A
 90%| | 392/435 [00:08<00:00, 44.68it/s][A
 91%|| 397/435 [00:08<00:00, 44.62it/s][A
 92%|| 402/435 [00:08<00:00, 44.58it/s][A
 94%|| 407/435 [00:09<00:00, 44.45it/s][A
 95%|| 412/435 [00:09<00:00, 40.41it/s][A
 96%|| 418/435 [00:09<00:00, 43.01it/s][A
 97%|| 423/435 [00:09<00:00, 43.54it/s][A
 98%|| 428/435 [00:09<00:00, 43.94it/s][A
100%|| 433/435 [00:09<00:00, 44.26it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.26it/s][A 40%|      | 312/780 [02:02<02:20,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:44:17,041 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 00:44:17,311 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:44:20,626 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:44:20,789 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:44:20,874 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:13<50:29,  6.49s/it] 40%|      | 314/780 [02:13<36:00,  4.64s/it] 40%|      | 315/780 [02:13<25:50,  3.33s/it] 41%|      | 316/780 [02:14<18:44,  2.42s/it] 41%|      | 317/780 [02:14<13:46,  1.78s/it] 41%|      | 318/780 [02:14<10:17,  1.34s/it] 41%|      | 319/780 [02:15<07:52,  1.03s/it] 41%|      | 320/780 [02:15<06:11,  1.24it/s] 41%|      | 321/780 [02:15<04:59,  1.53it/s] 41%|     | 322/780 [02:16<04:10,  1.83it/s] 41%|     | 323/780 [02:16<03:35,  2.13it/s] 42%|     | 324/780 [02:16<03:10,  2.40it/s] 42%|     | 325/780 [02:16<02:58,  2.55it/s] 42%|     | 326/780 [02:17<02:44,  2.76it/s] 42%|     | 327/780 [02:17<02:34,  2.93it/s] 42%|     | 328/780 [02:17<02:27,  3.07it/s] 42%|     | 329/780 [02:18<02:22,  3.17it/s] 42%|     | 330/780 [02:18<02:18,  3.24it/s] 42%|     | 331/780 [02:18<02:16,  3.29it/s] 43%|     | 332/780 [02:19<02:14,  3.33it/s] 43%|     | 333/780 [02:19<02:12,  3.36it/s] 43%|     | 334/780 [02:19<02:11,  3.38it/s] 43%|     | 335/780 [02:19<02:11,  3.40it/s] 43%|     | 336/780 [02:20<02:17,  3.22it/s] 43%|     | 337/780 [02:20<02:15,  3.28it/s] 43%|     | 338/780 [02:20<02:13,  3.32it/s] 43%|     | 339/780 [02:21<02:11,  3.35it/s] 44%|     | 340/780 [02:21<02:10,  3.38it/s] 44%|     | 341/780 [02:21<02:09,  3.39it/s] 44%|     | 342/780 [02:21<02:08,  3.40it/s] 44%|     | 343/780 [02:22<02:08,  3.41it/s] 44%|     | 344/780 [02:22<02:07,  3.41it/s] 44%|     | 345/780 [02:22<02:07,  3.42it/s] 44%|     | 346/780 [02:23<02:06,  3.42it/s] 44%|     | 347/780 [02:23<02:12,  3.28it/s] 45%|     | 348/780 [02:23<02:10,  3.32it/s] 45%|     | 349/780 [02:24<02:08,  3.35it/s] 45%|     | 350/780 [02:24<02:07,  3.38it/s] 45%|     | 351/780 [02:24<02:06,  3.39it/s] 45%|     | 352/780 [02:24<02:05,  3.40it/s] 45%|     | 353/780 [02:25<02:05,  3.41it/s] 45%|     | 354/780 [02:25<02:04,  3.42it/s] 46%|     | 355/780 [02:25<02:04,  3.42it/s] 46%|     | 356/780 [02:26<02:03,  3.42it/s] 46%|     | 357/780 [02:26<02:03,  3.42it/s] 46%|     | 358/780 [02:26<02:07,  3.31it/s] 46%|     | 359/780 [02:27<02:06,  3.33it/s] 46%|     | 360/780 [02:27<02:05,  3.34it/s] 46%|     | 361/780 [02:27<02:04,  3.35it/s] 46%|     | 362/780 [02:27<02:04,  3.36it/s] 47%|     | 363/780 [02:28<02:03,  3.37it/s] 47%|     | 364/780 [02:28<02:03,  3.37it/s] 47%|     | 365/780 [02:28<02:02,  3.38it/s] 47%|     | 366/780 [02:29<02:02,  3.37it/s] 47%|     | 367/780 [02:29<02:02,  3.37it/s] 47%|     | 368/780 [02:29<02:02,  3.38it/s] 47%|     | 369/780 [02:29<02:01,  3.38it/s] 47%|     | 370/780 [02:30<02:01,  3.38it/s] 48%|     | 371/780 [02:30<02:01,  3.38it/s] 48%|     | 372/780 [02:30<02:04,  3.27it/s] 48%|     | 373/780 [02:31<02:03,  3.30it/s] 48%|     | 374/780 [02:31<02:02,  3.32it/s] 48%|     | 375/780 [02:31<02:01,  3.34it/s] 48%|     | 376/780 [02:32<02:00,  3.35it/s] 48%|     | 377/780 [02:32<02:00,  3.35it/s] 48%|     | 378/780 [02:32<01:59,  3.36it/s] 49%|     | 379/780 [02:32<01:59,  3.37it/s] 49%|     | 380/780 [02:33<01:58,  3.37it/s] 49%|     | 381/780 [02:33<01:58,  3.37it/s] 49%|     | 382/780 [02:33<01:58,  3.37it/s] 49%|     | 383/780 [02:34<02:00,  3.28it/s] 49%|     | 384/780 [02:34<01:59,  3.31it/s] 49%|     | 385/780 [02:34<01:58,  3.33it/s] 49%|     | 386/780 [02:35<01:57,  3.34it/s] 50%|     | 387/780 [02:35<01:57,  3.35it/s] 50%|     | 388/780 [02:35<01:56,  3.36it/s] 50%|     | 389/780 [02:35<01:56,  3.36it/s] 50%|     | 390/780 [02:36<01:55,  3.36it/s] 50%|     | 391/780 [02:36<01:55,  3.37it/s] 50%|     | 392/780 [02:36<01:55,  3.37it/s] 50%|     | 393/780 [02:37<01:54,  3.38it/s] 51%|     | 394/780 [02:37<01:59,  3.24it/s] 51%|     | 395/780 [02:37<01:57,  3.28it/s] 51%|     | 396/780 [02:38<01:56,  3.31it/s] 51%|     | 397/780 [02:38<01:55,  3.32it/s] 51%|     | 398/780 [02:38<01:54,  3.33it/s] 51%|     | 399/780 [02:38<01:53,  3.35it/s] 51%|    | 400/780 [02:39<01:53,  3.36it/s] 51%|    | 401/780 [02:39<01:52,  3.36it/s] 52%|    | 402/780 [02:39<01:52,  3.36it/s] 52%|    | 403/780 [02:40<01:51,  3.37it/s] 52%|    | 404/780 [02:40<01:55,  3.25it/s] 52%|    | 405/780 [02:40<01:54,  3.29it/s] 52%|    | 406/780 [02:41<01:52,  3.31it/s] 52%|    | 407/780 [02:41<01:51,  3.33it/s] 52%|    | 408/780 [02:41<01:51,  3.35it/s] 52%|    | 409/780 [02:41<01:50,  3.36it/s] 53%|    | 410/780 [02:42<01:50,  3.36it/s] 53%|    | 411/780 [02:42<01:49,  3.36it/s] 53%|    | 412/780 [02:42<01:49,  3.37it/s] 53%|    | 413/780 [02:43<01:48,  3.38it/s] 53%|    | 414/780 [02:43<01:47,  3.39it/s] 53%|    | 415/780 [02:43<01:52,  3.26it/s] 53%|    | 416/780 [02:44<01:50,  3.31it/s] 53%|    | 417/780 [02:44<01:48,  3.34it/s] 54%|    | 418/780 [02:44<01:47,  3.37it/s] 54%|    | 419/780 [02:44<01:46,  3.38it/s] 54%|    | 420/780 [02:45<01:45,  3.40it/s] 54%|    | 421/780 [02:45<01:45,  3.41it/s] 54%|    | 422/780 [02:45<01:44,  3.41it/s] 54%|    | 423/780 [02:46<01:44,  3.41it/s] 54%|    | 424/780 [02:46<01:44,  3.42it/s] 54%|    | 425/780 [02:46<01:43,  3.42it/s] 55%|    | 426/780 [02:47<01:49,  3.23it/s] 55%|    | 427/780 [02:47<01:47,  3.29it/s] 55%|    | 428/780 [02:47<01:45,  3.33it/s] 55%|    | 429/780 [02:47<01:44,  3.36it/s] 55%|    | 430/780 [02:48<01:43,  3.38it/s] 55%|    | 431/780 [02:48<01:42,  3.39it/s] 55%|    | 432/780 [02:48<01:42,  3.40it/s] 56%|    | 433/780 [02:49<02:11,  2.63it/s] 56%|    | 434/780 [02:49<02:02,  2.83it/s] 56%|    | 435/780 [02:49<01:55,  2.99it/s] 56%|    | 436/780 [02:50<01:53,  3.04it/s] 56%|    | 437/780 [02:50<01:48,  3.15it/s] 56%|    | 438/780 [02:50<01:45,  3.23it/s] 56%|    | 439/780 [02:51<01:43,  3.29it/s] 56%|    | 440/780 [02:51<01:42,  3.33it/s] 57%|    | 441/780 [02:51<01:40,  3.36it/s] 57%|    | 442/780 [02:52<01:39,  3.38it/s] 57%|    | 443/780 [02:52<01:39,  3.39it/s] 57%|    | 444/780 [02:52<01:38,  3.41it/s] 57%|    | 445/780 [02:52<01:38,  3.41it/s] 57%|    | 446/780 [02:53<01:37,  3.42it/s] 57%|    | 447/780 [02:53<01:40,  3.31it/s] 57%|    | 448/780 [02:53<01:39,  3.35it/s] 58%|    | 449/780 [02:54<01:38,  3.37it/s] 58%|    | 450/780 [02:54<01:37,  3.39it/s] 58%|    | 451/780 [02:54<01:36,  3.40it/s] 58%|    | 452/780 [02:54<01:36,  3.41it/s] 58%|    | 453/780 [02:55<01:35,  3.42it/s] 58%|    | 454/780 [02:55<01:35,  3.42it/s] 58%|    | 455/780 [02:55<01:34,  3.43it/s] 58%|    | 456/780 [02:56<01:34,  3.42it/s] 59%|    | 457/780 [02:56<01:34,  3.42it/s] 59%|    | 458/780 [02:56<01:36,  3.34it/s] 59%|    | 459/780 [02:57<01:35,  3.37it/s] 59%|    | 460/780 [02:57<01:34,  3.38it/s] 59%|    | 461/780 [02:57<01:33,  3.40it/s] 59%|    | 462/780 [02:57<01:33,  3.41it/s] 59%|    | 463/780 [02:58<01:32,  3.42it/s] 59%|    | 464/780 [02:58<01:32,  3.42it/s] 60%|    | 465/780 [02:58<01:32,  3.42it/s] 60%|    | 466/780 [02:59<01:31,  3.42it/s] 60%|    | 467/780 [02:59<01:31,  3.43it/s] 60%|    | 468/780 [02:59<01:31,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 00:45:14,159 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:45:14,159 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:45:14,159 >>   Batch size = 8
{'eval_loss': 0.9387341141700745, 'eval_runtime': 9.7745, 'eval_samples_per_second': 355.723, 'eval_steps_per_second': 44.504, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.03it/s][A
  3%|         | 12/435 [00:00<00:08, 48.89it/s][A
  4%|         | 17/435 [00:00<00:08, 47.01it/s][A
  5%|         | 22/435 [00:00<00:08, 46.15it/s][A
  6%|         | 27/435 [00:00<00:08, 45.48it/s][A
  7%|         | 32/435 [00:00<00:08, 45.09it/s][A
  9%|         | 37/435 [00:00<00:08, 44.88it/s][A
 10%|         | 42/435 [00:00<00:08, 44.61it/s][A
 11%|         | 47/435 [00:01<00:08, 44.71it/s][A
 12%|        | 52/435 [00:01<00:08, 44.74it/s][A
 13%|        | 57/435 [00:01<00:08, 44.81it/s][A
 14%|        | 62/435 [00:01<00:08, 44.90it/s][A
 15%|        | 67/435 [00:01<00:08, 43.30it/s][A
 17%|        | 72/435 [00:01<00:08, 43.78it/s][A
 18%|        | 77/435 [00:01<00:08, 44.07it/s][A
 19%|        | 82/435 [00:01<00:07, 44.16it/s][A
 20%|        | 87/435 [00:01<00:07, 44.30it/s][A
 21%|        | 92/435 [00:02<00:08, 42.69it/s][A
 22%|       | 97/435 [00:02<00:07, 43.99it/s][A
 23%|       | 102/435 [00:02<00:07, 44.28it/s][A
 25%|       | 107/435 [00:02<00:07, 44.38it/s][A
 26%|       | 112/435 [00:02<00:07, 44.50it/s][A
 27%|       | 117/435 [00:02<00:07, 44.56it/s][A
 28%|       | 122/435 [00:02<00:07, 44.55it/s][A
 29%|       | 127/435 [00:02<00:06, 44.54it/s][A
 30%|       | 132/435 [00:02<00:06, 44.53it/s][A
 31%|      | 137/435 [00:03<00:06, 44.52it/s][A
 33%|      | 142/435 [00:03<00:06, 44.52it/s][A
 34%|      | 147/435 [00:03<00:06, 44.72it/s][A
 35%|      | 152/435 [00:03<00:06, 44.77it/s][A
 36%|      | 157/435 [00:03<00:06, 44.81it/s][A
 37%|      | 162/435 [00:03<00:06, 44.74it/s][A
 38%|      | 167/435 [00:03<00:05, 44.72it/s][A
 40%|      | 172/435 [00:03<00:05, 44.69it/s][A
 41%|      | 177/435 [00:03<00:05, 44.53it/s][A
 42%|     | 182/435 [00:04<00:05, 44.51it/s][A
 43%|     | 187/435 [00:04<00:05, 44.57it/s][A
 44%|     | 192/435 [00:04<00:05, 44.69it/s][A
 45%|     | 197/435 [00:04<00:05, 44.78it/s][A
 46%|     | 202/435 [00:04<00:05, 44.45it/s][A
 48%|     | 207/435 [00:04<00:05, 44.55it/s][A
 49%|     | 212/435 [00:04<00:04, 44.64it/s][A
 50%|     | 217/435 [00:04<00:04, 44.52it/s][A
 51%|     | 222/435 [00:04<00:04, 44.41it/s][A
 52%|    | 227/435 [00:05<00:04, 44.39it/s][A
 53%|    | 232/435 [00:05<00:04, 44.55it/s][A
 54%|    | 237/435 [00:05<00:04, 44.67it/s][A
 56%|    | 242/435 [00:05<00:04, 44.76it/s][A
 57%|    | 247/435 [00:05<00:04, 44.87it/s][A
 58%|    | 252/435 [00:05<00:04, 44.93it/s][A
 59%|    | 257/435 [00:05<00:03, 44.86it/s][A
 60%|    | 262/435 [00:05<00:03, 44.68it/s][A
 61%|   | 267/435 [00:05<00:03, 44.61it/s][A
 63%|   | 272/435 [00:06<00:03, 44.60it/s][A
 64%|   | 277/435 [00:06<00:03, 44.63it/s][A
 65%|   | 282/435 [00:06<00:03, 44.67it/s][A
 66%|   | 287/435 [00:06<00:03, 44.72it/s][A
 67%|   | 292/435 [00:06<00:03, 44.76it/s][A
 68%|   | 297/435 [00:06<00:03, 44.83it/s][A
 69%|   | 302/435 [00:06<00:02, 44.74it/s][A
 71%|   | 307/435 [00:06<00:02, 44.74it/s][A
 72%|  | 312/435 [00:06<00:02, 44.70it/s][A
 73%|  | 317/435 [00:07<00:02, 44.61it/s][A
 74%|  | 322/435 [00:07<00:02, 44.62it/s][A
 75%|  | 327/435 [00:07<00:02, 44.49it/s][A
 76%|  | 332/435 [00:07<00:02, 44.69it/s][A
 77%|  | 337/435 [00:07<00:02, 42.96it/s][A
 79%|  | 342/435 [00:07<00:02, 43.72it/s][A
 80%|  | 347/435 [00:07<00:01, 44.03it/s][A
 81%|  | 352/435 [00:07<00:01, 44.25it/s][A
 82%| | 357/435 [00:08<00:01, 44.25it/s][A
 83%| | 362/435 [00:08<00:01, 44.40it/s][A
 84%| | 367/435 [00:08<00:01, 44.44it/s][A
 86%| | 372/435 [00:08<00:01, 44.45it/s][A
 87%| | 377/435 [00:08<00:01, 44.27it/s][A
 88%| | 382/435 [00:08<00:01, 44.53it/s][A
 89%| | 387/435 [00:08<00:01, 44.60it/s][A
 90%| | 392/435 [00:08<00:00, 44.83it/s][A
 91%|| 397/435 [00:08<00:00, 44.82it/s][A
 92%|| 402/435 [00:09<00:00, 44.77it/s][A
 94%|| 407/435 [00:09<00:00, 44.76it/s][A
 95%|| 412/435 [00:09<00:00, 44.72it/s][A
 96%|| 417/435 [00:09<00:00, 44.68it/s][A
 97%|| 422/435 [00:09<00:00, 44.51it/s][A
 98%|| 427/435 [00:09<00:00, 44.66it/s][A
 99%|| 432/435 [00:09<00:00, 44.70it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.70it/s][A 60%|    | 468/780 [03:09<01:31,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:45:24,069 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 00:45:24,185 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:45:27,329 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:45:27,477 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:45:27,557 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:22<35:50,  6.92s/it] 60%|    | 470/780 [03:22<25:32,  4.94s/it] 60%|    | 471/780 [03:22<18:16,  3.55s/it] 61%|    | 472/780 [03:22<13:12,  2.57s/it] 61%|    | 473/780 [03:23<09:40,  1.89s/it] 61%|    | 474/780 [03:23<07:11,  1.41s/it] 61%|    | 475/780 [03:23<05:28,  1.08s/it] 61%|    | 476/780 [03:24<04:16,  1.19it/s] 61%|    | 477/780 [03:24<03:25,  1.47it/s] 61%|   | 478/780 [03:24<02:49,  1.78it/s] 61%|   | 479/780 [03:25<02:24,  2.08it/s] 62%|   | 480/780 [03:25<02:07,  2.36it/s] 62%|   | 481/780 [03:25<01:59,  2.51it/s] 62%|   | 482/780 [03:25<01:49,  2.73it/s] 62%|   | 483/780 [03:26<01:42,  2.91it/s] 62%|   | 484/780 [03:26<01:37,  3.05it/s] 62%|   | 485/780 [03:26<01:33,  3.15it/s] 62%|   | 486/780 [03:27<01:30,  3.23it/s] 62%|   | 487/780 [03:27<01:29,  3.29it/s] 63%|   | 488/780 [03:27<01:27,  3.33it/s] 63%|   | 489/780 [03:28<01:26,  3.36it/s] 63%|   | 490/780 [03:28<01:25,  3.38it/s] 63%|   | 491/780 [03:28<01:25,  3.40it/s] 63%|   | 492/780 [03:28<01:29,  3.22it/s] 63%|   | 493/780 [03:29<01:27,  3.28it/s] 63%|   | 494/780 [03:29<01:26,  3.32it/s] 63%|   | 495/780 [03:29<01:24,  3.35it/s] 64%|   | 496/780 [03:30<01:24,  3.38it/s] 64%|   | 497/780 [03:30<01:23,  3.39it/s] 64%|   | 498/780 [03:30<01:22,  3.40it/s] 64%|   | 499/780 [03:30<01:22,  3.41it/s] 64%|   | 500/780 [03:31<01:21,  3.42it/s]                                                  64%|   | 500/780 [03:31<01:21,  3.42it/s] 64%|   | 501/780 [03:31<01:21,  3.42it/s] 64%|   | 502/780 [03:31<01:21,  3.42it/s] 64%|   | 503/780 [03:32<01:23,  3.32it/s] 65%|   | 504/780 [03:32<01:22,  3.35it/s] 65%|   | 505/780 [03:32<01:21,  3.37it/s] 65%|   | 506/780 [03:33<01:20,  3.39it/s] 65%|   | 507/780 [03:33<01:20,  3.40it/s] 65%|   | 508/780 [03:33<01:19,  3.41it/s] 65%|   | 509/780 [03:33<01:19,  3.42it/s] 65%|   | 510/780 [03:34<01:18,  3.42it/s] 66%|   | 511/780 [03:34<01:18,  3.43it/s] 66%|   | 512/780 [03:34<01:18,  3.43it/s] 66%|   | 513/780 [03:35<01:17,  3.43it/s] 66%|   | 514/780 [03:35<01:18,  3.38it/s] 66%|   | 515/780 [03:35<01:18,  3.39it/s] 66%|   | 516/780 [03:35<01:17,  3.40it/s] 66%|   | 517/780 [03:36<01:17,  3.41it/s] 66%|   | 518/780 [03:36<01:16,  3.42it/s] 67%|   | 519/780 [03:36<01:16,  3.42it/s] 67%|   | 520/780 [03:37<01:15,  3.43it/s] 67%|   | 521/780 [03:37<01:15,  3.43it/s] 67%|   | 522/780 [03:37<01:15,  3.43it/s] 67%|   | 523/780 [03:38<01:14,  3.43it/s] 67%|   | 524/780 [03:38<01:14,  3.43it/s] 67%|   | 525/780 [03:38<01:15,  3.39it/s] 67%|   | 526/780 [03:38<01:14,  3.41it/s] 68%|   | 527/780 [03:39<01:14,  3.41it/s] 68%|   | 528/780 [03:39<01:13,  3.42it/s] 68%|   | 529/780 [03:39<01:13,  3.42it/s] 68%|   | 530/780 [03:40<01:12,  3.42it/s] 68%|   | 531/780 [03:40<01:12,  3.42it/s] 68%|   | 532/780 [03:40<01:12,  3.43it/s] 68%|   | 533/780 [03:40<01:12,  3.43it/s] 68%|   | 534/780 [03:41<01:11,  3.43it/s] 69%|   | 535/780 [03:41<01:11,  3.43it/s] 69%|   | 536/780 [03:41<01:13,  3.32it/s] 69%|   | 537/780 [03:42<01:12,  3.35it/s] 69%|   | 538/780 [03:42<01:11,  3.37it/s] 69%|   | 539/780 [03:42<01:11,  3.39it/s] 69%|   | 540/780 [03:43<01:10,  3.40it/s] 69%|   | 541/780 [03:43<01:10,  3.41it/s] 69%|   | 542/780 [03:43<01:09,  3.41it/s] 70%|   | 543/780 [03:43<01:09,  3.42it/s] 70%|   | 544/780 [03:44<01:08,  3.42it/s] 70%|   | 545/780 [03:44<01:08,  3.42it/s] 70%|   | 546/780 [03:44<01:08,  3.43it/s] 70%|   | 547/780 [03:45<01:12,  3.22it/s] 70%|   | 548/780 [03:45<01:10,  3.28it/s] 70%|   | 549/780 [03:45<01:09,  3.32it/s] 71%|   | 550/780 [03:46<01:08,  3.35it/s] 71%|   | 551/780 [03:46<01:07,  3.37it/s] 71%|   | 552/780 [03:46<01:07,  3.39it/s] 71%|   | 553/780 [03:46<01:06,  3.40it/s] 71%|   | 554/780 [03:47<01:08,  3.31it/s] 71%|   | 555/780 [03:47<01:07,  3.35it/s] 71%|  | 556/780 [03:47<01:06,  3.37it/s] 71%|  | 557/780 [03:48<01:09,  3.22it/s] 72%|  | 558/780 [03:48<01:07,  3.28it/s] 72%|  | 559/780 [03:48<01:06,  3.32it/s] 72%|  | 560/780 [03:49<01:05,  3.35it/s] 72%|  | 561/780 [03:49<01:23,  2.63it/s] 72%|  | 562/780 [03:49<01:17,  2.82it/s] 72%|  | 563/780 [03:50<01:12,  2.98it/s] 72%|  | 564/780 [03:50<01:09,  3.11it/s] 72%|  | 565/780 [03:50<01:07,  3.20it/s] 73%|  | 566/780 [03:51<01:05,  3.26it/s] 73%|  | 567/780 [03:51<01:06,  3.22it/s] 73%|  | 568/780 [03:51<01:04,  3.28it/s] 73%|  | 569/780 [03:51<01:03,  3.32it/s] 73%|  | 570/780 [03:52<01:02,  3.35it/s] 73%|  | 571/780 [03:52<01:01,  3.37it/s] 73%|  | 572/780 [03:52<01:01,  3.39it/s] 73%|  | 573/780 [03:53<01:00,  3.40it/s] 74%|  | 574/780 [03:53<01:00,  3.41it/s] 74%|  | 575/780 [03:53<00:59,  3.42it/s] 74%|  | 576/780 [03:53<00:59,  3.42it/s] 74%|  | 577/780 [03:54<00:59,  3.43it/s] 74%|  | 578/780 [03:54<01:00,  3.36it/s] 74%|  | 579/780 [03:54<00:59,  3.38it/s] 74%|  | 580/780 [03:55<00:58,  3.39it/s] 74%|  | 581/780 [03:55<00:58,  3.40it/s] 75%|  | 582/780 [03:55<00:58,  3.41it/s] 75%|  | 583/780 [03:56<00:57,  3.42it/s] 75%|  | 584/780 [03:56<00:57,  3.42it/s] 75%|  | 585/780 [03:56<00:56,  3.43it/s] 75%|  | 586/780 [03:56<00:56,  3.43it/s] 75%|  | 587/780 [03:57<00:56,  3.43it/s] 75%|  | 588/780 [03:57<00:56,  3.43it/s] 76%|  | 589/780 [03:57<00:56,  3.36it/s] 76%|  | 590/780 [03:58<00:56,  3.38it/s] 76%|  | 591/780 [03:58<00:55,  3.39it/s] 76%|  | 592/780 [03:58<00:55,  3.40it/s] 76%|  | 593/780 [03:58<00:54,  3.41it/s] 76%|  | 594/780 [03:59<00:54,  3.42it/s] 76%|  | 595/780 [03:59<00:54,  3.42it/s] 76%|  | 596/780 [03:59<00:55,  3.31it/s] 77%|  | 597/780 [04:00<00:54,  3.35it/s] 77%|  | 598/780 [04:00<00:53,  3.37it/s] 77%|  | 599/780 [04:00<00:53,  3.39it/s] 77%|  | 600/780 [04:01<00:52,  3.40it/s] 77%|  | 601/780 [04:01<00:52,  3.41it/s] 77%|  | 602/780 [04:01<00:52,  3.42it/s] 77%|  | 603/780 [04:01<00:51,  3.42it/s] 77%|  | 604/780 [04:02<00:51,  3.42it/s] 78%|  | 605/780 [04:02<00:51,  3.43it/s] 78%|  | 606/780 [04:02<00:50,  3.43it/s] 78%|  | 607/780 [04:03<00:52,  3.31it/s] 78%|  | 608/780 [04:03<00:51,  3.35it/s] 78%|  | 609/780 [04:03<00:50,  3.37it/s] 78%|  | 610/780 [04:03<00:50,  3.39it/s] 78%|  | 611/780 [04:04<00:49,  3.40it/s] 78%|  | 612/780 [04:04<00:49,  3.41it/s] 79%|  | 613/780 [04:04<00:48,  3.42it/s] 79%|  | 614/780 [04:05<00:48,  3.42it/s] 79%|  | 615/780 [04:05<00:48,  3.42it/s] 79%|  | 616/780 [04:05<00:47,  3.43it/s] 79%|  | 617/780 [04:06<00:47,  3.43it/s] 79%|  | 618/780 [04:06<00:49,  3.30it/s] 79%|  | 619/780 [04:06<00:48,  3.34it/s] 79%|  | 620/780 [04:06<00:47,  3.37it/s] 80%|  | 621/780 [04:07<00:46,  3.39it/s] 80%|  | 622/780 [04:07<00:46,  3.40it/s] 80%|  | 623/780 [04:07<00:46,  3.41it/s] 80%|  | 624/780 [04:08<00:45,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 00:46:22,587 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:46:22,587 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:46:22,587 >>   Batch size = 8
{'eval_loss': 0.9445056915283203, 'eval_runtime': 9.7649, 'eval_samples_per_second': 356.069, 'eval_steps_per_second': 44.547, 'epoch': 3.0}
{'loss': 0.7563, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.79it/s][A
  3%|         | 12/435 [00:00<00:08, 48.83it/s][A
  4%|         | 17/435 [00:00<00:08, 47.12it/s][A
  5%|         | 22/435 [00:00<00:08, 46.25it/s][A
  6%|         | 27/435 [00:00<00:08, 45.55it/s][A
  7%|         | 32/435 [00:00<00:08, 45.24it/s][A
  9%|         | 37/435 [00:00<00:08, 44.86it/s][A
 10%|         | 42/435 [00:00<00:08, 44.18it/s][A
 11%|         | 47/435 [00:01<00:08, 44.38it/s][A
 12%|        | 52/435 [00:01<00:08, 44.56it/s][A
 13%|        | 57/435 [00:01<00:08, 44.81it/s][A
 14%|        | 62/435 [00:01<00:08, 44.88it/s][A
 15%|        | 67/435 [00:01<00:08, 44.72it/s][A
 17%|        | 72/435 [00:01<00:08, 44.63it/s][A
 18%|        | 77/435 [00:01<00:08, 44.60it/s][A
 19%|        | 82/435 [00:01<00:07, 44.27it/s][A
 20%|        | 87/435 [00:01<00:07, 44.42it/s][A
 21%|        | 92/435 [00:02<00:07, 44.60it/s][A
 22%|       | 97/435 [00:02<00:07, 44.81it/s][A
 23%|       | 102/435 [00:02<00:07, 44.85it/s][A
 25%|       | 107/435 [00:02<00:07, 44.91it/s][A
 26%|       | 112/435 [00:02<00:07, 44.90it/s][A
 27%|       | 117/435 [00:02<00:07, 44.86it/s][A
 28%|       | 122/435 [00:02<00:07, 44.61it/s][A
 29%|       | 127/435 [00:02<00:06, 44.57it/s][A
 30%|       | 132/435 [00:02<00:06, 44.56it/s][A
 31%|      | 137/435 [00:03<00:06, 44.66it/s][A
 33%|      | 142/435 [00:03<00:06, 44.76it/s][A
 34%|      | 147/435 [00:03<00:06, 44.83it/s][A
 35%|      | 152/435 [00:03<00:06, 44.86it/s][A
 36%|      | 157/435 [00:03<00:06, 44.83it/s][A
 37%|      | 162/435 [00:03<00:06, 44.80it/s][A
 38%|      | 167/435 [00:03<00:05, 44.73it/s][A
 40%|      | 172/435 [00:03<00:05, 44.60it/s][A
 41%|      | 177/435 [00:03<00:05, 44.59it/s][A
 42%|     | 182/435 [00:04<00:05, 44.61it/s][A
 43%|     | 187/435 [00:04<00:05, 44.78it/s][A
 44%|     | 192/435 [00:04<00:05, 44.84it/s][A
 45%|     | 197/435 [00:04<00:05, 44.76it/s][A
 46%|     | 202/435 [00:04<00:05, 44.75it/s][A
 48%|     | 207/435 [00:04<00:05, 44.82it/s][A
 49%|     | 212/435 [00:04<00:04, 44.79it/s][A
 50%|     | 217/435 [00:04<00:04, 44.63it/s][A
 51%|     | 222/435 [00:04<00:04, 44.59it/s][A
 52%|    | 227/435 [00:05<00:04, 44.67it/s][A
 53%|    | 232/435 [00:05<00:04, 44.72it/s][A
 54%|    | 237/435 [00:05<00:04, 44.76it/s][A
 56%|    | 242/435 [00:05<00:04, 44.76it/s][A
 57%|    | 247/435 [00:05<00:04, 44.77it/s][A
 58%|    | 252/435 [00:05<00:04, 44.82it/s][A
 59%|    | 257/435 [00:05<00:03, 44.67it/s][A
 60%|    | 262/435 [00:05<00:03, 44.56it/s][A
 61%|   | 267/435 [00:05<00:03, 44.58it/s][A
 63%|   | 272/435 [00:06<00:03, 44.68it/s][A
 64%|   | 277/435 [00:06<00:03, 44.67it/s][A
 65%|   | 282/435 [00:06<00:03, 44.67it/s][A
 66%|   | 287/435 [00:06<00:03, 44.64it/s][A
 67%|   | 292/435 [00:06<00:03, 44.76it/s][A
 68%|   | 297/435 [00:06<00:03, 44.80it/s][A
 69%|   | 302/435 [00:06<00:02, 44.67it/s][A
 71%|   | 307/435 [00:06<00:02, 44.58it/s][A
 72%|  | 312/435 [00:06<00:02, 43.85it/s][A
 73%|  | 317/435 [00:07<00:02, 44.17it/s][A
 74%|  | 322/435 [00:07<00:02, 44.36it/s][A
 75%|  | 327/435 [00:07<00:02, 44.46it/s][A
 76%|  | 332/435 [00:07<00:02, 44.61it/s][A
 77%|  | 337/435 [00:07<00:02, 44.79it/s][A
 79%|  | 342/435 [00:07<00:02, 44.76it/s][A
 80%|  | 347/435 [00:07<00:01, 44.53it/s][A
 81%|  | 352/435 [00:07<00:01, 44.34it/s][A
 82%| | 357/435 [00:07<00:01, 44.43it/s][A
 83%| | 362/435 [00:08<00:01, 44.57it/s][A
 84%| | 367/435 [00:08<00:01, 44.68it/s][A
 86%| | 372/435 [00:08<00:01, 44.61it/s][A
 87%| | 377/435 [00:08<00:01, 44.68it/s][A
 88%| | 382/435 [00:08<00:01, 44.75it/s][A
 89%| | 387/435 [00:08<00:01, 44.65it/s][A
 90%| | 392/435 [00:08<00:00, 44.59it/s][A
 91%|| 397/435 [00:08<00:00, 44.64it/s][A
 92%|| 402/435 [00:08<00:00, 44.64it/s][A
 94%|| 407/435 [00:09<00:00, 44.63it/s][A
 95%|| 412/435 [00:09<00:00, 44.65it/s][A
 96%|| 417/435 [00:09<00:00, 44.70it/s][A
 97%|| 422/435 [00:09<00:00, 44.81it/s][A
 98%|| 427/435 [00:09<00:00, 44.79it/s][A
 99%|| 432/435 [00:09<00:00, 44.65it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.65it/s][A 80%|  | 624/780 [04:17<00:45,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:46:32,556 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 00:46:32,863 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:46:36,547 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:46:36,887 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:46:37,091 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:32<19:42,  7.63s/it] 80%|  | 626/780 [04:33<13:56,  5.43s/it] 80%|  | 627/780 [04:33<09:55,  3.89s/it] 81%|  | 628/780 [04:33<07:10,  2.83s/it] 81%|  | 629/780 [04:34<05:12,  2.07s/it] 81%|  | 630/780 [04:34<03:50,  1.54s/it] 81%|  | 631/780 [04:34<02:53,  1.17s/it] 81%|  | 632/780 [04:34<02:13,  1.11it/s] 81%|  | 633/780 [04:35<01:46,  1.39it/s] 81%| | 634/780 [04:35<01:26,  1.68it/s] 81%| | 635/780 [04:35<01:13,  1.98it/s] 82%| | 636/780 [04:36<01:03,  2.26it/s] 82%| | 637/780 [04:36<00:56,  2.51it/s] 82%| | 638/780 [04:36<00:53,  2.68it/s] 82%| | 639/780 [04:37<00:49,  2.85it/s] 82%| | 640/780 [04:37<00:46,  2.99it/s] 82%| | 641/780 [04:37<00:44,  3.10it/s] 82%| | 642/780 [04:37<00:43,  3.18it/s] 82%| | 643/780 [04:38<00:42,  3.24it/s] 83%| | 644/780 [04:38<00:41,  3.28it/s] 83%| | 645/780 [04:38<00:40,  3.31it/s] 83%| | 646/780 [04:39<00:40,  3.33it/s] 83%| | 647/780 [04:39<00:39,  3.35it/s] 83%| | 648/780 [04:39<00:39,  3.36it/s] 83%| | 649/780 [04:40<00:40,  3.27it/s] 83%| | 650/780 [04:40<00:39,  3.30it/s] 83%| | 651/780 [04:40<00:38,  3.32it/s] 84%| | 652/780 [04:40<00:38,  3.34it/s] 84%| | 653/780 [04:41<00:37,  3.35it/s] 84%| | 654/780 [04:41<00:37,  3.36it/s] 84%| | 655/780 [04:41<00:37,  3.37it/s] 84%| | 656/780 [04:42<00:36,  3.37it/s] 84%| | 657/780 [04:42<00:36,  3.37it/s] 84%| | 658/780 [04:42<00:36,  3.38it/s] 84%| | 659/780 [04:43<00:35,  3.38it/s] 85%| | 660/780 [04:43<00:36,  3.30it/s] 85%| | 661/780 [04:43<00:35,  3.32it/s] 85%| | 662/780 [04:43<00:35,  3.34it/s] 85%| | 663/780 [04:44<00:34,  3.35it/s] 85%| | 664/780 [04:44<00:34,  3.36it/s] 85%| | 665/780 [04:44<00:34,  3.37it/s] 85%| | 666/780 [04:45<00:33,  3.37it/s] 86%| | 667/780 [04:45<00:33,  3.37it/s] 86%| | 668/780 [04:45<00:33,  3.37it/s] 86%| | 669/780 [04:46<00:32,  3.37it/s] 86%| | 670/780 [04:46<00:32,  3.37it/s] 86%| | 671/780 [04:46<00:33,  3.25it/s] 86%| | 672/780 [04:46<00:32,  3.29it/s] 86%| | 673/780 [04:47<00:32,  3.26it/s] 86%| | 674/780 [04:47<00:32,  3.31it/s] 87%| | 675/780 [04:47<00:31,  3.34it/s] 87%| | 676/780 [04:48<00:30,  3.37it/s] 87%| | 677/780 [04:48<00:30,  3.39it/s] 87%| | 678/780 [04:48<00:29,  3.40it/s] 87%| | 679/780 [04:49<00:29,  3.41it/s] 87%| | 680/780 [04:49<00:29,  3.36it/s] 87%| | 681/780 [04:49<00:39,  2.53it/s] 87%| | 682/780 [04:50<00:37,  2.61it/s] 88%| | 683/780 [04:50<00:34,  2.81it/s] 88%| | 684/780 [04:50<00:32,  2.97it/s] 88%| | 685/780 [04:51<00:30,  3.10it/s] 88%| | 686/780 [04:51<00:29,  3.19it/s] 88%| | 687/780 [04:51<00:28,  3.25it/s] 88%| | 688/780 [04:52<00:28,  3.28it/s] 88%| | 689/780 [04:52<00:27,  3.31it/s] 88%| | 690/780 [04:52<00:27,  3.33it/s] 89%| | 691/780 [04:52<00:26,  3.35it/s] 89%| | 692/780 [04:53<00:27,  3.24it/s] 89%| | 693/780 [04:53<00:26,  3.27it/s] 89%| | 694/780 [04:53<00:26,  3.31it/s] 89%| | 695/780 [04:54<00:25,  3.32it/s] 89%| | 696/780 [04:54<00:25,  3.34it/s] 89%| | 697/780 [04:54<00:24,  3.35it/s] 89%| | 698/780 [04:55<00:24,  3.36it/s] 90%| | 699/780 [04:55<00:23,  3.38it/s] 90%| | 700/780 [04:55<00:23,  3.40it/s] 90%| | 701/780 [04:55<00:23,  3.41it/s] 90%| | 702/780 [04:56<00:22,  3.42it/s] 90%| | 703/780 [04:56<00:22,  3.35it/s] 90%| | 704/780 [04:56<00:22,  3.37it/s] 90%| | 705/780 [04:57<00:22,  3.39it/s] 91%| | 706/780 [04:57<00:21,  3.40it/s] 91%| | 707/780 [04:57<00:21,  3.41it/s] 91%| | 708/780 [04:57<00:21,  3.41it/s] 91%| | 709/780 [04:58<00:20,  3.42it/s] 91%| | 710/780 [04:58<00:20,  3.42it/s] 91%| | 711/780 [04:58<00:20,  3.42it/s] 91%|| 712/780 [04:59<00:19,  3.43it/s] 91%|| 713/780 [04:59<00:19,  3.43it/s] 92%|| 714/780 [04:59<00:19,  3.33it/s] 92%|| 715/780 [05:00<00:19,  3.36it/s] 92%|| 716/780 [05:00<00:18,  3.38it/s] 92%|| 717/780 [05:00<00:18,  3.39it/s] 92%|| 718/780 [05:00<00:18,  3.40it/s] 92%|| 719/780 [05:01<00:17,  3.41it/s] 92%|| 720/780 [05:01<00:17,  3.42it/s] 92%|| 721/780 [05:01<00:17,  3.42it/s] 93%|| 722/780 [05:02<00:16,  3.42it/s] 93%|| 723/780 [05:02<00:16,  3.43it/s] 93%|| 724/780 [05:02<00:16,  3.43it/s] 93%|| 725/780 [05:02<00:16,  3.43it/s] 93%|| 726/780 [05:03<00:15,  3.43it/s] 93%|| 727/780 [05:03<00:15,  3.42it/s] 93%|| 728/780 [05:03<00:15,  3.42it/s] 93%|| 729/780 [05:04<00:14,  3.42it/s] 94%|| 730/780 [05:04<00:14,  3.37it/s] 94%|| 731/780 [05:04<00:14,  3.38it/s] 94%|| 732/780 [05:05<00:14,  3.40it/s] 94%|| 733/780 [05:05<00:13,  3.40it/s] 94%|| 734/780 [05:05<00:13,  3.41it/s] 94%|| 735/780 [05:05<00:13,  3.42it/s] 94%|| 736/780 [05:06<00:12,  3.42it/s] 94%|| 737/780 [05:06<00:12,  3.42it/s] 95%|| 738/780 [05:06<00:12,  3.42it/s] 95%|| 739/780 [05:07<00:11,  3.42it/s] 95%|| 740/780 [05:07<00:11,  3.43it/s] 95%|| 741/780 [05:07<00:11,  3.39it/s] 95%|| 742/780 [05:07<00:11,  3.40it/s] 95%|| 743/780 [05:08<00:10,  3.41it/s] 95%|| 744/780 [05:08<00:10,  3.41it/s] 96%|| 745/780 [05:08<00:10,  3.42it/s] 96%|| 746/780 [05:09<00:09,  3.42it/s] 96%|| 747/780 [05:09<00:09,  3.42it/s] 96%|| 748/780 [05:09<00:09,  3.42it/s] 96%|| 749/780 [05:10<00:09,  3.42it/s] 96%|| 750/780 [05:10<00:08,  3.42it/s] 96%|| 751/780 [05:10<00:08,  3.43it/s] 96%|| 752/780 [05:10<00:08,  3.33it/s] 97%|| 753/780 [05:11<00:08,  3.36it/s] 97%|| 754/780 [05:11<00:07,  3.38it/s] 97%|| 755/780 [05:11<00:07,  3.39it/s] 97%|| 756/780 [05:12<00:07,  3.40it/s] 97%|| 757/780 [05:12<00:06,  3.41it/s] 97%|| 758/780 [05:12<00:06,  3.42it/s] 97%|| 759/780 [05:12<00:06,  3.42it/s] 97%|| 760/780 [05:13<00:05,  3.42it/s] 98%|| 761/780 [05:13<00:05,  3.43it/s] 98%|| 762/780 [05:13<00:05,  3.43it/s] 98%|| 763/780 [05:14<00:05,  3.33it/s] 98%|| 764/780 [05:14<00:04,  3.36it/s] 98%|| 765/780 [05:14<00:04,  3.38it/s] 98%|| 766/780 [05:15<00:04,  3.39it/s] 98%|| 767/780 [05:15<00:03,  3.40it/s] 98%|| 768/780 [05:15<00:03,  3.41it/s] 99%|| 769/780 [05:15<00:03,  3.42it/s] 99%|| 770/780 [05:16<00:02,  3.42it/s] 99%|| 771/780 [05:16<00:02,  3.42it/s] 99%|| 772/780 [05:16<00:02,  3.42it/s] 99%|| 773/780 [05:17<00:02,  3.42it/s] 99%|| 774/780 [05:17<00:01,  3.33it/s] 99%|| 775/780 [05:17<00:01,  3.36it/s] 99%|| 776/780 [05:17<00:01,  3.38it/s]100%|| 777/780 [05:18<00:00,  3.39it/s]100%|| 778/780 [05:18<00:00,  3.40it/s]100%|| 779/780 [05:18<00:00,  3.41it/s]100%|| 780/780 [05:19<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 00:47:33,491 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:47:33,491 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:47:33,491 >>   Batch size = 8
{'eval_loss': 0.9511211514472961, 'eval_runtime': 9.7361, 'eval_samples_per_second': 357.123, 'eval_steps_per_second': 44.679, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.98it/s][A
  3%|         | 12/435 [00:00<00:08, 48.92it/s][A
  4%|         | 17/435 [00:00<00:08, 47.25it/s][A
  5%|         | 22/435 [00:00<00:08, 46.26it/s][A
  6%|         | 27/435 [00:00<00:08, 45.70it/s][A
  7%|         | 32/435 [00:00<00:08, 45.14it/s][A
  9%|         | 37/435 [00:00<00:08, 44.92it/s][A
 10%|         | 42/435 [00:00<00:08, 44.69it/s][A
 11%|         | 47/435 [00:01<00:09, 40.77it/s][A
 12%|        | 52/435 [00:01<00:09, 42.09it/s][A
 13%|        | 57/435 [00:01<00:08, 42.93it/s][A
 14%|        | 62/435 [00:01<00:08, 43.63it/s][A
 15%|        | 67/435 [00:01<00:08, 44.05it/s][A
 17%|        | 72/435 [00:01<00:08, 44.45it/s][A
 18%|        | 77/435 [00:01<00:08, 44.43it/s][A
 19%|        | 82/435 [00:01<00:07, 44.43it/s][A
 20%|        | 87/435 [00:01<00:07, 44.11it/s][A
 21%|        | 92/435 [00:02<00:07, 44.11it/s][A
 22%|       | 97/435 [00:02<00:07, 43.99it/s][A
 23%|       | 102/435 [00:02<00:07, 44.60it/s][A
 25%|       | 107/435 [00:02<00:07, 44.80it/s][A
 26%|       | 112/435 [00:02<00:07, 44.95it/s][A
 27%|       | 117/435 [00:02<00:07, 44.96it/s][A
 28%|       | 122/435 [00:02<00:06, 44.85it/s][A
 29%|       | 127/435 [00:02<00:06, 44.65it/s][A
 30%|       | 132/435 [00:02<00:06, 44.44it/s][A
 31%|      | 137/435 [00:03<00:06, 44.36it/s][A
 33%|      | 142/435 [00:03<00:06, 44.50it/s][A
 34%|      | 147/435 [00:03<00:06, 44.53it/s][A
 35%|      | 152/435 [00:03<00:06, 44.71it/s][A
 36%|      | 157/435 [00:03<00:06, 44.82it/s][A
 37%|      | 162/435 [00:03<00:06, 44.90it/s][A
 38%|      | 167/435 [00:03<00:05, 44.78it/s][A
 40%|      | 172/435 [00:03<00:05, 44.64it/s][A
 41%|      | 177/435 [00:04<00:05, 44.53it/s][A
 42%|     | 182/435 [00:04<00:06, 40.21it/s][A
 43%|     | 187/435 [00:04<00:05, 41.53it/s][A
 44%|     | 192/435 [00:04<00:05, 42.54it/s][A
 45%|     | 197/435 [00:04<00:05, 43.25it/s][A
 46%|     | 202/435 [00:04<00:05, 43.75it/s][A
 48%|     | 207/435 [00:04<00:05, 44.16it/s][A
 49%|     | 212/435 [00:04<00:05, 44.35it/s][A
 50%|     | 217/435 [00:04<00:04, 44.55it/s][A
 51%|     | 222/435 [00:05<00:04, 44.21it/s][A
 52%|    | 227/435 [00:05<00:04, 44.25it/s][A
 53%|    | 232/435 [00:05<00:04, 44.38it/s][A
 54%|    | 237/435 [00:05<00:04, 44.55it/s][A
 56%|    | 242/435 [00:05<00:04, 44.65it/s][A
 57%|    | 247/435 [00:05<00:04, 44.79it/s][A
 58%|    | 252/435 [00:05<00:04, 44.66it/s][A
 59%|    | 257/435 [00:05<00:03, 44.89it/s][A
 60%|    | 262/435 [00:05<00:03, 44.87it/s][A
 61%|   | 267/435 [00:06<00:03, 44.46it/s][A
 63%|   | 272/435 [00:06<00:03, 44.67it/s][A
 64%|   | 277/435 [00:06<00:03, 44.70it/s][A
 65%|   | 282/435 [00:06<00:03, 44.76it/s][A
 66%|   | 287/435 [00:06<00:03, 44.73it/s][A
 67%|   | 292/435 [00:06<00:03, 44.76it/s][A
 68%|   | 297/435 [00:06<00:03, 44.82it/s][A
 69%|   | 302/435 [00:06<00:02, 44.75it/s][A
 71%|   | 307/435 [00:06<00:02, 44.66it/s][A
 72%|  | 312/435 [00:07<00:02, 44.61it/s][A
 73%|  | 317/435 [00:07<00:02, 40.36it/s][A
 74%|  | 322/435 [00:07<00:02, 41.74it/s][A
 75%|  | 327/435 [00:07<00:02, 42.61it/s][A
 76%|  | 332/435 [00:07<00:02, 43.35it/s][A
 77%|  | 337/435 [00:07<00:02, 43.84it/s][A
 79%|  | 342/435 [00:07<00:02, 44.23it/s][A
 80%|  | 347/435 [00:07<00:01, 44.46it/s][A
 81%|  | 352/435 [00:07<00:01, 44.51it/s][A
 82%| | 357/435 [00:08<00:01, 44.31it/s][A
 83%| | 362/435 [00:08<00:01, 44.33it/s][A
 84%| | 367/435 [00:08<00:01, 44.42it/s][A
 86%| | 372/435 [00:08<00:01, 44.60it/s][A
 87%| | 377/435 [00:08<00:01, 44.74it/s][A
 88%| | 382/435 [00:08<00:01, 44.85it/s][A
 89%| | 387/435 [00:08<00:01, 44.87it/s][A
 90%| | 392/435 [00:08<00:00, 44.87it/s][A
 91%|| 397/435 [00:08<00:00, 44.71it/s][A
 92%|| 402/435 [00:09<00:00, 44.44it/s][A
 94%|| 407/435 [00:09<00:00, 44.45it/s][A
 95%|| 412/435 [00:09<00:00, 44.53it/s][A
 96%|| 417/435 [00:09<00:00, 44.68it/s][A
 97%|| 422/435 [00:09<00:00, 44.77it/s][A
 98%|| 427/435 [00:09<00:00, 44.89it/s][A
 99%|| 432/435 [00:09<00:00, 44.95it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.95it/s][A100%|| 780/780 [05:28<00:00,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:47:43,685 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 00:47:44,074 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:47:47,778 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:47:47,891 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:47:47,960 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:47:54,506 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:47:54,524 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-156 (score: 0.9278207421302795).
                                                 100%|| 780/780 [05:52<00:00,  3.41it/s]100%|| 780/780 [05:52<00:00,  2.21it/s]
[INFO|trainer.py:1894] 2023-08-29 00:48:06,905 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 00:48:07,228 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:48:11,561 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:48:11,795 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:48:11,889 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:48:12,383 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:12,383 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:12,383 >>   train_loss               =     0.7433
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:12,383 >>   train_runtime            = 0:05:52.42
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:12,383 >>   train_samples            =      10010
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:12,383 >>   train_samples_per_second =    142.015
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:12,383 >>   train_steps_per_second   =      2.213
{'eval_loss': 0.9551570415496826, 'eval_runtime': 9.8333, 'eval_samples_per_second': 353.593, 'eval_steps_per_second': 44.237, 'epoch': 5.0}
{'train_runtime': 352.4264, 'train_samples_per_second': 142.015, 'train_steps_per_second': 2.213, 'train_loss': 0.7432918059520233, 'epoch': 5.0}
08/29/2023 00:48:12 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:48:12,676 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:48:12,676 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:48:12,676 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 55.65it/s]  3%|         | 12/435 [00:00<00:08, 49.22it/s]  4%|         | 17/435 [00:00<00:08, 47.73it/s]  5%|         | 22/435 [00:00<00:08, 46.83it/s]  6%|         | 27/435 [00:00<00:08, 46.34it/s]  7%|         | 32/435 [00:00<00:08, 46.06it/s]  9%|         | 37/435 [00:00<00:08, 45.82it/s] 10%|         | 42/435 [00:00<00:08, 45.49it/s] 11%|         | 47/435 [00:01<00:08, 45.01it/s] 12%|        | 52/435 [00:01<00:08, 44.95it/s] 13%|        | 57/435 [00:01<00:08, 45.02it/s] 14%|        | 62/435 [00:01<00:08, 45.10it/s] 15%|        | 67/435 [00:01<00:08, 45.17it/s] 17%|        | 72/435 [00:01<00:08, 45.29it/s] 18%|        | 77/435 [00:01<00:07, 45.28it/s] 19%|        | 82/435 [00:01<00:07, 45.24it/s] 20%|        | 87/435 [00:01<00:07, 45.04it/s] 21%|        | 92/435 [00:02<00:08, 41.71it/s] 22%|       | 97/435 [00:02<00:07, 42.79it/s] 23%|       | 102/435 [00:02<00:07, 43.47it/s] 25%|       | 107/435 [00:02<00:07, 44.02it/s] 26%|       | 112/435 [00:02<00:07, 44.40it/s] 27%|       | 117/435 [00:02<00:07, 44.69it/s] 28%|       | 122/435 [00:02<00:06, 44.89it/s] 29%|       | 127/435 [00:02<00:06, 44.94it/s] 30%|       | 132/435 [00:02<00:06, 44.58it/s] 31%|      | 137/435 [00:03<00:06, 44.60it/s] 33%|      | 142/435 [00:03<00:06, 44.79it/s] 34%|      | 147/435 [00:03<00:06, 44.94it/s] 35%|      | 152/435 [00:03<00:06, 45.00it/s] 36%|      | 157/435 [00:03<00:06, 45.10it/s] 37%|      | 162/435 [00:03<00:06, 39.83it/s] 38%|      | 167/435 [00:03<00:06, 41.98it/s] 40%|      | 172/435 [00:03<00:06, 42.86it/s] 41%|      | 177/435 [00:03<00:05, 43.70it/s] 42%|     | 182/435 [00:04<00:05, 44.22it/s] 43%|     | 187/435 [00:04<00:06, 39.52it/s] 44%|     | 192/435 [00:04<00:05, 41.16it/s] 45%|     | 197/435 [00:04<00:05, 42.32it/s] 46%|     | 202/435 [00:04<00:05, 43.14it/s] 48%|     | 207/435 [00:04<00:05, 43.67it/s] 49%|     | 212/435 [00:04<00:05, 44.07it/s] 50%|     | 217/435 [00:04<00:04, 44.44it/s] 51%|     | 222/435 [00:05<00:04, 44.65it/s] 52%|    | 227/435 [00:05<00:04, 44.50it/s] 53%|    | 232/435 [00:05<00:04, 44.43it/s] 54%|    | 237/435 [00:05<00:04, 44.62it/s] 56%|    | 242/435 [00:05<00:04, 44.80it/s] 57%|    | 247/435 [00:05<00:04, 44.90it/s] 58%|    | 252/435 [00:05<00:04, 44.86it/s] 59%|    | 257/435 [00:05<00:03, 44.90it/s] 60%|    | 262/435 [00:05<00:03, 45.01it/s] 61%|   | 267/435 [00:06<00:03, 45.00it/s] 63%|   | 272/435 [00:06<00:03, 44.85it/s] 64%|   | 277/435 [00:06<00:03, 44.77it/s] 65%|   | 282/435 [00:06<00:03, 44.46it/s] 66%|   | 287/435 [00:06<00:03, 44.71it/s] 67%|   | 292/435 [00:06<00:03, 44.89it/s] 68%|   | 297/435 [00:06<00:03, 44.90it/s] 69%|   | 302/435 [00:06<00:02, 44.95it/s] 71%|   | 307/435 [00:06<00:02, 45.01it/s] 72%|  | 312/435 [00:07<00:02, 45.06it/s] 73%|  | 317/435 [00:07<00:02, 44.99it/s] 74%|  | 322/435 [00:07<00:02, 42.41it/s] 75%|  | 327/435 [00:07<00:02, 43.23it/s] 76%|  | 332/435 [00:07<00:02, 43.82it/s] 77%|  | 337/435 [00:07<00:02, 44.26it/s] 79%|  | 342/435 [00:07<00:02, 44.54it/s] 80%|  | 347/435 [00:07<00:01, 44.55it/s] 81%|  | 352/435 [00:07<00:01, 44.74it/s] 82%| | 357/435 [00:08<00:01, 44.68it/s] 83%| | 362/435 [00:08<00:01, 44.52it/s] 84%| | 367/435 [00:08<00:01, 44.59it/s] 86%| | 372/435 [00:08<00:01, 44.71it/s] 87%| | 377/435 [00:08<00:01, 44.90it/s] 88%| | 382/435 [00:08<00:01, 44.95it/s] 89%| | 387/435 [00:08<00:01, 45.06it/s] 90%| | 392/435 [00:08<00:00, 45.04it/s] 91%|| 397/435 [00:08<00:00, 45.03it/s] 92%|| 402/435 [00:09<00:00, 44.87it/s] 94%|| 407/435 [00:09<00:00, 44.69it/s] 95%|| 412/435 [00:09<00:00, 44.68it/s] 96%|| 417/435 [00:09<00:00, 44.74it/s] 97%|| 422/435 [00:09<00:00, 44.89it/s] 98%|| 427/435 [00:09<00:00, 45.04it/s] 99%|| 432/435 [00:09<00:00, 45.10it/s]100%|| 435/435 [00:09<00:00, 44.49it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:48:22,471 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:22,471 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:22,471 >>   eval_loss               =     0.9278
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:22,471 >>   eval_runtime            = 0:00:09.79
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:22,471 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:22,471 >>   eval_samples_per_second =    354.979
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:22,471 >>   eval_steps_per_second   =     44.411
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:22,472 >>   perplexity              =      2.529
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:32,629 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:32,639 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:32,639 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:32,639 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:32,639 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:48:33,362 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:48:33,363 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:48:33,967 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:48:35,083 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:48:35,083 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:38,253 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:38,293 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:38,293 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:38,293 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:38,293 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:48:39,254 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:48:39,255 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:48:39,949 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:48:40,222 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:48:40,222 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.47it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.56it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:13,  1.56it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:23,  1.60it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:27,  1.59it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:30,  1.58it/s]Extractor Predicting: 48it [00:31,  1.54it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:32,  1.63it/s]Extractor Predicting: 51it [00:32,  1.62it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:34,  1.59it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:36,  1.53it/s]Extractor Predicting: 58it [00:37,  1.57it/s]Extractor Predicting: 59it [00:37,  1.60it/s]Extractor Predicting: 60it [00:38,  1.62it/s]Extractor Predicting: 61it [00:39,  1.61it/s]Extractor Predicting: 62it [00:39,  1.63it/s]Extractor Predicting: 63it [00:40,  1.62it/s]Extractor Predicting: 64it [00:41,  1.62it/s]Extractor Predicting: 65it [00:41,  1.62it/s]Extractor Predicting: 66it [00:42,  1.58it/s]Extractor Predicting: 67it [00:42,  1.59it/s]Extractor Predicting: 68it [00:43,  1.60it/s]Extractor Predicting: 69it [00:44,  1.60it/s]Extractor Predicting: 70it [00:44,  1.60it/s]Extractor Predicting: 71it [00:45,  1.57it/s]Extractor Predicting: 72it [00:46,  1.59it/s]Extractor Predicting: 73it [00:46,  1.59it/s]Extractor Predicting: 74it [00:47,  1.62it/s]Extractor Predicting: 75it [00:48,  1.50it/s]Extractor Predicting: 76it [00:48,  1.48it/s]Extractor Predicting: 77it [00:49,  1.49it/s]Extractor Predicting: 78it [00:50,  1.51it/s]Extractor Predicting: 79it [00:50,  1.51it/s]Extractor Predicting: 80it [00:51,  1.51it/s]Extractor Predicting: 81it [00:52,  1.47it/s]Extractor Predicting: 82it [00:52,  1.51it/s]Extractor Predicting: 83it [00:53,  1.53it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:54,  1.54it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:56,  1.52it/s]Extractor Predicting: 89it [00:57,  1.56it/s]Extractor Predicting: 90it [00:57,  1.57it/s]Extractor Predicting: 91it [00:58,  1.59it/s]Extractor Predicting: 92it [00:59,  1.62it/s]Extractor Predicting: 93it [00:59,  1.64it/s]Extractor Predicting: 94it [01:00,  1.61it/s]Extractor Predicting: 95it [01:00,  1.65it/s]Extractor Predicting: 96it [01:01,  1.60it/s]Extractor Predicting: 97it [01:02,  1.61it/s]Extractor Predicting: 98it [01:02,  1.61it/s]Extractor Predicting: 99it [01:03,  1.58it/s]Extractor Predicting: 100it [01:04,  1.55it/s]Extractor Predicting: 101it [01:04,  1.56it/s]Extractor Predicting: 102it [01:05,  1.64it/s]Extractor Predicting: 103it [01:05,  1.66it/s]Extractor Predicting: 104it [01:06,  1.63it/s]Extractor Predicting: 105it [01:07,  1.63it/s]Extractor Predicting: 106it [01:07,  1.63it/s]Extractor Predicting: 107it [01:08,  1.65it/s]Extractor Predicting: 108it [01:08,  1.63it/s]Extractor Predicting: 109it [01:09,  1.63it/s]Extractor Predicting: 110it [01:10,  1.63it/s]Extractor Predicting: 111it [01:10,  1.64it/s]Extractor Predicting: 112it [01:11,  1.64it/s]Extractor Predicting: 113it [01:11,  1.68it/s]Extractor Predicting: 114it [01:12,  1.72it/s]Extractor Predicting: 115it [01:13,  1.68it/s]Extractor Predicting: 116it [01:13,  1.69it/s]Extractor Predicting: 117it [01:14,  1.69it/s]Extractor Predicting: 118it [01:14,  1.67it/s]Extractor Predicting: 119it [01:15,  1.68it/s]Extractor Predicting: 120it [01:16,  1.61it/s]Extractor Predicting: 121it [01:16,  1.58it/s]Extractor Predicting: 122it [01:17,  1.60it/s]Extractor Predicting: 123it [01:18,  1.60it/s]Extractor Predicting: 124it [01:18,  1.59it/s]Extractor Predicting: 125it [01:19,  1.59it/s]Extractor Predicting: 126it [01:19,  1.61it/s]Extractor Predicting: 127it [01:20,  1.60it/s]Extractor Predicting: 128it [01:21,  1.60it/s]Extractor Predicting: 129it [01:21,  1.60it/s]Extractor Predicting: 130it [01:22,  1.55it/s]Extractor Predicting: 131it [01:23,  1.57it/s]Extractor Predicting: 132it [01:23,  1.55it/s]Extractor Predicting: 133it [01:24,  1.58it/s]Extractor Predicting: 134it [01:24,  1.62it/s]Extractor Predicting: 135it [01:25,  1.58it/s]Extractor Predicting: 136it [01:26,  1.56it/s]Extractor Predicting: 137it [01:26,  1.60it/s]Extractor Predicting: 138it [01:27,  1.59it/s]Extractor Predicting: 139it [01:28,  1.57it/s]Extractor Predicting: 140it [01:28,  1.57it/s]Extractor Predicting: 141it [01:29,  1.60it/s]Extractor Predicting: 142it [01:30,  1.59it/s]Extractor Predicting: 143it [01:30,  1.62it/s]Extractor Predicting: 144it [01:31,  1.66it/s]Extractor Predicting: 144it [01:31,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:27,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:27,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:27,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:27,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:27,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:50:28,123 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:50:28,125 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:50:28,414 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:50:29,481 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:50:29,481 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:32,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:32,166 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:32,166 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:32,166 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:32,166 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:50:32,915 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:50:32,916 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:50:33,607 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:50:33,767 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:50:33,767 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.39775051124744376,
  "recall": 0.11187805579522576,
  "score": 0.17463524130190794,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:13,  1.62it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.64it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.64it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:19,  1.64it/s]Extractor Predicting: 32it [00:19,  1.67it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:20,  1.63it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:22,  1.65it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.67it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:24,  1.67it/s]Extractor Predicting: 41it [00:25,  1.67it/s]Extractor Predicting: 42it [00:25,  1.70it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:27,  1.58it/s]Extractor Predicting: 46it [00:28,  1.57it/s]Extractor Predicting: 47it [00:28,  1.62it/s]Extractor Predicting: 48it [00:29,  1.54it/s]Extractor Predicting: 49it [00:30,  1.60it/s]Extractor Predicting: 50it [00:30,  1.61it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:32,  1.59it/s]Extractor Predicting: 53it [00:32,  1.59it/s]Extractor Predicting: 54it [00:33,  1.58it/s]Extractor Predicting: 55it [00:33,  1.63it/s]Extractor Predicting: 56it [00:34,  1.62it/s]Extractor Predicting: 57it [00:35,  1.60it/s]Extractor Predicting: 58it [00:35,  1.55it/s]Extractor Predicting: 59it [00:36,  1.54it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:37,  1.57it/s]Extractor Predicting: 62it [00:38,  1.58it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:39,  1.58it/s]Extractor Predicting: 65it [00:40,  1.64it/s]Extractor Predicting: 66it [00:40,  1.61it/s]Extractor Predicting: 67it [00:41,  1.60it/s]Extractor Predicting: 68it [00:42,  1.55it/s]Extractor Predicting: 69it [00:42,  1.57it/s]Extractor Predicting: 70it [00:43,  1.55it/s]Extractor Predicting: 71it [00:44,  1.58it/s]Extractor Predicting: 72it [00:44,  1.57it/s]Extractor Predicting: 73it [00:45,  1.55it/s]Extractor Predicting: 74it [00:46,  1.58it/s]Extractor Predicting: 75it [00:46,  1.58it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:47,  1.60it/s]Extractor Predicting: 78it [00:48,  1.62it/s]Extractor Predicting: 79it [00:49,  1.62it/s]Extractor Predicting: 80it [00:49,  1.51it/s]Extractor Predicting: 81it [00:50,  1.53it/s]Extractor Predicting: 82it [00:51,  1.56it/s]Extractor Predicting: 83it [00:51,  1.58it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:53,  1.57it/s]Extractor Predicting: 86it [00:53,  1.51it/s]Extractor Predicting: 87it [00:54,  1.49it/s]Extractor Predicting: 88it [00:55,  1.53it/s]Extractor Predicting: 89it [00:55,  1.52it/s]Extractor Predicting: 90it [00:56,  1.50it/s]Extractor Predicting: 91it [00:57,  1.48it/s]Extractor Predicting: 92it [00:57,  1.50it/s]Extractor Predicting: 93it [00:58,  1.52it/s]Extractor Predicting: 94it [00:59,  1.54it/s]Extractor Predicting: 95it [00:59,  1.54it/s]Extractor Predicting: 96it [01:00,  1.53it/s]Extractor Predicting: 97it [01:00,  1.52it/s]Extractor Predicting: 98it [01:01,  1.49it/s]Extractor Predicting: 99it [01:02,  1.51it/s]Extractor Predicting: 100it [01:02,  1.53it/s]Extractor Predicting: 101it [01:03,  1.52it/s]Extractor Predicting: 102it [01:04,  1.53it/s]Extractor Predicting: 103it [01:04,  1.52it/s]Extractor Predicting: 104it [01:05,  1.51it/s]Extractor Predicting: 105it [01:06,  1.55it/s]Extractor Predicting: 106it [01:06,  1.51it/s]Extractor Predicting: 107it [01:07,  1.54it/s]Extractor Predicting: 108it [01:08,  1.56it/s]Extractor Predicting: 109it [01:08,  1.55it/s]Extractor Predicting: 110it [01:09,  1.54it/s]Extractor Predicting: 111it [01:10,  1.54it/s]Extractor Predicting: 112it [01:10,  1.54it/s]Extractor Predicting: 113it [01:11,  1.55it/s]Extractor Predicting: 114it [01:12,  1.58it/s]Extractor Predicting: 115it [01:12,  1.57it/s]Extractor Predicting: 116it [01:13,  1.50it/s]Extractor Predicting: 117it [01:14,  1.51it/s]Extractor Predicting: 118it [01:14,  1.54it/s]Extractor Predicting: 119it [01:15,  1.53it/s]Extractor Predicting: 120it [01:15,  1.53it/s]Extractor Predicting: 121it [01:16,  1.54it/s]Extractor Predicting: 122it [01:17,  1.58it/s]Extractor Predicting: 123it [01:17,  1.58it/s]Extractor Predicting: 124it [01:18,  1.56it/s]Extractor Predicting: 125it [01:19,  1.56it/s]Extractor Predicting: 126it [01:19,  1.63it/s]Extractor Predicting: 127it [01:20,  1.64it/s]Extractor Predicting: 128it [01:20,  1.60it/s]Extractor Predicting: 129it [01:21,  1.66it/s]Extractor Predicting: 130it [01:22,  1.64it/s]Extractor Predicting: 131it [01:22,  1.63it/s]Extractor Predicting: 132it [01:23,  1.61it/s]Extractor Predicting: 133it [01:23,  1.66it/s]Extractor Predicting: 134it [01:24,  1.68it/s]Extractor Predicting: 135it [01:25,  1.63it/s]Extractor Predicting: 136it [01:25,  1.64it/s]Extractor Predicting: 137it [01:26,  1.64it/s]Extractor Predicting: 138it [01:26,  1.69it/s]Extractor Predicting: 139it [01:27,  1.67it/s]Extractor Predicting: 140it [01:28,  1.66it/s]Extractor Predicting: 141it [01:28,  1.65it/s]Extractor Predicting: 142it [01:29,  1.67it/s]Extractor Predicting: 143it [01:30,  1.58it/s]Extractor Predicting: 144it [01:30,  1.58it/s]Extractor Predicting: 145it [01:31,  1.59it/s]Extractor Predicting: 146it [01:31,  1.62it/s]Extractor Predicting: 147it [01:32,  1.67it/s]Extractor Predicting: 148it [01:33,  1.65it/s]Extractor Predicting: 149it [01:33,  1.67it/s]Extractor Predicting: 150it [01:34,  1.71it/s]Extractor Predicting: 151it [01:34,  1.71it/s]Extractor Predicting: 152it [01:35,  1.73it/s]Extractor Predicting: 153it [01:35,  1.72it/s]Extractor Predicting: 154it [01:36,  1.71it/s]Extractor Predicting: 155it [01:37,  1.70it/s]Extractor Predicting: 156it [01:37,  1.73it/s]Extractor Predicting: 157it [01:38,  1.76it/s]Extractor Predicting: 158it [01:38,  1.75it/s]Extractor Predicting: 159it [01:39,  1.81it/s]Extractor Predicting: 160it [01:39,  1.83it/s]Extractor Predicting: 161it [01:40,  1.78it/s]Extractor Predicting: 162it [01:41,  1.73it/s]Extractor Predicting: 163it [01:41,  1.73it/s]Extractor Predicting: 164it [01:42,  1.74it/s]Extractor Predicting: 165it [01:42,  1.78it/s]Extractor Predicting: 166it [01:43,  1.76it/s]Extractor Predicting: 167it [01:43,  1.76it/s]Extractor Predicting: 168it [01:44,  1.74it/s]Extractor Predicting: 169it [01:45,  1.77it/s]Extractor Predicting: 170it [01:45,  1.76it/s]Extractor Predicting: 171it [01:46,  1.80it/s]Extractor Predicting: 172it [01:46,  1.77it/s]Extractor Predicting: 173it [01:47,  1.78it/s]Extractor Predicting: 174it [01:47,  1.68it/s]Extractor Predicting: 175it [01:48,  1.68it/s]Extractor Predicting: 176it [01:49,  1.65it/s]Extractor Predicting: 177it [01:49,  1.62it/s]Extractor Predicting: 178it [01:50,  1.58it/s]Extractor Predicting: 179it [01:51,  1.56it/s]Extractor Predicting: 180it [01:51,  1.59it/s]Extractor Predicting: 181it [01:52,  1.58it/s]Extractor Predicting: 182it [01:53,  1.58it/s]Extractor Predicting: 183it [01:53,  1.60it/s]Extractor Predicting: 184it [01:54,  1.58it/s]Extractor Predicting: 185it [01:54,  1.58it/s]Extractor Predicting: 186it [01:55,  1.61it/s]Extractor Predicting: 187it [01:56,  1.60it/s]Extractor Predicting: 188it [01:56,  1.58it/s]Extractor Predicting: 189it [01:57,  1.59it/s]Extractor Predicting: 190it [01:58,  1.57it/s]Extractor Predicting: 191it [01:58,  1.55it/s]Extractor Predicting: 192it [01:59,  1.55it/s]Extractor Predicting: 193it [02:00,  1.54it/s]Extractor Predicting: 194it [02:00,  1.52it/s]Extractor Predicting: 195it [02:01,  1.54it/s]Extractor Predicting: 196it [02:01,  1.58it/s]Extractor Predicting: 197it [02:02,  1.60it/s]Extractor Predicting: 198it [02:03,  1.59it/s]Extractor Predicting: 199it [02:03,  1.58it/s]Extractor Predicting: 200it [02:04,  1.55it/s]Extractor Predicting: 201it [02:05,  1.37it/s]Extractor Predicting: 202it [02:06,  1.38it/s]Extractor Predicting: 203it [02:06,  1.40it/s]Extractor Predicting: 204it [02:07,  1.41it/s]Extractor Predicting: 205it [02:08,  1.43it/s]Extractor Predicting: 206it [02:08,  1.45it/s]Extractor Predicting: 207it [02:09,  1.48it/s]Extractor Predicting: 208it [02:10,  1.47it/s]Extractor Predicting: 209it [02:11,  1.41it/s]Extractor Predicting: 210it [02:11,  1.43it/s]Extractor Predicting: 211it [02:12,  1.43it/s]Extractor Predicting: 212it [02:13,  1.46it/s]Extractor Predicting: 213it [02:13,  1.47it/s]Extractor Predicting: 214it [02:14,  1.48it/s]Extractor Predicting: 215it [02:15,  1.46it/s]Extractor Predicting: 216it [02:15,  1.47it/s]Extractor Predicting: 217it [02:16,  1.48it/s]Extractor Predicting: 218it [02:17,  1.48it/s]Extractor Predicting: 219it [02:17,  1.44it/s]Extractor Predicting: 220it [02:18,  1.42it/s]Extractor Predicting: 221it [02:19,  1.43it/s]Extractor Predicting: 222it [02:19,  1.43it/s]Extractor Predicting: 223it [02:20,  1.47it/s]Extractor Predicting: 224it [02:21,  1.47it/s]Extractor Predicting: 225it [02:21,  1.48it/s]Extractor Predicting: 226it [02:22,  1.47it/s]Extractor Predicting: 227it [02:23,  1.49it/s]Extractor Predicting: 228it [02:23,  1.50it/s]Extractor Predicting: 229it [02:24,  1.49it/s]Extractor Predicting: 230it [02:25,  1.52it/s]Extractor Predicting: 231it [02:25,  1.57it/s]Extractor Predicting: 232it [02:26,  1.61it/s]Extractor Predicting: 233it [02:26,  1.63it/s]Extractor Predicting: 234it [02:27,  1.56it/s]Extractor Predicting: 235it [02:28,  1.59it/s]Extractor Predicting: 236it [02:28,  1.60it/s]Extractor Predicting: 237it [02:29,  1.60it/s]Extractor Predicting: 238it [02:30,  1.61it/s]Extractor Predicting: 239it [02:30,  1.54it/s]Extractor Predicting: 240it [02:31,  1.59it/s]Extractor Predicting: 241it [02:32,  1.61it/s]Extractor Predicting: 242it [02:32,  1.64it/s]Extractor Predicting: 243it [02:33,  1.64it/s]Extractor Predicting: 244it [02:33,  1.67it/s]Extractor Predicting: 245it [02:34,  1.63it/s]Extractor Predicting: 246it [02:35,  1.60it/s]Extractor Predicting: 247it [02:35,  1.59it/s]Extractor Predicting: 248it [02:36,  1.57it/s]Extractor Predicting: 249it [02:37,  1.59it/s]Extractor Predicting: 250it [02:37,  1.59it/s]Extractor Predicting: 251it [02:38,  1.62it/s]Extractor Predicting: 252it [02:38,  1.66it/s]Extractor Predicting: 253it [02:39,  1.64it/s]Extractor Predicting: 254it [02:40,  1.57it/s]Extractor Predicting: 255it [02:40,  1.59it/s]Extractor Predicting: 256it [02:41,  1.59it/s]Extractor Predicting: 257it [02:42,  1.54it/s]Extractor Predicting: 258it [02:42,  1.55it/s]Extractor Predicting: 259it [02:43,  1.54it/s]Extractor Predicting: 260it [02:44,  1.54it/s]Extractor Predicting: 261it [02:44,  1.55it/s]Extractor Predicting: 262it [02:45,  1.56it/s]Extractor Predicting: 263it [02:45,  1.59it/s]Extractor Predicting: 264it [02:46,  1.54it/s]Extractor Predicting: 265it [02:47,  1.55it/s]Extractor Predicting: 266it [02:47,  1.54it/s]Extractor Predicting: 267it [02:48,  1.54it/s]Extractor Predicting: 268it [02:49,  1.53it/s]Extractor Predicting: 269it [02:49,  1.51it/s]Extractor Predicting: 270it [02:50,  1.53it/s]Extractor Predicting: 271it [02:51,  1.55it/s]Extractor Predicting: 272it [02:51,  1.58it/s]Extractor Predicting: 273it [02:52,  1.53it/s]Extractor Predicting: 274it [02:53,  1.52it/s]Extractor Predicting: 275it [02:53,  1.53it/s]Extractor Predicting: 276it [02:54,  1.53it/s]Extractor Predicting: 277it [02:55,  1.54it/s]Extractor Predicting: 278it [02:55,  1.54it/s]Extractor Predicting: 279it [02:56,  1.53it/s]Extractor Predicting: 280it [02:57,  1.54it/s]Extractor Predicting: 281it [02:57,  1.57it/s]Extractor Predicting: 282it [02:58,  1.56it/s]Extractor Predicting: 283it [02:58,  1.51it/s]Extractor Predicting: 284it [02:59,  1.58it/s]Extractor Predicting: 285it [03:00,  1.56it/s]Extractor Predicting: 286it [03:00,  1.60it/s]Extractor Predicting: 287it [03:01,  1.59it/s]Extractor Predicting: 288it [03:02,  1.56it/s]Extractor Predicting: 289it [03:02,  1.56it/s]Extractor Predicting: 290it [03:03,  1.56it/s]Extractor Predicting: 291it [03:04,  1.51it/s]Extractor Predicting: 292it [03:04,  1.51it/s]Extractor Predicting: 293it [03:05,  1.52it/s]Extractor Predicting: 294it [03:06,  1.50it/s]Extractor Predicting: 295it [03:06,  1.52it/s]Extractor Predicting: 296it [03:07,  1.51it/s]Extractor Predicting: 297it [03:08,  1.54it/s]Extractor Predicting: 298it [03:08,  1.49it/s]Extractor Predicting: 299it [03:09,  1.49it/s]Extractor Predicting: 300it [03:10,  1.52it/s]Extractor Predicting: 301it [03:10,  1.51it/s]Extractor Predicting: 302it [03:11,  1.55it/s]Extractor Predicting: 303it [03:12,  1.48it/s]Extractor Predicting: 304it [03:13,  1.32it/s]Extractor Predicting: 305it [03:13,  1.37it/s]Extractor Predicting: 306it [03:14,  1.44it/s]Extractor Predicting: 307it [03:14,  1.45it/s]Extractor Predicting: 308it [03:15,  1.47it/s]Extractor Predicting: 309it [03:16,  1.49it/s]Extractor Predicting: 310it [03:16,  1.52it/s]Extractor Predicting: 311it [03:17,  1.51it/s]Extractor Predicting: 312it [03:18,  1.56it/s]Extractor Predicting: 313it [03:18,  1.60it/s]Extractor Predicting: 314it [03:19,  1.62it/s]Extractor Predicting: 315it [03:19,  1.64it/s]Extractor Predicting: 316it [03:20,  1.61it/s]Extractor Predicting: 317it [03:21,  1.58it/s]Extractor Predicting: 318it [03:21,  1.55it/s]Extractor Predicting: 319it [03:22,  1.55it/s]Extractor Predicting: 320it [03:23,  1.54it/s]Extractor Predicting: 321it [03:23,  1.55it/s]Extractor Predicting: 322it [03:24,  1.56it/s]Extractor Predicting: 323it [03:25,  1.52it/s]Extractor Predicting: 324it [03:25,  1.52it/s]Extractor Predicting: 325it [03:26,  1.51it/s]Extractor Predicting: 326it [03:27,  1.53it/s]Extractor Predicting: 327it [03:27,  1.57it/s]Extractor Predicting: 328it [03:28,  1.56it/s]Extractor Predicting: 329it [03:29,  1.58it/s]Extractor Predicting: 330it [03:29,  1.57it/s]Extractor Predicting: 331it [03:30,  1.56it/s]Extractor Predicting: 332it [03:30,  1.55it/s]Extractor Predicting: 333it [03:31,  1.58it/s]Extractor Predicting: 334it [03:32,  1.58it/s]Extractor Predicting: 335it [03:32,  1.55it/s]Extractor Predicting: 336it [03:33,  1.54it/s]Extractor Predicting: 337it [03:34,  1.55it/s]Extractor Predicting: 338it [03:34,  1.58it/s]Extractor Predicting: 339it [03:35,  1.62it/s]Extractor Predicting: 340it [03:36,  1.59it/s]Extractor Predicting: 341it [03:36,  1.59it/s]Extractor Predicting: 342it [03:37,  1.58it/s]Extractor Predicting: 343it [03:37,  1.58it/s]Extractor Predicting: 344it [03:38,  1.61it/s]Extractor Predicting: 345it [03:39,  1.54it/s]Extractor Predicting: 346it [03:39,  1.55it/s]Extractor Predicting: 347it [03:40,  1.56it/s]Extractor Predicting: 348it [03:41,  1.57it/s]Extractor Predicting: 349it [03:41,  1.60it/s]Extractor Predicting: 350it [03:42,  1.59it/s]Extractor Predicting: 351it [03:42,  1.61it/s]Extractor Predicting: 352it [03:43,  1.58it/s]Extractor Predicting: 353it [03:44,  1.60it/s]Extractor Predicting: 354it [03:44,  1.63it/s]Extractor Predicting: 355it [03:45,  1.64it/s]Extractor Predicting: 356it [03:46,  1.61it/s]Extractor Predicting: 357it [03:46,  1.61it/s]Extractor Predicting: 358it [03:47,  1.62it/s]Extractor Predicting: 359it [03:47,  1.61it/s]Extractor Predicting: 360it [03:48,  1.59it/s]Extractor Predicting: 361it [03:49,  1.61it/s]Extractor Predicting: 362it [03:49,  1.61it/s]Extractor Predicting: 363it [03:50,  1.63it/s]Extractor Predicting: 364it [03:50,  1.65it/s]Extractor Predicting: 365it [03:51,  1.65it/s]Extractor Predicting: 366it [03:52,  1.63it/s]Extractor Predicting: 367it [03:52,  1.58it/s]Extractor Predicting: 368it [03:53,  1.59it/s]Extractor Predicting: 369it [03:54,  1.60it/s]Extractor Predicting: 370it [03:54,  1.65it/s]Extractor Predicting: 371it [03:55,  1.66it/s]Extractor Predicting: 372it [03:55,  1.66it/s]Extractor Predicting: 373it [03:56,  1.62it/s]Extractor Predicting: 374it [03:57,  1.63it/s]Extractor Predicting: 375it [03:57,  1.64it/s]Extractor Predicting: 376it [03:58,  1.65it/s]Extractor Predicting: 377it [03:58,  1.66it/s]Extractor Predicting: 378it [03:59,  1.69it/s]Extractor Predicting: 379it [04:00,  1.63it/s]Extractor Predicting: 380it [04:00,  1.61it/s]Extractor Predicting: 381it [04:01,  1.63it/s]Extractor Predicting: 382it [04:02,  1.63it/s]Extractor Predicting: 383it [04:02,  1.66it/s]Extractor Predicting: 384it [04:03,  1.69it/s]Extractor Predicting: 385it [04:03,  1.65it/s]Extractor Predicting: 386it [04:04,  1.62it/s]Extractor Predicting: 387it [04:05,  1.62it/s]Extractor Predicting: 388it [04:05,  1.61it/s]Extractor Predicting: 389it [04:06,  1.62it/s]Extractor Predicting: 390it [04:06,  1.62it/s]Extractor Predicting: 391it [04:07,  1.57it/s]Extractor Predicting: 392it [04:08,  1.59it/s]Extractor Predicting: 393it [04:08,  1.63it/s]Extractor Predicting: 394it [04:09,  1.57it/s]Extractor Predicting: 395it [04:10,  1.52it/s]Extractor Predicting: 396it [04:10,  1.49it/s]Extractor Predicting: 397it [04:11,  1.49it/s]Extractor Predicting: 398it [04:12,  1.50it/s]Extractor Predicting: 399it [04:12,  1.49it/s]Extractor Predicting: 400it [04:13,  1.53it/s]Extractor Predicting: 401it [04:14,  1.48it/s]Extractor Predicting: 402it [04:14,  1.47it/s]Extractor Predicting: 403it [04:15,  1.49it/s]Extractor Predicting: 404it [04:16,  1.44it/s]Extractor Predicting: 405it [04:17,  1.45it/s]Extractor Predicting: 406it [04:17,  1.44it/s]Extractor Predicting: 407it [04:18,  1.43it/s]Extractor Predicting: 408it [04:19,  1.47it/s]Extractor Predicting: 409it [04:19,  1.50it/s]Extractor Predicting: 410it [04:20,  1.30it/s]Extractor Predicting: 411it [04:21,  1.34it/s]Extractor Predicting: 412it [04:22,  1.39it/s]Extractor Predicting: 413it [04:22,  1.44it/s]Extractor Predicting: 414it [04:23,  1.48it/s]Extractor Predicting: 415it [04:23,  1.53it/s]Extractor Predicting: 416it [04:24,  1.54it/s]Extractor Predicting: 417it [04:25,  1.54it/s]Extractor Predicting: 418it [04:25,  1.54it/s]Extractor Predicting: 419it [04:26,  1.48it/s]Extractor Predicting: 420it [04:27,  1.50it/s]Extractor Predicting: 421it [04:27,  1.57it/s]Extractor Predicting: 421it [04:27,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:13,060 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:13,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:13,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:13,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:13,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:55:13,504 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:55:13,505 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:13,809 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:14,890 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:14,891 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:16,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:16,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:16,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:16,806 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:16,806 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:55:17,169 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:55:17,170 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:17,467 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:17,643 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:17,643 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.21085972850678733,
  "recall": 0.04616146607231303,
  "score": 0.0757415684681024,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.44it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.94it/s]Extractor Predicting: 9it [00:05,  1.61it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:55:24,756 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:55:24,781 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:55:24,806 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:55:24,807 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:55:24,820 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:55:34,593 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:55:34,618 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:55:34,731 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:55:34,732 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:55:34,795 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:55:34,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:55:34,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:55:34,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:55:34,846 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:55:34,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:55:34,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.14814814814814814,
  "recall": 0.009876543209876543,
  "score": 0.018518518518518517,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:55:35,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:35,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:36,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:37,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:37,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:38,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:39,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:39,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:40,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:40,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:41,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:42,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:42,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:43,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:43,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:44,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:45,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:45,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:46,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:47,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:47,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:48,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:49,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:49,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:15<04:45, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-29 00:55:50,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:50,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:51,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:52,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:52,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:53,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:54,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:54,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:55,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:56,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:56,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:57,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:58,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:58,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:55:59,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:00,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:00,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:01,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:02,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:03,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:03,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:04,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:05,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:05,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:06,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:32<04:51, 16.19s/it][WARNING|generation_utils.py:914] 2023-08-29 00:56:07,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:07,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:08,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:09,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:09,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:10,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:10,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:11,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:12,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:12,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:13,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:14,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:14,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:15,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:16,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:16,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:17,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:18,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:18,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:19,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:19,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:20,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:21,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:46<04:22, 15.45s/it][WARNING|generation_utils.py:914] 2023-08-29 00:56:21,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:22,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:23,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:23,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:24,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:25,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:25,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:26,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:27,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:27,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:28,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:29,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:29,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:30,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:31,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:31,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:32,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:33,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:34,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:34,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:35,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:36,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:01<04:05, 15.32s/it][WARNING|generation_utils.py:914] 2023-08-29 00:56:36,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:37,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:38,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:38,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:39,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:40,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:40,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:41,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:41,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:42,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:43,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:43,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:44,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:45,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:45,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:46,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:47,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:47,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:48,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:49,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:49,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:50,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:15<03:42, 14.84s/it][WARNING|generation_utils.py:914] 2023-08-29 00:56:50,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:51,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:52,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:53,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:53,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:54,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:54,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:55,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:56,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:56,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:57,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:58,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:58,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:59,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:00,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:00,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:01,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:02,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:02,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:03,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:04,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:29<03:22, 14.48s/it][WARNING|generation_utils.py:914] 2023-08-29 00:57:04,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:05,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:06,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:07,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:08,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:08,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:09,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:10,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:11,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:11,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:12,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:13,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:14,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:14,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:15,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:16,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:17,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:17,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:18,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:19,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:19,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:20,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:46<03:17, 15.18s/it][WARNING|generation_utils.py:914] 2023-08-29 00:57:21,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:21,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:22,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:23,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:24,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:24,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:25,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:26,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:26,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:27,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:28,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:29,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:30,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:30,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:31,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:32,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:33,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:33,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:34,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:35,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:35,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:36,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:37,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:03<03:08, 15.74s/it][WARNING|generation_utils.py:914] 2023-08-29 00:57:38,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:38,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:39,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:40,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:40,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:41,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:42,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:42,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:43,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:44,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:44,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:45,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:46,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:47,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:47,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:48,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:49,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:49,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:50,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:51,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:51,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:17<02:48, 15.30s/it][WARNING|generation_utils.py:914] 2023-08-29 00:57:52,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:53,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:54,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:54,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:55,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:56,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:57,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:57,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:58,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:59,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:00,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:00,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:01,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:02,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:02,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:03,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:04,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:05,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:06,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:06,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:07,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:08,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:09,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:34<02:39, 15.95s/it][WARNING|generation_utils.py:914] 2023-08-29 00:58:10,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:10,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:11,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:12,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:12,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:13,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:14,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:14,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:15,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:16,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:16,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:17,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:18,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:18,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:19,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:20,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:20,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:21,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:22,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:23,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:23,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:49<02:19, 15.51s/it][WARNING|generation_utils.py:914] 2023-08-29 00:58:24,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:25,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:25,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:26,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:27,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:27,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:28,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:29,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:30,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:31,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:31,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:32,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:33,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:33,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:34,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:35,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:35,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:36,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:37,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:37,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:38,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:04<02:02, 15.29s/it][WARNING|generation_utils.py:914] 2023-08-29 00:58:39,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:40,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:40,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:41,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:42,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:43,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:44,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:45,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:45,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:46,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:47,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:48,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:48,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:49,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:50,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:51,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:51,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:52,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:53,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:53,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:54,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:55,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:20<01:50, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-29 00:58:56,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:56,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:57,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:57,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:58,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:59,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:59,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:00,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:00,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:01,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:02,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:02,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:03,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:03,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:04,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:04,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:05,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:06,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:07,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:07,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:33<01:27, 14.66s/it][WARNING|generation_utils.py:914] 2023-08-29 00:59:08,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:08,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:09,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:10,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:10,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:11,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:12,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:12,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:13,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:14,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:14,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:15,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:16,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:16,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:17,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:18,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:18,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:19,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:20,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:20,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:21,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:22,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:22,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:23,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [03:48<01:14, 14.95s/it][WARNING|generation_utils.py:914] 2023-08-29 00:59:23,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:24,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:25,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:25,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:26,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:26,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:27,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:28,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:28,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:29,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:30,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:30,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:31,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:32,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:32,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:33,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:33,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:34,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:35,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:35,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:36,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:36,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:02<00:58, 14.60s/it][WARNING|generation_utils.py:914] 2023-08-29 00:59:37,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:38,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:38,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:39,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:40,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:40,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:41,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:41,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:42,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:42,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:43,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:44,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:44,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:45,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:46,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:46,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:47,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:47,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:48,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:48,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:49,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:15<00:41, 13.99s/it][WARNING|generation_utils.py:914] 2023-08-29 00:59:50,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:50,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:51,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:52,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:52,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:53,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:53,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:54,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:55,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:55,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:56,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:57,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:57,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:58,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:59,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:59,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:00,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:00,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:01,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:02,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:02,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:03,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [04:28<00:27, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-29 01:00:03,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:04,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:05,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:05,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:06,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:07,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:07,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:08,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:09,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:09,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:10,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:11,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:11,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:12,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:13,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:14,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:14,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:15,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:16,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:17,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:17,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:18,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [04:43<00:14, 14.27s/it][WARNING|generation_utils.py:914] 2023-08-29 01:00:19,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:19,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:20,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:21,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:21,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:22,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:23,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:24,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:24,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:25,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:26,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:26,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:27,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:28,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:28,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:29,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:30,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:30,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:31,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:32,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:33,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:33,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [04:59<00:00, 14.56s/it]Generating: 100%|| 20/20 [04:59<00:00, 14.96s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:42,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:42,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:42,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:42,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:42,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:00:43,225 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:00:43,226 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:00:43,803 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:00:44,885 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:00:44,885 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:47,902 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:47,925 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:47,925 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:47,925 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:47,925 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:00:48,581 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:00:48,582 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:00:49,180 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:00:49,353 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:00:49,353 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Pristina and Daralu could continue working together as co - prime ministers until 2018 , when they could resign for failing to meet the constitution requirement . Head Entity : Daralu , Tail Entity : Turkey .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Isabelle', 'country of citizenship', '', 'He is the son of President Manuel Mrquez and his wife Isabelle .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 137, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 280, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 404, 'raw': 544}
{'target': 600, 'success': 429, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 478, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.76125, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : head of government .', 'success_rate': 0.8396739130434783, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8565340909090909, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : crosses . Context : Later in the year ( October 1887 ) , a young French soldier named Louis Boulogne had completed his tour of France . Head Entity : Louis Boulogne , Tail Entity : Charente .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : crosses .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8505434782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8693181818181818, 'errors': {'', '(\'Lord James of Normandy\', \'occupation\', \'\', \'" The Old Guard , " in his edition of the " The " , Lord James of Normandy was at one point a prisoner of war . He went on to write , " The " may be the most famous of all the British prisoners of war . "\')', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : participant .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The game is a co - produced by Sony Computer Entertainment Japan and published by Square Enix . Head Entity : PlayStation PlayStation , Tail Entity : Sony Computer Entertainment Japan .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.8928571428571429, 'errors': {''}}
['Relation : position played on team / speciality . Context : On 31 March 2014 , the Armenian national team returned to their World Cup tournament spot at the end of the World Cup , having lost 21 to eventual winner Brazil . Head Entity : Karim Chlopas , Tail Entity : Armenian national team .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8536931818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 15070
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15170, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.40it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.52it/s]Extractor Estimating: 4it [00:02,  1.50it/s]Extractor Estimating: 5it [00:03,  1.48it/s]Extractor Estimating: 6it [00:04,  1.51it/s]Extractor Estimating: 7it [00:04,  1.55it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.55it/s]Extractor Estimating: 11it [00:07,  1.53it/s]Extractor Estimating: 12it [00:07,  1.47it/s]Extractor Estimating: 13it [00:08,  1.56it/s]Extractor Estimating: 14it [00:09,  1.56it/s]Extractor Estimating: 15it [00:09,  1.58it/s]Extractor Estimating: 16it [00:10,  1.59it/s]Extractor Estimating: 17it [00:11,  1.57it/s]Extractor Estimating: 18it [00:11,  1.50it/s]Extractor Estimating: 19it [00:12,  1.49it/s]Extractor Estimating: 20it [00:13,  1.47it/s]Extractor Estimating: 21it [00:13,  1.49it/s]Extractor Estimating: 22it [00:14,  1.57it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:15,  1.62it/s]Extractor Estimating: 25it [00:16,  1.61it/s]Extractor Estimating: 26it [00:16,  1.58it/s]Extractor Estimating: 27it [00:17,  1.52it/s]Extractor Estimating: 28it [00:18,  1.44it/s]Extractor Estimating: 29it [00:19,  1.45it/s]Extractor Estimating: 30it [00:19,  1.48it/s]Extractor Estimating: 31it [00:20,  1.52it/s]Extractor Estimating: 32it [00:20,  1.48it/s]Extractor Estimating: 33it [00:21,  1.42it/s]Extractor Estimating: 34it [00:22,  1.43it/s]Extractor Estimating: 35it [00:23,  1.35it/s]Extractor Estimating: 36it [00:23,  1.43it/s]Extractor Estimating: 37it [00:24,  1.41it/s]Extractor Estimating: 38it [00:25,  1.37it/s]Extractor Estimating: 39it [00:26,  1.38it/s]Extractor Estimating: 40it [00:26,  1.38it/s]Extractor Estimating: 41it [00:27,  1.38it/s]Extractor Estimating: 42it [00:28,  1.37it/s]Extractor Estimating: 43it [00:29,  1.37it/s]Extractor Estimating: 44it [00:29,  1.37it/s]Extractor Estimating: 45it [00:30,  1.42it/s]Extractor Estimating: 46it [00:31,  1.41it/s]Extractor Estimating: 47it [00:31,  1.35it/s]Extractor Estimating: 48it [00:32,  1.40it/s]Extractor Estimating: 49it [00:33,  1.41it/s]Extractor Estimating: 50it [00:34,  1.39it/s]Extractor Estimating: 51it [00:34,  1.38it/s]Extractor Estimating: 52it [00:35,  1.44it/s]Extractor Estimating: 53it [00:35,  1.53it/s]Extractor Estimating: 54it [00:36,  1.56it/s]Extractor Estimating: 55it [00:37,  1.59it/s]Extractor Estimating: 56it [00:37,  1.60it/s]Extractor Estimating: 57it [00:38,  1.64it/s]Extractor Estimating: 58it [00:38,  1.66it/s]Extractor Estimating: 59it [00:39,  1.63it/s]Extractor Estimating: 60it [00:40,  1.59it/s]Extractor Estimating: 61it [00:40,  1.61it/s]Extractor Estimating: 62it [00:41,  1.69it/s]Extractor Estimating: 63it [00:42,  1.59it/s]Extractor Estimating: 64it [00:42,  1.59it/s]Extractor Estimating: 65it [00:43,  1.67it/s]Extractor Estimating: 66it [00:43,  1.65it/s]Extractor Estimating: 67it [00:44,  1.63it/s]Extractor Estimating: 68it [00:45,  1.59it/s]Extractor Estimating: 69it [00:45,  1.61it/s]Extractor Estimating: 70it [00:46,  1.58it/s]Extractor Estimating: 71it [00:47,  1.55it/s]Extractor Estimating: 72it [00:47,  1.57it/s]Extractor Estimating: 73it [00:48,  1.62it/s]Extractor Estimating: 74it [00:48,  1.59it/s]Extractor Estimating: 75it [00:49,  1.62it/s]Extractor Estimating: 76it [00:50,  1.55it/s]Extractor Estimating: 77it [00:50,  1.49it/s]Extractor Estimating: 78it [00:51,  1.46it/s]Extractor Estimating: 79it [00:52,  1.52it/s]Extractor Estimating: 80it [00:52,  1.52it/s]Extractor Estimating: 81it [00:53,  1.56it/s]Extractor Estimating: 82it [00:54,  1.59it/s]Extractor Estimating: 83it [00:54,  1.55it/s]Extractor Estimating: 84it [00:55,  1.53it/s]Extractor Estimating: 85it [00:56,  1.54it/s]Extractor Estimating: 86it [00:56,  1.52it/s]Extractor Estimating: 87it [00:57,  1.53it/s]Extractor Estimating: 88it [00:58,  1.53it/s]Extractor Estimating: 89it [00:58,  1.56it/s]Extractor Estimating: 90it [00:59,  1.58it/s]Extractor Estimating: 91it [00:59,  1.57it/s]Extractor Estimating: 92it [01:00,  1.50it/s]Extractor Estimating: 93it [01:01,  1.52it/s]Extractor Estimating: 94it [01:02,  1.47it/s]Extractor Estimating: 95it [01:02,  1.40it/s]Extractor Estimating: 96it [01:03,  1.38it/s]Extractor Estimating: 97it [01:04,  1.40it/s]Extractor Estimating: 98it [01:05,  1.42it/s]Extractor Estimating: 99it [01:05,  1.31it/s]Extractor Estimating: 100it [01:06,  1.34it/s]Extractor Estimating: 101it [01:07,  1.40it/s]Extractor Estimating: 102it [01:07,  1.41it/s]Extractor Estimating: 103it [01:08,  1.50it/s]Extractor Estimating: 104it [01:09,  1.54it/s]Extractor Estimating: 105it [01:09,  1.54it/s]Extractor Estimating: 106it [01:10,  1.57it/s]Extractor Estimating: 107it [01:11,  1.57it/s]Extractor Estimating: 108it [01:11,  1.62it/s]Extractor Estimating: 109it [01:12,  1.64it/s]Extractor Estimating: 110it [01:12,  1.55it/s]Extractor Estimating: 111it [01:13,  1.54it/s]Extractor Estimating: 112it [01:14,  1.61it/s]Extractor Estimating: 113it [01:14,  1.61it/s]Extractor Estimating: 114it [01:15,  1.63it/s]Extractor Estimating: 115it [01:15,  1.61it/s]Extractor Estimating: 116it [01:16,  1.63it/s]Extractor Estimating: 117it [01:17,  1.60it/s]Extractor Estimating: 118it [01:17,  1.60it/s]Extractor Estimating: 119it [01:18,  1.57it/s]Extractor Estimating: 120it [01:19,  1.52it/s]Extractor Estimating: 121it [01:19,  1.56it/s]Extractor Estimating: 122it [01:20,  1.56it/s]Extractor Estimating: 123it [01:21,  1.54it/s]Extractor Estimating: 124it [01:21,  1.58it/s]Extractor Estimating: 125it [01:22,  1.56it/s]Extractor Estimating: 126it [01:23,  1.50it/s]Extractor Estimating: 127it [01:23,  1.41it/s]Extractor Estimating: 128it [01:24,  1.37it/s]Extractor Estimating: 129it [01:25,  1.41it/s]Extractor Estimating: 130it [01:26,  1.41it/s]Extractor Estimating: 131it [01:26,  1.44it/s]Extractor Estimating: 132it [01:27,  1.49it/s]Extractor Estimating: 133it [01:28,  1.45it/s]Extractor Estimating: 134it [01:28,  1.44it/s]Extractor Estimating: 135it [01:29,  1.40it/s]Extractor Estimating: 136it [01:30,  1.43it/s]Extractor Estimating: 137it [01:30,  1.45it/s]Extractor Estimating: 138it [01:31,  1.48it/s]Extractor Estimating: 139it [01:32,  1.44it/s]Extractor Estimating: 140it [01:32,  1.42it/s]Extractor Estimating: 141it [01:33,  1.49it/s]Extractor Estimating: 142it [01:34,  1.45it/s]Extractor Estimating: 143it [01:35,  1.37it/s]Extractor Estimating: 144it [01:35,  1.37it/s]Extractor Estimating: 145it [01:36,  1.43it/s]Extractor Estimating: 146it [01:37,  1.46it/s]Extractor Estimating: 147it [01:37,  1.46it/s]Extractor Estimating: 148it [01:38,  1.42it/s]Extractor Estimating: 149it [01:39,  1.42it/s]Extractor Estimating: 150it [01:39,  1.42it/s]Extractor Estimating: 151it [01:40,  1.43it/s]Extractor Estimating: 152it [01:41,  1.47it/s]Extractor Estimating: 153it [01:42,  1.45it/s]Extractor Estimating: 154it [01:42,  1.45it/s]Extractor Estimating: 155it [01:43,  1.52it/s]Extractor Estimating: 156it [01:43,  1.56it/s]Extractor Estimating: 157it [01:44,  1.61it/s]Extractor Estimating: 158it [01:45,  1.63it/s]Extractor Estimating: 159it [01:45,  1.67it/s]Extractor Estimating: 160it [01:46,  1.68it/s]Extractor Estimating: 161it [01:46,  1.64it/s]Extractor Estimating: 162it [01:47,  1.61it/s]Extractor Estimating: 163it [01:48,  1.64it/s]Extractor Estimating: 164it [01:48,  1.62it/s]Extractor Estimating: 165it [01:49,  1.64it/s]Extractor Estimating: 166it [01:50,  1.57it/s]Extractor Estimating: 167it [01:50,  1.54it/s]Extractor Estimating: 168it [01:51,  1.53it/s]Extractor Estimating: 169it [01:51,  1.54it/s]Extractor Estimating: 170it [01:52,  1.50it/s]Extractor Estimating: 171it [01:53,  1.35it/s]Extractor Estimating: 172it [01:54,  1.42it/s]Extractor Estimating: 173it [01:54,  1.44it/s]Extractor Estimating: 174it [01:55,  1.50it/s]Extractor Estimating: 175it [01:56,  1.49it/s]Extractor Estimating: 176it [01:56,  1.54it/s]Extractor Estimating: 177it [01:57,  1.48it/s]Extractor Estimating: 178it [01:58,  1.51it/s]Extractor Estimating: 179it [01:58,  1.53it/s]Extractor Estimating: 180it [01:59,  1.50it/s]Extractor Estimating: 181it [02:00,  1.55it/s]Extractor Estimating: 182it [02:00,  1.55it/s]Extractor Estimating: 183it [02:01,  1.52it/s]Extractor Estimating: 184it [02:02,  1.50it/s]Extractor Estimating: 185it [02:02,  1.49it/s]Extractor Estimating: 186it [02:03,  1.42it/s]Extractor Estimating: 187it [02:04,  1.44it/s]Extractor Estimating: 188it [02:04,  1.46it/s]Extractor Estimating: 189it [02:05,  1.49it/s]Extractor Estimating: 190it [02:06,  1.51it/s]Extractor Estimating: 191it [02:06,  1.52it/s]Extractor Estimating: 192it [02:07,  1.47it/s]Extractor Estimating: 193it [02:08,  1.46it/s]Extractor Estimating: 194it [02:08,  1.49it/s]Extractor Estimating: 195it [02:09,  1.50it/s]Extractor Estimating: 196it [02:10,  1.55it/s]Extractor Estimating: 197it [02:10,  1.54it/s]Extractor Estimating: 198it [02:11,  1.54it/s]Extractor Estimating: 199it [02:12,  1.54it/s]Extractor Estimating: 200it [02:12,  1.54it/s]Extractor Estimating: 201it [02:13,  1.56it/s]Extractor Estimating: 202it [02:14,  1.47it/s]Extractor Estimating: 203it [02:14,  1.50it/s]Extractor Estimating: 204it [02:15,  1.56it/s]Extractor Estimating: 205it [02:16,  1.51it/s]Extractor Estimating: 206it [02:16,  1.53it/s]Extractor Estimating: 207it [02:17,  1.49it/s]Extractor Estimating: 208it [02:18,  1.51it/s]Extractor Estimating: 209it [02:18,  1.46it/s]Extractor Estimating: 210it [02:19,  1.42it/s]Extractor Estimating: 211it [02:20,  1.45it/s]Extractor Estimating: 212it [02:20,  1.44it/s]Extractor Estimating: 213it [02:21,  1.42it/s]Extractor Estimating: 214it [02:22,  1.42it/s]Extractor Estimating: 215it [02:23,  1.41it/s]Extractor Estimating: 216it [02:23,  1.36it/s]Extractor Estimating: 217it [02:24,  1.38it/s]Extractor Estimating: 218it [02:25,  1.45it/s]Extractor Estimating: 219it [02:25,  1.46it/s]Extractor Estimating: 220it [02:26,  1.43it/s]Extractor Estimating: 221it [02:27,  1.40it/s]Extractor Estimating: 222it [02:27,  1.41it/s]Extractor Estimating: 223it [02:28,  1.43it/s]Extractor Estimating: 224it [02:29,  1.48it/s]Extractor Estimating: 225it [02:29,  1.50it/s]Extractor Estimating: 226it [02:30,  1.51it/s]Extractor Estimating: 227it [02:31,  1.42it/s]Extractor Estimating: 228it [02:32,  1.42it/s]Extractor Estimating: 229it [02:32,  1.37it/s]Extractor Estimating: 230it [02:33,  1.42it/s]Extractor Estimating: 231it [02:34,  1.43it/s]Extractor Estimating: 232it [02:34,  1.44it/s]Extractor Estimating: 233it [02:35,  1.44it/s]Extractor Estimating: 234it [02:36,  1.47it/s]Extractor Estimating: 235it [02:36,  1.44it/s]Extractor Estimating: 236it [02:37,  1.44it/s]Extractor Estimating: 237it [02:38,  1.47it/s]Extractor Estimating: 238it [02:39,  1.46it/s]Extractor Estimating: 239it [02:39,  1.46it/s]Extractor Estimating: 240it [02:40,  1.44it/s]Extractor Estimating: 241it [02:41,  1.45it/s]Extractor Estimating: 242it [02:41,  1.44it/s]Extractor Estimating: 243it [02:42,  1.41it/s]Extractor Estimating: 244it [02:43,  1.33it/s]Extractor Estimating: 245it [02:44,  1.33it/s]Extractor Estimating: 246it [02:44,  1.37it/s]Extractor Estimating: 247it [02:45,  1.40it/s]Extractor Estimating: 248it [02:46,  1.43it/s]Extractor Estimating: 249it [02:46,  1.46it/s]Extractor Estimating: 250it [02:47,  1.33it/s]Extractor Estimating: 251it [02:48,  1.39it/s]Extractor Estimating: 252it [02:49,  1.36it/s]Extractor Estimating: 253it [02:49,  1.39it/s]Extractor Estimating: 254it [02:50,  1.41it/s]Extractor Estimating: 255it [02:51,  1.40it/s]Extractor Estimating: 256it [02:51,  1.38it/s]Extractor Estimating: 257it [02:52,  1.38it/s]Extractor Estimating: 258it [02:53,  1.41it/s]Extractor Estimating: 259it [02:53,  1.48it/s]Extractor Estimating: 260it [02:54,  1.49it/s]Extractor Estimating: 261it [02:55,  1.46it/s]Extractor Estimating: 262it [02:56,  1.46it/s]Extractor Estimating: 263it [02:56,  1.50it/s]Extractor Estimating: 264it [02:57,  1.45it/s]Extractor Estimating: 265it [02:58,  1.43it/s]Extractor Estimating: 266it [02:58,  1.47it/s]Extractor Estimating: 267it [02:59,  1.45it/s]Extractor Estimating: 268it [03:00,  1.44it/s]Extractor Estimating: 269it [03:00,  1.48it/s]Extractor Estimating: 270it [03:01,  1.49it/s]Extractor Estimating: 271it [03:02,  1.48it/s]Extractor Estimating: 272it [03:02,  1.50it/s]Extractor Estimating: 273it [03:03,  1.45it/s]Extractor Estimating: 274it [03:04,  1.46it/s]Extractor Estimating: 275it [03:04,  1.43it/s]Extractor Estimating: 276it [03:05,  1.42it/s]Extractor Estimating: 277it [03:06,  1.44it/s]Extractor Estimating: 278it [03:06,  1.49it/s]Extractor Estimating: 279it [03:07,  1.49it/s]Extractor Estimating: 280it [03:08,  1.50it/s]Extractor Estimating: 281it [03:08,  1.49it/s]Extractor Estimating: 282it [03:09,  1.53it/s]Extractor Estimating: 283it [03:10,  1.44it/s]Extractor Estimating: 284it [03:11,  1.45it/s]Extractor Estimating: 285it [03:11,  1.46it/s]Extractor Estimating: 286it [03:12,  1.51it/s]Extractor Estimating: 287it [03:12,  1.55it/s]Extractor Estimating: 288it [03:13,  1.54it/s]Extractor Estimating: 289it [03:14,  1.57it/s]Extractor Estimating: 290it [03:14,  1.56it/s]Extractor Estimating: 291it [03:15,  1.50it/s]Extractor Estimating: 292it [03:16,  1.48it/s]Extractor Estimating: 293it [03:16,  1.49it/s]Extractor Estimating: 294it [03:17,  1.50it/s]Extractor Estimating: 295it [03:18,  1.53it/s]Extractor Estimating: 296it [03:18,  1.55it/s]Extractor Estimating: 297it [03:19,  1.56it/s]Extractor Estimating: 298it [03:20,  1.54it/s]Extractor Estimating: 299it [03:20,  1.54it/s]Extractor Estimating: 300it [03:21,  1.53it/s]Extractor Estimating: 301it [03:22,  1.48it/s]Extractor Estimating: 302it [03:22,  1.52it/s]Extractor Estimating: 303it [03:23,  1.51it/s]Extractor Estimating: 304it [03:24,  1.47it/s]Extractor Estimating: 305it [03:24,  1.46it/s]Extractor Estimating: 306it [03:25,  1.40it/s]Extractor Estimating: 307it [03:26,  1.45it/s]Extractor Estimating: 308it [03:27,  1.44it/s]Extractor Estimating: 309it [03:27,  1.45it/s]Extractor Estimating: 310it [03:28,  1.43it/s]Extractor Estimating: 311it [03:29,  1.40it/s]Extractor Estimating: 312it [03:29,  1.41it/s]Extractor Estimating: 313it [03:30,  1.45it/s]Extractor Estimating: 314it [03:31,  1.46it/s]Extractor Estimating: 315it [03:31,  1.45it/s]Extractor Estimating: 316it [03:32,  1.42it/s]Extractor Estimating: 317it [03:33,  1.40it/s]Extractor Estimating: 318it [03:34,  1.41it/s]Extractor Estimating: 319it [03:34,  1.43it/s]Extractor Estimating: 320it [03:35,  1.45it/s]Extractor Estimating: 321it [03:36,  1.40it/s]Extractor Estimating: 322it [03:36,  1.45it/s]Extractor Estimating: 323it [03:37,  1.44it/s]Extractor Estimating: 324it [03:38,  1.44it/s]Extractor Estimating: 325it [03:39,  1.35it/s]Extractor Estimating: 326it [03:39,  1.48it/s]Extractor Estimating: 327it [03:40,  1.58it/s]Extractor Estimating: 328it [03:40,  1.68it/s]Extractor Estimating: 329it [03:41,  1.62it/s]Extractor Estimating: 330it [03:41,  1.67it/s]Extractor Estimating: 331it [03:42,  1.68it/s]Extractor Estimating: 332it [03:42,  1.72it/s]Extractor Estimating: 333it [03:43,  1.77it/s]Extractor Estimating: 334it [03:44,  1.80it/s]Extractor Estimating: 335it [03:44,  1.78it/s]Extractor Estimating: 336it [03:45,  1.79it/s]Extractor Estimating: 337it [03:45,  1.83it/s]Extractor Estimating: 338it [03:46,  1.76it/s]Extractor Estimating: 339it [03:46,  1.75it/s]Extractor Estimating: 340it [03:47,  1.82it/s]Extractor Estimating: 341it [03:47,  1.80it/s]Extractor Estimating: 342it [03:48,  1.88it/s]Extractor Estimating: 343it [03:48,  1.89it/s]Extractor Estimating: 344it [03:49,  1.92it/s]Extractor Estimating: 345it [03:49,  1.94it/s]Extractor Estimating: 346it [03:50,  1.89it/s]Extractor Estimating: 347it [03:51,  1.80it/s]Extractor Estimating: 348it [03:51,  1.65it/s]Extractor Estimating: 349it [03:52,  1.70it/s]Extractor Estimating: 350it [03:52,  1.76it/s]Extractor Estimating: 351it [03:53,  1.76it/s]Extractor Estimating: 352it [03:54,  1.71it/s]Extractor Estimating: 353it [03:54,  1.68it/s]Extractor Estimating: 354it [03:55,  1.65it/s]Extractor Estimating: 355it [03:56,  1.60it/s]Extractor Estimating: 356it [03:56,  1.59it/s]Extractor Estimating: 357it [03:57,  1.55it/s]Extractor Estimating: 358it [03:57,  1.58it/s]Extractor Estimating: 359it [03:58,  1.54it/s]Extractor Estimating: 360it [03:59,  1.55it/s]Extractor Estimating: 361it [03:59,  1.56it/s]Extractor Estimating: 362it [04:00,  1.53it/s]Extractor Estimating: 363it [04:01,  1.59it/s]Extractor Estimating: 364it [04:01,  1.63it/s]Extractor Estimating: 365it [04:02,  1.62it/s]Extractor Estimating: 366it [04:03,  1.56it/s]Extractor Estimating: 367it [04:03,  1.60it/s]Extractor Estimating: 368it [04:04,  1.61it/s]Extractor Estimating: 369it [04:04,  1.66it/s]Extractor Estimating: 370it [04:05,  1.59it/s]Extractor Estimating: 371it [04:06,  1.56it/s]Extractor Estimating: 372it [04:06,  1.53it/s]Extractor Estimating: 373it [04:07,  1.60it/s]Extractor Estimating: 374it [04:08,  1.63it/s]Extractor Estimating: 375it [04:08,  1.65it/s]Extractor Estimating: 376it [04:09,  1.66it/s]Extractor Estimating: 377it [04:09,  1.69it/s]Extractor Estimating: 378it [04:10,  1.65it/s]Extractor Estimating: 379it [04:11,  1.65it/s]Extractor Estimating: 380it [04:11,  1.58it/s]Extractor Estimating: 381it [04:12,  1.63it/s]Extractor Estimating: 382it [04:12,  1.66it/s]Extractor Estimating: 383it [04:13,  1.71it/s]Extractor Estimating: 384it [04:13,  1.71it/s]Extractor Estimating: 385it [04:14,  1.71it/s]Extractor Estimating: 386it [04:15,  1.74it/s]Extractor Estimating: 387it [04:15,  1.72it/s]Extractor Estimating: 388it [04:16,  1.69it/s]Extractor Estimating: 389it [04:16,  1.66it/s]Extractor Estimating: 390it [04:17,  1.65it/s]Extractor Estimating: 391it [04:18,  1.71it/s]Extractor Estimating: 392it [04:18,  1.61it/s]Extractor Estimating: 393it [04:19,  1.61it/s]Extractor Estimating: 394it [04:20,  1.65it/s]Extractor Estimating: 395it [04:20,  1.65it/s]Extractor Estimating: 396it [04:21,  1.64it/s]Extractor Estimating: 397it [04:21,  1.65it/s]Extractor Estimating: 398it [04:22,  1.70it/s]Extractor Estimating: 399it [04:22,  1.68it/s]Extractor Estimating: 400it [04:23,  1.70it/s]Extractor Estimating: 401it [04:24,  1.67it/s]Extractor Estimating: 402it [04:24,  1.66it/s]Extractor Estimating: 403it [04:25,  1.68it/s]Extractor Estimating: 404it [04:25,  1.69it/s]Extractor Estimating: 405it [04:26,  1.68it/s]Extractor Estimating: 406it [04:27,  1.59it/s]Extractor Estimating: 407it [04:27,  1.58it/s]Extractor Estimating: 408it [04:28,  1.62it/s]Extractor Estimating: 409it [04:29,  1.66it/s]Extractor Estimating: 410it [04:29,  1.63it/s]Extractor Estimating: 411it [04:30,  1.67it/s]Extractor Estimating: 412it [04:30,  1.64it/s]Extractor Estimating: 413it [04:31,  1.64it/s]Extractor Estimating: 414it [04:32,  1.56it/s]Extractor Estimating: 415it [04:32,  1.57it/s]Extractor Estimating: 416it [04:33,  1.38it/s]Extractor Estimating: 417it [04:34,  1.45it/s]Extractor Estimating: 418it [04:34,  1.53it/s]Extractor Estimating: 419it [04:35,  1.55it/s]Extractor Estimating: 420it [04:36,  1.58it/s]Extractor Estimating: 421it [04:36,  1.63it/s]Extractor Estimating: 422it [04:37,  1.62it/s]Extractor Estimating: 423it [04:37,  1.65it/s]Extractor Estimating: 424it [04:38,  1.59it/s]Extractor Estimating: 425it [04:39,  1.61it/s]Extractor Estimating: 426it [04:39,  1.67it/s]Extractor Estimating: 427it [04:40,  1.61it/s]Extractor Estimating: 428it [04:41,  1.62it/s]Extractor Estimating: 429it [04:41,  1.67it/s]Extractor Estimating: 430it [04:42,  1.68it/s]Extractor Estimating: 431it [04:42,  1.65it/s]Extractor Estimating: 432it [04:43,  1.64it/s]Extractor Estimating: 433it [04:44,  1.64it/s]Extractor Estimating: 434it [04:44,  1.70it/s]Extractor Estimating: 435it [04:45,  1.66it/s]Extractor Estimating: 436it [04:45,  1.60it/s]Extractor Estimating: 437it [04:46,  1.68it/s]Extractor Estimating: 438it [04:47,  1.67it/s]Extractor Estimating: 439it [04:47,  1.61it/s]Extractor Estimating: 440it [04:48,  1.65it/s]Extractor Estimating: 441it [04:48,  1.63it/s]Extractor Estimating: 442it [04:49,  1.64it/s]Extractor Estimating: 443it [04:50,  1.65it/s]Extractor Estimating: 444it [04:50,  1.67it/s]Extractor Estimating: 445it [04:51,  1.73it/s]Extractor Estimating: 446it [04:51,  1.74it/s]Extractor Estimating: 447it [04:52,  1.69it/s]Extractor Estimating: 448it [04:53,  1.68it/s]Extractor Estimating: 449it [04:53,  1.65it/s]Extractor Estimating: 450it [04:54,  1.61it/s]Extractor Estimating: 451it [04:55,  1.45it/s]Extractor Estimating: 452it [04:55,  1.44it/s]Extractor Estimating: 453it [04:56,  1.46it/s]Extractor Estimating: 454it [04:57,  1.47it/s]Extractor Estimating: 455it [04:57,  1.45it/s]Extractor Estimating: 456it [04:58,  1.43it/s]Extractor Estimating: 457it [04:59,  1.44it/s]Extractor Estimating: 458it [05:00,  1.45it/s]Extractor Estimating: 459it [05:00,  1.46it/s]Extractor Estimating: 460it [05:01,  1.52it/s]Extractor Estimating: 461it [05:01,  1.54it/s]Extractor Estimating: 462it [05:02,  1.43it/s]Extractor Estimating: 463it [05:03,  1.38it/s]Extractor Estimating: 464it [05:04,  1.42it/s]Extractor Estimating: 465it [05:04,  1.39it/s]Extractor Estimating: 466it [05:05,  1.42it/s]Extractor Estimating: 467it [05:06,  1.45it/s]Extractor Estimating: 468it [05:06,  1.47it/s]Extractor Estimating: 469it [05:07,  1.49it/s]Extractor Estimating: 470it [05:08,  1.50it/s]Extractor Estimating: 471it [05:08,  1.48it/s]Extractor Estimating: 472it [05:09,  1.49it/s]Extractor Estimating: 473it [05:10,  1.49it/s]Extractor Estimating: 474it [05:10,  1.52it/s]Extractor Estimating: 475it [05:11,  1.48it/s]Extractor Estimating: 476it [05:12,  1.52it/s]Extractor Estimating: 477it [05:12,  1.48it/s]Extractor Estimating: 478it [05:13,  1.47it/s]Extractor Estimating: 479it [05:14,  1.51it/s]Extractor Estimating: 480it [05:14,  1.46it/s]Extractor Estimating: 481it [05:15,  1.56it/s]Extractor Estimating: 482it [05:16,  1.55it/s]Extractor Estimating: 483it [05:16,  1.56it/s]Extractor Estimating: 484it [05:17,  1.54it/s]Extractor Estimating: 485it [05:18,  1.50it/s]Extractor Estimating: 486it [05:18,  1.50it/s]Extractor Estimating: 487it [05:19,  1.46it/s]Extractor Estimating: 488it [05:20,  1.48it/s]Extractor Estimating: 489it [05:20,  1.50it/s]Extractor Estimating: 490it [05:21,  1.51it/s]Extractor Estimating: 491it [05:22,  1.56it/s]Extractor Estimating: 492it [05:22,  1.55it/s]Extractor Estimating: 493it [05:23,  1.58it/s]Extractor Estimating: 494it [05:24,  1.57it/s]Extractor Estimating: 495it [05:24,  1.51it/s]Extractor Estimating: 496it [05:25,  1.49it/s]Extractor Estimating: 497it [05:26,  1.53it/s]Extractor Estimating: 498it [05:26,  1.40it/s]Extractor Estimating: 499it [05:27,  1.44it/s]Extractor Estimating: 500it [05:27,  1.67it/s]Extractor Estimating: 500it [05:27,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:35,290 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:35,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:35,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:35,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:35,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:06:36,048 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:06:36,049 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:06:36,740 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:06:37,820 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:06:37,820 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:40,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:40,059 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:40,059 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:40,059 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:40,059 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:06:40,801 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:06:40,822 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:06:41,520 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:06:41,690 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:06:41,690 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 04:02:28,866 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 04:02:28,889 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9980 mean pseudo reward: 0.9236480405935406
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 25836
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25936, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25936, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.049, loss:788.3827
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.026, loss:758.9734
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.047, loss:774.6766
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.027, loss:747.7349
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.048, loss:690.2219
>> valid entity prec:0.5747, rec:0.5976, f1:0.5859
>> valid relation prec:0.2126, rec:0.0880, f1:0.1245
>> valid relation with NER prec:0.2126, rec:0.0880, f1:0.1245
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.274, loss:737.3413
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.028, loss:735.2239
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.046, loss:756.8427
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.039, loss:743.9966
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.040, loss:739.5924
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5575, rec:0.5751, f1:0.5662
>> valid relation prec:0.1232, rec:0.0495, f1:0.0706
>> valid relation with NER prec:0.1232, rec:0.0495, f1:0.0706
g_step 1100, step 268, avg_time 2.278, loss:750.7230
g_step 1200, step 368, avg_time 1.048, loss:746.6267
g_step 1300, step 52, avg_time 1.045, loss:734.8372
g_step 1400, step 152, avg_time 1.044, loss:710.3740
g_step 1500, step 252, avg_time 1.039, loss:716.0098
>> valid entity prec:0.5437, rec:0.5601, f1:0.5517
>> valid relation prec:0.1623, rec:0.0935, f1:0.1186
>> valid relation with NER prec:0.1623, rec:0.0935, f1:0.1186
g_step 1600, step 352, avg_time 2.282, loss:717.7531
g_step 1700, step 36, avg_time 1.040, loss:696.4196
g_step 1800, step 136, avg_time 1.046, loss:667.4514
g_step 1900, step 236, avg_time 1.031, loss:680.5094
g_step 2000, step 336, avg_time 1.042, loss:684.9507
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5575, rec:0.5437, f1:0.5505
>> valid relation prec:0.1381, rec:0.0667, f1:0.0900
>> valid relation with NER prec:0.1381, rec:0.0667, f1:0.0900
g_step 2100, step 20, avg_time 2.291, loss:675.9834
g_step 2200, step 120, avg_time 1.042, loss:641.7275
g_step 2300, step 220, avg_time 1.027, loss:644.6768
g_step 2400, step 320, avg_time 1.040, loss:635.3501
g_step 2500, step 4, avg_time 1.045, loss:643.4537
>> valid entity prec:0.5645, rec:0.5617, f1:0.5631
>> valid relation prec:0.1209, rec:0.0546, f1:0.0753
>> valid relation with NER prec:0.1209, rec:0.0546, f1:0.0753
g_step 2600, step 104, avg_time 2.248, loss:585.5133
g_step 2700, step 204, avg_time 1.068, loss:618.8213
g_step 2800, step 304, avg_time 1.031, loss:624.0092
g_step 2900, step 404, avg_time 1.025, loss:629.5672
g_step 3000, step 88, avg_time 1.043, loss:582.1252
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5630, rec:0.5872, f1:0.5748
>> valid relation prec:0.1462, rec:0.0820, f1:0.1050
>> valid relation with NER prec:0.1462, rec:0.0820, f1:0.1050
g_step 3100, step 188, avg_time 2.281, loss:571.9668
g_step 3200, step 288, avg_time 1.044, loss:585.2741
g_step 3300, step 388, avg_time 1.021, loss:613.8393
g_step 3400, step 72, avg_time 1.023, loss:573.4107
g_step 3500, step 172, avg_time 1.026, loss:547.0562
>> valid entity prec:0.5862, rec:0.5292, f1:0.5562
>> valid relation prec:0.1222, rec:0.0592, f1:0.0798
>> valid relation with NER prec:0.1222, rec:0.0592, f1:0.0798
g_step 3600, step 272, avg_time 2.268, loss:556.5601
g_step 3700, step 372, avg_time 1.037, loss:579.0536
g_step 3800, step 56, avg_time 1.048, loss:529.1583
g_step 3900, step 156, avg_time 1.022, loss:524.3050
g_step 4000, step 256, avg_time 1.049, loss:546.7691
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5294, rec:0.5302, f1:0.5298
>> valid relation prec:0.0870, rec:0.0529, f1:0.0658
>> valid relation with NER prec:0.0870, rec:0.0529, f1:0.0658
g_step 4100, step 356, avg_time 2.269, loss:555.9693
g_step 4200, step 40, avg_time 1.043, loss:542.7366
g_step 4300, step 140, avg_time 1.030, loss:492.2193
g_step 4400, step 240, avg_time 1.025, loss:525.2284
g_step 4500, step 340, avg_time 1.039, loss:552.4976
>> valid entity prec:0.5430, rec:0.5549, f1:0.5489
>> valid relation prec:0.1351, rec:0.0756, f1:0.0970
>> valid relation with NER prec:0.1351, rec:0.0756, f1:0.0970
g_step 4600, step 24, avg_time 2.263, loss:514.8557
g_step 4700, step 124, avg_time 1.033, loss:493.7666
g_step 4800, step 224, avg_time 1.018, loss:497.4681
g_step 4900, step 324, avg_time 1.031, loss:510.1387
g_step 5000, step 8, avg_time 1.056, loss:508.6392
learning rate was adjusted to 0.0008
>> valid entity prec:0.5595, rec:0.5484, f1:0.5539
>> valid relation prec:0.1072, rec:0.0587, f1:0.0758
>> valid relation with NER prec:0.1072, rec:0.0587, f1:0.0758
g_step 5100, step 108, avg_time 2.260, loss:483.9544
g_step 5200, step 208, avg_time 1.038, loss:458.5547
g_step 5300, step 308, avg_time 1.036, loss:471.0321
g_step 5400, step 408, avg_time 1.024, loss:518.0497
g_step 5500, step 92, avg_time 1.034, loss:450.2119
>> valid entity prec:0.5379, rec:0.5046, f1:0.5207
>> valid relation prec:0.0886, rec:0.0486, f1:0.0628
>> valid relation with NER prec:0.0886, rec:0.0486, f1:0.0628
g_step 5600, step 192, avg_time 2.257, loss:464.9008
g_step 5700, step 292, avg_time 1.043, loss:474.0144
g_step 5800, step 392, avg_time 1.039, loss:491.3107
g_step 5900, step 76, avg_time 1.032, loss:445.4833
g_step 6000, step 176, avg_time 1.025, loss:429.8051
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5481, rec:0.5725, f1:0.5600
>> valid relation prec:0.1423, rec:0.0880, f1:0.1088
>> valid relation with NER prec:0.1423, rec:0.0880, f1:0.1088
g_step 6100, step 276, avg_time 2.257, loss:466.7045
g_step 6200, step 376, avg_time 1.043, loss:458.5672
g_step 6300, step 60, avg_time 1.036, loss:428.2911
g_step 6400, step 160, avg_time 1.028, loss:434.6554
g_step 6500, step 260, avg_time 1.022, loss:443.0016
>> valid entity prec:0.5612, rec:0.4861, f1:0.5210
>> valid relation prec:0.1137, rec:0.0506, f1:0.0700
>> valid relation with NER prec:0.1137, rec:0.0506, f1:0.0700
g_step 6600, step 360, avg_time 2.272, loss:442.2997
g_step 6700, step 44, avg_time 1.047, loss:435.9662
g_step 6800, step 144, avg_time 1.039, loss:400.7787
g_step 6900, step 244, avg_time 1.041, loss:398.0791
g_step 7000, step 344, avg_time 1.013, loss:422.5900
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5596, rec:0.5134, f1:0.5355
>> valid relation prec:0.1276, rec:0.0664, f1:0.0874
>> valid relation with NER prec:0.1276, rec:0.0664, f1:0.0874
g_step 7100, step 28, avg_time 2.188, loss:413.1260
g_step 7200, step 128, avg_time 0.998, loss:390.9230
g_step 7300, step 228, avg_time 1.015, loss:396.8249
g_step 7400, step 328, avg_time 0.996, loss:401.6564
g_step 7500, step 12, avg_time 1.001, loss:425.1912
>> valid entity prec:0.5568, rec:0.5133, f1:0.5341
>> valid relation prec:0.1325, rec:0.0696, f1:0.0913
>> valid relation with NER prec:0.1325, rec:0.0696, f1:0.0913
g_step 7600, step 112, avg_time 2.204, loss:373.7381
g_step 7700, step 212, avg_time 1.001, loss:386.9813
g_step 7800, step 312, avg_time 1.008, loss:398.3832
g_step 7900, step 412, avg_time 1.008, loss:408.0330
g_step 8000, step 96, avg_time 0.996, loss:370.4950
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5388, rec:0.5321, f1:0.5354
>> valid relation prec:0.1077, rec:0.0650, f1:0.0811
>> valid relation with NER prec:0.1077, rec:0.0650, f1:0.0811
g_step 8100, step 196, avg_time 2.194, loss:369.6792
g_step 8200, step 296, avg_time 0.993, loss:380.8143
g_step 8300, step 396, avg_time 1.016, loss:390.9737
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 04:02:28 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 04:02:28 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_04-02-28_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 04:02:30 - WARNING - datasets.builder -   Using custom data configuration default-937adbc71d5c01ac
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-937adbc71d5c01ac/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 04:02:31,818 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:02:31,819 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:02:31,820 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:02:31,821 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:02:31,898 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:31,939 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:31,940 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:31,940 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:31,940 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:31,940 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:31,940 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 04:02:32,227 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:02:35,391 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 04:02:35,422 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-937adbc71d5c01ac/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:04,  2.47ba/s] 18%|        | 2/11 [00:00<00:02,  3.47ba/s] 27%|       | 3/11 [00:00<00:02,  3.94ba/s] 36%|      | 4/11 [00:01<00:01,  4.20ba/s] 45%|     | 5/11 [00:01<00:01,  4.37ba/s] 55%|    | 6/11 [00:01<00:01,  4.48ba/s] 64%|   | 7/11 [00:01<00:00,  4.54ba/s] 73%|  | 8/11 [00:01<00:00,  4.58ba/s] 82%| | 9/11 [00:02<00:00,  4.63ba/s] 91%| | 10/11 [00:02<00:00,  3.87ba/s]100%|| 11/11 [00:02<00:00,  4.46ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.14ba/s] 50%|     | 2/4 [00:00<00:00,  3.63ba/s] 75%|  | 3/4 [00:00<00:00,  4.01ba/s]100%|| 4/4 [00:00<00:00,  5.15ba/s]100%|| 4/4 [00:00<00:00,  4.49ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  6.49ba/s] 27%|       | 3/11 [00:00<00:00,  9.47ba/s] 45%|     | 5/11 [00:00<00:00, 10.47ba/s] 64%|   | 7/11 [00:00<00:00, 10.87ba/s] 82%| | 9/11 [00:00<00:00, 11.05ba/s]100%|| 11/11 [00:00<00:00, 11.65ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  6.06ba/s] 75%|  | 3/4 [00:00<00:00,  9.11ba/s]100%|| 4/4 [00:00<00:00, 10.31ba/s]
[INFO|trainer.py:414] 2023-08-29 04:02:41,739 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 04:02:41,831 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 04:02:41,831 >>   Num examples = 10009
[INFO|trainer.py:1149] 2023-08-29 04:02:41,831 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 04:02:41,831 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 04:02:41,831 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 04:02:41,831 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 04:02:41,831 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:54,  3.32it/s]  0%|          | 2/780 [00:00<03:49,  3.39it/s]  0%|          | 3/780 [00:00<03:47,  3.42it/s]  1%|          | 4/780 [00:01<03:46,  3.43it/s]  1%|          | 5/780 [00:01<03:46,  3.42it/s]  1%|          | 6/780 [00:01<03:46,  3.41it/s]  1%|          | 7/780 [00:02<03:46,  3.41it/s]  1%|          | 8/780 [00:02<03:46,  3.41it/s]  1%|          | 9/780 [00:02<03:46,  3.40it/s]  1%|         | 10/780 [00:02<03:46,  3.41it/s]  1%|         | 11/780 [00:03<03:51,  3.32it/s]  2%|         | 12/780 [00:03<03:49,  3.34it/s]  2%|         | 13/780 [00:03<03:48,  3.36it/s]  2%|         | 14/780 [00:04<03:47,  3.37it/s]  2%|         | 15/780 [00:04<03:46,  3.37it/s]  2%|         | 16/780 [00:04<03:45,  3.38it/s]  2%|         | 17/780 [00:05<03:45,  3.39it/s]  2%|         | 18/780 [00:05<03:44,  3.39it/s]  2%|         | 19/780 [00:05<03:43,  3.41it/s]  3%|         | 20/780 [00:05<03:42,  3.42it/s]  3%|         | 21/780 [00:06<03:41,  3.43it/s]  3%|         | 22/780 [00:06<03:46,  3.35it/s]  3%|         | 23/780 [00:06<03:44,  3.38it/s]  3%|         | 24/780 [00:07<03:42,  3.40it/s]  3%|         | 25/780 [00:07<03:41,  3.41it/s]  3%|         | 26/780 [00:07<03:40,  3.43it/s]  3%|         | 27/780 [00:07<03:39,  3.43it/s]  4%|         | 28/780 [00:08<03:38,  3.44it/s]  4%|         | 29/780 [00:08<03:38,  3.44it/s]  4%|         | 30/780 [00:08<03:37,  3.45it/s]  4%|         | 31/780 [00:09<03:37,  3.45it/s]  4%|         | 32/780 [00:09<03:36,  3.45it/s]  4%|         | 33/780 [00:09<03:41,  3.37it/s]  4%|         | 34/780 [00:09<03:39,  3.40it/s]  4%|         | 35/780 [00:10<03:38,  3.41it/s]  5%|         | 36/780 [00:10<03:37,  3.42it/s]  5%|         | 37/780 [00:10<03:36,  3.43it/s]  5%|         | 38/780 [00:11<03:36,  3.43it/s]  5%|         | 39/780 [00:11<03:35,  3.44it/s]  5%|         | 40/780 [00:11<03:35,  3.44it/s]  5%|         | 41/780 [00:12<03:34,  3.44it/s]  5%|         | 42/780 [00:12<03:34,  3.44it/s]  6%|         | 43/780 [00:12<03:33,  3.45it/s]  6%|         | 44/780 [00:12<03:36,  3.40it/s]  6%|         | 45/780 [00:13<03:35,  3.42it/s]  6%|         | 46/780 [00:13<03:34,  3.42it/s]  6%|         | 47/780 [00:13<03:33,  3.43it/s]  6%|         | 48/780 [00:14<03:33,  3.43it/s]  6%|         | 49/780 [00:14<03:32,  3.43it/s]  6%|         | 50/780 [00:14<03:32,  3.43it/s]  7%|         | 51/780 [00:14<03:32,  3.43it/s]  7%|         | 52/780 [00:15<03:31,  3.44it/s]  7%|         | 53/780 [00:15<03:31,  3.44it/s]  7%|         | 54/780 [00:15<03:30,  3.45it/s]  7%|         | 55/780 [00:16<03:35,  3.36it/s]  7%|         | 56/780 [00:16<03:33,  3.39it/s]  7%|         | 57/780 [00:16<03:32,  3.41it/s]  7%|         | 58/780 [00:16<03:31,  3.42it/s]  8%|         | 59/780 [00:17<03:30,  3.42it/s]  8%|         | 60/780 [00:17<03:29,  3.43it/s]  8%|         | 61/780 [00:17<03:29,  3.44it/s]  8%|         | 62/780 [00:18<03:28,  3.44it/s]  8%|         | 63/780 [00:18<03:28,  3.44it/s]  8%|         | 64/780 [00:18<03:27,  3.44it/s]  8%|         | 65/780 [00:19<03:27,  3.44it/s]  8%|         | 66/780 [00:19<03:32,  3.36it/s]  9%|         | 67/780 [00:19<03:30,  3.39it/s]  9%|         | 68/780 [00:19<03:29,  3.40it/s]  9%|         | 69/780 [00:20<03:28,  3.42it/s]  9%|         | 70/780 [00:20<03:27,  3.43it/s]  9%|         | 71/780 [00:20<03:26,  3.43it/s]  9%|         | 72/780 [00:21<03:26,  3.43it/s]  9%|         | 73/780 [00:21<03:25,  3.44it/s]  9%|         | 74/780 [00:21<03:25,  3.44it/s] 10%|         | 75/780 [00:21<03:24,  3.44it/s] 10%|         | 76/780 [00:22<03:24,  3.44it/s] 10%|         | 77/780 [00:22<03:24,  3.44it/s] 10%|         | 78/780 [00:22<03:24,  3.44it/s] 10%|         | 79/780 [00:23<03:23,  3.44it/s] 10%|         | 80/780 [00:23<03:23,  3.44it/s] 10%|         | 81/780 [00:23<03:23,  3.44it/s] 11%|         | 82/780 [00:23<03:23,  3.44it/s] 11%|         | 83/780 [00:24<03:22,  3.44it/s] 11%|         | 84/780 [00:24<03:22,  3.44it/s] 11%|         | 85/780 [00:24<03:22,  3.44it/s] 11%|         | 86/780 [00:25<03:21,  3.44it/s] 11%|         | 87/780 [00:25<03:26,  3.36it/s] 11%|        | 88/780 [00:25<03:24,  3.39it/s] 11%|        | 89/780 [00:26<03:23,  3.40it/s] 12%|        | 90/780 [00:26<03:21,  3.42it/s] 12%|        | 91/780 [00:26<03:21,  3.42it/s] 12%|        | 92/780 [00:26<03:20,  3.43it/s] 12%|        | 93/780 [00:27<03:20,  3.43it/s] 12%|        | 94/780 [00:27<03:20,  3.43it/s] 12%|        | 95/780 [00:27<03:19,  3.44it/s] 12%|        | 96/780 [00:28<03:19,  3.44it/s] 12%|        | 97/780 [00:28<03:18,  3.44it/s] 13%|        | 98/780 [00:28<03:26,  3.31it/s] 13%|        | 99/780 [00:28<03:23,  3.35it/s] 13%|        | 100/780 [00:29<03:21,  3.37it/s] 13%|        | 101/780 [00:29<03:19,  3.40it/s] 13%|        | 102/780 [00:29<03:18,  3.41it/s] 13%|        | 103/780 [00:30<03:17,  3.42it/s] 13%|        | 104/780 [00:30<03:17,  3.43it/s] 13%|        | 105/780 [00:30<03:16,  3.43it/s] 14%|        | 106/780 [00:31<03:16,  3.43it/s] 14%|        | 107/780 [00:31<03:15,  3.43it/s] 14%|        | 108/780 [00:31<03:15,  3.43it/s] 14%|        | 109/780 [00:31<03:25,  3.26it/s] 14%|        | 110/780 [00:32<03:22,  3.31it/s] 14%|        | 111/780 [00:32<03:19,  3.35it/s] 14%|        | 112/780 [00:32<03:17,  3.38it/s] 14%|        | 113/780 [00:33<03:16,  3.40it/s] 15%|        | 114/780 [00:33<03:15,  3.41it/s] 15%|        | 115/780 [00:33<03:14,  3.42it/s] 15%|        | 116/780 [00:33<03:14,  3.42it/s] 15%|        | 117/780 [00:34<03:13,  3.43it/s] 15%|        | 118/780 [00:34<03:12,  3.43it/s] 15%|        | 119/780 [00:34<03:12,  3.44it/s] 15%|        | 120/780 [00:35<03:20,  3.30it/s] 16%|        | 121/780 [00:35<03:17,  3.34it/s] 16%|        | 122/780 [00:35<03:15,  3.37it/s] 16%|        | 123/780 [00:36<03:13,  3.39it/s] 16%|        | 124/780 [00:36<03:12,  3.40it/s] 16%|        | 125/780 [00:36<03:11,  3.41it/s] 16%|        | 126/780 [00:36<03:10,  3.42it/s] 16%|        | 127/780 [00:37<03:10,  3.43it/s] 16%|        | 128/780 [00:37<03:09,  3.43it/s] 17%|        | 129/780 [00:37<03:09,  3.44it/s] 17%|        | 130/780 [00:38<03:09,  3.44it/s] 17%|        | 131/780 [00:38<03:11,  3.39it/s] 17%|        | 132/780 [00:38<03:10,  3.41it/s] 17%|        | 133/780 [00:38<03:09,  3.42it/s] 17%|        | 134/780 [00:39<03:08,  3.42it/s] 17%|        | 135/780 [00:39<03:08,  3.43it/s] 17%|        | 136/780 [00:39<03:07,  3.44it/s] 18%|        | 137/780 [00:40<03:06,  3.44it/s] 18%|        | 138/780 [00:40<03:06,  3.44it/s] 18%|        | 139/780 [00:40<03:06,  3.44it/s] 18%|        | 140/780 [00:41<03:06,  3.44it/s] 18%|        | 141/780 [00:41<03:05,  3.44it/s] 18%|        | 142/780 [00:41<03:10,  3.35it/s] 18%|        | 143/780 [00:41<03:08,  3.38it/s] 18%|        | 144/780 [00:42<03:07,  3.40it/s] 19%|        | 145/780 [00:42<03:06,  3.41it/s] 19%|        | 146/780 [00:42<03:05,  3.42it/s] 19%|        | 147/780 [00:43<03:04,  3.42it/s] 19%|        | 148/780 [00:43<03:08,  3.35it/s] 19%|        | 149/780 [00:43<03:07,  3.37it/s] 19%|        | 150/780 [00:43<03:06,  3.39it/s] 19%|        | 151/780 [00:44<03:17,  3.18it/s] 19%|        | 152/780 [00:44<03:45,  2.78it/s] 20%|        | 153/780 [00:45<03:32,  2.95it/s] 20%|        | 154/780 [00:45<03:23,  3.08it/s] 20%|        | 155/780 [00:45<03:16,  3.19it/s] 20%|        | 156/780 [00:45<03:11,  3.26it/s][INFO|trainer.py:2140] 2023-08-29 04:03:27,905 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:03:27,905 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:03:27,905 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.93it/s][A
  3%|         | 12/435 [00:00<00:08, 48.89it/s][A
  4%|         | 17/435 [00:00<00:08, 47.20it/s][A
  5%|         | 22/435 [00:00<00:08, 46.20it/s][A
  6%|         | 27/435 [00:00<00:08, 45.49it/s][A
  7%|         | 32/435 [00:00<00:08, 45.17it/s][A
  9%|         | 37/435 [00:00<00:08, 44.86it/s][A
 10%|         | 42/435 [00:00<00:08, 44.72it/s][A
 11%|         | 47/435 [00:01<00:08, 44.76it/s][A
 12%|        | 52/435 [00:01<00:08, 44.82it/s][A
 13%|        | 57/435 [00:01<00:08, 44.91it/s][A
 14%|        | 62/435 [00:01<00:08, 44.41it/s][A
 15%|        | 67/435 [00:01<00:08, 44.57it/s][A
 17%|        | 72/435 [00:01<00:08, 44.52it/s][A
 18%|        | 77/435 [00:01<00:08, 44.48it/s][A
 19%|        | 82/435 [00:01<00:07, 44.36it/s][A
 20%|        | 87/435 [00:01<00:07, 44.42it/s][A
 21%|        | 92/435 [00:02<00:07, 44.60it/s][A
 22%|       | 97/435 [00:02<00:07, 44.67it/s][A
 23%|       | 102/435 [00:02<00:07, 44.79it/s][A
 25%|       | 107/435 [00:02<00:07, 44.71it/s][A
 26%|       | 112/435 [00:02<00:07, 44.90it/s][A
 27%|       | 117/435 [00:02<00:07, 44.79it/s][A
 28%|       | 122/435 [00:02<00:07, 44.67it/s][A
 29%|       | 127/435 [00:02<00:06, 44.68it/s][A
 30%|       | 132/435 [00:02<00:06, 44.67it/s][A
 31%|      | 137/435 [00:03<00:06, 44.23it/s][A
 33%|      | 142/435 [00:03<00:06, 44.75it/s][A
 34%|      | 147/435 [00:03<00:06, 44.79it/s][A
 35%|      | 152/435 [00:03<00:06, 44.88it/s][A
 36%|      | 157/435 [00:03<00:06, 44.89it/s][A
 37%|      | 162/435 [00:03<00:06, 44.87it/s][A
 38%|      | 167/435 [00:03<00:05, 44.67it/s][A
 40%|      | 172/435 [00:03<00:05, 44.66it/s][A
 41%|      | 177/435 [00:03<00:05, 44.67it/s][A
 42%|     | 182/435 [00:04<00:05, 44.62it/s][A
 43%|     | 187/435 [00:04<00:05, 44.60it/s][A
 44%|     | 192/435 [00:04<00:05, 44.76it/s][A
 45%|     | 197/435 [00:04<00:05, 44.66it/s][A
 46%|     | 202/435 [00:04<00:05, 44.78it/s][A
 48%|     | 207/435 [00:04<00:05, 44.65it/s][A
 49%|     | 212/435 [00:04<00:04, 44.78it/s][A
 50%|     | 217/435 [00:04<00:04, 44.78it/s][A
 51%|     | 222/435 [00:04<00:04, 44.75it/s][A
 52%|    | 227/435 [00:05<00:04, 44.65it/s][A
 53%|    | 232/435 [00:05<00:04, 44.63it/s][A
 54%|    | 237/435 [00:05<00:04, 44.77it/s][A
 56%|    | 242/435 [00:05<00:04, 44.74it/s][A
 57%|    | 247/435 [00:05<00:04, 44.77it/s][A
 58%|    | 252/435 [00:05<00:04, 44.69it/s][A
 59%|    | 257/435 [00:05<00:03, 44.64it/s][A
 60%|    | 262/435 [00:05<00:03, 44.75it/s][A
 61%|   | 267/435 [00:05<00:03, 44.74it/s][A
 63%|   | 272/435 [00:06<00:03, 44.72it/s][A
 64%|   | 277/435 [00:06<00:03, 44.75it/s][A
 65%|   | 282/435 [00:06<00:03, 44.79it/s][A
 66%|   | 287/435 [00:06<00:03, 44.71it/s][A
 67%|   | 292/435 [00:06<00:03, 44.59it/s][A
 68%|   | 297/435 [00:06<00:03, 44.70it/s][A
 69%|   | 302/435 [00:06<00:02, 44.70it/s][A
 71%|   | 307/435 [00:06<00:02, 44.74it/s][A
 72%|  | 312/435 [00:06<00:02, 44.71it/s][A
 73%|  | 317/435 [00:07<00:02, 44.63it/s][A
 74%|  | 322/435 [00:07<00:02, 44.73it/s][A
 75%|  | 327/435 [00:07<00:02, 44.80it/s][A
 76%|  | 332/435 [00:07<00:02, 44.80it/s][A
 77%|  | 337/435 [00:07<00:02, 44.70it/s][A
 79%|  | 342/435 [00:07<00:02, 44.60it/s][A
 80%|  | 347/435 [00:07<00:01, 44.67it/s][A
 81%|  | 352/435 [00:07<00:01, 44.65it/s][A
 82%| | 357/435 [00:07<00:01, 44.63it/s][A
 83%| | 362/435 [00:08<00:01, 44.54it/s][A
 84%| | 367/435 [00:08<00:01, 44.60it/s][A
 86%| | 372/435 [00:08<00:01, 44.69it/s][A
 87%| | 377/435 [00:08<00:01, 44.72it/s][A
 88%| | 382/435 [00:08<00:01, 44.55it/s][A
 89%| | 387/435 [00:08<00:01, 44.75it/s][A
 90%| | 392/435 [00:08<00:00, 44.71it/s][A
 91%|| 397/435 [00:08<00:00, 44.65it/s][A
 92%|| 402/435 [00:08<00:00, 44.65it/s][A
 94%|| 407/435 [00:09<00:00, 44.56it/s][A
 95%|| 412/435 [00:09<00:00, 44.59it/s][A
 96%|| 417/435 [00:09<00:00, 44.64it/s][A
 97%|| 422/435 [00:09<00:00, 44.70it/s][A
 98%|| 427/435 [00:09<00:00, 44.64it/s][A
 99%|| 432/435 [00:09<00:00, 44.75it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.75it/s][A 20%|        | 156/780 [00:55<03:11,  3.26it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:03:37,930 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 04:03:38,098 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:03:41,034 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:03:41,148 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:03:41,212 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:05<1:03:46,  6.14s/it] 20%|        | 158/780 [01:06<45:34,  4.40s/it]   20%|        | 159/780 [01:06<32:46,  3.17s/it] 21%|        | 160/780 [01:06<23:48,  2.30s/it] 21%|        | 161/780 [01:06<17:33,  1.70s/it] 21%|        | 162/780 [01:07<13:10,  1.28s/it] 21%|        | 163/780 [01:07<10:07,  1.02it/s] 21%|        | 164/780 [01:07<07:58,  1.29it/s] 21%|        | 165/780 [01:08<06:28,  1.58it/s] 21%|       | 166/780 [01:08<05:25,  1.88it/s] 21%|       | 167/780 [01:08<04:41,  2.17it/s] 22%|       | 168/780 [01:08<04:11,  2.44it/s] 22%|       | 169/780 [01:09<03:54,  2.60it/s] 22%|       | 170/780 [01:09<03:37,  2.80it/s] 22%|       | 171/780 [01:09<03:26,  2.95it/s] 22%|       | 172/780 [01:10<03:17,  3.08it/s] 22%|       | 173/780 [01:10<03:11,  3.17it/s] 22%|       | 174/780 [01:10<03:07,  3.23it/s] 22%|       | 175/780 [01:11<03:04,  3.28it/s] 23%|       | 176/780 [01:11<03:02,  3.31it/s] 23%|       | 177/780 [01:11<03:00,  3.34it/s] 23%|       | 178/780 [01:11<02:59,  3.35it/s] 23%|       | 179/780 [01:12<02:58,  3.36it/s] 23%|       | 180/780 [01:12<03:00,  3.32it/s] 23%|       | 181/780 [01:12<02:59,  3.34it/s] 23%|       | 182/780 [01:13<02:58,  3.35it/s] 23%|       | 183/780 [01:13<02:57,  3.36it/s] 24%|       | 184/780 [01:13<02:56,  3.37it/s] 24%|       | 185/780 [01:14<02:56,  3.38it/s] 24%|       | 186/780 [01:14<02:55,  3.38it/s] 24%|       | 187/780 [01:14<02:55,  3.38it/s] 24%|       | 188/780 [01:14<02:54,  3.39it/s] 24%|       | 189/780 [01:15<02:54,  3.39it/s] 24%|       | 190/780 [01:15<02:54,  3.38it/s] 24%|       | 191/780 [01:15<02:59,  3.28it/s] 25%|       | 192/780 [01:16<02:57,  3.31it/s] 25%|       | 193/780 [01:16<02:56,  3.33it/s] 25%|       | 194/780 [01:16<02:55,  3.34it/s] 25%|       | 195/780 [01:17<02:54,  3.36it/s] 25%|       | 196/780 [01:17<02:53,  3.37it/s] 25%|       | 197/780 [01:17<02:52,  3.38it/s] 25%|       | 198/780 [01:17<02:52,  3.38it/s] 26%|       | 199/780 [01:18<02:51,  3.38it/s] 26%|       | 200/780 [01:18<02:51,  3.38it/s] 26%|       | 201/780 [01:18<02:51,  3.38it/s] 26%|       | 202/780 [01:19<02:55,  3.29it/s] 26%|       | 203/780 [01:19<02:53,  3.32it/s] 26%|       | 204/780 [01:19<02:52,  3.35it/s] 26%|       | 205/780 [01:20<02:51,  3.36it/s] 26%|       | 206/780 [01:20<02:50,  3.37it/s] 27%|       | 207/780 [01:20<02:49,  3.37it/s] 27%|       | 208/780 [01:20<02:49,  3.38it/s] 27%|       | 209/780 [01:21<02:48,  3.38it/s] 27%|       | 210/780 [01:21<02:48,  3.38it/s] 27%|       | 211/780 [01:21<02:48,  3.38it/s] 27%|       | 212/780 [01:22<02:47,  3.39it/s] 27%|       | 213/780 [01:22<02:51,  3.31it/s] 27%|       | 214/780 [01:22<02:49,  3.35it/s] 28%|       | 215/780 [01:22<02:47,  3.37it/s] 28%|       | 216/780 [01:23<02:46,  3.40it/s] 28%|       | 217/780 [01:23<02:45,  3.41it/s] 28%|       | 218/780 [01:23<02:48,  3.34it/s] 28%|       | 219/780 [01:24<02:46,  3.37it/s] 28%|       | 220/780 [01:24<02:45,  3.39it/s] 28%|       | 221/780 [01:24<02:44,  3.40it/s] 28%|       | 222/780 [01:25<02:43,  3.41it/s] 29%|       | 223/780 [01:25<02:42,  3.42it/s] 29%|       | 224/780 [01:25<02:42,  3.43it/s] 29%|       | 225/780 [01:25<02:41,  3.44it/s] 29%|       | 226/780 [01:26<02:41,  3.44it/s] 29%|       | 227/780 [01:26<02:40,  3.44it/s] 29%|       | 228/780 [01:26<02:40,  3.44it/s] 29%|       | 229/780 [01:27<02:46,  3.31it/s] 29%|       | 230/780 [01:27<02:44,  3.35it/s] 30%|       | 231/780 [01:27<02:42,  3.38it/s] 30%|       | 232/780 [01:27<02:41,  3.40it/s] 30%|       | 233/780 [01:28<02:40,  3.41it/s] 30%|       | 234/780 [01:28<02:39,  3.42it/s] 30%|       | 235/780 [01:28<02:38,  3.43it/s] 30%|       | 236/780 [01:29<02:38,  3.43it/s] 30%|       | 237/780 [01:29<02:37,  3.44it/s] 31%|       | 238/780 [01:29<02:37,  3.44it/s] 31%|       | 239/780 [01:30<02:37,  3.44it/s] 31%|       | 240/780 [01:30<02:47,  3.21it/s] 31%|       | 241/780 [01:30<02:44,  3.28it/s] 31%|       | 242/780 [01:30<02:41,  3.33it/s] 31%|       | 243/780 [01:31<02:39,  3.36it/s] 31%|      | 244/780 [01:31<02:38,  3.39it/s] 31%|      | 245/780 [01:31<02:37,  3.40it/s] 32%|      | 246/780 [01:32<02:36,  3.42it/s] 32%|      | 247/780 [01:32<02:35,  3.42it/s] 32%|      | 248/780 [01:32<02:35,  3.43it/s] 32%|      | 249/780 [01:32<02:34,  3.43it/s] 32%|      | 250/780 [01:33<02:34,  3.43it/s] 32%|      | 251/780 [01:33<02:40,  3.29it/s] 32%|      | 252/780 [01:33<02:38,  3.33it/s] 32%|      | 253/780 [01:34<02:36,  3.37it/s] 33%|      | 254/780 [01:34<02:35,  3.39it/s] 33%|      | 255/780 [01:34<02:34,  3.41it/s] 33%|      | 256/780 [01:35<02:33,  3.41it/s] 33%|      | 257/780 [01:35<02:32,  3.42it/s] 33%|      | 258/780 [01:35<02:32,  3.43it/s] 33%|      | 259/780 [01:35<02:31,  3.43it/s] 33%|      | 260/780 [01:36<02:31,  3.44it/s] 33%|      | 261/780 [01:36<02:30,  3.44it/s] 34%|      | 262/780 [01:36<02:39,  3.24it/s] 34%|      | 263/780 [01:37<02:36,  3.30it/s] 34%|      | 264/780 [01:37<02:34,  3.34it/s] 34%|      | 265/780 [01:37<02:32,  3.37it/s] 34%|      | 266/780 [01:38<02:31,  3.39it/s] 34%|      | 267/780 [01:38<02:30,  3.40it/s] 34%|      | 268/780 [01:38<02:29,  3.42it/s] 34%|      | 269/780 [01:38<02:29,  3.42it/s] 35%|      | 270/780 [01:39<02:28,  3.43it/s] 35%|      | 271/780 [01:39<02:33,  3.33it/s] 35%|      | 272/780 [01:39<02:31,  3.36it/s] 35%|      | 273/780 [01:40<02:34,  3.28it/s] 35%|      | 274/780 [01:40<02:32,  3.33it/s] 35%|      | 275/780 [01:40<02:30,  3.36it/s] 35%|      | 276/780 [01:40<02:28,  3.38it/s] 36%|      | 277/780 [01:41<02:27,  3.40it/s] 36%|      | 278/780 [01:41<02:27,  3.41it/s] 36%|      | 279/780 [01:41<02:26,  3.42it/s] 36%|      | 280/780 [01:42<02:25,  3.43it/s] 36%|      | 281/780 [01:42<02:25,  3.43it/s] 36%|      | 282/780 [01:42<02:25,  3.43it/s] 36%|      | 283/780 [01:43<02:24,  3.43it/s] 36%|      | 284/780 [01:43<02:28,  3.34it/s] 37%|      | 285/780 [01:43<02:30,  3.28it/s] 37%|      | 286/780 [01:43<02:28,  3.33it/s] 37%|      | 287/780 [01:44<02:26,  3.37it/s] 37%|      | 288/780 [01:44<02:34,  3.18it/s] 37%|      | 289/780 [01:45<02:54,  2.81it/s] 37%|      | 290/780 [01:45<02:44,  2.97it/s] 37%|      | 291/780 [01:45<02:37,  3.10it/s] 37%|      | 292/780 [01:45<02:32,  3.20it/s] 38%|      | 293/780 [01:46<02:29,  3.26it/s] 38%|      | 294/780 [01:46<02:35,  3.12it/s] 38%|      | 295/780 [01:46<02:31,  3.21it/s] 38%|      | 296/780 [01:47<02:27,  3.28it/s] 38%|      | 297/780 [01:47<02:25,  3.32it/s] 38%|      | 298/780 [01:47<02:23,  3.36it/s] 38%|      | 299/780 [01:48<02:22,  3.38it/s] 38%|      | 300/780 [01:48<02:21,  3.40it/s] 39%|      | 301/780 [01:48<02:20,  3.41it/s] 39%|      | 302/780 [01:48<02:19,  3.42it/s] 39%|      | 303/780 [01:49<02:19,  3.43it/s] 39%|      | 304/780 [01:49<02:18,  3.43it/s] 39%|      | 305/780 [01:49<02:22,  3.34it/s] 39%|      | 306/780 [01:50<02:20,  3.37it/s] 39%|      | 307/780 [01:50<02:19,  3.39it/s] 39%|      | 308/780 [01:50<02:18,  3.41it/s] 40%|      | 309/780 [01:50<02:17,  3.42it/s] 40%|      | 310/780 [01:51<02:17,  3.43it/s] 40%|      | 311/780 [01:51<02:16,  3.43it/s] 40%|      | 312/780 [01:51<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 04:04:33,752 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:04:33,752 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:04:33,752 >>   Batch size = 8
{'eval_loss': 0.9404196739196777, 'eval_runtime': 9.7304, 'eval_samples_per_second': 357.334, 'eval_steps_per_second': 44.705, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.78it/s][A
  3%|         | 12/435 [00:00<00:08, 48.80it/s][A
  4%|         | 17/435 [00:00<00:08, 47.03it/s][A
  5%|         | 22/435 [00:00<00:08, 46.11it/s][A
  6%|         | 27/435 [00:00<00:08, 45.40it/s][A
  7%|         | 32/435 [00:00<00:09, 43.66it/s][A
  9%|         | 37/435 [00:00<00:09, 44.02it/s][A
 10%|         | 42/435 [00:00<00:08, 44.12it/s][A
 11%|         | 47/435 [00:01<00:08, 44.32it/s][A
 12%|        | 52/435 [00:01<00:08, 44.50it/s][A
 13%|        | 57/435 [00:01<00:08, 44.75it/s][A
 14%|        | 62/435 [00:01<00:08, 44.84it/s][A
 15%|        | 67/435 [00:01<00:08, 44.85it/s][A
 17%|        | 72/435 [00:01<00:08, 44.63it/s][A
 18%|        | 77/435 [00:01<00:08, 44.57it/s][A
 19%|        | 82/435 [00:01<00:07, 44.57it/s][A
 20%|        | 87/435 [00:01<00:07, 44.59it/s][A
 21%|        | 92/435 [00:02<00:07, 44.59it/s][A
 22%|       | 97/435 [00:02<00:07, 44.77it/s][A
 23%|       | 102/435 [00:02<00:07, 44.81it/s][A
 25%|       | 107/435 [00:02<00:07, 44.91it/s][A
 26%|       | 112/435 [00:02<00:07, 42.02it/s][A
 27%|       | 117/435 [00:02<00:07, 42.89it/s][A
 28%|       | 122/435 [00:02<00:07, 43.54it/s][A
 29%|       | 127/435 [00:02<00:07, 43.85it/s][A
 30%|       | 132/435 [00:02<00:06, 44.03it/s][A
 31%|      | 137/435 [00:03<00:06, 44.30it/s][A
 33%|      | 142/435 [00:03<00:06, 44.47it/s][A
 34%|      | 147/435 [00:03<00:06, 44.61it/s][A
 35%|      | 152/435 [00:03<00:06, 44.50it/s][A
 36%|      | 157/435 [00:03<00:06, 44.29it/s][A
 37%|      | 162/435 [00:03<00:06, 44.64it/s][A
 38%|      | 167/435 [00:03<00:06, 44.64it/s][A
 40%|      | 172/435 [00:03<00:05, 44.76it/s][A
 41%|      | 177/435 [00:03<00:05, 44.70it/s][A
 42%|     | 182/435 [00:04<00:05, 44.74it/s][A
 43%|     | 187/435 [00:04<00:05, 44.74it/s][A
 44%|     | 192/435 [00:04<00:05, 44.66it/s][A
 45%|     | 197/435 [00:04<00:05, 44.59it/s][A
 46%|     | 202/435 [00:04<00:05, 44.58it/s][A
 48%|     | 207/435 [00:04<00:05, 44.69it/s][A
 49%|     | 212/435 [00:04<00:04, 44.72it/s][A
 50%|     | 217/435 [00:04<00:04, 44.65it/s][A
 51%|     | 222/435 [00:04<00:04, 44.75it/s][A
 52%|    | 227/435 [00:05<00:04, 44.77it/s][A
 53%|    | 232/435 [00:05<00:04, 44.71it/s][A
 54%|    | 237/435 [00:05<00:04, 44.66it/s][A
 56%|    | 242/435 [00:05<00:04, 44.29it/s][A
 57%|    | 247/435 [00:05<00:04, 39.36it/s][A
 58%|    | 252/435 [00:05<00:04, 40.93it/s][A
 59%|    | 257/435 [00:05<00:04, 42.05it/s][A
 60%|    | 262/435 [00:05<00:04, 42.99it/s][A
 61%|   | 267/435 [00:06<00:03, 43.63it/s][A
 63%|   | 272/435 [00:06<00:03, 44.05it/s][A
 64%|   | 277/435 [00:06<00:03, 44.39it/s][A
 65%|   | 282/435 [00:06<00:03, 44.49it/s][A
 66%|   | 287/435 [00:06<00:03, 44.26it/s][A
 67%|   | 292/435 [00:06<00:03, 44.12it/s][A
 68%|   | 297/435 [00:06<00:03, 44.32it/s][A
 69%|   | 302/435 [00:06<00:02, 44.41it/s][A
 71%|   | 307/435 [00:06<00:02, 44.65it/s][A
 72%|  | 312/435 [00:07<00:02, 44.79it/s][A
 73%|  | 317/435 [00:07<00:02, 44.94it/s][A
 74%|  | 322/435 [00:07<00:02, 44.94it/s][A
 75%|  | 327/435 [00:07<00:02, 44.82it/s][A
 76%|  | 332/435 [00:07<00:02, 44.55it/s][A
 77%|  | 337/435 [00:07<00:02, 44.41it/s][A
 79%|  | 342/435 [00:07<00:02, 44.44it/s][A
 80%|  | 347/435 [00:07<00:01, 44.50it/s][A
 81%|  | 352/435 [00:07<00:01, 44.66it/s][A
 82%| | 357/435 [00:08<00:01, 44.68it/s][A
 83%| | 362/435 [00:08<00:01, 44.83it/s][A
 84%| | 367/435 [00:08<00:01, 44.88it/s][A
 86%| | 372/435 [00:08<00:01, 44.90it/s][A
 87%| | 377/435 [00:08<00:01, 44.74it/s][A
 88%| | 382/435 [00:08<00:01, 43.48it/s][A
 89%| | 387/435 [00:08<00:01, 43.86it/s][A
 90%| | 392/435 [00:08<00:00, 44.06it/s][A
 91%|| 397/435 [00:08<00:00, 44.29it/s][A
 92%|| 402/435 [00:09<00:00, 44.48it/s][A
 94%|| 407/435 [00:09<00:00, 44.54it/s][A
 95%|| 412/435 [00:09<00:00, 44.67it/s][A
 96%|| 417/435 [00:09<00:00, 44.66it/s][A
 97%|| 422/435 [00:09<00:00, 44.46it/s][A
 98%|| 427/435 [00:09<00:00, 44.46it/s][A
 99%|| 432/435 [00:09<00:00, 44.52it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.52it/s][A 40%|      | 312/780 [02:01<02:16,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:04:43,796 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 04:04:43,990 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:04:47,077 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:04:47,430 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:04:47,528 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:12<50:41,  6.51s/it] 40%|      | 314/780 [02:13<36:09,  4.66s/it] 40%|      | 315/780 [02:13<25:56,  3.35s/it] 41%|      | 316/780 [02:13<18:48,  2.43s/it] 41%|      | 317/780 [02:14<13:48,  1.79s/it] 41%|      | 318/780 [02:14<10:19,  1.34s/it] 41%|      | 319/780 [02:14<07:53,  1.03s/it] 41%|      | 320/780 [02:14<06:11,  1.24it/s] 41%|      | 321/780 [02:15<04:59,  1.53it/s] 41%|     | 322/780 [02:15<04:10,  1.83it/s] 41%|     | 323/780 [02:15<03:35,  2.13it/s] 42%|     | 324/780 [02:16<03:10,  2.40it/s] 42%|     | 325/780 [02:16<02:56,  2.58it/s] 42%|     | 326/780 [02:16<02:43,  2.78it/s] 42%|     | 327/780 [02:17<02:33,  2.94it/s] 42%|     | 328/780 [02:17<02:27,  3.07it/s] 42%|     | 329/780 [02:17<02:22,  3.16it/s] 42%|     | 330/780 [02:17<02:19,  3.22it/s] 42%|     | 331/780 [02:18<02:17,  3.27it/s] 43%|     | 332/780 [02:18<02:15,  3.31it/s] 43%|     | 333/780 [02:18<02:14,  3.34it/s] 43%|     | 334/780 [02:19<02:13,  3.35it/s] 43%|     | 335/780 [02:19<02:12,  3.37it/s] 43%|     | 336/780 [02:19<02:12,  3.34it/s] 43%|     | 337/780 [02:19<02:11,  3.36it/s] 43%|     | 338/780 [02:20<02:11,  3.37it/s] 43%|     | 339/780 [02:20<02:11,  3.36it/s] 44%|     | 340/780 [02:20<02:10,  3.37it/s] 44%|     | 341/780 [02:21<02:10,  3.38it/s] 44%|     | 342/780 [02:21<02:09,  3.39it/s] 44%|     | 343/780 [02:21<02:09,  3.38it/s] 44%|     | 344/780 [02:22<02:08,  3.39it/s] 44%|     | 345/780 [02:22<02:08,  3.39it/s] 44%|     | 346/780 [02:22<02:07,  3.39it/s] 44%|     | 347/780 [02:22<02:10,  3.32it/s] 45%|     | 348/780 [02:23<02:09,  3.34it/s] 45%|     | 349/780 [02:23<02:08,  3.36it/s] 45%|     | 350/780 [02:23<02:07,  3.37it/s] 45%|     | 351/780 [02:24<02:07,  3.37it/s] 45%|     | 352/780 [02:24<02:06,  3.38it/s] 45%|     | 353/780 [02:24<02:06,  3.39it/s] 45%|     | 354/780 [02:25<02:05,  3.39it/s] 46%|     | 355/780 [02:25<02:07,  3.33it/s] 46%|     | 356/780 [02:25<02:06,  3.35it/s] 46%|     | 357/780 [02:25<02:05,  3.36it/s] 46%|     | 358/780 [02:26<02:05,  3.37it/s] 46%|     | 359/780 [02:26<02:04,  3.37it/s] 46%|     | 360/780 [02:26<02:04,  3.38it/s] 46%|     | 361/780 [02:27<02:03,  3.38it/s] 46%|     | 362/780 [02:27<02:03,  3.38it/s] 47%|     | 363/780 [02:27<02:03,  3.38it/s] 47%|     | 364/780 [02:27<02:02,  3.39it/s] 47%|     | 365/780 [02:28<02:02,  3.39it/s] 47%|     | 366/780 [02:28<02:05,  3.30it/s] 47%|     | 367/780 [02:28<02:04,  3.33it/s] 47%|     | 368/780 [02:29<02:03,  3.35it/s] 47%|     | 369/780 [02:29<02:02,  3.36it/s] 47%|     | 370/780 [02:29<02:01,  3.37it/s] 48%|     | 371/780 [02:30<02:01,  3.38it/s] 48%|     | 372/780 [02:30<02:00,  3.38it/s] 48%|     | 373/780 [02:30<02:00,  3.39it/s] 48%|     | 374/780 [02:30<01:59,  3.39it/s] 48%|     | 375/780 [02:31<01:59,  3.39it/s] 48%|     | 376/780 [02:31<01:59,  3.39it/s] 48%|     | 377/780 [02:31<02:04,  3.25it/s] 48%|     | 378/780 [02:32<02:02,  3.29it/s] 49%|     | 379/780 [02:32<02:00,  3.32it/s] 49%|     | 380/780 [02:32<01:59,  3.34it/s] 49%|     | 381/780 [02:33<01:59,  3.35it/s] 49%|     | 382/780 [02:33<01:58,  3.37it/s] 49%|     | 383/780 [02:33<01:57,  3.38it/s] 49%|     | 384/780 [02:33<01:57,  3.38it/s] 49%|     | 385/780 [02:34<01:56,  3.39it/s] 49%|     | 386/780 [02:34<01:56,  3.39it/s] 50%|     | 387/780 [02:34<01:55,  3.39it/s] 50%|     | 388/780 [02:35<02:02,  3.20it/s] 50%|     | 389/780 [02:35<02:00,  3.26it/s] 50%|     | 390/780 [02:35<01:58,  3.30it/s] 50%|     | 391/780 [02:36<01:56,  3.33it/s] 50%|     | 392/780 [02:36<01:56,  3.34it/s] 50%|     | 393/780 [02:36<01:55,  3.36it/s] 51%|     | 394/780 [02:36<01:54,  3.37it/s] 51%|     | 395/780 [02:37<01:53,  3.38it/s] 51%|     | 396/780 [02:37<01:53,  3.38it/s] 51%|     | 397/780 [02:37<01:53,  3.39it/s] 51%|     | 398/780 [02:38<01:57,  3.25it/s] 51%|     | 399/780 [02:38<01:55,  3.29it/s] 51%|    | 400/780 [02:38<01:54,  3.32it/s] 51%|    | 401/780 [02:39<01:53,  3.34it/s] 52%|    | 402/780 [02:39<01:52,  3.36it/s] 52%|    | 403/780 [02:39<01:52,  3.36it/s] 52%|    | 404/780 [02:39<01:51,  3.37it/s] 52%|    | 405/780 [02:40<01:50,  3.38it/s] 52%|    | 406/780 [02:40<01:50,  3.38it/s] 52%|    | 407/780 [02:40<01:50,  3.38it/s] 52%|    | 408/780 [02:41<01:49,  3.38it/s] 52%|    | 409/780 [02:41<01:55,  3.23it/s] 53%|    | 410/780 [02:41<01:53,  3.27it/s] 53%|    | 411/780 [02:42<01:51,  3.31it/s] 53%|    | 412/780 [02:42<01:50,  3.33it/s] 53%|    | 413/780 [02:42<01:49,  3.35it/s] 53%|    | 414/780 [02:42<01:48,  3.37it/s] 53%|    | 415/780 [02:43<01:47,  3.39it/s] 53%|    | 416/780 [02:43<01:46,  3.40it/s] 53%|    | 417/780 [02:43<01:50,  3.29it/s] 54%|    | 418/780 [02:44<01:48,  3.34it/s] 54%|    | 419/780 [02:44<01:50,  3.28it/s] 54%|    | 420/780 [02:44<01:54,  3.13it/s] 54%|    | 421/780 [02:45<02:08,  2.79it/s] 54%|    | 422/780 [02:45<02:01,  2.95it/s] 54%|    | 423/780 [02:45<01:55,  3.08it/s] 54%|    | 424/780 [02:46<01:51,  3.19it/s] 54%|    | 425/780 [02:46<01:48,  3.26it/s] 55%|    | 426/780 [02:46<01:46,  3.31it/s] 55%|    | 427/780 [02:46<01:45,  3.35it/s] 55%|    | 428/780 [02:47<01:44,  3.38it/s] 55%|    | 429/780 [02:47<01:46,  3.29it/s] 55%|    | 430/780 [02:47<01:44,  3.34it/s] 55%|    | 431/780 [02:48<01:43,  3.37it/s] 55%|    | 432/780 [02:48<01:42,  3.39it/s] 56%|    | 433/780 [02:48<01:41,  3.41it/s] 56%|    | 434/780 [02:49<01:41,  3.42it/s] 56%|    | 435/780 [02:49<01:40,  3.43it/s] 56%|    | 436/780 [02:49<01:40,  3.43it/s] 56%|    | 437/780 [02:49<01:39,  3.44it/s] 56%|    | 438/780 [02:50<01:39,  3.44it/s] 56%|    | 439/780 [02:50<01:39,  3.44it/s] 56%|    | 440/780 [02:50<01:42,  3.33it/s] 57%|    | 441/780 [02:51<01:40,  3.37it/s] 57%|    | 442/780 [02:51<01:39,  3.39it/s] 57%|    | 443/780 [02:51<01:38,  3.41it/s] 57%|    | 444/780 [02:51<01:38,  3.42it/s] 57%|    | 445/780 [02:52<01:37,  3.43it/s] 57%|    | 446/780 [02:52<01:37,  3.43it/s] 57%|    | 447/780 [02:52<01:36,  3.44it/s] 57%|    | 448/780 [02:53<01:36,  3.44it/s] 58%|    | 449/780 [02:53<01:36,  3.45it/s] 58%|    | 450/780 [02:53<01:35,  3.45it/s] 58%|    | 451/780 [02:54<01:37,  3.39it/s] 58%|    | 452/780 [02:54<01:36,  3.41it/s] 58%|    | 453/780 [02:54<01:35,  3.42it/s] 58%|    | 454/780 [02:54<01:35,  3.43it/s] 58%|    | 455/780 [02:55<01:34,  3.43it/s] 58%|    | 456/780 [02:55<01:34,  3.44it/s] 59%|    | 457/780 [02:55<01:33,  3.44it/s] 59%|    | 458/780 [02:56<01:33,  3.44it/s] 59%|    | 459/780 [02:56<01:33,  3.44it/s] 59%|    | 460/780 [02:56<01:32,  3.44it/s] 59%|    | 461/780 [02:56<01:32,  3.44it/s] 59%|    | 462/780 [02:57<01:32,  3.45it/s] 59%|    | 463/780 [02:57<01:32,  3.45it/s] 59%|    | 464/780 [02:57<01:31,  3.45it/s] 60%|    | 465/780 [02:58<01:31,  3.45it/s] 60%|    | 466/780 [02:58<01:31,  3.45it/s] 60%|    | 467/780 [02:58<01:30,  3.45it/s] 60%|    | 468/780 [02:58<01:30,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 04:05:40,934 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:05:40,934 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:05:40,934 >>   Batch size = 8
{'eval_loss': 0.9541786313056946, 'eval_runtime': 9.8167, 'eval_samples_per_second': 354.193, 'eval_steps_per_second': 44.312, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.24it/s][A
  3%|         | 12/435 [00:00<00:08, 49.09it/s][A
  4%|         | 17/435 [00:00<00:08, 47.34it/s][A
  5%|         | 22/435 [00:00<00:08, 46.38it/s][A
  6%|         | 27/435 [00:00<00:08, 45.95it/s][A
  7%|         | 32/435 [00:00<00:08, 45.58it/s][A
  9%|         | 37/435 [00:00<00:08, 45.07it/s][A
 10%|         | 42/435 [00:00<00:08, 44.54it/s][A
 11%|         | 47/435 [00:01<00:08, 44.50it/s][A
 12%|        | 52/435 [00:01<00:08, 44.59it/s][A
 13%|        | 57/435 [00:01<00:08, 44.79it/s][A
 14%|        | 62/435 [00:01<00:08, 44.83it/s][A
 15%|        | 67/435 [00:01<00:08, 44.85it/s][A
 17%|        | 72/435 [00:01<00:08, 44.87it/s][A
 18%|        | 77/435 [00:01<00:07, 44.75it/s][A
 19%|        | 82/435 [00:01<00:07, 44.59it/s][A
 20%|        | 87/435 [00:01<00:07, 44.34it/s][A
 21%|        | 92/435 [00:02<00:07, 44.42it/s][A
 22%|       | 97/435 [00:02<00:07, 44.54it/s][A
 23%|       | 102/435 [00:02<00:07, 44.69it/s][A
 25%|       | 107/435 [00:02<00:07, 44.70it/s][A
 26%|       | 112/435 [00:02<00:07, 44.73it/s][A
 27%|       | 117/435 [00:02<00:07, 44.81it/s][A
 28%|       | 122/435 [00:02<00:06, 44.76it/s][A
 29%|       | 127/435 [00:02<00:06, 44.54it/s][A
 30%|       | 132/435 [00:02<00:06, 44.44it/s][A
 31%|      | 137/435 [00:03<00:06, 43.08it/s][A
 33%|      | 142/435 [00:03<00:06, 43.67it/s][A
 34%|      | 147/435 [00:03<00:06, 44.05it/s][A
 35%|      | 152/435 [00:03<00:06, 44.38it/s][A
 36%|      | 157/435 [00:03<00:06, 44.36it/s][A
 37%|      | 162/435 [00:03<00:06, 44.69it/s][A
 38%|      | 167/435 [00:03<00:06, 44.61it/s][A
 40%|      | 172/435 [00:03<00:05, 44.58it/s][A
 41%|      | 177/435 [00:03<00:05, 44.36it/s][A
 42%|     | 182/435 [00:04<00:05, 44.34it/s][A
 43%|     | 187/435 [00:04<00:05, 44.46it/s][A
 44%|     | 192/435 [00:04<00:05, 44.57it/s][A
 45%|     | 197/435 [00:04<00:05, 44.75it/s][A
 46%|     | 202/435 [00:04<00:05, 44.86it/s][A
 48%|     | 207/435 [00:04<00:05, 44.81it/s][A
 49%|     | 212/435 [00:04<00:04, 44.83it/s][A
 50%|     | 217/435 [00:04<00:04, 44.51it/s][A
 51%|     | 222/435 [00:04<00:04, 44.40it/s][A
 52%|    | 227/435 [00:05<00:04, 44.36it/s][A
 53%|    | 232/435 [00:05<00:04, 44.40it/s][A
 54%|    | 237/435 [00:05<00:04, 44.54it/s][A
 56%|    | 242/435 [00:05<00:04, 44.74it/s][A
 57%|    | 247/435 [00:05<00:04, 44.70it/s][A
 58%|    | 252/435 [00:05<00:04, 44.81it/s][A
 59%|    | 257/435 [00:05<00:03, 44.79it/s][A
 60%|    | 262/435 [00:05<00:03, 44.80it/s][A
 61%|   | 267/435 [00:05<00:03, 44.65it/s][A
 63%|   | 272/435 [00:06<00:03, 43.67it/s][A
 64%|   | 277/435 [00:06<00:03, 43.85it/s][A
 65%|   | 282/435 [00:06<00:03, 44.11it/s][A
 66%|   | 287/435 [00:06<00:03, 44.32it/s][A
 67%|   | 292/435 [00:06<00:03, 44.50it/s][A
 68%|   | 297/435 [00:06<00:03, 44.64it/s][A
 69%|   | 302/435 [00:06<00:02, 44.64it/s][A
 71%|   | 307/435 [00:06<00:02, 44.58it/s][A
 72%|  | 312/435 [00:06<00:02, 44.35it/s][A
 73%|  | 317/435 [00:07<00:02, 44.49it/s][A
 74%|  | 322/435 [00:07<00:02, 44.52it/s][A
 75%|  | 327/435 [00:07<00:02, 44.64it/s][A
 76%|  | 332/435 [00:07<00:02, 44.70it/s][A
 77%|  | 337/435 [00:07<00:02, 44.68it/s][A
 79%|  | 342/435 [00:07<00:02, 44.81it/s][A
 80%|  | 347/435 [00:07<00:01, 44.78it/s][A
 81%|  | 352/435 [00:07<00:01, 44.61it/s][A
 82%| | 357/435 [00:07<00:01, 44.56it/s][A
 83%| | 362/435 [00:08<00:01, 44.37it/s][A
 84%| | 367/435 [00:08<00:01, 44.51it/s][A
 86%| | 372/435 [00:08<00:01, 44.58it/s][A
 87%| | 377/435 [00:08<00:01, 44.66it/s][A
 88%| | 382/435 [00:08<00:01, 44.71it/s][A
 89%| | 387/435 [00:08<00:01, 44.81it/s][A
 90%| | 392/435 [00:08<00:00, 44.79it/s][A
 91%|| 397/435 [00:08<00:00, 44.74it/s][A
 92%|| 402/435 [00:09<00:00, 44.69it/s][A
 94%|| 407/435 [00:09<00:00, 42.72it/s][A
 95%|| 412/435 [00:09<00:00, 43.36it/s][A
 96%|| 417/435 [00:09<00:00, 43.72it/s][A
 97%|| 422/435 [00:09<00:00, 44.06it/s][A
 98%|| 427/435 [00:09<00:00, 44.33it/s][A
 99%|| 432/435 [00:09<00:00, 44.47it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.47it/s][A 60%|    | 468/780 [03:08<01:30,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:05:50,919 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 04:05:51,082 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:05:54,202 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:05:54,345 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:05:54,408 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:19<33:42,  6.50s/it] 60%|    | 470/780 [03:20<24:01,  4.65s/it] 60%|    | 471/780 [03:20<17:13,  3.34s/it] 61%|    | 472/780 [03:20<12:28,  2.43s/it] 61%|    | 473/780 [03:21<09:09,  1.79s/it] 61%|    | 474/780 [03:21<06:50,  1.34s/it] 61%|    | 475/780 [03:21<05:13,  1.03s/it] 61%|    | 476/780 [03:22<04:05,  1.24it/s] 61%|    | 477/780 [03:22<03:17,  1.53it/s] 61%|   | 478/780 [03:22<02:44,  1.83it/s] 61%|   | 479/780 [03:22<02:21,  2.13it/s] 62%|   | 480/780 [03:23<02:05,  2.40it/s] 62%|   | 481/780 [03:23<01:56,  2.58it/s] 62%|   | 482/780 [03:23<01:47,  2.78it/s] 62%|   | 483/780 [03:24<01:41,  2.94it/s] 62%|   | 484/780 [03:24<01:36,  3.06it/s] 62%|   | 485/780 [03:24<01:33,  3.15it/s] 62%|   | 486/780 [03:25<01:31,  3.23it/s] 62%|   | 487/780 [03:25<01:29,  3.27it/s] 63%|   | 488/780 [03:25<01:28,  3.31it/s] 63%|   | 489/780 [03:25<01:27,  3.33it/s] 63%|   | 490/780 [03:26<01:26,  3.35it/s] 63%|   | 491/780 [03:26<01:25,  3.36it/s] 63%|   | 492/780 [03:26<01:25,  3.37it/s] 63%|   | 493/780 [03:27<01:25,  3.37it/s] 63%|   | 494/780 [03:27<01:24,  3.38it/s] 63%|   | 495/780 [03:27<01:24,  3.38it/s] 64%|   | 496/780 [03:27<01:24,  3.38it/s] 64%|   | 497/780 [03:28<01:23,  3.39it/s] 64%|   | 498/780 [03:28<01:23,  3.39it/s] 64%|   | 499/780 [03:28<01:22,  3.39it/s] 64%|   | 500/780 [03:29<01:22,  3.39it/s]                                                  64%|   | 500/780 [03:29<01:22,  3.39it/s] 64%|   | 501/780 [03:29<01:22,  3.39it/s] 64%|   | 502/780 [03:29<01:23,  3.32it/s] 64%|   | 503/780 [03:30<01:22,  3.34it/s] 65%|   | 504/780 [03:30<01:22,  3.36it/s] 65%|   | 505/780 [03:30<01:21,  3.37it/s] 65%|   | 506/780 [03:30<01:21,  3.38it/s] 65%|   | 507/780 [03:31<01:20,  3.38it/s] 65%|   | 508/780 [03:31<01:20,  3.38it/s] 65%|   | 509/780 [03:31<01:20,  3.39it/s] 65%|   | 510/780 [03:32<01:19,  3.39it/s] 66%|   | 511/780 [03:32<01:19,  3.39it/s] 66%|   | 512/780 [03:32<01:19,  3.39it/s] 66%|   | 513/780 [03:33<01:22,  3.25it/s] 66%|   | 514/780 [03:33<01:20,  3.29it/s] 66%|   | 515/780 [03:33<01:19,  3.32it/s] 66%|   | 516/780 [03:33<01:19,  3.34it/s] 66%|   | 517/780 [03:34<01:18,  3.36it/s] 66%|   | 518/780 [03:34<01:17,  3.37it/s] 67%|   | 519/780 [03:34<01:17,  3.37it/s] 67%|   | 520/780 [03:35<01:16,  3.38it/s] 67%|   | 521/780 [03:35<01:16,  3.38it/s] 67%|   | 522/780 [03:35<01:16,  3.39it/s] 67%|   | 523/780 [03:35<01:15,  3.39it/s] 67%|   | 524/780 [03:36<01:21,  3.16it/s] 67%|   | 525/780 [03:36<01:19,  3.22it/s] 67%|   | 526/780 [03:36<01:17,  3.27it/s] 68%|   | 527/780 [03:37<01:16,  3.31it/s] 68%|   | 528/780 [03:37<01:15,  3.33it/s] 68%|   | 529/780 [03:37<01:14,  3.35it/s] 68%|   | 530/780 [03:38<01:14,  3.36it/s] 68%|   | 531/780 [03:38<01:13,  3.37it/s] 68%|   | 532/780 [03:38<01:13,  3.38it/s] 68%|   | 533/780 [03:39<01:13,  3.38it/s] 68%|   | 534/780 [03:39<01:15,  3.24it/s] 69%|   | 535/780 [03:39<01:14,  3.29it/s] 69%|   | 536/780 [03:39<01:13,  3.32it/s] 69%|   | 537/780 [03:40<01:12,  3.34it/s] 69%|   | 538/780 [03:40<01:12,  3.35it/s] 69%|   | 539/780 [03:40<01:11,  3.36it/s] 69%|   | 540/780 [03:41<01:11,  3.37it/s] 69%|   | 541/780 [03:41<01:10,  3.38it/s] 69%|   | 542/780 [03:41<01:10,  3.38it/s] 70%|   | 543/780 [03:42<01:10,  3.39it/s] 70%|   | 544/780 [03:42<01:09,  3.39it/s] 70%|   | 545/780 [03:42<01:11,  3.29it/s] 70%|   | 546/780 [03:42<01:10,  3.32it/s] 70%|   | 547/780 [03:43<01:09,  3.34it/s] 70%|   | 548/780 [03:43<01:09,  3.35it/s] 70%|   | 549/780 [03:43<01:10,  3.30it/s] 71%|   | 550/780 [03:44<01:09,  3.33it/s] 71%|   | 551/780 [03:44<01:08,  3.34it/s] 71%|   | 552/780 [03:44<01:08,  3.35it/s] 71%|   | 553/780 [03:45<01:29,  2.53it/s] 71%|   | 554/780 [03:45<01:25,  2.65it/s] 71%|   | 555/780 [03:45<01:19,  2.84it/s] 71%|  | 556/780 [03:46<01:15,  2.98it/s] 71%|  | 557/780 [03:46<01:11,  3.10it/s] 72%|  | 558/780 [03:46<01:09,  3.18it/s] 72%|  | 559/780 [03:47<01:08,  3.24it/s] 72%|  | 560/780 [03:47<01:06,  3.28it/s] 72%|  | 561/780 [03:47<01:06,  3.32it/s] 72%|  | 562/780 [03:48<01:05,  3.33it/s] 72%|  | 563/780 [03:48<01:04,  3.34it/s] 72%|  | 564/780 [03:48<01:04,  3.35it/s] 72%|  | 565/780 [03:48<01:05,  3.30it/s] 73%|  | 566/780 [03:49<01:04,  3.33it/s] 73%|  | 567/780 [03:49<01:03,  3.35it/s] 73%|  | 568/780 [03:49<01:03,  3.36it/s] 73%|  | 569/780 [03:50<01:02,  3.37it/s] 73%|  | 570/780 [03:50<01:02,  3.38it/s] 73%|  | 571/780 [03:50<01:01,  3.38it/s] 73%|  | 572/780 [03:51<01:01,  3.39it/s] 73%|  | 573/780 [03:51<01:01,  3.39it/s] 74%|  | 574/780 [03:51<01:00,  3.39it/s] 74%|  | 575/780 [03:51<01:00,  3.39it/s] 74%|  | 576/780 [03:52<01:02,  3.24it/s] 74%|  | 577/780 [03:52<01:01,  3.29it/s] 74%|  | 578/780 [03:52<01:00,  3.32it/s] 74%|  | 579/780 [03:53<01:00,  3.34it/s] 74%|  | 580/780 [03:53<00:59,  3.36it/s] 74%|  | 581/780 [03:53<00:59,  3.37it/s] 75%|  | 582/780 [03:53<00:58,  3.38it/s] 75%|  | 583/780 [03:54<00:58,  3.38it/s] 75%|  | 584/780 [03:54<00:57,  3.39it/s] 75%|  | 585/780 [03:54<00:57,  3.39it/s] 75%|  | 586/780 [03:55<00:57,  3.39it/s] 75%|  | 587/780 [03:55<00:57,  3.34it/s] 75%|  | 588/780 [03:55<00:57,  3.36it/s] 76%|  | 589/780 [03:56<00:56,  3.37it/s] 76%|  | 590/780 [03:56<00:56,  3.38it/s] 76%|  | 591/780 [03:56<00:55,  3.38it/s] 76%|  | 592/780 [03:56<00:55,  3.38it/s] 76%|  | 593/780 [03:57<00:55,  3.39it/s] 76%|  | 594/780 [03:57<00:54,  3.39it/s] 76%|  | 595/780 [03:57<00:54,  3.39it/s] 76%|  | 596/780 [03:58<00:54,  3.39it/s] 77%|  | 597/780 [03:58<00:53,  3.39it/s] 77%|  | 598/780 [03:58<00:53,  3.39it/s] 77%|  | 599/780 [03:59<00:53,  3.39it/s] 77%|  | 600/780 [03:59<00:53,  3.39it/s] 77%|  | 601/780 [03:59<00:52,  3.39it/s] 77%|  | 602/780 [03:59<00:52,  3.40it/s] 77%|  | 603/780 [04:00<00:52,  3.40it/s] 77%|  | 604/780 [04:00<00:53,  3.27it/s] 78%|  | 605/780 [04:00<00:52,  3.31it/s] 78%|  | 606/780 [04:01<00:52,  3.34it/s] 78%|  | 607/780 [04:01<00:51,  3.35it/s] 78%|  | 608/780 [04:01<00:51,  3.36it/s] 78%|  | 609/780 [04:01<00:50,  3.37it/s] 78%|  | 610/780 [04:02<00:50,  3.38it/s] 78%|  | 611/780 [04:02<00:49,  3.38it/s] 78%|  | 612/780 [04:02<00:49,  3.39it/s] 79%|  | 613/780 [04:03<00:49,  3.39it/s] 79%|  | 614/780 [04:03<00:48,  3.39it/s] 79%|  | 615/780 [04:03<00:50,  3.27it/s] 79%|  | 616/780 [04:04<00:49,  3.31it/s] 79%|  | 617/780 [04:04<00:48,  3.35it/s] 79%|  | 618/780 [04:04<00:47,  3.39it/s] 79%|  | 619/780 [04:04<00:47,  3.40it/s] 79%|  | 620/780 [04:05<00:46,  3.42it/s] 80%|  | 621/780 [04:05<00:46,  3.43it/s] 80%|  | 622/780 [04:05<00:45,  3.44it/s] 80%|  | 623/780 [04:06<00:45,  3.44it/s] 80%|  | 624/780 [04:06<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 04:06:48,355 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:06:48,368 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:06:48,368 >>   Batch size = 8
{'eval_loss': 0.9664760828018188, 'eval_runtime': 9.7676, 'eval_samples_per_second': 355.974, 'eval_steps_per_second': 44.535, 'epoch': 3.0}
{'loss': 0.6776, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.99it/s][A
  3%|         | 12/435 [00:00<00:08, 49.39it/s][A
  4%|         | 18/435 [00:00<00:08, 47.34it/s][A
  5%|         | 23/435 [00:00<00:08, 46.48it/s][A
  6%|         | 28/435 [00:00<00:08, 45.82it/s][A
  8%|         | 33/435 [00:00<00:08, 45.51it/s][A
  9%|         | 38/435 [00:00<00:08, 45.07it/s][A
 10%|         | 43/435 [00:00<00:08, 44.77it/s][A
 11%|         | 48/435 [00:01<00:08, 44.71it/s][A
 12%|        | 53/435 [00:01<00:08, 44.71it/s][A
 13%|        | 58/435 [00:01<00:08, 44.84it/s][A
 14%|        | 63/435 [00:01<00:08, 44.93it/s][A
 16%|        | 68/435 [00:01<00:08, 44.94it/s][A
 17%|        | 73/435 [00:01<00:08, 44.84it/s][A
 18%|        | 78/435 [00:01<00:07, 44.79it/s][A
 19%|        | 83/435 [00:01<00:07, 44.70it/s][A
 20%|        | 88/435 [00:01<00:07, 44.43it/s][A
 21%|       | 93/435 [00:02<00:07, 44.53it/s][A
 23%|       | 98/435 [00:02<00:07, 44.57it/s][A
 24%|       | 103/435 [00:02<00:07, 44.67it/s][A
 25%|       | 108/435 [00:02<00:07, 44.80it/s][A
 26%|       | 113/435 [00:02<00:07, 44.91it/s][A
 27%|       | 118/435 [00:02<00:07, 44.95it/s][A
 28%|       | 123/435 [00:02<00:06, 44.89it/s][A
 29%|       | 128/435 [00:02<00:06, 44.74it/s][A
 31%|       | 133/435 [00:02<00:06, 44.62it/s][A
 32%|      | 138/435 [00:03<00:06, 44.41it/s][A
 33%|      | 143/435 [00:03<00:06, 43.36it/s][A
 34%|      | 148/435 [00:03<00:06, 43.94it/s][A
 35%|      | 153/435 [00:03<00:06, 44.29it/s][A
 36%|      | 158/435 [00:03<00:06, 44.47it/s][A
 37%|      | 163/435 [00:03<00:06, 44.60it/s][A
 39%|      | 168/435 [00:03<00:05, 44.73it/s][A
 40%|      | 173/435 [00:03<00:05, 44.60it/s][A
 41%|      | 178/435 [00:03<00:05, 44.66it/s][A
 42%|     | 183/435 [00:04<00:05, 44.34it/s][A
 43%|     | 188/435 [00:04<00:05, 44.45it/s][A
 44%|     | 193/435 [00:04<00:05, 44.65it/s][A
 46%|     | 198/435 [00:04<00:05, 44.75it/s][A
 47%|     | 203/435 [00:04<00:05, 44.76it/s][A
 48%|     | 208/435 [00:04<00:05, 44.86it/s][A
 49%|     | 213/435 [00:04<00:04, 44.70it/s][A
 50%|     | 218/435 [00:04<00:04, 44.67it/s][A
 51%|    | 223/435 [00:04<00:04, 44.63it/s][A
 52%|    | 228/435 [00:05<00:04, 44.44it/s][A
 54%|    | 233/435 [00:05<00:04, 44.60it/s][A
 55%|    | 238/435 [00:05<00:04, 44.67it/s][A
 56%|    | 243/435 [00:05<00:04, 44.67it/s][A
 57%|    | 248/435 [00:05<00:04, 44.60it/s][A
 58%|    | 253/435 [00:05<00:04, 44.73it/s][A
 59%|    | 258/435 [00:05<00:03, 44.65it/s][A
 60%|    | 263/435 [00:05<00:03, 44.56it/s][A
 62%|   | 268/435 [00:05<00:03, 44.58it/s][A
 63%|   | 273/435 [00:06<00:03, 44.45it/s][A
 64%|   | 278/435 [00:06<00:03, 43.81it/s][A
 65%|   | 283/435 [00:06<00:03, 44.02it/s][A
 66%|   | 288/435 [00:06<00:03, 44.40it/s][A
 67%|   | 293/435 [00:06<00:03, 44.45it/s][A
 69%|   | 298/435 [00:06<00:03, 44.63it/s][A
 70%|   | 303/435 [00:06<00:02, 44.65it/s][A
 71%|   | 308/435 [00:06<00:02, 44.61it/s][A
 72%|  | 313/435 [00:06<00:02, 44.51it/s][A
 73%|  | 318/435 [00:07<00:02, 44.27it/s][A
 74%|  | 323/435 [00:07<00:02, 44.48it/s][A
 75%|  | 328/435 [00:07<00:02, 44.66it/s][A
 77%|  | 333/435 [00:07<00:02, 44.79it/s][A
 78%|  | 338/435 [00:07<00:02, 44.89it/s][A
 79%|  | 343/435 [00:07<00:02, 44.74it/s][A
 80%|  | 348/435 [00:07<00:01, 44.81it/s][A
 81%|  | 353/435 [00:07<00:01, 44.81it/s][A
 82%| | 358/435 [00:07<00:01, 44.66it/s][A
 83%| | 363/435 [00:08<00:01, 44.55it/s][A
 85%| | 368/435 [00:08<00:01, 44.37it/s][A
 86%| | 373/435 [00:08<00:01, 44.62it/s][A
 87%| | 378/435 [00:08<00:01, 44.73it/s][A
 88%| | 383/435 [00:08<00:01, 44.78it/s][A
 89%| | 388/435 [00:08<00:01, 44.84it/s][A
 90%| | 393/435 [00:08<00:00, 44.73it/s][A
 91%|| 398/435 [00:08<00:00, 44.75it/s][A
 93%|| 403/435 [00:09<00:00, 44.77it/s][A
 94%|| 408/435 [00:09<00:00, 44.70it/s][A
 95%|| 413/435 [00:09<00:00, 44.31it/s][A
 96%|| 418/435 [00:09<00:00, 44.36it/s][A
 97%|| 423/435 [00:09<00:00, 44.57it/s][A
 98%|| 428/435 [00:09<00:00, 44.57it/s][A
100%|| 433/435 [00:09<00:00, 44.65it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.65it/s][A 80%|  | 624/780 [04:16<00:45,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:06:58,268 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 04:06:58,390 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:07:01,074 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:07:01,273 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:07:01,397 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:26<16:28,  6.38s/it] 80%|  | 626/780 [04:27<11:42,  4.56s/it] 80%|  | 627/780 [04:27<08:21,  3.28s/it] 81%|  | 628/780 [04:27<06:02,  2.38s/it] 81%|  | 629/780 [04:28<04:25,  1.76s/it] 81%|  | 630/780 [04:28<03:17,  1.32s/it] 81%|  | 631/780 [04:28<02:30,  1.01s/it] 81%|  | 632/780 [04:29<01:57,  1.26it/s] 81%|  | 633/780 [04:29<01:34,  1.55it/s] 81%| | 634/780 [04:29<01:18,  1.86it/s] 81%| | 635/780 [04:29<01:07,  2.16it/s] 82%| | 636/780 [04:30<00:59,  2.43it/s] 82%| | 637/780 [04:30<00:53,  2.67it/s] 82%| | 638/780 [04:30<00:49,  2.87it/s] 82%| | 639/780 [04:31<00:46,  3.02it/s] 82%| | 640/780 [04:31<00:45,  3.08it/s] 82%| | 641/780 [04:31<00:43,  3.19it/s] 82%| | 642/780 [04:31<00:42,  3.26it/s] 82%| | 643/780 [04:32<00:41,  3.32it/s] 83%| | 644/780 [04:32<00:40,  3.36it/s] 83%| | 645/780 [04:32<00:39,  3.39it/s] 83%| | 646/780 [04:33<00:39,  3.41it/s] 83%| | 647/780 [04:33<00:38,  3.42it/s] 83%| | 648/780 [04:33<00:38,  3.43it/s] 83%| | 649/780 [04:34<00:38,  3.44it/s] 83%| | 650/780 [04:34<00:37,  3.44it/s] 83%| | 651/780 [04:34<00:39,  3.30it/s] 84%| | 652/780 [04:34<00:38,  3.34it/s] 84%| | 653/780 [04:35<00:37,  3.38it/s] 84%| | 654/780 [04:35<00:37,  3.40it/s] 84%| | 655/780 [04:35<00:36,  3.41it/s] 84%| | 656/780 [04:36<00:36,  3.43it/s] 84%| | 657/780 [04:36<00:35,  3.44it/s] 84%| | 658/780 [04:36<00:35,  3.44it/s] 84%| | 659/780 [04:36<00:35,  3.45it/s] 85%| | 660/780 [04:37<00:34,  3.45it/s] 85%| | 661/780 [04:37<00:34,  3.45it/s] 85%| | 662/780 [04:37<00:35,  3.29it/s] 85%| | 663/780 [04:38<00:35,  3.34it/s] 85%| | 664/780 [04:38<00:34,  3.37it/s] 85%| | 665/780 [04:38<00:33,  3.39it/s] 85%| | 666/780 [04:39<00:33,  3.41it/s] 86%| | 667/780 [04:39<00:33,  3.42it/s] 86%| | 668/780 [04:39<00:34,  3.26it/s] 86%| | 669/780 [04:39<00:33,  3.30it/s] 86%| | 670/780 [04:40<00:32,  3.33it/s] 86%| | 671/780 [04:40<00:32,  3.35it/s] 86%| | 672/780 [04:40<00:33,  3.23it/s] 86%| | 673/780 [04:41<00:32,  3.28it/s] 86%| | 674/780 [04:41<00:31,  3.32it/s] 87%| | 675/780 [04:41<00:31,  3.34it/s] 87%| | 676/780 [04:42<00:30,  3.36it/s] 87%| | 677/780 [04:42<00:30,  3.37it/s] 87%| | 678/780 [04:42<00:30,  3.38it/s] 87%| | 679/780 [04:42<00:29,  3.39it/s] 87%| | 680/780 [04:43<00:29,  3.39it/s] 87%| | 681/780 [04:43<00:29,  3.39it/s] 87%| | 682/780 [04:43<00:29,  3.30it/s] 88%| | 683/780 [04:44<00:29,  3.24it/s] 88%| | 684/780 [04:44<00:29,  3.29it/s] 88%| | 685/780 [04:44<00:28,  3.32it/s] 88%| | 686/780 [04:45<00:28,  3.35it/s] 88%| | 687/780 [04:45<00:27,  3.36it/s] 88%| | 688/780 [04:45<00:27,  3.37it/s] 88%| | 689/780 [04:45<00:26,  3.38it/s] 88%| | 690/780 [04:46<00:26,  3.39it/s] 89%| | 691/780 [04:46<00:26,  3.39it/s] 89%| | 692/780 [04:46<00:25,  3.39it/s] 89%| | 693/780 [04:47<00:25,  3.40it/s] 89%| | 694/780 [04:47<00:25,  3.33it/s] 89%| | 695/780 [04:47<00:25,  3.35it/s] 89%| | 696/780 [04:48<00:24,  3.37it/s] 89%| | 697/780 [04:48<00:24,  3.38it/s] 89%| | 698/780 [04:48<00:24,  3.38it/s] 90%| | 699/780 [04:48<00:23,  3.39it/s] 90%| | 700/780 [04:49<00:23,  3.39it/s] 90%| | 701/780 [04:49<00:23,  3.39it/s] 90%| | 702/780 [04:49<00:23,  3.39it/s] 90%| | 703/780 [04:50<00:22,  3.39it/s] 90%| | 704/780 [04:50<00:22,  3.39it/s] 90%| | 705/780 [04:50<00:22,  3.28it/s] 91%| | 706/780 [04:50<00:22,  3.31it/s] 91%| | 707/780 [04:51<00:21,  3.34it/s] 91%| | 708/780 [04:51<00:21,  3.35it/s] 91%| | 709/780 [04:51<00:21,  3.37it/s] 91%| | 710/780 [04:52<00:20,  3.37it/s] 91%| | 711/780 [04:52<00:20,  3.38it/s] 91%|| 712/780 [04:52<00:20,  3.39it/s] 91%|| 713/780 [04:53<00:19,  3.38it/s] 92%|| 714/780 [04:53<00:19,  3.39it/s] 92%|| 715/780 [04:53<00:19,  3.39it/s] 92%|| 716/780 [04:53<00:19,  3.30it/s] 92%|| 717/780 [04:54<00:18,  3.32it/s] 92%|| 718/780 [04:54<00:18,  3.34it/s] 92%|| 719/780 [04:54<00:18,  3.35it/s] 92%|| 720/780 [04:55<00:17,  3.36it/s] 92%|| 721/780 [04:55<00:17,  3.38it/s] 93%|| 722/780 [04:55<00:17,  3.38it/s] 93%|| 723/780 [04:56<00:16,  3.38it/s] 93%|| 724/780 [04:56<00:16,  3.39it/s] 93%|| 725/780 [04:56<00:16,  3.39it/s] 93%|| 726/780 [04:56<00:15,  3.39it/s] 93%|| 727/780 [04:57<00:16,  3.25it/s] 93%|| 728/780 [04:57<00:15,  3.29it/s] 93%|| 729/780 [04:57<00:15,  3.32it/s] 94%|| 730/780 [04:58<00:14,  3.34it/s] 94%|| 731/780 [04:58<00:14,  3.36it/s] 94%|| 732/780 [04:58<00:14,  3.37it/s] 94%|| 733/780 [04:59<00:14,  3.18it/s] 94%|| 734/780 [04:59<00:14,  3.24it/s] 94%|| 735/780 [04:59<00:13,  3.28it/s] 94%|| 736/780 [04:59<00:13,  3.32it/s] 94%|| 737/780 [05:00<00:12,  3.34it/s] 95%|| 738/780 [05:00<00:12,  3.36it/s] 95%|| 739/780 [05:00<00:12,  3.37it/s] 95%|| 740/780 [05:01<00:11,  3.38it/s] 95%|| 741/780 [05:01<00:11,  3.38it/s] 95%|| 742/780 [05:01<00:11,  3.38it/s] 95%|| 743/780 [05:02<00:11,  3.30it/s] 95%|| 744/780 [05:02<00:10,  3.33it/s] 96%|| 745/780 [05:02<00:10,  3.35it/s] 96%|| 746/780 [05:02<00:10,  3.36it/s] 96%|| 747/780 [05:03<00:09,  3.37it/s] 96%|| 748/780 [05:03<00:09,  3.38it/s] 96%|| 749/780 [05:03<00:09,  3.38it/s] 96%|| 750/780 [05:04<00:08,  3.38it/s] 96%|| 751/780 [05:04<00:08,  3.39it/s] 96%|| 752/780 [05:04<00:08,  3.39it/s] 97%|| 753/780 [05:04<00:07,  3.40it/s] 97%|| 754/780 [05:05<00:07,  3.27it/s] 97%|| 755/780 [05:05<00:07,  3.31it/s] 97%|| 756/780 [05:05<00:07,  3.34it/s] 97%|| 757/780 [05:06<00:06,  3.35it/s] 97%|| 758/780 [05:06<00:06,  3.36it/s] 97%|| 759/780 [05:06<00:06,  3.37it/s] 97%|| 760/780 [05:07<00:05,  3.38it/s] 98%|| 761/780 [05:07<00:05,  3.38it/s] 98%|| 762/780 [05:07<00:05,  3.38it/s] 98%|| 763/780 [05:07<00:05,  3.39it/s] 98%|| 764/780 [05:08<00:04,  3.39it/s] 98%|| 765/780 [05:08<00:04,  3.30it/s] 98%|| 766/780 [05:08<00:04,  3.33it/s] 98%|| 767/780 [05:09<00:03,  3.35it/s] 98%|| 768/780 [05:09<00:03,  3.36it/s] 99%|| 769/780 [05:09<00:03,  3.37it/s] 99%|| 770/780 [05:10<00:02,  3.38it/s] 99%|| 771/780 [05:10<00:02,  3.38it/s] 99%|| 772/780 [05:10<00:02,  3.39it/s] 99%|| 773/780 [05:10<00:02,  3.39it/s] 99%|| 774/780 [05:11<00:01,  3.39it/s] 99%|| 775/780 [05:11<00:01,  3.39it/s] 99%|| 776/780 [05:11<00:01,  3.32it/s]100%|| 777/780 [05:12<00:00,  3.34it/s]100%|| 778/780 [05:12<00:00,  3.35it/s]100%|| 779/780 [05:12<00:00,  3.36it/s]100%|| 780/780 [05:13<00:00,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 04:07:54,872 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:07:54,873 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:07:54,873 >>   Batch size = 8
{'eval_loss': 0.9723892211914062, 'eval_runtime': 9.7574, 'eval_samples_per_second': 356.346, 'eval_steps_per_second': 44.582, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.56it/s][A
  3%|         | 12/435 [00:00<00:08, 48.68it/s][A
  4%|         | 17/435 [00:00<00:08, 47.22it/s][A
  5%|         | 22/435 [00:00<00:08, 46.38it/s][A
  6%|         | 27/435 [00:00<00:08, 45.90it/s][A
  7%|         | 32/435 [00:00<00:08, 45.38it/s][A
  9%|         | 37/435 [00:00<00:08, 44.98it/s][A
 10%|         | 42/435 [00:00<00:08, 44.74it/s][A
 11%|         | 47/435 [00:01<00:08, 44.69it/s][A
 12%|        | 52/435 [00:01<00:08, 44.80it/s][A
 13%|        | 57/435 [00:01<00:08, 44.88it/s][A
 14%|        | 62/435 [00:01<00:08, 44.88it/s][A
 15%|        | 67/435 [00:01<00:08, 44.96it/s][A
 17%|        | 72/435 [00:01<00:08, 44.90it/s][A
 18%|        | 77/435 [00:01<00:08, 43.62it/s][A
 19%|        | 82/435 [00:01<00:08, 43.87it/s][A
 20%|        | 87/435 [00:01<00:07, 43.96it/s][A
 21%|        | 92/435 [00:02<00:07, 44.20it/s][A
 22%|       | 97/435 [00:02<00:07, 44.36it/s][A
 23%|       | 102/435 [00:02<00:07, 44.55it/s][A
 25%|       | 107/435 [00:02<00:07, 44.66it/s][A
 26%|       | 112/435 [00:02<00:07, 44.75it/s][A
 27%|       | 117/435 [00:02<00:07, 44.63it/s][A
 28%|       | 122/435 [00:02<00:07, 44.63it/s][A
 29%|       | 127/435 [00:02<00:06, 44.56it/s][A
 30%|       | 132/435 [00:02<00:06, 44.46it/s][A
 31%|      | 137/435 [00:03<00:06, 44.58it/s][A
 33%|      | 142/435 [00:03<00:06, 44.62it/s][A
 34%|      | 147/435 [00:03<00:06, 44.80it/s][A
 35%|      | 152/435 [00:03<00:06, 44.72it/s][A
 36%|      | 157/435 [00:03<00:06, 44.80it/s][A
 37%|      | 162/435 [00:03<00:06, 44.87it/s][A
 38%|      | 167/435 [00:03<00:05, 44.81it/s][A
 40%|      | 172/435 [00:03<00:05, 44.64it/s][A
 41%|      | 177/435 [00:03<00:05, 44.59it/s][A
 42%|     | 182/435 [00:04<00:05, 44.63it/s][A
 43%|     | 187/435 [00:04<00:05, 44.68it/s][A
 44%|     | 192/435 [00:04<00:05, 44.72it/s][A
 45%|     | 197/435 [00:04<00:05, 44.78it/s][A
 46%|     | 202/435 [00:04<00:05, 44.78it/s][A
 48%|     | 207/435 [00:04<00:05, 44.79it/s][A
 49%|     | 212/435 [00:04<00:05, 42.84it/s][A
 50%|     | 217/435 [00:04<00:05, 43.53it/s][A
 51%|     | 222/435 [00:04<00:04, 43.94it/s][A
 52%|    | 227/435 [00:05<00:04, 44.12it/s][A
 53%|    | 232/435 [00:05<00:04, 44.23it/s][A
 54%|    | 237/435 [00:05<00:04, 44.34it/s][A
 56%|    | 242/435 [00:05<00:04, 44.52it/s][A
 57%|    | 247/435 [00:05<00:04, 44.62it/s][A
 58%|    | 252/435 [00:05<00:04, 44.45it/s][A
 59%|    | 257/435 [00:05<00:04, 44.46it/s][A
 60%|    | 262/435 [00:05<00:03, 44.50it/s][A
 61%|   | 267/435 [00:05<00:03, 44.69it/s][A
 63%|   | 272/435 [00:06<00:03, 44.70it/s][A
 64%|   | 277/435 [00:06<00:03, 44.71it/s][A
 65%|   | 282/435 [00:06<00:03, 44.78it/s][A
 66%|   | 287/435 [00:06<00:03, 44.62it/s][A
 67%|   | 292/435 [00:06<00:03, 44.72it/s][A
 68%|   | 297/435 [00:06<00:03, 44.68it/s][A
 69%|   | 302/435 [00:06<00:02, 44.64it/s][A
 71%|   | 307/435 [00:06<00:02, 44.55it/s][A
 72%|  | 312/435 [00:06<00:02, 44.58it/s][A
 73%|  | 317/435 [00:07<00:02, 44.62it/s][A
 74%|  | 322/435 [00:07<00:02, 44.67it/s][A
 75%|  | 327/435 [00:07<00:02, 44.78it/s][A
 76%|  | 332/435 [00:07<00:02, 44.86it/s][A
 77%|  | 337/435 [00:07<00:02, 44.80it/s][A
 79%|  | 342/435 [00:07<00:02, 44.75it/s][A
 80%|  | 347/435 [00:07<00:02, 43.08it/s][A
 81%|  | 352/435 [00:07<00:01, 43.71it/s][A
 82%| | 357/435 [00:07<00:01, 44.05it/s][A
 83%| | 362/435 [00:08<00:01, 44.32it/s][A
 84%| | 367/435 [00:08<00:01, 44.46it/s][A
 86%| | 372/435 [00:08<00:01, 44.53it/s][A
 87%| | 377/435 [00:08<00:01, 44.71it/s][A
 88%| | 382/435 [00:08<00:01, 44.66it/s][A
 89%| | 387/435 [00:08<00:01, 44.51it/s][A
 90%| | 392/435 [00:08<00:00, 44.45it/s][A
 91%|| 397/435 [00:08<00:00, 44.57it/s][A
 92%|| 402/435 [00:09<00:00, 44.57it/s][A
 94%|| 407/435 [00:09<00:00, 44.61it/s][A
 95%|| 412/435 [00:09<00:00, 44.72it/s][A
 96%|| 417/435 [00:09<00:00, 44.77it/s][A
 97%|| 422/435 [00:09<00:00, 44.80it/s][A
 98%|| 427/435 [00:09<00:00, 44.59it/s][A
 99%|| 432/435 [00:09<00:00, 44.55it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.55it/s][A100%|| 780/780 [05:22<00:00,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:08:04,815 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 04:08:04,966 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:08:07,459 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:08:07,606 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:08:07,683 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 04:08:13,765 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 04:08:13,765 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-156 (score: 0.9404196739196777).
                                                 100%|| 780/780 [05:42<00:00,  3.37it/s]100%|| 780/780 [05:42<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-29 04:08:24,849 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 04:08:25,097 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:08:28,677 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:08:28,919 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:08:29,021 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:08:29,413 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:29,414 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:29,414 >>   train_loss               =     0.6655
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:29,414 >>   train_runtime            = 0:05:42.93
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:29,414 >>   train_samples            =      10009
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:29,414 >>   train_samples_per_second =    145.932
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:29,414 >>   train_steps_per_second   =      2.274
{'eval_loss': 0.9753132462501526, 'eval_runtime': 9.7638, 'eval_samples_per_second': 356.11, 'eval_steps_per_second': 44.552, 'epoch': 5.0}
{'train_runtime': 342.9336, 'train_samples_per_second': 145.932, 'train_steps_per_second': 2.274, 'train_loss': 0.6654762854942908, 'epoch': 5.0}
08/29/2023 04:08:29 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 04:08:29,630 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:08:29,631 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:08:29,631 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 55.32it/s]  3%|         | 12/435 [00:00<00:08, 49.29it/s]  4%|         | 17/435 [00:00<00:08, 47.59it/s]  5%|         | 22/435 [00:00<00:08, 46.76it/s]  6%|         | 27/435 [00:00<00:08, 46.28it/s]  7%|         | 32/435 [00:00<00:08, 46.06it/s]  9%|         | 37/435 [00:00<00:08, 45.84it/s] 10%|         | 42/435 [00:00<00:08, 45.57it/s] 11%|         | 47/435 [00:01<00:08, 45.25it/s] 12%|        | 52/435 [00:01<00:08, 45.17it/s] 13%|        | 57/435 [00:01<00:08, 45.22it/s] 14%|        | 62/435 [00:01<00:08, 45.22it/s] 15%|        | 67/435 [00:01<00:08, 45.24it/s] 17%|        | 72/435 [00:01<00:08, 45.24it/s] 18%|        | 77/435 [00:01<00:07, 45.34it/s] 19%|        | 82/435 [00:01<00:07, 45.24it/s] 20%|        | 87/435 [00:01<00:07, 45.16it/s] 21%|        | 92/435 [00:02<00:07, 45.03it/s] 22%|       | 97/435 [00:02<00:07, 44.99it/s] 23%|       | 102/435 [00:02<00:07, 42.41it/s] 25%|       | 107/435 [00:02<00:07, 43.20it/s] 26%|       | 112/435 [00:02<00:07, 43.92it/s] 27%|       | 117/435 [00:02<00:07, 44.34it/s] 28%|       | 122/435 [00:02<00:07, 44.71it/s] 29%|       | 127/435 [00:02<00:06, 44.86it/s] 30%|       | 132/435 [00:02<00:06, 45.02it/s] 31%|      | 137/435 [00:03<00:06, 45.06it/s] 33%|      | 142/435 [00:03<00:06, 44.90it/s] 34%|      | 147/435 [00:03<00:06, 44.80it/s] 35%|      | 152/435 [00:03<00:06, 44.94it/s] 36%|      | 157/435 [00:03<00:06, 44.99it/s] 37%|      | 162/435 [00:03<00:06, 45.02it/s] 38%|      | 167/435 [00:03<00:05, 45.15it/s] 40%|      | 172/435 [00:03<00:05, 45.22it/s] 41%|      | 177/435 [00:03<00:05, 45.37it/s] 42%|     | 182/435 [00:04<00:05, 45.30it/s] 43%|     | 187/435 [00:04<00:05, 45.12it/s] 44%|     | 192/435 [00:04<00:05, 45.04it/s] 45%|     | 197/435 [00:04<00:05, 45.10it/s] 46%|     | 202/435 [00:04<00:05, 45.06it/s] 48%|     | 207/435 [00:04<00:05, 45.03it/s] 49%|     | 212/435 [00:04<00:04, 45.09it/s] 50%|     | 217/435 [00:04<00:04, 45.19it/s] 51%|     | 222/435 [00:04<00:04, 45.14it/s] 52%|    | 227/435 [00:05<00:04, 45.16it/s] 53%|    | 232/435 [00:05<00:04, 45.08it/s] 54%|    | 237/435 [00:05<00:04, 44.17it/s] 56%|    | 242/435 [00:05<00:04, 44.50it/s] 57%|    | 247/435 [00:05<00:04, 44.61it/s] 58%|    | 252/435 [00:05<00:04, 44.66it/s] 59%|    | 257/435 [00:05<00:03, 44.86it/s] 60%|    | 262/435 [00:05<00:03, 44.96it/s] 61%|   | 267/435 [00:05<00:03, 45.14it/s] 63%|   | 272/435 [00:06<00:03, 45.11it/s] 64%|   | 277/435 [00:06<00:03, 44.94it/s] 65%|   | 282/435 [00:06<00:03, 45.01it/s] 66%|   | 287/435 [00:06<00:03, 45.09it/s] 67%|   | 292/435 [00:06<00:03, 45.09it/s] 68%|   | 297/435 [00:06<00:03, 45.03it/s] 69%|   | 302/435 [00:06<00:02, 45.10it/s] 71%|   | 307/435 [00:06<00:02, 45.06it/s] 72%|  | 312/435 [00:06<00:02, 45.15it/s] 73%|  | 317/435 [00:07<00:02, 45.03it/s] 74%|  | 322/435 [00:07<00:02, 45.10it/s] 75%|  | 327/435 [00:07<00:02, 45.11it/s] 76%|  | 332/435 [00:07<00:02, 45.07it/s] 77%|  | 337/435 [00:07<00:02, 44.99it/s] 79%|  | 342/435 [00:07<00:02, 44.93it/s] 80%|  | 347/435 [00:07<00:01, 44.99it/s] 81%|  | 352/435 [00:07<00:01, 45.08it/s] 82%| | 357/435 [00:07<00:01, 45.15it/s] 83%| | 362/435 [00:08<00:01, 45.04it/s] 84%| | 367/435 [00:08<00:01, 45.10it/s] 86%| | 372/435 [00:08<00:01, 43.11it/s] 87%| | 377/435 [00:08<00:01, 43.86it/s] 88%| | 382/435 [00:08<00:01, 44.30it/s] 89%| | 387/435 [00:08<00:01, 44.52it/s] 90%| | 392/435 [00:08<00:00, 44.62it/s] 91%|| 397/435 [00:08<00:00, 44.70it/s] 92%|| 402/435 [00:08<00:00, 44.75it/s] 94%|| 407/435 [00:09<00:00, 44.76it/s] 95%|| 412/435 [00:09<00:00, 44.51it/s] 96%|| 417/435 [00:09<00:00, 44.77it/s] 97%|| 422/435 [00:09<00:00, 44.80it/s] 98%|| 427/435 [00:09<00:00, 44.99it/s] 99%|| 432/435 [00:09<00:00, 45.05it/s]100%|| 435/435 [00:09<00:00, 44.99it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:08:39,316 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:39,316 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:39,316 >>   eval_loss               =     0.9404
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:39,316 >>   eval_runtime            = 0:00:09.68
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:39,316 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:39,317 >>   eval_samples_per_second =    358.983
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:39,317 >>   eval_steps_per_second   =     44.912
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:08:39,317 >>   perplexity              =     2.5611
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:52,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:52,531 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:52,531 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:52,532 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:52,532 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:08:53,019 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:08:53,020 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:08:53,314 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:08:54,435 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:08:54,435 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:57,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:57,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:57,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:57,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:57,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:08:58,183 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:08:58,185 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:08:58,497 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:08:58,709 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:08:58,709 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.80it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.73it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.55it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:09,  1.58it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:18,  1.48it/s]Extractor Predicting: 31it [00:19,  1.47it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:20,  1.50it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:23,  1.56it/s]Extractor Predicting: 39it [00:24,  1.56it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:26,  1.62it/s]Extractor Predicting: 43it [00:27,  1.59it/s]Extractor Predicting: 44it [00:27,  1.59it/s]Extractor Predicting: 45it [00:28,  1.57it/s]Extractor Predicting: 46it [00:28,  1.59it/s]Extractor Predicting: 47it [00:29,  1.60it/s]Extractor Predicting: 48it [00:30,  1.58it/s]Extractor Predicting: 49it [00:30,  1.61it/s]Extractor Predicting: 50it [00:31,  1.66it/s]Extractor Predicting: 51it [00:31,  1.64it/s]Extractor Predicting: 52it [00:32,  1.60it/s]Extractor Predicting: 53it [00:33,  1.60it/s]Extractor Predicting: 54it [00:33,  1.62it/s]Extractor Predicting: 55it [00:34,  1.60it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:35,  1.58it/s]Extractor Predicting: 58it [00:36,  1.61it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:37,  1.66it/s]Extractor Predicting: 61it [00:38,  1.66it/s]Extractor Predicting: 62it [00:38,  1.68it/s]Extractor Predicting: 63it [00:39,  1.67it/s]Extractor Predicting: 64it [00:39,  1.66it/s]Extractor Predicting: 65it [00:40,  1.66it/s]Extractor Predicting: 66it [00:41,  1.64it/s]Extractor Predicting: 67it [00:41,  1.65it/s]Extractor Predicting: 68it [00:42,  1.65it/s]Extractor Predicting: 69it [00:42,  1.66it/s]Extractor Predicting: 70it [00:43,  1.65it/s]Extractor Predicting: 71it [00:44,  1.63it/s]Extractor Predicting: 72it [00:44,  1.66it/s]Extractor Predicting: 73it [00:45,  1.63it/s]Extractor Predicting: 74it [00:45,  1.65it/s]Extractor Predicting: 75it [00:46,  1.69it/s]Extractor Predicting: 76it [00:47,  1.66it/s]Extractor Predicting: 77it [00:47,  1.63it/s]Extractor Predicting: 78it [00:48,  1.60it/s]Extractor Predicting: 79it [00:49,  1.59it/s]Extractor Predicting: 80it [00:49,  1.58it/s]Extractor Predicting: 81it [00:50,  1.56it/s]Extractor Predicting: 82it [00:50,  1.60it/s]Extractor Predicting: 83it [00:51,  1.59it/s]Extractor Predicting: 84it [00:52,  1.60it/s]Extractor Predicting: 85it [00:52,  1.60it/s]Extractor Predicting: 86it [00:53,  1.62it/s]Extractor Predicting: 87it [00:54,  1.61it/s]Extractor Predicting: 88it [00:54,  1.58it/s]Extractor Predicting: 89it [00:55,  1.62it/s]Extractor Predicting: 90it [00:55,  1.63it/s]Extractor Predicting: 91it [00:56,  1.52it/s]Extractor Predicting: 92it [00:57,  1.59it/s]Extractor Predicting: 93it [00:57,  1.60it/s]Extractor Predicting: 94it [00:58,  1.59it/s]Extractor Predicting: 95it [00:59,  1.65it/s]Extractor Predicting: 96it [00:59,  1.65it/s]Extractor Predicting: 97it [01:00,  1.66it/s]Extractor Predicting: 98it [01:00,  1.65it/s]Extractor Predicting: 99it [01:01,  1.62it/s]Extractor Predicting: 100it [01:02,  1.59it/s]Extractor Predicting: 101it [01:02,  1.59it/s]Extractor Predicting: 102it [01:03,  1.66it/s]Extractor Predicting: 103it [01:03,  1.67it/s]Extractor Predicting: 104it [01:04,  1.67it/s]Extractor Predicting: 105it [01:05,  1.66it/s]Extractor Predicting: 106it [01:05,  1.65it/s]Extractor Predicting: 107it [01:06,  1.67it/s]Extractor Predicting: 108it [01:06,  1.64it/s]Extractor Predicting: 109it [01:07,  1.64it/s]Extractor Predicting: 110it [01:08,  1.65it/s]Extractor Predicting: 111it [01:08,  1.67it/s]Extractor Predicting: 112it [01:09,  1.66it/s]Extractor Predicting: 113it [01:09,  1.69it/s]Extractor Predicting: 114it [01:10,  1.72it/s]Extractor Predicting: 115it [01:11,  1.71it/s]Extractor Predicting: 116it [01:11,  1.71it/s]Extractor Predicting: 117it [01:12,  1.71it/s]Extractor Predicting: 118it [01:12,  1.69it/s]Extractor Predicting: 119it [01:13,  1.70it/s]Extractor Predicting: 120it [01:14,  1.66it/s]Extractor Predicting: 121it [01:14,  1.62it/s]Extractor Predicting: 122it [01:15,  1.64it/s]Extractor Predicting: 123it [01:15,  1.63it/s]Extractor Predicting: 124it [01:16,  1.61it/s]Extractor Predicting: 125it [01:17,  1.62it/s]Extractor Predicting: 126it [01:17,  1.64it/s]Extractor Predicting: 127it [01:18,  1.62it/s]Extractor Predicting: 128it [01:19,  1.60it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:20,  1.59it/s]Extractor Predicting: 131it [01:20,  1.60it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.59it/s]Extractor Predicting: 134it [01:22,  1.58it/s]Extractor Predicting: 135it [01:23,  1.59it/s]Extractor Predicting: 136it [01:24,  1.57it/s]Extractor Predicting: 137it [01:24,  1.62it/s]Extractor Predicting: 138it [01:25,  1.61it/s]Extractor Predicting: 139it [01:26,  1.56it/s]Extractor Predicting: 140it [01:26,  1.59it/s]Extractor Predicting: 141it [01:27,  1.61it/s]Extractor Predicting: 142it [01:27,  1.61it/s]Extractor Predicting: 143it [01:28,  1.64it/s]Extractor Predicting: 144it [01:29,  1.68it/s]Extractor Predicting: 144it [01:29,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:39,564 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:39,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:39,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:39,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:39,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:10:40,300 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:10:40,301 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:10:41,031 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:10:42,068 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:10:42,068 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:44,286 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:44,322 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:44,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:44,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:10:44,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:10:44,759 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:10:44,761 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:10:45,472 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:10:45,626 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:10:45,626 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.46164978292329956,
  "recall": 0.09174575783721599,
  "score": 0.15307101727447214,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.70it/s]Extractor Predicting: 8it [00:04,  1.73it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:08,  1.67it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:10,  1.69it/s]Extractor Predicting: 19it [00:11,  1.72it/s]Extractor Predicting: 20it [00:11,  1.65it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.69it/s]Extractor Predicting: 23it [00:13,  1.70it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:14,  1.65it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:16,  1.66it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:17,  1.68it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.73it/s]Extractor Predicting: 33it [00:19,  1.70it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:20,  1.66it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:22,  1.67it/s]Extractor Predicting: 38it [00:22,  1.69it/s]Extractor Predicting: 39it [00:23,  1.70it/s]Extractor Predicting: 40it [00:23,  1.69it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.70it/s]Extractor Predicting: 43it [00:25,  1.65it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:26,  1.65it/s]Extractor Predicting: 46it [00:27,  1.64it/s]Extractor Predicting: 47it [00:28,  1.64it/s]Extractor Predicting: 48it [00:28,  1.64it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:29,  1.68it/s]Extractor Predicting: 51it [00:30,  1.66it/s]Extractor Predicting: 52it [00:31,  1.64it/s]Extractor Predicting: 53it [00:31,  1.66it/s]Extractor Predicting: 54it [00:32,  1.64it/s]Extractor Predicting: 55it [00:32,  1.69it/s]Extractor Predicting: 56it [00:33,  1.67it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:34,  1.63it/s]Extractor Predicting: 59it [00:35,  1.57it/s]Extractor Predicting: 60it [00:36,  1.60it/s]Extractor Predicting: 61it [00:36,  1.60it/s]Extractor Predicting: 62it [00:37,  1.61it/s]Extractor Predicting: 63it [00:37,  1.64it/s]Extractor Predicting: 64it [00:38,  1.60it/s]Extractor Predicting: 65it [00:39,  1.67it/s]Extractor Predicting: 66it [00:39,  1.64it/s]Extractor Predicting: 67it [00:40,  1.62it/s]Extractor Predicting: 68it [00:40,  1.62it/s]Extractor Predicting: 69it [00:41,  1.58it/s]Extractor Predicting: 70it [00:42,  1.57it/s]Extractor Predicting: 71it [00:42,  1.60it/s]Extractor Predicting: 72it [00:43,  1.60it/s]Extractor Predicting: 73it [00:44,  1.58it/s]Extractor Predicting: 74it [00:44,  1.58it/s]Extractor Predicting: 75it [00:45,  1.59it/s]Extractor Predicting: 76it [00:46,  1.62it/s]Extractor Predicting: 77it [00:46,  1.62it/s]Extractor Predicting: 78it [00:47,  1.65it/s]Extractor Predicting: 79it [00:47,  1.63it/s]Extractor Predicting: 80it [00:48,  1.69it/s]Extractor Predicting: 81it [00:48,  1.70it/s]Extractor Predicting: 82it [00:49,  1.70it/s]Extractor Predicting: 83it [00:50,  1.68it/s]Extractor Predicting: 84it [00:50,  1.67it/s]Extractor Predicting: 85it [00:51,  1.61it/s]Extractor Predicting: 86it [00:52,  1.57it/s]Extractor Predicting: 87it [00:52,  1.55it/s]Extractor Predicting: 88it [00:53,  1.59it/s]Extractor Predicting: 89it [00:54,  1.58it/s]Extractor Predicting: 90it [00:54,  1.54it/s]Extractor Predicting: 91it [00:55,  1.54it/s]Extractor Predicting: 92it [00:55,  1.56it/s]Extractor Predicting: 93it [00:56,  1.42it/s]Extractor Predicting: 94it [00:57,  1.48it/s]Extractor Predicting: 95it [00:58,  1.49it/s]Extractor Predicting: 96it [00:58,  1.52it/s]Extractor Predicting: 97it [00:59,  1.51it/s]Extractor Predicting: 98it [01:00,  1.49it/s]Extractor Predicting: 99it [01:00,  1.51it/s]Extractor Predicting: 100it [01:01,  1.54it/s]Extractor Predicting: 101it [01:01,  1.56it/s]Extractor Predicting: 102it [01:02,  1.58it/s]Extractor Predicting: 103it [01:03,  1.56it/s]Extractor Predicting: 104it [01:03,  1.55it/s]Extractor Predicting: 105it [01:04,  1.58it/s]Extractor Predicting: 106it [01:05,  1.56it/s]Extractor Predicting: 107it [01:05,  1.56it/s]Extractor Predicting: 108it [01:06,  1.58it/s]Extractor Predicting: 109it [01:07,  1.57it/s]Extractor Predicting: 110it [01:07,  1.56it/s]Extractor Predicting: 111it [01:08,  1.58it/s]Extractor Predicting: 112it [01:08,  1.55it/s]Extractor Predicting: 113it [01:09,  1.56it/s]Extractor Predicting: 114it [01:10,  1.59it/s]Extractor Predicting: 115it [01:10,  1.58it/s]Extractor Predicting: 116it [01:11,  1.54it/s]Extractor Predicting: 117it [01:12,  1.52it/s]Extractor Predicting: 118it [01:12,  1.55it/s]Extractor Predicting: 119it [01:13,  1.54it/s]Extractor Predicting: 120it [01:14,  1.55it/s]Extractor Predicting: 121it [01:14,  1.57it/s]Extractor Predicting: 122it [01:15,  1.60it/s]Extractor Predicting: 123it [01:15,  1.60it/s]Extractor Predicting: 124it [01:16,  1.61it/s]Extractor Predicting: 125it [01:17,  1.60it/s]Extractor Predicting: 126it [01:17,  1.67it/s]Extractor Predicting: 127it [01:18,  1.67it/s]Extractor Predicting: 128it [01:19,  1.63it/s]Extractor Predicting: 129it [01:19,  1.68it/s]Extractor Predicting: 130it [01:20,  1.66it/s]Extractor Predicting: 131it [01:20,  1.65it/s]Extractor Predicting: 132it [01:21,  1.65it/s]Extractor Predicting: 133it [01:21,  1.70it/s]Extractor Predicting: 134it [01:22,  1.72it/s]Extractor Predicting: 135it [01:23,  1.66it/s]Extractor Predicting: 136it [01:23,  1.67it/s]Extractor Predicting: 137it [01:24,  1.69it/s]Extractor Predicting: 138it [01:24,  1.74it/s]Extractor Predicting: 139it [01:25,  1.72it/s]Extractor Predicting: 140it [01:26,  1.69it/s]Extractor Predicting: 141it [01:26,  1.67it/s]Extractor Predicting: 142it [01:27,  1.69it/s]Extractor Predicting: 143it [01:27,  1.66it/s]Extractor Predicting: 144it [01:28,  1.64it/s]Extractor Predicting: 145it [01:29,  1.64it/s]Extractor Predicting: 146it [01:29,  1.67it/s]Extractor Predicting: 147it [01:30,  1.72it/s]Extractor Predicting: 148it [01:30,  1.73it/s]Extractor Predicting: 149it [01:31,  1.73it/s]Extractor Predicting: 150it [01:31,  1.77it/s]Extractor Predicting: 151it [01:32,  1.76it/s]Extractor Predicting: 152it [01:33,  1.77it/s]Extractor Predicting: 153it [01:33,  1.76it/s]Extractor Predicting: 154it [01:34,  1.79it/s]Extractor Predicting: 155it [01:34,  1.77it/s]Extractor Predicting: 156it [01:35,  1.78it/s]Extractor Predicting: 157it [01:35,  1.81it/s]Extractor Predicting: 158it [01:36,  1.77it/s]Extractor Predicting: 159it [01:36,  1.84it/s]Extractor Predicting: 160it [01:37,  1.90it/s]Extractor Predicting: 161it [01:38,  1.83it/s]Extractor Predicting: 162it [01:38,  1.74it/s]Extractor Predicting: 163it [01:39,  1.74it/s]Extractor Predicting: 164it [01:39,  1.76it/s]Extractor Predicting: 165it [01:40,  1.80it/s]Extractor Predicting: 166it [01:40,  1.83it/s]Extractor Predicting: 167it [01:41,  1.82it/s]Extractor Predicting: 168it [01:41,  1.78it/s]Extractor Predicting: 169it [01:42,  1.82it/s]Extractor Predicting: 170it [01:43,  1.81it/s]Extractor Predicting: 171it [01:43,  1.85it/s]Extractor Predicting: 172it [01:44,  1.84it/s]Extractor Predicting: 173it [01:44,  1.84it/s]Extractor Predicting: 174it [01:45,  1.74it/s]Extractor Predicting: 175it [01:45,  1.73it/s]Extractor Predicting: 176it [01:46,  1.70it/s]Extractor Predicting: 177it [01:47,  1.67it/s]Extractor Predicting: 178it [01:47,  1.63it/s]Extractor Predicting: 179it [01:48,  1.62it/s]Extractor Predicting: 180it [01:48,  1.64it/s]Extractor Predicting: 181it [01:49,  1.63it/s]Extractor Predicting: 182it [01:50,  1.63it/s]Extractor Predicting: 183it [01:50,  1.65it/s]Extractor Predicting: 184it [01:51,  1.64it/s]Extractor Predicting: 185it [01:52,  1.64it/s]Extractor Predicting: 186it [01:52,  1.66it/s]Extractor Predicting: 187it [01:53,  1.65it/s]Extractor Predicting: 188it [01:53,  1.63it/s]Extractor Predicting: 189it [01:54,  1.64it/s]Extractor Predicting: 190it [01:55,  1.62it/s]Extractor Predicting: 191it [01:55,  1.60it/s]Extractor Predicting: 192it [01:56,  1.59it/s]Extractor Predicting: 193it [01:57,  1.57it/s]Extractor Predicting: 194it [01:57,  1.57it/s]Extractor Predicting: 195it [01:58,  1.59it/s]Extractor Predicting: 196it [01:58,  1.63it/s]Extractor Predicting: 197it [01:59,  1.64it/s]Extractor Predicting: 198it [02:00,  1.63it/s]Extractor Predicting: 199it [02:00,  1.65it/s]Extractor Predicting: 200it [02:01,  1.61it/s]Extractor Predicting: 201it [02:01,  1.59it/s]Extractor Predicting: 202it [02:02,  1.55it/s]Extractor Predicting: 203it [02:03,  1.53it/s]Extractor Predicting: 204it [02:04,  1.52it/s]Extractor Predicting: 205it [02:04,  1.53it/s]Extractor Predicting: 206it [02:05,  1.53it/s]Extractor Predicting: 207it [02:05,  1.55it/s]Extractor Predicting: 208it [02:06,  1.53it/s]Extractor Predicting: 209it [02:07,  1.48it/s]Extractor Predicting: 210it [02:08,  1.49it/s]Extractor Predicting: 211it [02:08,  1.33it/s]Extractor Predicting: 212it [02:09,  1.39it/s]Extractor Predicting: 213it [02:10,  1.42it/s]Extractor Predicting: 214it [02:10,  1.47it/s]Extractor Predicting: 215it [02:11,  1.45it/s]Extractor Predicting: 216it [02:12,  1.47it/s]Extractor Predicting: 217it [02:12,  1.48it/s]Extractor Predicting: 218it [02:13,  1.48it/s]Extractor Predicting: 219it [02:14,  1.48it/s]Extractor Predicting: 220it [02:14,  1.45it/s]Extractor Predicting: 221it [02:15,  1.45it/s]Extractor Predicting: 222it [02:16,  1.45it/s]Extractor Predicting: 223it [02:16,  1.49it/s]Extractor Predicting: 224it [02:17,  1.49it/s]Extractor Predicting: 225it [02:18,  1.50it/s]Extractor Predicting: 226it [02:19,  1.48it/s]Extractor Predicting: 227it [02:19,  1.50it/s]Extractor Predicting: 228it [02:20,  1.51it/s]Extractor Predicting: 229it [02:20,  1.53it/s]Extractor Predicting: 230it [02:21,  1.56it/s]Extractor Predicting: 231it [02:22,  1.60it/s]Extractor Predicting: 232it [02:22,  1.62it/s]Extractor Predicting: 233it [02:23,  1.65it/s]Extractor Predicting: 234it [02:23,  1.60it/s]Extractor Predicting: 235it [02:24,  1.63it/s]Extractor Predicting: 236it [02:25,  1.63it/s]Extractor Predicting: 237it [02:25,  1.63it/s]Extractor Predicting: 238it [02:26,  1.64it/s]Extractor Predicting: 239it [02:27,  1.61it/s]Extractor Predicting: 240it [02:27,  1.64it/s]Extractor Predicting: 241it [02:28,  1.66it/s]Extractor Predicting: 242it [02:28,  1.69it/s]Extractor Predicting: 243it [02:29,  1.68it/s]Extractor Predicting: 244it [02:29,  1.74it/s]Extractor Predicting: 245it [02:30,  1.69it/s]Extractor Predicting: 246it [02:31,  1.67it/s]Extractor Predicting: 247it [02:31,  1.64it/s]Extractor Predicting: 248it [02:32,  1.62it/s]Extractor Predicting: 249it [02:33,  1.64it/s]Extractor Predicting: 250it [02:33,  1.64it/s]Extractor Predicting: 251it [02:34,  1.66it/s]Extractor Predicting: 252it [02:34,  1.70it/s]Extractor Predicting: 253it [02:35,  1.67it/s]Extractor Predicting: 254it [02:36,  1.64it/s]Extractor Predicting: 255it [02:36,  1.64it/s]Extractor Predicting: 256it [02:37,  1.63it/s]Extractor Predicting: 257it [02:37,  1.56it/s]Extractor Predicting: 258it [02:38,  1.58it/s]Extractor Predicting: 259it [02:39,  1.59it/s]Extractor Predicting: 260it [02:39,  1.58it/s]Extractor Predicting: 261it [02:40,  1.59it/s]Extractor Predicting: 262it [02:41,  1.58it/s]Extractor Predicting: 263it [02:41,  1.62it/s]Extractor Predicting: 264it [02:42,  1.60it/s]Extractor Predicting: 265it [02:42,  1.61it/s]Extractor Predicting: 266it [02:43,  1.59it/s]Extractor Predicting: 267it [02:44,  1.56it/s]Extractor Predicting: 268it [02:44,  1.55it/s]Extractor Predicting: 269it [02:45,  1.57it/s]Extractor Predicting: 270it [02:46,  1.58it/s]Extractor Predicting: 271it [02:46,  1.59it/s]Extractor Predicting: 272it [02:47,  1.59it/s]Extractor Predicting: 273it [02:48,  1.56it/s]Extractor Predicting: 274it [02:48,  1.54it/s]Extractor Predicting: 275it [02:49,  1.56it/s]Extractor Predicting: 276it [02:50,  1.55it/s]Extractor Predicting: 277it [02:50,  1.53it/s]Extractor Predicting: 278it [02:51,  1.56it/s]Extractor Predicting: 279it [02:51,  1.55it/s]Extractor Predicting: 280it [02:52,  1.56it/s]Extractor Predicting: 281it [02:53,  1.59it/s]Extractor Predicting: 282it [02:53,  1.57it/s]Extractor Predicting: 283it [02:54,  1.55it/s]Extractor Predicting: 284it [02:55,  1.61it/s]Extractor Predicting: 285it [02:55,  1.58it/s]Extractor Predicting: 286it [02:56,  1.62it/s]Extractor Predicting: 287it [02:56,  1.62it/s]Extractor Predicting: 288it [02:57,  1.61it/s]Extractor Predicting: 289it [02:58,  1.60it/s]Extractor Predicting: 290it [02:58,  1.58it/s]Extractor Predicting: 291it [02:59,  1.53it/s]Extractor Predicting: 292it [03:00,  1.53it/s]Extractor Predicting: 293it [03:00,  1.56it/s]Extractor Predicting: 294it [03:01,  1.54it/s]Extractor Predicting: 295it [03:02,  1.56it/s]Extractor Predicting: 296it [03:02,  1.55it/s]Extractor Predicting: 297it [03:03,  1.58it/s]Extractor Predicting: 298it [03:04,  1.55it/s]Extractor Predicting: 299it [03:04,  1.54it/s]Extractor Predicting: 300it [03:05,  1.56it/s]Extractor Predicting: 301it [03:05,  1.55it/s]Extractor Predicting: 302it [03:06,  1.58it/s]Extractor Predicting: 303it [03:07,  1.53it/s]Extractor Predicting: 304it [03:07,  1.53it/s]Extractor Predicting: 305it [03:08,  1.54it/s]Extractor Predicting: 306it [03:09,  1.58it/s]Extractor Predicting: 307it [03:09,  1.57it/s]Extractor Predicting: 308it [03:10,  1.58it/s]Extractor Predicting: 309it [03:11,  1.58it/s]Extractor Predicting: 310it [03:11,  1.61it/s]Extractor Predicting: 311it [03:12,  1.59it/s]Extractor Predicting: 312it [03:12,  1.64it/s]Extractor Predicting: 313it [03:13,  1.66it/s]Extractor Predicting: 314it [03:14,  1.69it/s]Extractor Predicting: 315it [03:14,  1.71it/s]Extractor Predicting: 316it [03:15,  1.67it/s]Extractor Predicting: 317it [03:15,  1.64it/s]Extractor Predicting: 318it [03:16,  1.64it/s]Extractor Predicting: 319it [03:17,  1.61it/s]Extractor Predicting: 320it [03:17,  1.60it/s]Extractor Predicting: 321it [03:18,  1.42it/s]Extractor Predicting: 322it [03:19,  1.47it/s]Extractor Predicting: 323it [03:19,  1.47it/s]Extractor Predicting: 324it [03:20,  1.46it/s]Extractor Predicting: 325it [03:21,  1.49it/s]Extractor Predicting: 326it [03:21,  1.53it/s]Extractor Predicting: 327it [03:22,  1.57it/s]Extractor Predicting: 328it [03:23,  1.57it/s]Extractor Predicting: 329it [03:23,  1.59it/s]Extractor Predicting: 330it [03:24,  1.58it/s]Extractor Predicting: 331it [03:25,  1.57it/s]Extractor Predicting: 332it [03:25,  1.57it/s]Extractor Predicting: 333it [03:26,  1.60it/s]Extractor Predicting: 334it [03:26,  1.60it/s]Extractor Predicting: 335it [03:27,  1.61it/s]Extractor Predicting: 336it [03:28,  1.59it/s]Extractor Predicting: 337it [03:28,  1.59it/s]Extractor Predicting: 338it [03:29,  1.60it/s]Extractor Predicting: 339it [03:29,  1.64it/s]Extractor Predicting: 340it [03:30,  1.65it/s]Extractor Predicting: 341it [03:31,  1.65it/s]Extractor Predicting: 342it [03:31,  1.63it/s]Extractor Predicting: 343it [03:32,  1.62it/s]Extractor Predicting: 344it [03:33,  1.64it/s]Extractor Predicting: 345it [03:33,  1.60it/s]Extractor Predicting: 346it [03:34,  1.60it/s]Extractor Predicting: 347it [03:34,  1.60it/s]Extractor Predicting: 348it [03:35,  1.60it/s]Extractor Predicting: 349it [03:36,  1.63it/s]Extractor Predicting: 350it [03:36,  1.64it/s]Extractor Predicting: 351it [03:37,  1.65it/s]Extractor Predicting: 352it [03:38,  1.61it/s]Extractor Predicting: 353it [03:38,  1.63it/s]Extractor Predicting: 354it [03:39,  1.66it/s]Extractor Predicting: 355it [03:39,  1.66it/s]Extractor Predicting: 356it [03:40,  1.63it/s]Extractor Predicting: 357it [03:41,  1.64it/s]Extractor Predicting: 358it [03:41,  1.65it/s]Extractor Predicting: 359it [03:42,  1.64it/s]Extractor Predicting: 360it [03:42,  1.64it/s]Extractor Predicting: 361it [03:43,  1.65it/s]Extractor Predicting: 362it [03:44,  1.66it/s]Extractor Predicting: 363it [03:44,  1.64it/s]Extractor Predicting: 364it [03:45,  1.67it/s]Extractor Predicting: 365it [03:45,  1.68it/s]Extractor Predicting: 366it [03:46,  1.66it/s]Extractor Predicting: 367it [03:47,  1.61it/s]Extractor Predicting: 368it [03:47,  1.62it/s]Extractor Predicting: 369it [03:48,  1.62it/s]Extractor Predicting: 370it [03:48,  1.67it/s]Extractor Predicting: 371it [03:49,  1.68it/s]Extractor Predicting: 372it [03:50,  1.67it/s]Extractor Predicting: 373it [03:50,  1.63it/s]Extractor Predicting: 374it [03:51,  1.65it/s]Extractor Predicting: 375it [03:51,  1.66it/s]Extractor Predicting: 376it [03:52,  1.67it/s]Extractor Predicting: 377it [03:53,  1.68it/s]Extractor Predicting: 378it [03:53,  1.71it/s]Extractor Predicting: 379it [03:54,  1.66it/s]Extractor Predicting: 380it [03:54,  1.64it/s]Extractor Predicting: 381it [03:55,  1.66it/s]Extractor Predicting: 382it [03:56,  1.66it/s]Extractor Predicting: 383it [03:56,  1.69it/s]Extractor Predicting: 384it [03:57,  1.72it/s]Extractor Predicting: 385it [03:57,  1.68it/s]Extractor Predicting: 386it [03:58,  1.69it/s]Extractor Predicting: 387it [03:59,  1.68it/s]Extractor Predicting: 388it [03:59,  1.66it/s]Extractor Predicting: 389it [04:00,  1.66it/s]Extractor Predicting: 390it [04:00,  1.65it/s]Extractor Predicting: 391it [04:01,  1.64it/s]Extractor Predicting: 392it [04:02,  1.65it/s]Extractor Predicting: 393it [04:02,  1.68it/s]Extractor Predicting: 394it [04:03,  1.60it/s]Extractor Predicting: 395it [04:04,  1.55it/s]Extractor Predicting: 396it [04:04,  1.55it/s]Extractor Predicting: 397it [04:05,  1.52it/s]Extractor Predicting: 398it [04:06,  1.53it/s]Extractor Predicting: 399it [04:06,  1.53it/s]Extractor Predicting: 400it [04:07,  1.58it/s]Extractor Predicting: 401it [04:07,  1.55it/s]Extractor Predicting: 402it [04:08,  1.53it/s]Extractor Predicting: 403it [04:09,  1.54it/s]Extractor Predicting: 404it [04:09,  1.49it/s]Extractor Predicting: 405it [04:10,  1.49it/s]Extractor Predicting: 406it [04:11,  1.50it/s]Extractor Predicting: 407it [04:11,  1.49it/s]Extractor Predicting: 408it [04:12,  1.53it/s]Extractor Predicting: 409it [04:13,  1.55it/s]Extractor Predicting: 410it [04:13,  1.52it/s]Extractor Predicting: 411it [04:14,  1.53it/s]Extractor Predicting: 412it [04:15,  1.53it/s]Extractor Predicting: 413it [04:15,  1.56it/s]Extractor Predicting: 414it [04:16,  1.58it/s]Extractor Predicting: 415it [04:17,  1.62it/s]Extractor Predicting: 416it [04:17,  1.62it/s]Extractor Predicting: 417it [04:18,  1.62it/s]Extractor Predicting: 418it [04:18,  1.60it/s]Extractor Predicting: 419it [04:19,  1.54it/s]Extractor Predicting: 420it [04:20,  1.54it/s]Extractor Predicting: 421it [04:20,  1.63it/s]Extractor Predicting: 421it [04:20,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:18,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:18,123 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:18,123 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:18,123 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:18,123 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:15:18,745 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:15:18,746 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:15:19,353 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:15:20,426 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:15:20,426 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:23,425 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:23,427 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:23,427 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:23,427 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:15:23,427 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:15:24,063 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:15:24,064 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:15:24,637 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:15:24,828 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:15:24,828 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.23717948717948717,
  "recall": 0.05131253095591877,
  "score": 0.08437169150582295,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.95it/s]Extractor Predicting: 9it [00:05,  1.63it/s]
[INFO|configuration_utils.py:515] 2023-08-29 04:15:32,338 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:15:32,339 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:15:32,426 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:15:32,427 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 04:15:32,453 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:15:43,134 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 04:15:43,152 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 04:15:43,238 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:15:43,239 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:15:43,294 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:15:43,333 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:15:43,334 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:15:43,334 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:15:43,334 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:15:43,334 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:15:43,334 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3181818181818182,
  "recall": 0.01728395061728395,
  "score": 0.032786885245901634,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 04:15:43,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:44,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:44,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:45,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:45,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:46,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:47,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:47,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:48,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:48,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:49,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:50,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:50,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:51,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:51,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:52,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:53,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:53,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:54,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:54,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:55,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:55,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:56,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:13<04:13, 13.36s/it][WARNING|generation_utils.py:914] 2023-08-29 04:15:56,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:57,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:58,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:58,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:59,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:15:59,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:00,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:01,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:01,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:02,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:02,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:03,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:04,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:04,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:05,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:06,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:06,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:07,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:07,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:08,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:09,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:09,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:10,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:27<04:08, 13.79s/it][WARNING|generation_utils.py:914] 2023-08-29 04:16:11,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:11,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:12,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:12,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:13,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:13,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:14,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:15,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:15,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:16,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:16,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:17,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:18,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:18,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:19,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:19,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:20,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:21,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:21,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:22,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:22,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:39<03:42, 13.12s/it][WARNING|generation_utils.py:914] 2023-08-29 04:16:23,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:23,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:24,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:25,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:25,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:26,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:26,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:27,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:28,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:28,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:29,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:30,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:30,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:31,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:32,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:32,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:33,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:33,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:34,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:35,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:36,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [00:53<03:35, 13.49s/it][WARNING|generation_utils.py:914] 2023-08-29 04:16:37,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:38,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:38,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:39,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:39,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:40,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:41,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:41,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:42,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:42,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:43,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:44,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:44,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:45,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:45,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:46,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:47,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:47,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:48,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:48,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:49,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:06<03:17, 13.16s/it][WARNING|generation_utils.py:914] 2023-08-29 04:16:50,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:50,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:51,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:51,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:52,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:53,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:53,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:54,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:54,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:55,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:56,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:57,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:57,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:58,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:58,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:16:59,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:00,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:00,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:01,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:02,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:02,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:19<03:04, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-29 04:17:03,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:03,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:04,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:05,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:05,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:06,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:07,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:07,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:08,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:08,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:09,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:10,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:10,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:11,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:12,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:13,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:13,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:14,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:14,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:15,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:16,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:16,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:33<02:56, 13.56s/it][WARNING|generation_utils.py:914] 2023-08-29 04:17:17,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:18,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:18,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:19,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:20,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:20,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:21,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:21,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:22,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:23,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:23,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:24,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:24,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:25,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:26,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:26,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:27,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:27,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:28,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:29,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:29,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:30,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [01:47<02:43, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-29 04:17:31,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:31,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:32,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:33,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:33,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:34,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:35,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:35,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:36,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:36,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:37,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:38,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:38,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:39,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:39,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:40,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:41,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:41,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:42,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:43,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:43,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:00<02:27, 13.45s/it][WARNING|generation_utils.py:914] 2023-08-29 04:17:44,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:44,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:45,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:46,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:46,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:47,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:48,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:48,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:49,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:50,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:50,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:51,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:51,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:52,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:53,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:53,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:54,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:55,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:55,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:56,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:56,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:14<02:13, 13.40s/it][WARNING|generation_utils.py:914] 2023-08-29 04:17:57,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:58,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:58,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:17:59,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:00,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:00,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:01,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:02,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:02,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:03,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:03,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:04,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:05,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:05,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:06,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:06,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:07,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:08,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:08,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:09,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:26<01:58, 13.16s/it][WARNING|generation_utils.py:914] 2023-08-29 04:18:10,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:10,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:11,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:12,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:12,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:13,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:14,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:14,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:15,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:16,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:16,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:17,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:17,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:18,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:19,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:19,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:20,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:21,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:21,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:22,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:23,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [02:40<01:46, 13.28s/it][WARNING|generation_utils.py:914] 2023-08-29 04:18:23,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:24,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:25,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:26,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:26,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:27,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:27,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:28,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:29,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:29,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:30,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:31,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:31,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:32,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:33,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:33,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:34,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:35,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:35,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:36,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:37,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [02:54<01:35, 13.59s/it][WARNING|generation_utils.py:914] 2023-08-29 04:18:38,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:38,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:39,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:39,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:40,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:40,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:41,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:41,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:42,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:42,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:43,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:43,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:44,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:44,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:45,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:46,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:46,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:47,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:47,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:48,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:05<01:16, 12.67s/it][WARNING|generation_utils.py:914] 2023-08-29 04:18:48,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:49,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:50,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:50,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:51,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:52,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:52,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:53,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:54,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:54,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:55,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:56,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:56,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:57,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:58,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:58,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:59,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:59,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:00,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:01,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:01,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [03:18<01:04, 12.93s/it][WARNING|generation_utils.py:914] 2023-08-29 04:19:02,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:02,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:03,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:03,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:04,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:05,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:05,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:06,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:06,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:07,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:08,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:08,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:09,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:09,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:10,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:11,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:11,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:12,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:12,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:13,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:13,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [03:30<00:50, 12.71s/it][WARNING|generation_utils.py:914] 2023-08-29 04:19:14,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:14,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:15,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:15,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:16,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:17,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:17,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:18,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:18,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:19,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:19,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:20,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:20,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:21,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:21,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:22,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:22,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:23,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:24,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:24,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:25,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [03:42<00:37, 12.40s/it][WARNING|generation_utils.py:914] 2023-08-29 04:19:26,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:26,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:27,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:28,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:28,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:29,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:29,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:30,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:31,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:31,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:32,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:32,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:33,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:33,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:34,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:35,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:35,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:36,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:36,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:37,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:38,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:38,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [03:55<00:25, 12.63s/it][WARNING|generation_utils.py:914] 2023-08-29 04:19:39,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:39,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:40,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:41,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:41,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:42,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:42,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:43,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:44,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:44,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:45,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:45,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:46,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:47,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:47,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:48,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:48,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:49,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:50,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:50,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:51,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [04:08<00:12, 12.64s/it][WARNING|generation_utils.py:914] 2023-08-29 04:19:51,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:52,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:53,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:53,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:54,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:55,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:55,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:56,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:56,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:57,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:58,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:59,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:59,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:00,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:00,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:01,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:02,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:03,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:03,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:04,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:04,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:05,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [04:22<00:00, 13.17s/it]Generating: 100%|| 20/20 [04:22<00:00, 13.13s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:14,394 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:14,414 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:14,414 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:14,414 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:14,414 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:20:15,054 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:20:15,055 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:20:15,649 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:20:16,730 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:20:16,730 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:20,095 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:20,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:20,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:20,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:20:20,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:20:20,768 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:20:20,769 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:20:21,337 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:20:21,505 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:20:21,505 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Pristina and Daralu could continue working together as co - prime ministers until 2018 , when they could resign for failing to meet the constitutionality of their former government . Head Entity : Daralu , Tail Entity : Turkish .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8206521739130435, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8274456521739131, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : military branch .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8821022727272727, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : crosses .', 'success_rate': 0.8764204545454546, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.946875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9047619047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : Later in the year ( 17901814 ) he married Brigadier John Baskin s sister Emma ( born 1793 ) . Head Entity : Emma ( , Tail Entity : Brigadier ) , Tail Entity : Revolution .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8958333333333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : participant . Context : On 31 March 2014 , the Brazilian national squad played a friendly at Euro 2012 in Baku in which they won 20 . Head Entity : Brazil national squad , Tail Entity : Euro 2012 .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : participant .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9211309523809523, 'errors': {'', '(\'Star Wars Battlefront II\', \'platform\', \'\', \'The successor to " Star Wars Battlefront II " was announced in 2017 and is supported by the original trilogy expansion , " Star Wars Battlefront II - Imperial Assault " .\')'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8892045454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 13184
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13284, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.63it/s]Extractor Estimating: 2it [00:01,  1.45it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.65it/s]Extractor Estimating: 5it [00:03,  1.66it/s]Extractor Estimating: 6it [00:03,  1.65it/s]Extractor Estimating: 7it [00:04,  1.71it/s]Extractor Estimating: 8it [00:04,  1.73it/s]Extractor Estimating: 9it [00:05,  1.74it/s]Extractor Estimating: 10it [00:05,  1.70it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:07,  1.56it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.63it/s]Extractor Estimating: 16it [00:09,  1.66it/s]Extractor Estimating: 17it [00:10,  1.67it/s]Extractor Estimating: 18it [00:11,  1.56it/s]Extractor Estimating: 19it [00:11,  1.59it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:12,  1.61it/s]Extractor Estimating: 22it [00:13,  1.67it/s]Extractor Estimating: 23it [00:14,  1.70it/s]Extractor Estimating: 24it [00:14,  1.69it/s]Extractor Estimating: 25it [00:15,  1.72it/s]Extractor Estimating: 26it [00:15,  1.63it/s]Extractor Estimating: 27it [00:16,  1.59it/s]Extractor Estimating: 28it [00:17,  1.59it/s]Extractor Estimating: 29it [00:17,  1.60it/s]Extractor Estimating: 30it [00:18,  1.47it/s]Extractor Estimating: 31it [00:19,  1.51it/s]Extractor Estimating: 32it [00:19,  1.53it/s]Extractor Estimating: 33it [00:20,  1.51it/s]Extractor Estimating: 34it [00:21,  1.50it/s]Extractor Estimating: 35it [00:21,  1.52it/s]Extractor Estimating: 36it [00:22,  1.55it/s]Extractor Estimating: 37it [00:23,  1.52it/s]Extractor Estimating: 38it [00:23,  1.49it/s]Extractor Estimating: 39it [00:24,  1.51it/s]Extractor Estimating: 40it [00:25,  1.48it/s]Extractor Estimating: 41it [00:25,  1.47it/s]Extractor Estimating: 42it [00:26,  1.53it/s]Extractor Estimating: 43it [00:27,  1.51it/s]Extractor Estimating: 44it [00:27,  1.53it/s]Extractor Estimating: 45it [00:28,  1.51it/s]Extractor Estimating: 46it [00:29,  1.50it/s]Extractor Estimating: 47it [00:29,  1.44it/s]Extractor Estimating: 48it [00:30,  1.45it/s]Extractor Estimating: 49it [00:31,  1.48it/s]Extractor Estimating: 50it [00:31,  1.46it/s]Extractor Estimating: 51it [00:32,  1.55it/s]Extractor Estimating: 52it [00:33,  1.56it/s]Extractor Estimating: 53it [00:33,  1.56it/s]Extractor Estimating: 54it [00:34,  1.59it/s]Extractor Estimating: 55it [00:34,  1.62it/s]Extractor Estimating: 56it [00:35,  1.66it/s]Extractor Estimating: 57it [00:36,  1.70it/s]Extractor Estimating: 58it [00:36,  1.69it/s]Extractor Estimating: 59it [00:37,  1.77it/s]Extractor Estimating: 60it [00:37,  1.73it/s]Extractor Estimating: 61it [00:38,  1.62it/s]Extractor Estimating: 62it [00:39,  1.63it/s]Extractor Estimating: 63it [00:39,  1.64it/s]Extractor Estimating: 64it [00:40,  1.62it/s]Extractor Estimating: 65it [00:40,  1.62it/s]Extractor Estimating: 66it [00:41,  1.68it/s]Extractor Estimating: 67it [00:42,  1.69it/s]Extractor Estimating: 68it [00:42,  1.64it/s]Extractor Estimating: 69it [00:43,  1.67it/s]Extractor Estimating: 70it [00:43,  1.66it/s]Extractor Estimating: 71it [00:44,  1.75it/s]Extractor Estimating: 72it [00:45,  1.65it/s]Extractor Estimating: 73it [00:45,  1.64it/s]Extractor Estimating: 74it [00:46,  1.64it/s]Extractor Estimating: 75it [00:46,  1.66it/s]Extractor Estimating: 76it [00:47,  1.67it/s]Extractor Estimating: 77it [00:48,  1.63it/s]Extractor Estimating: 78it [00:48,  1.60it/s]Extractor Estimating: 79it [00:49,  1.62it/s]Extractor Estimating: 80it [00:50,  1.61it/s]Extractor Estimating: 81it [00:50,  1.42it/s]Extractor Estimating: 82it [00:51,  1.46it/s]Extractor Estimating: 83it [00:52,  1.49it/s]Extractor Estimating: 84it [00:52,  1.54it/s]Extractor Estimating: 85it [00:53,  1.56it/s]Extractor Estimating: 86it [00:54,  1.53it/s]Extractor Estimating: 87it [00:54,  1.50it/s]Extractor Estimating: 88it [00:55,  1.54it/s]Extractor Estimating: 89it [00:56,  1.51it/s]Extractor Estimating: 90it [00:56,  1.48it/s]Extractor Estimating: 91it [00:57,  1.47it/s]Extractor Estimating: 92it [00:58,  1.57it/s]Extractor Estimating: 93it [00:58,  1.52it/s]Extractor Estimating: 94it [00:59,  1.55it/s]Extractor Estimating: 95it [01:00,  1.49it/s]Extractor Estimating: 96it [01:00,  1.51it/s]Extractor Estimating: 97it [01:01,  1.44it/s]Extractor Estimating: 98it [01:02,  1.45it/s]Extractor Estimating: 99it [01:03,  1.34it/s]Extractor Estimating: 100it [01:03,  1.38it/s]Extractor Estimating: 101it [01:04,  1.47it/s]Extractor Estimating: 102it [01:04,  1.54it/s]Extractor Estimating: 103it [01:05,  1.54it/s]Extractor Estimating: 104it [01:06,  1.63it/s]Extractor Estimating: 105it [01:06,  1.61it/s]Extractor Estimating: 106it [01:07,  1.63it/s]Extractor Estimating: 107it [01:07,  1.66it/s]Extractor Estimating: 108it [01:08,  1.67it/s]Extractor Estimating: 109it [01:09,  1.64it/s]Extractor Estimating: 110it [01:09,  1.61it/s]Extractor Estimating: 111it [01:10,  1.60it/s]Extractor Estimating: 112it [01:10,  1.66it/s]Extractor Estimating: 113it [01:11,  1.68it/s]Extractor Estimating: 114it [01:12,  1.71it/s]Extractor Estimating: 115it [01:12,  1.74it/s]Extractor Estimating: 116it [01:13,  1.74it/s]Extractor Estimating: 117it [01:13,  1.77it/s]Extractor Estimating: 118it [01:14,  1.71it/s]Extractor Estimating: 119it [01:14,  1.71it/s]Extractor Estimating: 120it [01:15,  1.72it/s]Extractor Estimating: 121it [01:16,  1.71it/s]Extractor Estimating: 122it [01:16,  1.72it/s]Extractor Estimating: 123it [01:17,  1.75it/s]Extractor Estimating: 124it [01:17,  1.71it/s]Extractor Estimating: 125it [01:18,  1.59it/s]Extractor Estimating: 126it [01:19,  1.59it/s]Extractor Estimating: 127it [01:19,  1.51it/s]Extractor Estimating: 128it [01:20,  1.54it/s]Extractor Estimating: 129it [01:21,  1.54it/s]Extractor Estimating: 130it [01:21,  1.54it/s]Extractor Estimating: 131it [01:22,  1.49it/s]Extractor Estimating: 132it [01:23,  1.49it/s]Extractor Estimating: 133it [01:23,  1.49it/s]Extractor Estimating: 134it [01:24,  1.45it/s]Extractor Estimating: 135it [01:25,  1.44it/s]Extractor Estimating: 136it [01:26,  1.42it/s]Extractor Estimating: 137it [01:26,  1.42it/s]Extractor Estimating: 138it [01:27,  1.41it/s]Extractor Estimating: 139it [01:28,  1.41it/s]Extractor Estimating: 140it [01:28,  1.45it/s]Extractor Estimating: 141it [01:29,  1.47it/s]Extractor Estimating: 142it [01:30,  1.48it/s]Extractor Estimating: 143it [01:30,  1.47it/s]Extractor Estimating: 144it [01:31,  1.46it/s]Extractor Estimating: 145it [01:32,  1.43it/s]Extractor Estimating: 146it [01:32,  1.48it/s]Extractor Estimating: 147it [01:33,  1.47it/s]Extractor Estimating: 148it [01:34,  1.42it/s]Extractor Estimating: 149it [01:35,  1.40it/s]Extractor Estimating: 150it [01:35,  1.41it/s]Extractor Estimating: 151it [01:36,  1.47it/s]Extractor Estimating: 152it [01:37,  1.51it/s]Extractor Estimating: 153it [01:37,  1.51it/s]Extractor Estimating: 154it [01:38,  1.53it/s]Extractor Estimating: 155it [01:38,  1.61it/s]Extractor Estimating: 156it [01:39,  1.66it/s]Extractor Estimating: 157it [01:40,  1.57it/s]Extractor Estimating: 158it [01:40,  1.64it/s]Extractor Estimating: 159it [01:41,  1.65it/s]Extractor Estimating: 160it [01:41,  1.66it/s]Extractor Estimating: 161it [01:42,  1.71it/s]Extractor Estimating: 162it [01:43,  1.68it/s]Extractor Estimating: 163it [01:43,  1.52it/s]Extractor Estimating: 164it [01:44,  1.56it/s]Extractor Estimating: 165it [01:45,  1.46it/s]Extractor Estimating: 166it [01:45,  1.51it/s]Extractor Estimating: 167it [01:46,  1.55it/s]Extractor Estimating: 168it [01:47,  1.59it/s]Extractor Estimating: 169it [01:47,  1.67it/s]Extractor Estimating: 170it [01:48,  1.70it/s]Extractor Estimating: 171it [01:48,  1.74it/s]Extractor Estimating: 172it [01:49,  1.73it/s]Extractor Estimating: 173it [01:49,  1.75it/s]Extractor Estimating: 174it [01:50,  1.66it/s]Extractor Estimating: 175it [01:51,  1.63it/s]Extractor Estimating: 176it [01:51,  1.65it/s]Extractor Estimating: 177it [01:52,  1.69it/s]Extractor Estimating: 178it [01:52,  1.72it/s]Extractor Estimating: 179it [01:53,  1.66it/s]Extractor Estimating: 180it [01:54,  1.64it/s]Extractor Estimating: 181it [01:54,  1.68it/s]Extractor Estimating: 182it [01:55,  1.60it/s]Extractor Estimating: 183it [01:55,  1.63it/s]Extractor Estimating: 184it [01:56,  1.60it/s]Extractor Estimating: 185it [01:57,  1.54it/s]Extractor Estimating: 186it [01:57,  1.58it/s]Extractor Estimating: 187it [01:58,  1.60it/s]Extractor Estimating: 188it [01:59,  1.61it/s]Extractor Estimating: 189it [01:59,  1.61it/s]Extractor Estimating: 190it [02:00,  1.54it/s]Extractor Estimating: 191it [02:01,  1.58it/s]Extractor Estimating: 192it [02:01,  1.56it/s]Extractor Estimating: 193it [02:02,  1.59it/s]Extractor Estimating: 194it [02:03,  1.57it/s]Extractor Estimating: 195it [02:03,  1.59it/s]Extractor Estimating: 196it [02:04,  1.59it/s]Extractor Estimating: 197it [02:05,  1.50it/s]Extractor Estimating: 198it [02:05,  1.54it/s]Extractor Estimating: 199it [02:06,  1.54it/s]Extractor Estimating: 200it [02:06,  1.53it/s]Extractor Estimating: 201it [02:07,  1.51it/s]Extractor Estimating: 202it [02:08,  1.49it/s]Extractor Estimating: 203it [02:08,  1.52it/s]Extractor Estimating: 204it [02:09,  1.55it/s]Extractor Estimating: 205it [02:10,  1.50it/s]Extractor Estimating: 206it [02:10,  1.52it/s]Extractor Estimating: 207it [02:11,  1.52it/s]Extractor Estimating: 208it [02:12,  1.51it/s]Extractor Estimating: 209it [02:12,  1.49it/s]Extractor Estimating: 210it [02:13,  1.49it/s]Extractor Estimating: 211it [02:14,  1.47it/s]Extractor Estimating: 212it [02:14,  1.47it/s]Extractor Estimating: 213it [02:15,  1.48it/s]Extractor Estimating: 214it [02:16,  1.47it/s]Extractor Estimating: 215it [02:17,  1.47it/s]Extractor Estimating: 216it [02:17,  1.50it/s]Extractor Estimating: 217it [02:18,  1.52it/s]Extractor Estimating: 218it [02:18,  1.54it/s]Extractor Estimating: 219it [02:19,  1.54it/s]Extractor Estimating: 220it [02:20,  1.51it/s]Extractor Estimating: 221it [02:20,  1.50it/s]Extractor Estimating: 222it [02:21,  1.58it/s]Extractor Estimating: 223it [02:22,  1.58it/s]Extractor Estimating: 224it [02:22,  1.60it/s]Extractor Estimating: 225it [02:23,  1.59it/s]Extractor Estimating: 226it [02:23,  1.64it/s]Extractor Estimating: 227it [02:24,  1.59it/s]Extractor Estimating: 228it [02:25,  1.58it/s]Extractor Estimating: 229it [02:25,  1.54it/s]Extractor Estimating: 230it [02:26,  1.51it/s]Extractor Estimating: 231it [02:27,  1.56it/s]Extractor Estimating: 232it [02:27,  1.59it/s]Extractor Estimating: 233it [02:28,  1.61it/s]Extractor Estimating: 234it [02:29,  1.54it/s]Extractor Estimating: 235it [02:29,  1.49it/s]Extractor Estimating: 236it [02:30,  1.53it/s]Extractor Estimating: 237it [02:31,  1.55it/s]Extractor Estimating: 238it [02:31,  1.49it/s]Extractor Estimating: 239it [02:32,  1.52it/s]Extractor Estimating: 240it [02:33,  1.57it/s]Extractor Estimating: 241it [02:33,  1.58it/s]Extractor Estimating: 242it [02:34,  1.54it/s]Extractor Estimating: 243it [02:34,  1.56it/s]Extractor Estimating: 244it [02:35,  1.49it/s]Extractor Estimating: 245it [02:36,  1.51it/s]Extractor Estimating: 246it [02:37,  1.51it/s]Extractor Estimating: 247it [02:37,  1.54it/s]Extractor Estimating: 248it [02:38,  1.52it/s]Extractor Estimating: 249it [02:38,  1.55it/s]Extractor Estimating: 250it [02:39,  1.54it/s]Extractor Estimating: 251it [02:40,  1.55it/s]Extractor Estimating: 252it [02:40,  1.50it/s]Extractor Estimating: 253it [02:41,  1.35it/s]Extractor Estimating: 254it [02:42,  1.41it/s]Extractor Estimating: 255it [02:43,  1.42it/s]Extractor Estimating: 256it [02:43,  1.46it/s]Extractor Estimating: 257it [02:44,  1.46it/s]Extractor Estimating: 258it [02:45,  1.50it/s]Extractor Estimating: 259it [02:45,  1.50it/s]Extractor Estimating: 260it [02:46,  1.49it/s]Extractor Estimating: 261it [02:47,  1.52it/s]Extractor Estimating: 262it [02:47,  1.55it/s]Extractor Estimating: 263it [02:48,  1.56it/s]Extractor Estimating: 264it [02:48,  1.56it/s]Extractor Estimating: 265it [02:49,  1.54it/s]Extractor Estimating: 266it [02:50,  1.49it/s]Extractor Estimating: 267it [02:51,  1.46it/s]Extractor Estimating: 268it [02:51,  1.46it/s]Extractor Estimating: 269it [02:52,  1.49it/s]Extractor Estimating: 270it [02:53,  1.50it/s]Extractor Estimating: 271it [02:53,  1.51it/s]Extractor Estimating: 272it [02:54,  1.49it/s]Extractor Estimating: 273it [02:55,  1.48it/s]Extractor Estimating: 274it [02:55,  1.44it/s]Extractor Estimating: 275it [02:56,  1.46it/s]Extractor Estimating: 276it [02:57,  1.51it/s]Extractor Estimating: 277it [02:57,  1.53it/s]Extractor Estimating: 278it [02:58,  1.51it/s]Extractor Estimating: 279it [02:59,  1.51it/s]Extractor Estimating: 280it [02:59,  1.54it/s]Extractor Estimating: 281it [03:00,  1.60it/s]Extractor Estimating: 282it [03:00,  1.60it/s]Extractor Estimating: 283it [03:01,  1.64it/s]Extractor Estimating: 284it [03:02,  1.63it/s]Extractor Estimating: 285it [03:02,  1.54it/s]Extractor Estimating: 286it [03:03,  1.58it/s]Extractor Estimating: 287it [03:04,  1.63it/s]Extractor Estimating: 288it [03:04,  1.65it/s]Extractor Estimating: 289it [03:05,  1.68it/s]Extractor Estimating: 290it [03:05,  1.65it/s]Extractor Estimating: 291it [03:06,  1.59it/s]Extractor Estimating: 292it [03:07,  1.57it/s]Extractor Estimating: 293it [03:07,  1.59it/s]Extractor Estimating: 294it [03:08,  1.61it/s]Extractor Estimating: 295it [03:08,  1.63it/s]Extractor Estimating: 296it [03:09,  1.68it/s]Extractor Estimating: 297it [03:10,  1.65it/s]Extractor Estimating: 298it [03:10,  1.63it/s]Extractor Estimating: 299it [03:11,  1.55it/s]Extractor Estimating: 300it [03:12,  1.52it/s]Extractor Estimating: 301it [03:12,  1.53it/s]Extractor Estimating: 302it [03:13,  1.47it/s]Extractor Estimating: 303it [03:14,  1.46it/s]Extractor Estimating: 304it [03:14,  1.49it/s]Extractor Estimating: 305it [03:15,  1.51it/s]Extractor Estimating: 306it [03:16,  1.51it/s]Extractor Estimating: 307it [03:16,  1.51it/s]Extractor Estimating: 308it [03:17,  1.56it/s]Extractor Estimating: 309it [03:18,  1.53it/s]Extractor Estimating: 310it [03:18,  1.54it/s]Extractor Estimating: 311it [03:19,  1.52it/s]Extractor Estimating: 312it [03:20,  1.52it/s]Extractor Estimating: 313it [03:20,  1.49it/s]Extractor Estimating: 314it [03:21,  1.53it/s]Extractor Estimating: 315it [03:22,  1.56it/s]Extractor Estimating: 316it [03:22,  1.51it/s]Extractor Estimating: 317it [03:23,  1.53it/s]Extractor Estimating: 318it [03:24,  1.52it/s]Extractor Estimating: 319it [03:24,  1.57it/s]Extractor Estimating: 320it [03:25,  1.57it/s]Extractor Estimating: 321it [03:25,  1.57it/s]Extractor Estimating: 322it [03:26,  1.53it/s]Extractor Estimating: 323it [03:27,  1.50it/s]Extractor Estimating: 324it [03:27,  1.52it/s]Extractor Estimating: 325it [03:28,  1.56it/s]Extractor Estimating: 326it [03:29,  1.64it/s]Extractor Estimating: 327it [03:29,  1.79it/s]Extractor Estimating: 328it [03:30,  1.87it/s]Extractor Estimating: 329it [03:30,  1.90it/s]Extractor Estimating: 330it [03:30,  1.97it/s]Extractor Estimating: 331it [03:31,  1.98it/s]Extractor Estimating: 332it [03:31,  2.05it/s]Extractor Estimating: 333it [03:32,  1.86it/s]Extractor Estimating: 334it [03:32,  2.00it/s]Extractor Estimating: 335it [03:33,  2.03it/s]Extractor Estimating: 336it [03:34,  1.96it/s]Extractor Estimating: 337it [03:34,  1.98it/s]Extractor Estimating: 338it [03:34,  2.01it/s]Extractor Estimating: 339it [03:35,  1.90it/s]Extractor Estimating: 340it [03:36,  1.88it/s]Extractor Estimating: 341it [03:36,  1.89it/s]Extractor Estimating: 342it [03:37,  1.80it/s]Extractor Estimating: 343it [03:37,  1.73it/s]Extractor Estimating: 344it [03:38,  1.76it/s]Extractor Estimating: 345it [03:39,  1.75it/s]Extractor Estimating: 346it [03:39,  1.83it/s]Extractor Estimating: 347it [03:39,  1.92it/s]Extractor Estimating: 348it [03:40,  1.98it/s]Extractor Estimating: 349it [03:40,  2.00it/s]Extractor Estimating: 350it [03:41,  1.99it/s]Extractor Estimating: 351it [03:41,  1.92it/s]Extractor Estimating: 352it [03:42,  1.87it/s]Extractor Estimating: 353it [03:43,  1.86it/s]Extractor Estimating: 354it [03:43,  1.80it/s]Extractor Estimating: 355it [03:44,  1.77it/s]Extractor Estimating: 356it [03:44,  1.79it/s]Extractor Estimating: 357it [03:45,  1.72it/s]Extractor Estimating: 358it [03:46,  1.66it/s]Extractor Estimating: 359it [03:46,  1.70it/s]Extractor Estimating: 360it [03:47,  1.68it/s]Extractor Estimating: 361it [03:47,  1.71it/s]Extractor Estimating: 362it [03:48,  1.67it/s]Extractor Estimating: 363it [03:49,  1.70it/s]Extractor Estimating: 364it [03:49,  1.69it/s]Extractor Estimating: 365it [03:50,  1.73it/s]Extractor Estimating: 366it [03:50,  1.69it/s]Extractor Estimating: 367it [03:51,  1.71it/s]Extractor Estimating: 368it [03:52,  1.65it/s]Extractor Estimating: 369it [03:52,  1.70it/s]Extractor Estimating: 370it [03:53,  1.66it/s]Extractor Estimating: 371it [03:53,  1.72it/s]Extractor Estimating: 372it [03:54,  1.68it/s]Extractor Estimating: 373it [03:54,  1.68it/s]Extractor Estimating: 374it [03:55,  1.66it/s]Extractor Estimating: 375it [03:56,  1.69it/s]Extractor Estimating: 376it [03:56,  1.70it/s]Extractor Estimating: 377it [03:57,  1.76it/s]Extractor Estimating: 378it [03:57,  1.75it/s]Extractor Estimating: 379it [03:58,  1.72it/s]Extractor Estimating: 380it [03:59,  1.68it/s]Extractor Estimating: 381it [03:59,  1.75it/s]Extractor Estimating: 382it [04:00,  1.75it/s]Extractor Estimating: 383it [04:00,  1.72it/s]Extractor Estimating: 384it [04:01,  1.77it/s]Extractor Estimating: 385it [04:01,  1.75it/s]Extractor Estimating: 386it [04:02,  1.76it/s]Extractor Estimating: 387it [04:02,  1.77it/s]Extractor Estimating: 388it [04:03,  1.72it/s]Extractor Estimating: 389it [04:04,  1.76it/s]Extractor Estimating: 390it [04:04,  1.78it/s]Extractor Estimating: 391it [04:05,  1.74it/s]Extractor Estimating: 392it [04:05,  1.72it/s]Extractor Estimating: 393it [04:06,  1.73it/s]Extractor Estimating: 394it [04:07,  1.71it/s]Extractor Estimating: 395it [04:07,  1.68it/s]Extractor Estimating: 396it [04:08,  1.70it/s]Extractor Estimating: 397it [04:08,  1.74it/s]Extractor Estimating: 398it [04:09,  1.74it/s]Extractor Estimating: 399it [04:09,  1.80it/s]Extractor Estimating: 400it [04:10,  1.81it/s]Extractor Estimating: 401it [04:10,  1.79it/s]Extractor Estimating: 402it [04:11,  1.83it/s]Extractor Estimating: 403it [04:12,  1.81it/s]Extractor Estimating: 404it [04:12,  1.70it/s]Extractor Estimating: 405it [04:13,  1.68it/s]Extractor Estimating: 406it [04:13,  1.71it/s]Extractor Estimating: 407it [04:14,  1.79it/s]Extractor Estimating: 408it [04:14,  1.81it/s]Extractor Estimating: 409it [04:15,  1.79it/s]Extractor Estimating: 410it [04:16,  1.80it/s]Extractor Estimating: 411it [04:16,  1.73it/s]Extractor Estimating: 412it [04:17,  1.66it/s]Extractor Estimating: 413it [04:17,  1.68it/s]Extractor Estimating: 414it [04:18,  1.69it/s]Extractor Estimating: 415it [04:19,  1.74it/s]Extractor Estimating: 416it [04:19,  1.74it/s]Extractor Estimating: 417it [04:20,  1.68it/s]Extractor Estimating: 418it [04:20,  1.70it/s]Extractor Estimating: 419it [04:21,  1.75it/s]Extractor Estimating: 420it [04:22,  1.69it/s]Extractor Estimating: 421it [04:22,  1.72it/s]Extractor Estimating: 422it [04:23,  1.69it/s]Extractor Estimating: 423it [04:23,  1.71it/s]Extractor Estimating: 424it [04:24,  1.77it/s]Extractor Estimating: 425it [04:24,  1.71it/s]Extractor Estimating: 426it [04:25,  1.71it/s]Extractor Estimating: 427it [04:26,  1.70it/s]Extractor Estimating: 428it [04:26,  1.75it/s]Extractor Estimating: 429it [04:27,  1.71it/s]Extractor Estimating: 430it [04:27,  1.72it/s]Extractor Estimating: 431it [04:28,  1.70it/s]Extractor Estimating: 432it [04:29,  1.69it/s]Extractor Estimating: 433it [04:29,  1.71it/s]Extractor Estimating: 434it [04:30,  1.68it/s]Extractor Estimating: 435it [04:30,  1.72it/s]Extractor Estimating: 436it [04:31,  1.71it/s]Extractor Estimating: 437it [04:31,  1.74it/s]Extractor Estimating: 438it [04:32,  1.76it/s]Extractor Estimating: 439it [04:33,  1.72it/s]Extractor Estimating: 440it [04:33,  1.76it/s]Extractor Estimating: 441it [04:34,  1.71it/s]Extractor Estimating: 442it [04:34,  1.66it/s]Extractor Estimating: 443it [04:35,  1.68it/s]Extractor Estimating: 444it [04:36,  1.72it/s]Extractor Estimating: 445it [04:36,  1.70it/s]Extractor Estimating: 446it [04:37,  1.72it/s]Extractor Estimating: 447it [04:38,  1.52it/s]Extractor Estimating: 448it [04:38,  1.62it/s]Extractor Estimating: 449it [04:39,  1.66it/s]Extractor Estimating: 450it [04:39,  1.66it/s]Extractor Estimating: 451it [04:40,  1.61it/s]Extractor Estimating: 452it [04:41,  1.56it/s]Extractor Estimating: 453it [04:41,  1.56it/s]Extractor Estimating: 454it [04:42,  1.56it/s]Extractor Estimating: 455it [04:43,  1.51it/s]Extractor Estimating: 456it [04:43,  1.55it/s]Extractor Estimating: 457it [04:44,  1.53it/s]Extractor Estimating: 458it [04:44,  1.56it/s]Extractor Estimating: 459it [04:45,  1.55it/s]Extractor Estimating: 460it [04:46,  1.49it/s]Extractor Estimating: 461it [04:46,  1.53it/s]Extractor Estimating: 462it [04:47,  1.53it/s]Extractor Estimating: 463it [04:48,  1.49it/s]Extractor Estimating: 464it [04:48,  1.48it/s]Extractor Estimating: 465it [04:49,  1.48it/s]Extractor Estimating: 466it [04:50,  1.48it/s]Extractor Estimating: 467it [04:50,  1.55it/s]Extractor Estimating: 468it [04:51,  1.54it/s]Extractor Estimating: 469it [04:52,  1.58it/s]Extractor Estimating: 470it [04:52,  1.53it/s]Extractor Estimating: 471it [04:53,  1.50it/s]Extractor Estimating: 472it [04:54,  1.51it/s]Extractor Estimating: 473it [04:54,  1.55it/s]Extractor Estimating: 474it [04:55,  1.52it/s]Extractor Estimating: 475it [04:56,  1.53it/s]Extractor Estimating: 476it [04:56,  1.53it/s]Extractor Estimating: 477it [04:57,  1.54it/s]Extractor Estimating: 478it [04:58,  1.59it/s]Extractor Estimating: 479it [04:58,  1.58it/s]Extractor Estimating: 480it [04:59,  1.65it/s]Extractor Estimating: 481it [04:59,  1.65it/s]Extractor Estimating: 482it [05:00,  1.65it/s]Extractor Estimating: 483it [05:01,  1.63it/s]Extractor Estimating: 484it [05:01,  1.66it/s]Extractor Estimating: 485it [05:02,  1.56it/s]Extractor Estimating: 486it [05:02,  1.62it/s]Extractor Estimating: 487it [05:03,  1.65it/s]Extractor Estimating: 488it [05:04,  1.65it/s]Extractor Estimating: 489it [05:04,  1.61it/s]Extractor Estimating: 490it [05:05,  1.66it/s]Extractor Estimating: 491it [05:05,  1.68it/s]Extractor Estimating: 492it [05:06,  1.63it/s]Extractor Estimating: 493it [05:07,  1.65it/s]Extractor Estimating: 494it [05:07,  1.53it/s]Extractor Estimating: 495it [05:08,  1.53it/s]Extractor Estimating: 496it [05:09,  1.55it/s]Extractor Estimating: 497it [05:09,  1.59it/s]Extractor Estimating: 498it [05:10,  1.59it/s]Extractor Estimating: 499it [05:11,  1.62it/s]Extractor Estimating: 500it [05:11,  1.65it/s]Extractor Estimating: 500it [05:11,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:51,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:51,832 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:51,832 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:51,832 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:51,832 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:25:52,940 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:25:52,941 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:25:53,300 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:25:54,443 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:25:54,443 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:56,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:56,808 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:56,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:56,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:25:56,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:25:57,780 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:25:57,781 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:25:58,081 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:25:58,299 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:25:58,299 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 07:19:15,213 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 07:19:15,245 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9987 mean pseudo reward: 0.9244437359229878
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 22188
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22288, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22288, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.019, loss:727.7987
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.006, loss:714.8072
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.032, loss:702.5268
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.009, loss:710.0543
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.017, loss:672.3352
>> valid entity prec:0.5933, rec:0.6207, f1:0.6067
>> valid relation prec:0.2331, rec:0.1208, f1:0.1591
>> valid relation with NER prec:0.2331, rec:0.1208, f1:0.1591
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.210, loss:683.2636
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.022, loss:702.3513
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.006, loss:692.2635
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.004, loss:682.7565
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.003, loss:680.3264
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5900, rec:0.6294, f1:0.6091
>> valid relation prec:0.2208, rec:0.1148, f1:0.1510
>> valid relation with NER prec:0.2208, rec:0.1148, f1:0.1510
new max entity f1 on valid!
g_step 1100, step 266, avg_time 2.207, loss:722.2995
g_step 1200, step 366, avg_time 1.024, loss:710.5413
g_step 1300, step 49, avg_time 1.006, loss:686.7722
g_step 1400, step 149, avg_time 0.999, loss:655.9486
g_step 1500, step 249, avg_time 1.013, loss:641.1809
>> valid entity prec:0.5656, rec:0.6210, f1:0.5920
>> valid relation prec:0.2506, rec:0.1199, f1:0.1622
>> valid relation with NER prec:0.2506, rec:0.1199, f1:0.1622
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.199, loss:683.7171
g_step 1700, step 32, avg_time 1.045, loss:634.7089
g_step 1800, step 132, avg_time 1.008, loss:602.1217
g_step 1900, step 232, avg_time 1.031, loss:640.7636
g_step 2000, step 332, avg_time 1.005, loss:661.8906
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6624, rec:0.5329, f1:0.5907
>> valid relation prec:0.2590, rec:0.1136, f1:0.1579
>> valid relation with NER prec:0.2590, rec:0.1136, f1:0.1579
g_step 2100, step 15, avg_time 2.204, loss:625.3812
g_step 2200, step 115, avg_time 1.010, loss:590.1125
g_step 2300, step 215, avg_time 1.028, loss:578.1885
g_step 2400, step 315, avg_time 1.009, loss:591.3568
g_step 2500, step 415, avg_time 1.005, loss:609.7434
>> valid entity prec:0.6019, rec:0.5829, f1:0.5922
>> valid relation prec:0.1997, rec:0.1058, f1:0.1383
>> valid relation with NER prec:0.1997, rec:0.1058, f1:0.1383
g_step 2600, step 98, avg_time 2.191, loss:555.8548
g_step 2700, step 198, avg_time 1.011, loss:551.8772
g_step 2800, step 298, avg_time 1.038, loss:570.2660
g_step 2900, step 398, avg_time 1.013, loss:586.3764
g_step 3000, step 81, avg_time 0.994, loss:536.3993
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6169, rec:0.5872, f1:0.6017
>> valid relation prec:0.1971, rec:0.1119, f1:0.1427
>> valid relation with NER prec:0.1971, rec:0.1119, f1:0.1427
g_step 3100, step 181, avg_time 2.208, loss:551.3484
g_step 3200, step 281, avg_time 1.028, loss:545.5197
g_step 3300, step 381, avg_time 1.018, loss:550.9040
g_step 3400, step 64, avg_time 1.011, loss:529.9036
g_step 3500, step 164, avg_time 1.009, loss:491.9092
>> valid entity prec:0.5697, rec:0.5862, f1:0.5779
>> valid relation prec:0.2181, rec:0.1176, f1:0.1528
>> valid relation with NER prec:0.2181, rec:0.1176, f1:0.1528
g_step 3600, step 264, avg_time 2.230, loss:529.8389
g_step 3700, step 364, avg_time 1.019, loss:533.6319
g_step 3800, step 47, avg_time 1.024, loss:494.0762
g_step 3900, step 147, avg_time 1.019, loss:484.8439
g_step 4000, step 247, avg_time 1.011, loss:505.2324
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5983, rec:0.5683, f1:0.5829
>> valid relation prec:0.1868, rec:0.0938, f1:0.1249
>> valid relation with NER prec:0.1868, rec:0.0938, f1:0.1249
g_step 4100, step 347, avg_time 2.209, loss:497.3210
g_step 4200, step 30, avg_time 1.023, loss:512.9367
g_step 4300, step 130, avg_time 1.010, loss:462.1343
g_step 4400, step 230, avg_time 1.020, loss:488.0555
g_step 4500, step 330, avg_time 1.022, loss:477.1609
>> valid entity prec:0.5638, rec:0.5377, f1:0.5504
>> valid relation prec:0.1794, rec:0.0768, f1:0.1076
>> valid relation with NER prec:0.1794, rec:0.0768, f1:0.1076
g_step 4600, step 13, avg_time 2.231, loss:492.3690
g_step 4700, step 113, avg_time 1.021, loss:457.0135
g_step 4800, step 213, avg_time 1.009, loss:460.2823
g_step 4900, step 313, avg_time 1.029, loss:461.8619
g_step 5000, step 413, avg_time 1.011, loss:473.9344
learning rate was adjusted to 0.0008
>> valid entity prec:0.5299, rec:0.5709, f1:0.5496
>> valid relation prec:0.1460, rec:0.0837, f1:0.1064
>> valid relation with NER prec:0.1460, rec:0.0837, f1:0.1064
g_step 5100, step 96, avg_time 2.224, loss:423.3418
g_step 5200, step 196, avg_time 1.046, loss:426.2610
g_step 5300, step 296, avg_time 1.014, loss:448.8478
g_step 5400, step 396, avg_time 1.008, loss:459.1477
g_step 5500, step 79, avg_time 0.990, loss:418.5555
>> valid entity prec:0.5815, rec:0.5488, f1:0.5647
>> valid relation prec:0.1919, rec:0.1102, f1:0.1400
>> valid relation with NER prec:0.1919, rec:0.1102, f1:0.1400
g_step 5600, step 179, avg_time 2.225, loss:421.1009
g_step 5700, step 279, avg_time 1.034, loss:437.2200
g_step 5800, step 379, avg_time 1.015, loss:430.5857
g_step 5900, step 62, avg_time 1.017, loss:397.0296
g_step 6000, step 162, avg_time 1.016, loss:417.4660
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5537, rec:0.5437, f1:0.5487
>> valid relation prec:0.1703, rec:0.0851, f1:0.1135
>> valid relation with NER prec:0.1703, rec:0.0851, f1:0.1135
g_step 6100, step 262, avg_time 2.222, loss:409.4508
g_step 6200, step 362, avg_time 1.031, loss:427.1469
g_step 6300, step 45, avg_time 0.981, loss:396.0701
g_step 6400, step 145, avg_time 1.022, loss:380.6823
g_step 6500, step 245, avg_time 1.027, loss:413.5152
>> valid entity prec:0.5897, rec:0.5319, f1:0.5593
>> valid relation prec:0.2039, rec:0.1061, f1:0.1396
>> valid relation with NER prec:0.2039, rec:0.1061, f1:0.1396
g_step 6600, step 345, avg_time 2.234, loss:430.4479
g_step 6700, step 28, avg_time 1.007, loss:386.0981
g_step 6800, step 128, avg_time 1.029, loss:377.9214
g_step 6900, step 228, avg_time 1.008, loss:386.7201
g_step 7000, step 328, avg_time 1.037, loss:396.9639
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6053, rec:0.5589, f1:0.5812
>> valid relation prec:0.1870, rec:0.1090, f1:0.1377
>> valid relation with NER prec:0.1870, rec:0.1090, f1:0.1377
g_step 7100, step 11, avg_time 2.204, loss:394.9470
g_step 7200, step 111, avg_time 1.017, loss:378.7268
g_step 7300, step 211, avg_time 1.009, loss:370.6842
g_step 7400, step 311, avg_time 1.010, loss:409.1549
g_step 7500, step 411, avg_time 1.042, loss:387.4749
>> valid entity prec:0.5650, rec:0.5312, f1:0.5476
>> valid relation prec:0.1890, rec:0.1050, f1:0.1350
>> valid relation with NER prec:0.1890, rec:0.1050, f1:0.1350
g_step 7600, step 94, avg_time 2.196, loss:356.9380
g_step 7700, step 194, avg_time 1.013, loss:353.8092
g_step 7800, step 294, avg_time 1.015, loss:375.7349
g_step 7900, step 394, avg_time 1.022, loss:382.8998
g_step 8000, step 77, avg_time 1.006, loss:344.6531
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5810, rec:0.5393, f1:0.5593
>> valid relation prec:0.1748, rec:0.1044, f1:0.1307
>> valid relation with NER prec:0.1748, rec:0.1044, f1:0.1307
g_step 8100, step 177, avg_time 2.211, loss:343.3478
g_step 8200, step 277, avg_time 1.024, loss:361.4101
g_step 8300, step 377, avg_time 1.012, loss:349.7607
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 07:19:15 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 07:19:15 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_07-19-15_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 07:19:16 - WARNING - datasets.builder -   Using custom data configuration default-15fbde7fcb0eb84f
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-15fbde7fcb0eb84f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 07:19:18,156 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:19:18,158 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:19:18,158 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:19:18,159 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:19:18,236 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:18,290 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:18,290 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:18,290 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:18,290 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:18,290 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:18,290 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 07:19:18,699 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:19:21,744 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 07:19:21,760 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-15fbde7fcb0eb84f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:02,  3.07ba/s] 20%|        | 2/10 [00:00<00:02,  3.93ba/s] 30%|       | 3/10 [00:00<00:01,  4.31ba/s] 40%|      | 4/10 [00:00<00:01,  4.52ba/s] 50%|     | 5/10 [00:01<00:01,  4.60ba/s] 60%|    | 6/10 [00:01<00:00,  4.67ba/s] 70%|   | 7/10 [00:01<00:00,  4.71ba/s] 80%|  | 8/10 [00:01<00:00,  3.92ba/s] 90%| | 9/10 [00:02<00:00,  4.15ba/s]100%|| 10/10 [00:02<00:00,  4.32ba/s]100%|| 10/10 [00:02<00:00,  4.28ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.05ba/s] 50%|     | 2/4 [00:00<00:00,  3.78ba/s] 75%|  | 3/4 [00:00<00:00,  3.93ba/s]100%|| 4/4 [00:00<00:00,  5.09ba/s]100%|| 4/4 [00:00<00:00,  4.46ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  5.65ba/s] 30%|       | 3/10 [00:00<00:00,  9.05ba/s] 50%|     | 5/10 [00:00<00:00, 10.21ba/s] 70%|   | 7/10 [00:00<00:00, 10.74ba/s] 90%| | 9/10 [00:00<00:00, 10.96ba/s]100%|| 10/10 [00:00<00:00, 10.41ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  6.00ba/s] 75%|  | 3/4 [00:00<00:00,  9.29ba/s]100%|| 4/4 [00:00<00:00, 10.43ba/s]
[INFO|trainer.py:414] 2023-08-29 07:19:27,387 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 07:19:27,453 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 07:19:27,453 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-29 07:19:27,453 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 07:19:27,453 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 07:19:27,453 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 07:19:27,453 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 07:19:27,453 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:53,  3.34it/s]  0%|          | 2/780 [00:00<03:47,  3.42it/s]  0%|          | 3/780 [00:00<03:58,  3.26it/s]  1%|          | 4/780 [00:01<03:52,  3.34it/s]  1%|          | 5/780 [00:01<03:48,  3.39it/s]  1%|          | 6/780 [00:01<03:46,  3.42it/s]  1%|          | 7/780 [00:02<03:45,  3.43it/s]  1%|          | 8/780 [00:02<03:44,  3.45it/s]  1%|          | 9/780 [00:02<03:43,  3.45it/s]  1%|         | 10/780 [00:02<03:42,  3.46it/s]  1%|         | 11/780 [00:03<03:42,  3.46it/s]  2%|         | 12/780 [00:03<03:41,  3.46it/s]  2%|         | 13/780 [00:03<03:41,  3.46it/s]  2%|         | 14/780 [00:04<03:46,  3.39it/s]  2%|         | 15/780 [00:04<03:44,  3.41it/s]  2%|         | 16/780 [00:04<03:42,  3.43it/s]  2%|         | 17/780 [00:04<03:41,  3.44it/s]  2%|         | 18/780 [00:05<03:40,  3.45it/s]  2%|         | 19/780 [00:05<03:40,  3.45it/s]  3%|         | 20/780 [00:05<03:39,  3.46it/s]  3%|         | 21/780 [00:06<03:39,  3.46it/s]  3%|         | 22/780 [00:06<03:38,  3.46it/s]  3%|         | 23/780 [00:06<03:38,  3.47it/s]  3%|         | 24/780 [00:06<03:37,  3.47it/s]  3%|         | 25/780 [00:07<03:41,  3.41it/s]  3%|         | 26/780 [00:07<03:40,  3.43it/s]  3%|         | 27/780 [00:07<03:38,  3.44it/s]  4%|         | 28/780 [00:08<03:38,  3.45it/s]  4%|         | 29/780 [00:08<03:37,  3.45it/s]  4%|         | 30/780 [00:08<03:36,  3.46it/s]  4%|         | 31/780 [00:09<03:36,  3.46it/s]  4%|         | 32/780 [00:09<03:35,  3.46it/s]  4%|         | 33/780 [00:09<03:35,  3.46it/s]  4%|         | 34/780 [00:09<03:35,  3.47it/s]  4%|         | 35/780 [00:10<03:35,  3.46it/s]  5%|         | 36/780 [00:10<03:41,  3.37it/s]  5%|         | 37/780 [00:10<03:38,  3.40it/s]  5%|         | 38/780 [00:11<03:37,  3.42it/s]  5%|         | 39/780 [00:11<03:36,  3.43it/s]  5%|         | 40/780 [00:11<03:34,  3.44it/s]  5%|         | 41/780 [00:11<03:34,  3.45it/s]  5%|         | 42/780 [00:12<03:33,  3.45it/s]  6%|         | 43/780 [00:12<03:33,  3.46it/s]  6%|         | 44/780 [00:12<03:32,  3.46it/s]  6%|         | 45/780 [00:13<03:32,  3.45it/s]  6%|         | 46/780 [00:13<03:32,  3.45it/s]  6%|         | 47/780 [00:13<03:41,  3.32it/s]  6%|         | 48/780 [00:13<03:37,  3.36it/s]  6%|         | 49/780 [00:14<03:35,  3.39it/s]  6%|         | 50/780 [00:14<03:33,  3.42it/s]  7%|         | 51/780 [00:14<03:32,  3.43it/s]  7%|         | 52/780 [00:15<03:31,  3.44it/s]  7%|         | 53/780 [00:15<03:30,  3.45it/s]  7%|         | 54/780 [00:15<03:30,  3.45it/s]  7%|         | 55/780 [00:16<03:30,  3.45it/s]  7%|         | 56/780 [00:16<03:29,  3.46it/s]  7%|         | 57/780 [00:16<03:29,  3.46it/s]  7%|         | 58/780 [00:16<03:33,  3.38it/s]  8%|         | 59/780 [00:17<03:31,  3.40it/s]  8%|         | 60/780 [00:17<03:30,  3.42it/s]  8%|         | 61/780 [00:17<03:29,  3.43it/s]  8%|         | 62/780 [00:18<03:28,  3.44it/s]  8%|         | 63/780 [00:18<03:28,  3.44it/s]  8%|         | 64/780 [00:18<03:27,  3.45it/s]  8%|         | 65/780 [00:18<03:27,  3.45it/s]  8%|         | 66/780 [00:19<03:26,  3.45it/s]  9%|         | 67/780 [00:19<03:26,  3.46it/s]  9%|         | 68/780 [00:19<03:26,  3.46it/s]  9%|         | 69/780 [00:20<03:37,  3.26it/s]  9%|         | 70/780 [00:20<03:34,  3.32it/s]  9%|         | 71/780 [00:20<03:31,  3.36it/s]  9%|         | 72/780 [00:21<03:32,  3.34it/s]  9%|         | 73/780 [00:21<03:29,  3.38it/s]  9%|         | 74/780 [00:21<03:27,  3.41it/s] 10%|         | 75/780 [00:21<03:25,  3.43it/s] 10%|         | 76/780 [00:22<03:38,  3.23it/s] 10%|         | 77/780 [00:22<03:33,  3.29it/s] 10%|         | 78/780 [00:22<03:29,  3.35it/s] 10%|         | 79/780 [00:23<03:26,  3.39it/s] 10%|         | 80/780 [00:23<04:56,  2.36it/s] 10%|         | 81/780 [00:24<04:37,  2.52it/s] 11%|         | 82/780 [00:24<04:14,  2.74it/s] 11%|         | 83/780 [00:24<03:57,  2.93it/s] 11%|         | 84/780 [00:25<03:46,  3.08it/s] 11%|         | 85/780 [00:25<03:38,  3.19it/s] 11%|         | 86/780 [00:25<03:32,  3.27it/s] 11%|         | 87/780 [00:25<03:28,  3.33it/s] 11%|        | 88/780 [00:26<03:25,  3.37it/s] 11%|        | 89/780 [00:26<03:23,  3.40it/s] 12%|        | 90/780 [00:26<03:21,  3.42it/s] 12%|        | 91/780 [00:27<03:21,  3.43it/s] 12%|        | 92/780 [00:27<03:27,  3.31it/s] 12%|        | 93/780 [00:27<03:25,  3.35it/s] 12%|        | 94/780 [00:27<03:22,  3.38it/s] 12%|        | 95/780 [00:28<03:21,  3.40it/s] 12%|        | 96/780 [00:28<03:20,  3.41it/s] 12%|        | 97/780 [00:28<03:19,  3.42it/s] 13%|        | 98/780 [00:29<03:18,  3.43it/s] 13%|        | 99/780 [00:29<03:18,  3.44it/s] 13%|        | 100/780 [00:29<03:17,  3.44it/s] 13%|        | 101/780 [00:29<03:17,  3.45it/s] 13%|        | 102/780 [00:30<03:16,  3.45it/s] 13%|        | 103/780 [00:30<03:24,  3.31it/s] 13%|        | 104/780 [00:30<03:21,  3.35it/s] 13%|        | 105/780 [00:31<03:19,  3.38it/s] 14%|        | 106/780 [00:31<03:18,  3.40it/s] 14%|        | 107/780 [00:31<03:16,  3.42it/s] 14%|        | 108/780 [00:32<03:16,  3.43it/s] 14%|        | 109/780 [00:32<03:15,  3.44it/s] 14%|        | 110/780 [00:32<03:14,  3.44it/s] 14%|        | 111/780 [00:32<03:14,  3.44it/s] 14%|        | 112/780 [00:33<03:13,  3.45it/s] 14%|        | 113/780 [00:33<03:13,  3.45it/s] 15%|        | 114/780 [00:33<03:19,  3.34it/s] 15%|        | 115/780 [00:34<03:17,  3.37it/s] 15%|        | 116/780 [00:34<03:15,  3.40it/s] 15%|        | 117/780 [00:34<03:13,  3.42it/s] 15%|        | 118/780 [00:34<03:13,  3.43it/s] 15%|        | 119/780 [00:35<03:12,  3.44it/s] 15%|        | 120/780 [00:35<03:11,  3.45it/s] 16%|        | 121/780 [00:35<03:10,  3.46it/s] 16%|        | 122/780 [00:36<03:10,  3.46it/s] 16%|        | 123/780 [00:36<03:09,  3.46it/s] 16%|        | 124/780 [00:36<03:09,  3.47it/s] 16%|        | 125/780 [00:37<03:13,  3.38it/s] 16%|        | 126/780 [00:37<03:12,  3.40it/s] 16%|        | 127/780 [00:37<03:10,  3.42it/s] 16%|        | 128/780 [00:37<03:09,  3.44it/s] 17%|        | 129/780 [00:38<03:08,  3.45it/s] 17%|        | 130/780 [00:38<03:08,  3.45it/s] 17%|        | 131/780 [00:38<03:07,  3.46it/s] 17%|        | 132/780 [00:39<03:07,  3.46it/s] 17%|        | 133/780 [00:39<03:06,  3.46it/s] 17%|        | 134/780 [00:39<03:06,  3.46it/s] 17%|        | 135/780 [00:39<03:06,  3.47it/s] 17%|        | 136/780 [00:40<03:10,  3.39it/s] 18%|        | 137/780 [00:40<03:08,  3.41it/s] 18%|        | 138/780 [00:40<03:07,  3.42it/s] 18%|        | 139/780 [00:41<03:06,  3.44it/s] 18%|        | 140/780 [00:41<03:05,  3.45it/s] 18%|        | 141/780 [00:41<03:05,  3.45it/s] 18%|        | 142/780 [00:41<03:04,  3.45it/s] 18%|        | 143/780 [00:42<03:04,  3.46it/s] 18%|        | 144/780 [00:42<03:03,  3.46it/s] 19%|        | 145/780 [00:42<03:03,  3.46it/s] 19%|        | 146/780 [00:43<03:03,  3.46it/s] 19%|        | 147/780 [00:43<03:14,  3.25it/s] 19%|        | 148/780 [00:43<03:11,  3.31it/s] 19%|        | 149/780 [00:44<03:08,  3.35it/s] 19%|        | 150/780 [00:44<03:06,  3.38it/s] 19%|        | 151/780 [00:44<03:04,  3.40it/s] 19%|        | 152/780 [00:44<03:03,  3.42it/s] 20%|        | 153/780 [00:45<03:02,  3.43it/s] 20%|        | 154/780 [00:45<03:02,  3.44it/s] 20%|        | 155/780 [00:45<03:01,  3.44it/s] 20%|        | 156/780 [00:46<03:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 07:20:13,548 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:20:13,548 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 07:20:13,548 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  2%|         | 7/435 [00:00<00:07, 58.23it/s][A
  3%|         | 13/435 [00:00<00:08, 50.14it/s][A
  4%|         | 19/435 [00:00<00:08, 47.79it/s][A
  6%|         | 24/435 [00:00<00:08, 46.91it/s][A
  7%|         | 29/435 [00:00<00:08, 46.25it/s][A
  8%|         | 34/435 [00:00<00:08, 45.96it/s][A
  9%|         | 39/435 [00:00<00:08, 45.50it/s][A
 10%|         | 44/435 [00:00<00:08, 44.97it/s][A
 11%|        | 49/435 [00:01<00:08, 44.67it/s][A
 12%|        | 54/435 [00:01<00:08, 44.75it/s][A
 14%|        | 59/435 [00:01<00:08, 44.77it/s][A
 15%|        | 64/435 [00:01<00:08, 44.86it/s][A
 16%|        | 69/435 [00:01<00:08, 44.93it/s][A
 17%|        | 74/435 [00:01<00:08, 45.02it/s][A
 18%|        | 79/435 [00:01<00:07, 45.04it/s][A
 19%|        | 84/435 [00:01<00:07, 44.87it/s][A
 20%|        | 89/435 [00:01<00:07, 44.69it/s][A
 22%|       | 94/435 [00:02<00:07, 44.55it/s][A
 23%|       | 99/435 [00:02<00:07, 44.58it/s][A
 24%|       | 104/435 [00:02<00:07, 44.67it/s][A
 25%|       | 109/435 [00:02<00:07, 44.71it/s][A
 26%|       | 114/435 [00:02<00:07, 44.79it/s][A
 27%|       | 119/435 [00:02<00:07, 44.86it/s][A
 29%|       | 124/435 [00:02<00:06, 44.93it/s][A
 30%|       | 129/435 [00:02<00:06, 44.87it/s][A
 31%|       | 134/435 [00:02<00:06, 44.68it/s][A
 32%|      | 139/435 [00:03<00:06, 43.33it/s][A
 33%|      | 144/435 [00:03<00:06, 43.78it/s][A
 34%|      | 149/435 [00:03<00:06, 44.04it/s][A
 35%|      | 154/435 [00:03<00:06, 44.26it/s][A
 37%|      | 159/435 [00:03<00:06, 44.46it/s][A
 38%|      | 164/435 [00:03<00:06, 44.65it/s][A
 39%|      | 169/435 [00:03<00:05, 44.79it/s][A
 40%|      | 174/435 [00:03<00:05, 44.83it/s][A
 41%|      | 179/435 [00:03<00:05, 44.64it/s][A
 42%|     | 184/435 [00:04<00:05, 44.62it/s][A
 43%|     | 189/435 [00:04<00:05, 44.71it/s][A
 45%|     | 194/435 [00:04<00:05, 44.70it/s][A
 46%|     | 199/435 [00:04<00:05, 44.65it/s][A
 47%|     | 204/435 [00:04<00:05, 44.80it/s][A
 48%|     | 209/435 [00:04<00:05, 44.84it/s][A
 49%|     | 214/435 [00:04<00:04, 44.91it/s][A
 50%|     | 219/435 [00:04<00:04, 44.81it/s][A
 51%|    | 224/435 [00:04<00:04, 44.66it/s][A
 53%|    | 229/435 [00:05<00:04, 44.68it/s][A
 54%|    | 234/435 [00:05<00:04, 44.66it/s][A
 55%|    | 239/435 [00:05<00:04, 43.24it/s][A
 56%|    | 244/435 [00:05<00:04, 43.61it/s][A
 57%|    | 249/435 [00:05<00:04, 44.09it/s][A
 58%|    | 254/435 [00:05<00:04, 44.43it/s][A
 60%|    | 259/435 [00:05<00:03, 44.59it/s][A
 61%|    | 264/435 [00:05<00:03, 44.68it/s][A
 62%|   | 269/435 [00:05<00:03, 44.66it/s][A
 63%|   | 274/435 [00:06<00:03, 44.61it/s][A
 64%|   | 279/435 [00:06<00:03, 44.43it/s][A
 65%|   | 284/435 [00:06<00:03, 44.53it/s][A
 66%|   | 289/435 [00:06<00:03, 44.58it/s][A
 68%|   | 294/435 [00:06<00:03, 44.80it/s][A
 69%|   | 299/435 [00:06<00:03, 44.86it/s][A
 70%|   | 304/435 [00:06<00:02, 44.94it/s][A
 71%|   | 309/435 [00:06<00:02, 44.91it/s][A
 72%|  | 314/435 [00:06<00:02, 44.82it/s][A
 73%|  | 319/435 [00:07<00:02, 44.76it/s][A
 74%|  | 324/435 [00:07<00:02, 44.65it/s][A
 76%|  | 329/435 [00:07<00:02, 44.70it/s][A
 77%|  | 334/435 [00:07<00:02, 44.75it/s][A
 78%|  | 339/435 [00:07<00:02, 44.76it/s][A
 79%|  | 344/435 [00:07<00:02, 44.84it/s][A
 80%|  | 349/435 [00:07<00:01, 44.88it/s][A
 81%| | 354/435 [00:07<00:01, 44.91it/s][A
 83%| | 359/435 [00:08<00:01, 44.82it/s][A
 84%| | 364/435 [00:08<00:01, 44.78it/s][A
 85%| | 369/435 [00:08<00:01, 44.82it/s][A
 86%| | 374/435 [00:08<00:01, 43.50it/s][A
 87%| | 379/435 [00:08<00:01, 43.93it/s][A
 88%| | 384/435 [00:08<00:01, 44.16it/s][A
 89%| | 389/435 [00:08<00:01, 44.37it/s][A
 91%| | 394/435 [00:08<00:00, 44.50it/s][A
 92%|| 399/435 [00:08<00:00, 44.59it/s][A
 93%|| 404/435 [00:09<00:00, 44.64it/s][A
 94%|| 409/435 [00:09<00:00, 44.65it/s][A
 95%|| 414/435 [00:09<00:00, 44.50it/s][A
 96%|| 419/435 [00:09<00:00, 44.56it/s][A
 97%|| 424/435 [00:09<00:00, 44.65it/s][A
 99%|| 429/435 [00:09<00:00, 44.70it/s][A
100%|| 434/435 [00:09<00:00, 44.81it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.81it/s][A 20%|        | 156/780 [00:55<03:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:20:23,545 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 07:20:23,682 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:20:26,048 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:20:26,141 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:20:26,209 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:04<1:01:01,  5.88s/it] 20%|        | 158/780 [01:05<43:35,  4.20s/it]   20%|        | 159/780 [01:05<31:22,  3.03s/it] 21%|        | 160/780 [01:05<22:50,  2.21s/it] 21%|        | 161/780 [01:06<16:52,  1.64s/it] 21%|        | 162/780 [01:06<12:41,  1.23s/it] 21%|        | 163/780 [01:06<09:46,  1.05it/s] 21%|        | 164/780 [01:07<07:43,  1.33it/s] 21%|        | 165/780 [01:07<06:17,  1.63it/s] 21%|       | 166/780 [01:07<05:17,  1.93it/s] 21%|       | 167/780 [01:07<04:35,  2.22it/s] 22%|       | 168/780 [01:08<04:06,  2.48it/s] 22%|       | 169/780 [01:08<03:50,  2.65it/s] 22%|       | 170/780 [01:08<03:34,  2.84it/s] 22%|       | 171/780 [01:09<03:23,  2.99it/s] 22%|       | 172/780 [01:09<03:15,  3.11it/s] 22%|       | 173/780 [01:09<03:09,  3.20it/s] 22%|       | 174/780 [01:09<03:06,  3.26it/s] 22%|       | 175/780 [01:10<03:03,  3.30it/s] 23%|       | 176/780 [01:10<03:01,  3.34it/s] 23%|       | 177/780 [01:10<02:59,  3.36it/s] 23%|       | 178/780 [01:11<02:58,  3.38it/s] 23%|       | 179/780 [01:11<02:57,  3.39it/s] 23%|       | 180/780 [01:11<03:00,  3.33it/s] 23%|       | 181/780 [01:12<02:58,  3.36it/s] 23%|       | 182/780 [01:12<02:57,  3.37it/s] 23%|       | 183/780 [01:12<02:56,  3.38it/s] 24%|       | 184/780 [01:12<02:55,  3.40it/s] 24%|       | 185/780 [01:13<02:55,  3.40it/s] 24%|       | 186/780 [01:13<02:54,  3.40it/s] 24%|       | 187/780 [01:13<02:53,  3.41it/s] 24%|       | 188/780 [01:14<02:53,  3.41it/s] 24%|       | 189/780 [01:14<02:53,  3.41it/s] 24%|       | 190/780 [01:14<02:52,  3.41it/s] 24%|       | 191/780 [01:15<02:56,  3.34it/s] 25%|       | 192/780 [01:15<02:54,  3.36it/s] 25%|       | 193/780 [01:15<02:53,  3.38it/s] 25%|       | 194/780 [01:15<02:52,  3.39it/s] 25%|       | 195/780 [01:16<02:52,  3.40it/s] 25%|       | 196/780 [01:16<02:51,  3.40it/s] 25%|       | 197/780 [01:16<02:51,  3.41it/s] 25%|       | 198/780 [01:17<02:50,  3.41it/s] 26%|       | 199/780 [01:17<02:50,  3.41it/s] 26%|       | 200/780 [01:17<02:49,  3.41it/s] 26%|       | 201/780 [01:17<02:49,  3.42it/s] 26%|       | 202/780 [01:18<02:51,  3.37it/s] 26%|       | 203/780 [01:18<02:50,  3.38it/s] 26%|       | 204/780 [01:18<02:49,  3.39it/s] 26%|       | 205/780 [01:19<02:49,  3.40it/s] 26%|       | 206/780 [01:19<02:48,  3.40it/s] 27%|       | 207/780 [01:19<02:48,  3.41it/s] 27%|       | 208/780 [01:19<02:47,  3.41it/s] 27%|       | 209/780 [01:20<02:47,  3.41it/s] 27%|       | 210/780 [01:20<02:47,  3.41it/s] 27%|       | 211/780 [01:20<02:46,  3.41it/s] 27%|       | 212/780 [01:21<02:46,  3.41it/s] 27%|       | 213/780 [01:21<02:54,  3.26it/s] 27%|       | 214/780 [01:21<02:51,  3.30it/s] 28%|       | 215/780 [01:22<02:49,  3.34it/s] 28%|       | 216/780 [01:22<02:47,  3.36it/s] 28%|       | 217/780 [01:22<02:46,  3.39it/s] 28%|       | 218/780 [01:22<02:44,  3.41it/s] 28%|       | 219/780 [01:23<02:43,  3.43it/s] 28%|       | 220/780 [01:23<02:48,  3.31it/s] 28%|       | 221/780 [01:23<02:46,  3.36it/s] 28%|       | 222/780 [01:24<02:44,  3.40it/s] 29%|       | 223/780 [01:24<02:42,  3.42it/s] 29%|       | 224/780 [01:24<02:49,  3.28it/s] 29%|       | 225/780 [01:25<02:46,  3.33it/s] 29%|       | 226/780 [01:25<02:44,  3.37it/s] 29%|       | 227/780 [01:25<02:42,  3.40it/s] 29%|       | 228/780 [01:25<02:41,  3.42it/s] 29%|       | 229/780 [01:26<02:40,  3.43it/s] 29%|       | 230/780 [01:26<02:39,  3.44it/s] 30%|       | 231/780 [01:26<02:39,  3.45it/s] 30%|       | 232/780 [01:27<02:38,  3.45it/s] 30%|       | 233/780 [01:27<02:38,  3.45it/s] 30%|       | 234/780 [01:27<02:38,  3.45it/s] 30%|       | 235/780 [01:27<02:37,  3.46it/s] 30%|       | 236/780 [01:28<02:37,  3.46it/s] 30%|       | 237/780 [01:28<02:36,  3.46it/s] 31%|       | 238/780 [01:28<02:36,  3.46it/s] 31%|       | 239/780 [01:29<02:36,  3.46it/s] 31%|       | 240/780 [01:29<02:35,  3.46it/s] 31%|       | 241/780 [01:29<02:35,  3.46it/s] 31%|       | 242/780 [01:29<02:35,  3.46it/s] 31%|       | 243/780 [01:30<02:34,  3.47it/s] 31%|      | 244/780 [01:30<02:40,  3.35it/s] 31%|      | 245/780 [01:30<02:38,  3.38it/s] 32%|      | 246/780 [01:31<02:36,  3.41it/s] 32%|      | 247/780 [01:31<02:35,  3.42it/s] 32%|      | 248/780 [01:31<02:35,  3.43it/s] 32%|      | 249/780 [01:32<02:34,  3.43it/s] 32%|      | 250/780 [01:32<02:34,  3.43it/s] 32%|      | 251/780 [01:32<02:33,  3.45it/s] 32%|      | 252/780 [01:32<02:32,  3.45it/s] 32%|      | 253/780 [01:33<02:32,  3.45it/s] 33%|      | 254/780 [01:33<02:32,  3.46it/s] 33%|      | 255/780 [01:33<02:37,  3.33it/s] 33%|      | 256/780 [01:34<02:35,  3.37it/s] 33%|      | 257/780 [01:34<02:34,  3.39it/s] 33%|      | 258/780 [01:34<02:33,  3.41it/s] 33%|      | 259/780 [01:34<02:32,  3.41it/s] 33%|      | 260/780 [01:35<02:32,  3.42it/s] 33%|      | 261/780 [01:35<02:31,  3.42it/s] 34%|      | 262/780 [01:35<02:31,  3.43it/s] 34%|      | 263/780 [01:36<02:30,  3.43it/s] 34%|      | 264/780 [01:36<02:30,  3.43it/s] 34%|      | 265/780 [01:36<02:29,  3.44it/s] 34%|      | 266/780 [01:37<02:36,  3.29it/s] 34%|      | 267/780 [01:37<02:33,  3.34it/s] 34%|      | 268/780 [01:37<02:31,  3.38it/s] 34%|      | 269/780 [01:37<02:30,  3.40it/s] 35%|      | 270/780 [01:38<02:29,  3.42it/s] 35%|      | 271/780 [01:38<02:28,  3.44it/s] 35%|      | 272/780 [01:38<02:27,  3.45it/s] 35%|      | 273/780 [01:39<02:26,  3.45it/s] 35%|      | 274/780 [01:39<02:26,  3.46it/s] 35%|      | 275/780 [01:39<02:25,  3.46it/s] 35%|      | 276/780 [01:39<02:25,  3.46it/s] 36%|      | 277/780 [01:40<02:30,  3.35it/s] 36%|      | 278/780 [01:40<02:28,  3.38it/s] 36%|      | 279/780 [01:40<02:27,  3.41it/s] 36%|      | 280/780 [01:41<02:26,  3.42it/s] 36%|      | 281/780 [01:41<02:25,  3.43it/s] 36%|      | 282/780 [01:41<02:24,  3.44it/s] 36%|      | 283/780 [01:41<02:24,  3.44it/s] 36%|      | 284/780 [01:42<02:23,  3.45it/s] 37%|      | 285/780 [01:42<02:23,  3.45it/s] 37%|      | 286/780 [01:42<02:23,  3.45it/s] 37%|      | 287/780 [01:43<02:22,  3.45it/s] 37%|      | 288/780 [01:43<02:27,  3.34it/s] 37%|      | 289/780 [01:43<02:26,  3.36it/s] 37%|      | 290/780 [01:44<02:24,  3.39it/s] 37%|      | 291/780 [01:44<02:23,  3.41it/s] 37%|      | 292/780 [01:44<02:22,  3.42it/s] 38%|      | 293/780 [01:44<02:21,  3.43it/s] 38%|      | 294/780 [01:45<02:21,  3.44it/s] 38%|      | 295/780 [01:45<02:20,  3.44it/s] 38%|      | 296/780 [01:45<02:20,  3.45it/s] 38%|      | 297/780 [01:46<02:19,  3.45it/s] 38%|      | 298/780 [01:46<02:19,  3.45it/s] 38%|      | 299/780 [01:46<02:24,  3.34it/s] 38%|      | 300/780 [01:46<02:22,  3.37it/s] 39%|      | 301/780 [01:47<02:20,  3.40it/s] 39%|      | 302/780 [01:47<02:19,  3.42it/s] 39%|      | 303/780 [01:47<02:19,  3.43it/s] 39%|      | 304/780 [01:48<02:18,  3.44it/s] 39%|      | 305/780 [01:48<02:17,  3.45it/s] 39%|      | 306/780 [01:48<02:17,  3.45it/s] 39%|      | 307/780 [01:48<02:16,  3.45it/s] 39%|      | 308/780 [01:49<02:16,  3.46it/s] 40%|      | 309/780 [01:49<02:16,  3.46it/s] 40%|      | 310/780 [01:49<02:19,  3.38it/s] 40%|      | 311/780 [01:50<02:17,  3.40it/s] 40%|      | 312/780 [01:50<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 07:21:17,946 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:21:17,946 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 07:21:17,946 >>   Batch size = 8
{'eval_loss': 0.9695549607276917, 'eval_runtime': 9.8065, 'eval_samples_per_second': 354.562, 'eval_steps_per_second': 44.358, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.71it/s][A
  3%|         | 12/435 [00:00<00:08, 48.88it/s][A
  4%|         | 17/435 [00:00<00:08, 47.10it/s][A
  5%|         | 22/435 [00:00<00:08, 46.12it/s][A
  6%|         | 27/435 [00:00<00:08, 45.56it/s][A
  7%|         | 32/435 [00:00<00:08, 45.24it/s][A
  9%|         | 37/435 [00:00<00:08, 45.03it/s][A
 10%|         | 42/435 [00:00<00:08, 44.73it/s][A
 11%|         | 47/435 [00:01<00:08, 44.77it/s][A
 12%|        | 52/435 [00:01<00:08, 44.91it/s][A
 13%|        | 57/435 [00:01<00:08, 44.96it/s][A
 14%|        | 62/435 [00:01<00:08, 45.00it/s][A
 15%|        | 67/435 [00:01<00:08, 44.94it/s][A
 17%|        | 72/435 [00:01<00:08, 44.87it/s][A
 18%|        | 77/435 [00:01<00:07, 44.82it/s][A
 19%|        | 82/435 [00:01<00:07, 44.64it/s][A
 20%|        | 87/435 [00:01<00:07, 44.58it/s][A
 21%|        | 92/435 [00:02<00:07, 44.63it/s][A
 22%|       | 97/435 [00:02<00:07, 43.43it/s][A
 23%|       | 102/435 [00:02<00:07, 43.91it/s][A
 25%|       | 107/435 [00:02<00:07, 44.26it/s][A
 26%|       | 112/435 [00:02<00:07, 44.50it/s][A
 27%|       | 117/435 [00:02<00:07, 44.63it/s][A
 28%|       | 122/435 [00:02<00:07, 44.68it/s][A
 29%|       | 127/435 [00:02<00:06, 44.60it/s][A
 30%|       | 132/435 [00:02<00:06, 44.58it/s][A
 31%|      | 137/435 [00:03<00:06, 44.51it/s][A
 33%|      | 142/435 [00:03<00:06, 44.57it/s][A
 34%|      | 147/435 [00:03<00:06, 44.65it/s][A
 35%|      | 152/435 [00:03<00:06, 44.80it/s][A
 36%|      | 157/435 [00:03<00:06, 44.86it/s][A
 37%|      | 162/435 [00:03<00:06, 44.89it/s][A
 38%|      | 167/435 [00:03<00:05, 44.89it/s][A
 40%|      | 172/435 [00:03<00:05, 44.75it/s][A
 41%|      | 177/435 [00:03<00:05, 44.77it/s][A
 42%|     | 182/435 [00:04<00:05, 44.67it/s][A
 43%|     | 187/435 [00:04<00:05, 44.62it/s][A
 44%|     | 192/435 [00:04<00:05, 44.70it/s][A
 45%|     | 197/435 [00:04<00:05, 44.79it/s][A
 46%|     | 202/435 [00:04<00:05, 44.87it/s][A
 48%|     | 207/435 [00:04<00:05, 44.83it/s][A
 49%|     | 212/435 [00:04<00:04, 44.86it/s][A
 50%|     | 217/435 [00:04<00:04, 44.78it/s][A
 51%|     | 222/435 [00:04<00:04, 44.67it/s][A
 52%|    | 227/435 [00:05<00:04, 44.72it/s][A
 53%|    | 232/435 [00:05<00:04, 44.63it/s][A
 54%|    | 237/435 [00:05<00:04, 44.65it/s][A
 56%|    | 242/435 [00:05<00:04, 44.75it/s][A
 57%|    | 247/435 [00:05<00:04, 44.78it/s][A
 58%|    | 252/435 [00:05<00:04, 44.86it/s][A
 59%|    | 257/435 [00:05<00:03, 44.83it/s][A
 60%|    | 262/435 [00:05<00:03, 44.86it/s][A
 61%|   | 267/435 [00:05<00:03, 44.83it/s][A
 63%|   | 272/435 [00:06<00:03, 44.74it/s][A
 64%|   | 277/435 [00:06<00:03, 44.55it/s][A
 65%|   | 282/435 [00:06<00:03, 44.63it/s][A
 66%|   | 287/435 [00:06<00:03, 44.67it/s][A
 67%|   | 292/435 [00:06<00:03, 44.57it/s][A
 68%|   | 297/435 [00:06<00:03, 44.82it/s][A
 69%|   | 302/435 [00:06<00:02, 44.78it/s][A
 71%|   | 307/435 [00:06<00:02, 44.73it/s][A
 72%|  | 312/435 [00:06<00:02, 44.72it/s][A
 73%|  | 317/435 [00:07<00:02, 44.54it/s][A
 74%|  | 322/435 [00:07<00:02, 44.47it/s][A
 75%|  | 327/435 [00:07<00:02, 44.41it/s][A
 76%|  | 332/435 [00:07<00:02, 44.62it/s][A
 77%|  | 337/435 [00:07<00:02, 37.29it/s][A
 79%|  | 342/435 [00:07<00:02, 40.03it/s][A
 80%|  | 347/435 [00:07<00:02, 41.42it/s][A
 81%|  | 352/435 [00:07<00:01, 42.49it/s][A
 82%| | 357/435 [00:08<00:01, 43.24it/s][A
 83%| | 362/435 [00:08<00:01, 43.83it/s][A
 84%| | 367/435 [00:08<00:01, 44.20it/s][A
 86%| | 372/435 [00:08<00:01, 44.38it/s][A
 87%| | 377/435 [00:08<00:01, 44.19it/s][A
 88%| | 382/435 [00:08<00:01, 44.15it/s][A
 89%| | 387/435 [00:08<00:01, 44.34it/s][A
 90%| | 392/435 [00:08<00:00, 44.50it/s][A
 91%|| 397/435 [00:08<00:00, 44.63it/s][A
 92%|| 402/435 [00:09<00:00, 44.82it/s][A
 94%|| 407/435 [00:09<00:00, 44.89it/s][A
 95%|| 412/435 [00:09<00:00, 44.95it/s][A
 96%|| 417/435 [00:09<00:00, 44.83it/s][A
 97%|| 422/435 [00:09<00:00, 44.66it/s][A
 98%|| 427/435 [00:09<00:00, 44.59it/s][A
 99%|| 432/435 [00:09<00:00, 44.49it/s][A                                                 
                                                 [A 40%|      | 312/780 [02:00<02:16,  3.42it/s]
100%|| 435/435 [00:09<00:00, 44.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:21:27,888 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 07:21:27,988 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:21:31,353 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:21:31,460 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:21:31,525 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:11<50:38,  6.51s/it] 40%|      | 314/780 [02:11<36:08,  4.65s/it] 40%|      | 315/780 [02:12<25:56,  3.35s/it] 41%|      | 316/780 [02:12<18:48,  2.43s/it] 41%|      | 317/780 [02:12<13:48,  1.79s/it] 41%|      | 318/780 [02:12<10:19,  1.34s/it] 41%|      | 319/780 [02:13<07:53,  1.03s/it] 41%|      | 320/780 [02:13<06:11,  1.24it/s] 41%|      | 321/780 [02:13<04:59,  1.53it/s] 41%|     | 322/780 [02:14<04:09,  1.84it/s] 41%|     | 323/780 [02:14<03:34,  2.13it/s] 42%|     | 324/780 [02:14<03:09,  2.41it/s] 42%|     | 325/780 [02:15<03:00,  2.52it/s] 42%|     | 326/780 [02:15<02:45,  2.74it/s] 42%|     | 327/780 [02:15<02:35,  2.91it/s] 42%|     | 328/780 [02:15<02:28,  3.04it/s] 42%|     | 329/780 [02:16<02:23,  3.15it/s] 42%|     | 330/780 [02:16<02:19,  3.23it/s] 42%|     | 331/780 [02:16<02:16,  3.28it/s] 43%|     | 332/780 [02:17<02:14,  3.32it/s] 43%|     | 333/780 [02:17<02:13,  3.34it/s] 43%|     | 334/780 [02:17<02:12,  3.36it/s] 43%|     | 335/780 [02:18<02:11,  3.38it/s] 43%|     | 336/780 [02:18<02:14,  3.30it/s] 43%|     | 337/780 [02:18<02:12,  3.33it/s] 43%|     | 338/780 [02:18<02:12,  3.34it/s] 43%|     | 339/780 [02:19<02:11,  3.36it/s] 44%|     | 340/780 [02:19<02:10,  3.37it/s] 44%|     | 341/780 [02:19<02:09,  3.40it/s] 44%|     | 342/780 [02:20<02:08,  3.42it/s] 44%|     | 343/780 [02:20<02:07,  3.43it/s] 44%|     | 344/780 [02:20<02:06,  3.44it/s] 44%|     | 345/780 [02:20<02:06,  3.45it/s] 44%|     | 346/780 [02:21<02:05,  3.45it/s] 44%|     | 347/780 [02:21<02:10,  3.32it/s] 45%|     | 348/780 [02:21<02:08,  3.36it/s] 45%|     | 349/780 [02:22<02:07,  3.39it/s] 45%|     | 350/780 [02:22<02:06,  3.41it/s] 45%|     | 351/780 [02:22<02:05,  3.43it/s] 45%|     | 352/780 [02:23<02:04,  3.44it/s] 45%|     | 353/780 [02:23<02:03,  3.45it/s] 45%|     | 354/780 [02:23<02:07,  3.33it/s] 46%|     | 355/780 [02:23<02:05,  3.37it/s] 46%|     | 356/780 [02:24<02:04,  3.39it/s] 46%|     | 357/780 [02:24<02:03,  3.42it/s] 46%|     | 358/780 [02:24<02:08,  3.28it/s] 46%|     | 359/780 [02:25<02:06,  3.34it/s] 46%|     | 360/780 [02:25<02:04,  3.37it/s] 46%|     | 361/780 [02:25<02:03,  3.40it/s] 46%|     | 362/780 [02:25<02:02,  3.42it/s] 47%|     | 363/780 [02:26<02:01,  3.43it/s] 47%|     | 364/780 [02:26<02:00,  3.44it/s] 47%|     | 365/780 [02:26<02:00,  3.45it/s] 47%|     | 366/780 [02:27<01:59,  3.45it/s] 47%|     | 367/780 [02:27<01:59,  3.45it/s] 47%|     | 368/780 [02:27<01:59,  3.46it/s] 47%|     | 369/780 [02:28<02:05,  3.28it/s] 47%|     | 370/780 [02:28<02:02,  3.34it/s] 48%|     | 371/780 [02:28<02:01,  3.37it/s] 48%|     | 372/780 [02:28<01:59,  3.40it/s] 48%|     | 373/780 [02:29<01:58,  3.42it/s] 48%|     | 374/780 [02:29<01:58,  3.43it/s] 48%|     | 375/780 [02:29<01:57,  3.44it/s] 48%|     | 376/780 [02:30<01:57,  3.45it/s] 48%|     | 377/780 [02:30<01:56,  3.45it/s] 48%|     | 378/780 [02:30<01:56,  3.46it/s] 49%|     | 379/780 [02:30<01:55,  3.46it/s] 49%|     | 380/780 [02:31<01:55,  3.46it/s] 49%|     | 381/780 [02:31<01:55,  3.46it/s] 49%|     | 382/780 [02:31<01:54,  3.46it/s] 49%|     | 383/780 [02:32<01:59,  3.33it/s] 49%|     | 384/780 [02:32<01:57,  3.37it/s] 49%|     | 385/780 [02:32<01:56,  3.40it/s] 49%|     | 386/780 [02:32<01:55,  3.42it/s] 50%|     | 387/780 [02:33<01:54,  3.44it/s] 50%|     | 388/780 [02:33<01:53,  3.44it/s] 50%|     | 389/780 [02:33<01:53,  3.45it/s] 50%|     | 390/780 [02:34<01:53,  3.45it/s] 50%|     | 391/780 [02:34<01:52,  3.45it/s] 50%|     | 392/780 [02:34<01:52,  3.46it/s] 50%|     | 393/780 [02:35<01:52,  3.45it/s] 51%|     | 394/780 [02:35<01:55,  3.34it/s] 51%|     | 395/780 [02:35<01:54,  3.37it/s] 51%|     | 396/780 [02:35<01:52,  3.40it/s] 51%|     | 397/780 [02:36<01:52,  3.42it/s] 51%|     | 398/780 [02:36<01:51,  3.43it/s] 51%|     | 399/780 [02:36<01:50,  3.44it/s] 51%|    | 400/780 [02:37<01:50,  3.45it/s] 51%|    | 401/780 [02:37<01:50,  3.44it/s] 52%|    | 402/780 [02:37<01:49,  3.44it/s] 52%|    | 403/780 [02:37<01:49,  3.45it/s] 52%|    | 404/780 [02:38<01:48,  3.46it/s] 52%|    | 405/780 [02:38<01:54,  3.26it/s] 52%|    | 406/780 [02:38<01:52,  3.33it/s] 52%|    | 407/780 [02:39<01:51,  3.36it/s] 52%|    | 408/780 [02:39<01:49,  3.39it/s] 52%|    | 409/780 [02:39<01:48,  3.40it/s] 53%|    | 410/780 [02:40<01:48,  3.42it/s] 53%|    | 411/780 [02:40<01:47,  3.43it/s] 53%|    | 412/780 [02:40<01:46,  3.44it/s] 53%|    | 413/780 [02:40<01:46,  3.44it/s] 53%|    | 414/780 [02:41<01:46,  3.44it/s] 53%|    | 415/780 [02:41<01:46,  3.44it/s] 53%|    | 416/780 [02:41<01:49,  3.34it/s] 53%|    | 417/780 [02:42<01:47,  3.37it/s] 54%|    | 418/780 [02:42<01:46,  3.39it/s] 54%|    | 419/780 [02:42<01:46,  3.40it/s] 54%|    | 420/780 [02:42<01:45,  3.40it/s] 54%|    | 421/780 [02:43<01:45,  3.41it/s] 54%|    | 422/780 [02:43<01:44,  3.43it/s] 54%|    | 423/780 [02:43<01:43,  3.44it/s] 54%|    | 424/780 [02:44<01:43,  3.45it/s] 54%|    | 425/780 [02:44<01:42,  3.45it/s] 55%|    | 426/780 [02:44<01:42,  3.45it/s] 55%|    | 427/780 [02:45<01:47,  3.28it/s] 55%|    | 428/780 [02:45<01:45,  3.33it/s] 55%|    | 429/780 [02:45<01:44,  3.37it/s] 55%|    | 430/780 [02:45<01:42,  3.40it/s] 55%|    | 431/780 [02:46<01:42,  3.41it/s] 55%|    | 432/780 [02:46<01:41,  3.43it/s] 56%|    | 433/780 [02:46<01:41,  3.44it/s] 56%|    | 434/780 [02:47<01:40,  3.44it/s] 56%|    | 435/780 [02:47<01:40,  3.45it/s] 56%|    | 436/780 [02:47<01:39,  3.44it/s] 56%|    | 437/780 [02:47<01:39,  3.44it/s] 56%|    | 438/780 [02:48<01:45,  3.23it/s] 56%|    | 439/780 [02:48<01:43,  3.29it/s] 56%|    | 440/780 [02:48<01:41,  3.34it/s] 57%|    | 441/780 [02:49<01:40,  3.37it/s] 57%|    | 442/780 [02:49<01:39,  3.39it/s] 57%|    | 443/780 [02:49<01:38,  3.41it/s] 57%|    | 444/780 [02:50<01:38,  3.42it/s] 57%|    | 445/780 [02:50<01:37,  3.43it/s] 57%|    | 446/780 [02:50<01:37,  3.44it/s] 57%|    | 447/780 [02:50<01:36,  3.44it/s] 57%|    | 448/780 [02:51<01:36,  3.45it/s] 58%|    | 449/780 [02:51<01:41,  3.25it/s] 58%|    | 450/780 [02:51<01:39,  3.30it/s] 58%|    | 451/780 [02:52<01:38,  3.33it/s] 58%|    | 452/780 [02:52<01:37,  3.36it/s] 58%|    | 453/780 [02:52<01:36,  3.39it/s] 58%|    | 454/780 [02:52<01:35,  3.40it/s] 58%|    | 455/780 [02:53<01:35,  3.42it/s] 58%|    | 456/780 [02:53<01:34,  3.42it/s] 59%|    | 457/780 [02:53<01:34,  3.43it/s] 59%|    | 458/780 [02:54<01:33,  3.43it/s] 59%|    | 459/780 [02:54<01:33,  3.44it/s] 59%|    | 460/780 [02:54<01:34,  3.38it/s] 59%|    | 461/780 [02:55<01:33,  3.40it/s] 59%|    | 462/780 [02:55<01:33,  3.41it/s] 59%|    | 463/780 [02:55<01:32,  3.42it/s] 59%|    | 464/780 [02:55<01:32,  3.43it/s] 60%|    | 465/780 [02:56<01:31,  3.43it/s] 60%|    | 466/780 [02:56<01:31,  3.43it/s] 60%|    | 467/780 [02:56<01:31,  3.44it/s] 60%|    | 468/780 [02:57<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 07:22:24,563 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:22:24,563 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 07:22:24,563 >>   Batch size = 8
{'eval_loss': 0.9788810014724731, 'eval_runtime': 9.8222, 'eval_samples_per_second': 353.994, 'eval_steps_per_second': 44.287, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.94it/s][A
  3%|         | 12/435 [00:00<00:08, 48.91it/s][A
  4%|         | 17/435 [00:00<00:08, 47.26it/s][A
  5%|         | 22/435 [00:00<00:09, 44.65it/s][A
  6%|         | 27/435 [00:00<00:09, 44.68it/s][A
  7%|         | 32/435 [00:00<00:09, 44.59it/s][A
  9%|         | 37/435 [00:00<00:08, 44.51it/s][A
 10%|         | 42/435 [00:00<00:08, 44.40it/s][A
 11%|         | 47/435 [00:01<00:09, 42.59it/s][A
 12%|        | 52/435 [00:01<00:08, 43.43it/s][A
 13%|        | 57/435 [00:01<00:08, 43.96it/s][A
 14%|        | 62/435 [00:01<00:08, 44.25it/s][A
 15%|        | 67/435 [00:01<00:08, 44.55it/s][A
 17%|        | 72/435 [00:01<00:08, 44.65it/s][A
 18%|        | 77/435 [00:01<00:08, 44.54it/s][A
 19%|        | 82/435 [00:01<00:07, 44.53it/s][A
 20%|        | 87/435 [00:01<00:07, 44.44it/s][A
 21%|        | 92/435 [00:02<00:07, 44.48it/s][A
 22%|       | 97/435 [00:02<00:07, 44.56it/s][A
 23%|       | 102/435 [00:02<00:07, 44.64it/s][A
 25%|       | 107/435 [00:02<00:07, 44.75it/s][A
 26%|       | 112/435 [00:02<00:07, 44.84it/s][A
 27%|       | 117/435 [00:02<00:07, 44.84it/s][A
 28%|       | 122/435 [00:02<00:06, 44.75it/s][A
 29%|       | 127/435 [00:02<00:06, 44.52it/s][A
 30%|       | 132/435 [00:02<00:06, 44.37it/s][A
 31%|      | 137/435 [00:03<00:06, 44.57it/s][A
 33%|      | 142/435 [00:03<00:06, 44.59it/s][A
 34%|      | 147/435 [00:03<00:06, 44.73it/s][A
 35%|      | 152/435 [00:03<00:06, 44.79it/s][A
 36%|      | 157/435 [00:03<00:06, 44.77it/s][A
 37%|      | 162/435 [00:03<00:06, 44.97it/s][A
 38%|      | 167/435 [00:03<00:05, 44.88it/s][A
 40%|      | 172/435 [00:03<00:05, 44.84it/s][A
 41%|      | 177/435 [00:03<00:05, 44.74it/s][A
 42%|     | 182/435 [00:04<00:05, 44.62it/s][A
 43%|     | 187/435 [00:04<00:05, 44.57it/s][A
 44%|     | 192/435 [00:04<00:05, 44.67it/s][A
 45%|     | 197/435 [00:04<00:05, 44.75it/s][A
 46%|     | 202/435 [00:04<00:05, 44.87it/s][A
 48%|     | 207/435 [00:04<00:05, 44.78it/s][A
 49%|     | 212/435 [00:04<00:04, 44.79it/s][A
 50%|     | 217/435 [00:04<00:04, 44.74it/s][A
 51%|     | 222/435 [00:04<00:04, 44.73it/s][A
 52%|    | 227/435 [00:05<00:04, 44.71it/s][A
 53%|    | 232/435 [00:05<00:04, 44.55it/s][A
 54%|    | 237/435 [00:05<00:04, 44.63it/s][A
 56%|    | 242/435 [00:05<00:04, 44.12it/s][A
 57%|    | 247/435 [00:05<00:04, 44.42it/s][A
 58%|    | 252/435 [00:05<00:04, 44.52it/s][A
 59%|    | 257/435 [00:05<00:03, 44.53it/s][A
 60%|    | 262/435 [00:05<00:03, 44.60it/s][A
 61%|   | 267/435 [00:05<00:03, 44.57it/s][A
 63%|   | 272/435 [00:06<00:03, 44.58it/s][A
 64%|   | 277/435 [00:06<00:03, 44.62it/s][A
 65%|   | 282/435 [00:06<00:03, 44.50it/s][A
 66%|   | 287/435 [00:06<00:03, 44.66it/s][A
 67%|   | 292/435 [00:06<00:03, 44.71it/s][A
 68%|   | 297/435 [00:06<00:03, 44.76it/s][A
 69%|   | 302/435 [00:06<00:02, 44.82it/s][A
 71%|   | 307/435 [00:06<00:02, 44.70it/s][A
 72%|  | 312/435 [00:06<00:02, 44.71it/s][A
 73%|  | 317/435 [00:07<00:02, 44.51it/s][A
 74%|  | 322/435 [00:07<00:02, 44.51it/s][A
 75%|  | 327/435 [00:07<00:02, 44.59it/s][A
 76%|  | 332/435 [00:07<00:02, 44.58it/s][A
 77%|  | 337/435 [00:07<00:02, 44.75it/s][A
 79%|  | 342/435 [00:07<00:02, 44.85it/s][A
 80%|  | 347/435 [00:07<00:01, 44.81it/s][A
 81%|  | 352/435 [00:07<00:01, 44.83it/s][A
 82%| | 357/435 [00:07<00:01, 44.73it/s][A
 83%| | 362/435 [00:08<00:01, 44.74it/s][A
 84%| | 367/435 [00:08<00:01, 44.68it/s][A
 86%| | 372/435 [00:08<00:01, 44.55it/s][A
 87%| | 377/435 [00:08<00:01, 44.08it/s][A
 88%| | 382/435 [00:08<00:01, 44.22it/s][A
 89%| | 387/435 [00:08<00:01, 44.55it/s][A
 90%| | 392/435 [00:08<00:00, 44.70it/s][A
 91%|| 397/435 [00:08<00:00, 44.67it/s][A
 92%|| 402/435 [00:08<00:00, 44.80it/s][A
 94%|| 407/435 [00:09<00:00, 44.75it/s][A
 95%|| 412/435 [00:09<00:00, 44.71it/s][A
 96%|| 417/435 [00:09<00:00, 44.61it/s][A
 97%|| 422/435 [00:09<00:00, 44.62it/s][A
 98%|| 427/435 [00:09<00:00, 44.67it/s][A
 99%|| 432/435 [00:09<00:00, 44.67it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.67it/s][A 60%|    | 468/780 [03:06<01:30,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:22:34,503 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 07:22:34,650 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:22:37,438 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:22:37,541 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:22:37,592 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:15<29:57,  5.78s/it] 60%|    | 470/780 [03:15<21:24,  4.14s/it] 60%|    | 471/780 [03:16<15:23,  2.99s/it] 61%|    | 472/780 [03:16<11:11,  2.18s/it] 61%|    | 473/780 [03:16<08:15,  1.61s/it] 61%|    | 474/780 [03:17<06:12,  1.22s/it] 61%|    | 475/780 [03:17<04:46,  1.06it/s] 61%|    | 476/780 [03:17<03:47,  1.34it/s] 61%|    | 477/780 [03:18<03:04,  1.64it/s] 61%|   | 478/780 [03:18<02:35,  1.94it/s] 61%|   | 479/780 [03:18<02:15,  2.23it/s] 62%|   | 480/780 [03:18<02:00,  2.48it/s] 62%|   | 481/780 [03:19<01:52,  2.67it/s] 62%|   | 482/780 [03:19<01:44,  2.85it/s] 62%|   | 483/780 [03:19<01:38,  3.02it/s] 62%|   | 484/780 [03:20<01:34,  3.13it/s] 62%|   | 485/780 [03:20<01:31,  3.23it/s] 62%|   | 486/780 [03:20<01:29,  3.29it/s] 62%|   | 487/780 [03:20<01:27,  3.34it/s] 63%|   | 488/780 [03:21<01:26,  3.37it/s] 63%|   | 489/780 [03:21<01:25,  3.40it/s] 63%|   | 490/780 [03:21<01:24,  3.41it/s] 63%|   | 491/780 [03:22<01:24,  3.42it/s] 63%|   | 492/780 [03:22<01:25,  3.37it/s] 63%|   | 493/780 [03:22<01:24,  3.39it/s] 63%|   | 494/780 [03:23<01:23,  3.41it/s] 63%|   | 495/780 [03:23<01:23,  3.42it/s] 64%|   | 496/780 [03:24<01:58,  2.40it/s] 64%|   | 497/780 [03:24<01:46,  2.65it/s] 64%|   | 498/780 [03:24<01:39,  2.85it/s] 64%|   | 499/780 [03:24<01:33,  3.01it/s] 64%|   | 500/780 [03:25<01:29,  3.13it/s]                                                  64%|   | 500/780 [03:25<01:29,  3.13it/s] 64%|   | 501/780 [03:25<01:29,  3.12it/s] 64%|   | 502/780 [03:25<01:26,  3.21it/s] 64%|   | 503/780 [03:26<01:24,  3.29it/s] 65%|   | 504/780 [03:26<01:22,  3.33it/s] 65%|   | 505/780 [03:26<01:21,  3.37it/s] 65%|   | 506/780 [03:26<01:20,  3.39it/s] 65%|   | 507/780 [03:27<01:20,  3.41it/s] 65%|   | 508/780 [03:27<01:19,  3.42it/s] 65%|   | 509/780 [03:27<01:18,  3.43it/s] 65%|   | 510/780 [03:28<01:18,  3.44it/s] 66%|   | 511/780 [03:28<01:18,  3.44it/s] 66%|   | 512/780 [03:28<01:21,  3.30it/s] 66%|   | 513/780 [03:29<01:19,  3.34it/s] 66%|   | 514/780 [03:29<01:18,  3.38it/s] 66%|   | 515/780 [03:29<01:17,  3.40it/s] 66%|   | 516/780 [03:29<01:17,  3.42it/s] 66%|   | 517/780 [03:30<01:16,  3.43it/s] 66%|   | 518/780 [03:30<01:17,  3.36it/s] 67%|   | 519/780 [03:30<01:16,  3.39it/s] 67%|   | 520/780 [03:31<01:16,  3.41it/s] 67%|   | 521/780 [03:31<01:15,  3.42it/s] 67%|   | 522/780 [03:31<01:15,  3.43it/s] 67%|   | 523/780 [03:31<01:14,  3.44it/s] 67%|   | 524/780 [03:32<01:14,  3.44it/s] 67%|   | 525/780 [03:32<01:14,  3.44it/s] 67%|   | 526/780 [03:32<01:13,  3.45it/s] 68%|   | 527/780 [03:33<01:13,  3.45it/s] 68%|   | 528/780 [03:33<01:13,  3.45it/s] 68%|   | 529/780 [03:33<01:14,  3.39it/s] 68%|   | 530/780 [03:33<01:13,  3.41it/s] 68%|   | 531/780 [03:34<01:12,  3.42it/s] 68%|   | 532/780 [03:34<01:12,  3.43it/s] 68%|   | 533/780 [03:34<01:11,  3.44it/s] 68%|   | 534/780 [03:35<01:11,  3.45it/s] 69%|   | 535/780 [03:35<01:11,  3.45it/s] 69%|   | 536/780 [03:35<01:10,  3.45it/s] 69%|   | 537/780 [03:35<01:10,  3.45it/s] 69%|   | 538/780 [03:36<01:10,  3.45it/s] 69%|   | 539/780 [03:36<01:09,  3.45it/s] 69%|   | 540/780 [03:36<01:11,  3.37it/s] 69%|   | 541/780 [03:37<01:10,  3.40it/s] 69%|   | 542/780 [03:37<01:09,  3.41it/s] 70%|   | 543/780 [03:37<01:09,  3.43it/s] 70%|   | 544/780 [03:38<01:08,  3.43it/s] 70%|   | 545/780 [03:38<01:08,  3.44it/s] 70%|   | 546/780 [03:38<01:07,  3.44it/s] 70%|   | 547/780 [03:38<01:07,  3.45it/s] 70%|   | 548/780 [03:39<01:07,  3.45it/s] 70%|   | 549/780 [03:39<01:06,  3.45it/s] 71%|   | 550/780 [03:39<01:06,  3.45it/s] 71%|   | 551/780 [03:40<01:07,  3.39it/s] 71%|   | 552/780 [03:40<01:06,  3.40it/s] 71%|   | 553/780 [03:40<01:06,  3.42it/s] 71%|   | 554/780 [03:40<01:05,  3.43it/s] 71%|   | 555/780 [03:41<01:05,  3.44it/s] 71%|  | 556/780 [03:41<01:05,  3.44it/s] 71%|  | 557/780 [03:41<01:04,  3.45it/s] 72%|  | 558/780 [03:42<01:04,  3.45it/s] 72%|  | 559/780 [03:42<01:04,  3.45it/s] 72%|  | 560/780 [03:42<01:03,  3.45it/s] 72%|  | 561/780 [03:42<01:03,  3.45it/s] 72%|  | 562/780 [03:43<01:04,  3.37it/s] 72%|  | 563/780 [03:43<01:03,  3.39it/s] 72%|  | 564/780 [03:43<01:03,  3.41it/s] 72%|  | 565/780 [03:44<01:02,  3.42it/s] 73%|  | 566/780 [03:44<01:02,  3.43it/s] 73%|  | 567/780 [03:44<01:01,  3.44it/s] 73%|  | 568/780 [03:45<01:01,  3.45it/s] 73%|  | 569/780 [03:45<01:01,  3.45it/s] 73%|  | 570/780 [03:45<01:00,  3.45it/s] 73%|  | 571/780 [03:45<01:00,  3.45it/s] 73%|  | 572/780 [03:46<01:00,  3.45it/s] 73%|  | 573/780 [03:46<01:01,  3.36it/s] 74%|  | 574/780 [03:46<01:00,  3.39it/s] 74%|  | 575/780 [03:47<01:00,  3.40it/s] 74%|  | 576/780 [03:47<00:59,  3.41it/s] 74%|  | 577/780 [03:47<00:59,  3.41it/s] 74%|  | 578/780 [03:47<00:58,  3.43it/s] 74%|  | 579/780 [03:48<00:58,  3.43it/s] 74%|  | 580/780 [03:48<00:58,  3.44it/s] 74%|  | 581/780 [03:48<00:57,  3.44it/s] 75%|  | 582/780 [03:49<00:57,  3.45it/s] 75%|  | 583/780 [03:49<00:57,  3.45it/s] 75%|  | 584/780 [03:49<00:59,  3.28it/s] 75%|  | 585/780 [03:50<00:58,  3.32it/s] 75%|  | 586/780 [03:50<00:57,  3.36it/s] 75%|  | 587/780 [03:50<00:56,  3.39it/s] 75%|  | 588/780 [03:50<00:56,  3.41it/s] 76%|  | 589/780 [03:51<00:55,  3.42it/s] 76%|  | 590/780 [03:51<00:55,  3.43it/s] 76%|  | 591/780 [03:51<00:54,  3.44it/s] 76%|  | 592/780 [03:52<00:54,  3.45it/s] 76%|  | 593/780 [03:52<00:54,  3.44it/s] 76%|  | 594/780 [03:52<00:53,  3.45it/s] 76%|  | 595/780 [03:52<00:55,  3.33it/s] 76%|  | 596/780 [03:53<00:54,  3.36it/s] 77%|  | 597/780 [03:53<00:54,  3.39it/s] 77%|  | 598/780 [03:53<00:53,  3.40it/s] 77%|  | 599/780 [03:54<00:53,  3.41it/s] 77%|  | 600/780 [03:54<00:52,  3.43it/s] 77%|  | 601/780 [03:54<00:52,  3.43it/s] 77%|  | 602/780 [03:54<00:51,  3.43it/s] 77%|  | 603/780 [03:55<00:51,  3.44it/s] 77%|  | 604/780 [03:55<00:51,  3.44it/s] 78%|  | 605/780 [03:55<00:50,  3.44it/s] 78%|  | 606/780 [03:56<00:51,  3.37it/s] 78%|  | 607/780 [03:56<00:51,  3.39it/s] 78%|  | 608/780 [03:56<00:50,  3.41it/s] 78%|  | 609/780 [03:57<00:50,  3.41it/s] 78%|  | 610/780 [03:57<00:49,  3.42it/s] 78%|  | 611/780 [03:57<00:49,  3.43it/s] 78%|  | 612/780 [03:57<00:48,  3.43it/s] 79%|  | 613/780 [03:58<00:48,  3.43it/s] 79%|  | 614/780 [03:58<00:48,  3.44it/s] 79%|  | 615/780 [03:58<00:47,  3.44it/s] 79%|  | 616/780 [03:59<00:47,  3.44it/s] 79%|  | 617/780 [03:59<00:48,  3.37it/s] 79%|  | 618/780 [03:59<00:47,  3.39it/s] 79%|  | 619/780 [03:59<00:47,  3.41it/s] 79%|  | 620/780 [04:00<00:46,  3.42it/s] 80%|  | 621/780 [04:00<00:46,  3.43it/s] 80%|  | 622/780 [04:00<00:46,  3.43it/s] 80%|  | 623/780 [04:01<00:45,  3.44it/s] 80%|  | 624/780 [04:01<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 07:23:28,921 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:23:28,921 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 07:23:28,921 >>   Batch size = 8
{'eval_loss': 0.9944688081741333, 'eval_runtime': 9.7866, 'eval_samples_per_second': 355.282, 'eval_steps_per_second': 44.449, 'epoch': 3.0}
{'loss': 0.571, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.05it/s][A
  3%|         | 12/435 [00:00<00:08, 49.02it/s][A
  4%|         | 17/435 [00:00<00:08, 47.12it/s][A
  5%|         | 22/435 [00:00<00:08, 46.40it/s][A
  6%|         | 27/435 [00:00<00:08, 45.60it/s][A
  7%|         | 32/435 [00:00<00:08, 45.23it/s][A
  9%|         | 37/435 [00:00<00:08, 44.97it/s][A
 10%|         | 42/435 [00:00<00:08, 44.57it/s][A
 11%|         | 47/435 [00:01<00:08, 44.68it/s][A
 12%|        | 52/435 [00:01<00:08, 44.75it/s][A
 13%|        | 57/435 [00:01<00:08, 44.87it/s][A
 14%|        | 62/435 [00:01<00:08, 44.99it/s][A
 15%|        | 67/435 [00:01<00:08, 44.95it/s][A
 17%|        | 72/435 [00:01<00:08, 44.94it/s][A
 18%|        | 77/435 [00:01<00:07, 44.86it/s][A
 19%|        | 82/435 [00:01<00:07, 44.71it/s][A
 20%|        | 87/435 [00:01<00:07, 44.59it/s][A
 21%|        | 92/435 [00:02<00:07, 44.53it/s][A
 22%|       | 97/435 [00:02<00:07, 44.70it/s][A
 23%|       | 102/435 [00:02<00:07, 44.71it/s][A
 25%|       | 107/435 [00:02<00:07, 44.77it/s][A
 26%|       | 112/435 [00:02<00:07, 44.50it/s][A
 27%|       | 117/435 [00:02<00:07, 44.55it/s][A
 28%|       | 122/435 [00:02<00:07, 44.66it/s][A
 29%|       | 127/435 [00:02<00:06, 44.57it/s][A
 30%|       | 132/435 [00:02<00:06, 44.42it/s][A
 31%|      | 137/435 [00:03<00:06, 44.48it/s][A
 33%|      | 142/435 [00:03<00:06, 44.46it/s][A
 34%|      | 147/435 [00:03<00:06, 44.63it/s][A
 35%|      | 152/435 [00:03<00:06, 44.75it/s][A
 36%|      | 157/435 [00:03<00:06, 44.74it/s][A
 37%|      | 162/435 [00:03<00:06, 44.89it/s][A
 38%|      | 167/435 [00:03<00:05, 44.80it/s][A
 40%|      | 172/435 [00:03<00:05, 44.82it/s][A
 41%|      | 177/435 [00:03<00:05, 44.68it/s][A
 42%|     | 182/435 [00:04<00:05, 44.49it/s][A
 43%|     | 187/435 [00:04<00:05, 44.54it/s][A
 44%|     | 192/435 [00:04<00:05, 44.65it/s][A
 45%|     | 197/435 [00:04<00:05, 44.74it/s][A
 46%|     | 202/435 [00:04<00:05, 44.79it/s][A
 48%|     | 207/435 [00:04<00:05, 44.73it/s][A
 49%|     | 212/435 [00:04<00:04, 44.81it/s][A
 50%|     | 217/435 [00:04<00:04, 44.81it/s][A
 51%|     | 222/435 [00:04<00:04, 44.81it/s][A
 52%|    | 227/435 [00:05<00:04, 44.37it/s][A
 53%|    | 232/435 [00:05<00:04, 44.77it/s][A
 54%|    | 237/435 [00:05<00:04, 44.72it/s][A
 56%|    | 242/435 [00:05<00:04, 44.78it/s][A
 57%|    | 247/435 [00:05<00:04, 44.52it/s][A
 58%|    | 252/435 [00:05<00:04, 44.64it/s][A
 59%|    | 257/435 [00:05<00:03, 44.64it/s][A
 60%|    | 262/435 [00:05<00:03, 44.62it/s][A
 61%|   | 267/435 [00:05<00:03, 44.56it/s][A
 63%|   | 272/435 [00:06<00:03, 44.67it/s][A
 64%|   | 277/435 [00:06<00:03, 44.63it/s][A
 65%|   | 282/435 [00:06<00:03, 44.54it/s][A
 66%|   | 287/435 [00:06<00:03, 44.55it/s][A
 67%|   | 292/435 [00:06<00:03, 44.75it/s][A
 68%|   | 297/435 [00:06<00:03, 44.88it/s][A
 69%|   | 302/435 [00:06<00:02, 44.91it/s][A
 71%|   | 307/435 [00:06<00:02, 44.80it/s][A
 72%|  | 312/435 [00:06<00:02, 44.86it/s][A
 73%|  | 317/435 [00:07<00:02, 44.74it/s][A
 74%|  | 322/435 [00:07<00:02, 44.76it/s][A
 75%|  | 327/435 [00:07<00:02, 44.70it/s][A
 76%|  | 332/435 [00:07<00:02, 44.64it/s][A
 77%|  | 337/435 [00:07<00:02, 44.66it/s][A
 79%|  | 342/435 [00:07<00:02, 44.62it/s][A
 80%|  | 347/435 [00:07<00:01, 44.82it/s][A
 81%|  | 352/435 [00:07<00:01, 44.84it/s][A
 82%| | 357/435 [00:07<00:01, 44.83it/s][A
 83%| | 362/435 [00:08<00:01, 44.66it/s][A
 84%| | 367/435 [00:08<00:01, 44.66it/s][A
 86%| | 372/435 [00:08<00:01, 44.66it/s][A
 87%| | 377/435 [00:08<00:01, 44.63it/s][A
 88%| | 382/435 [00:08<00:01, 44.03it/s][A
 89%| | 387/435 [00:08<00:01, 44.20it/s][A
 90%| | 392/435 [00:08<00:00, 44.45it/s][A
 91%|| 397/435 [00:08<00:00, 44.63it/s][A
 92%|| 402/435 [00:08<00:00, 44.65it/s][A
 94%|| 407/435 [00:09<00:00, 44.76it/s][A
 95%|| 412/435 [00:09<00:00, 44.61it/s][A
 96%|| 417/435 [00:09<00:00, 44.67it/s][A
 97%|| 422/435 [00:09<00:00, 44.64it/s][A
 98%|| 427/435 [00:09<00:00, 44.61it/s][A
 99%|| 432/435 [00:09<00:00, 44.63it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.63it/s][A 80%|  | 624/780 [04:11<00:45,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:23:38,878 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 07:23:39,046 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:23:41,721 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:23:41,828 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:23:41,897 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:20<15:08,  5.86s/it] 80%|  | 626/780 [04:20<10:46,  4.20s/it] 80%|  | 627/780 [04:20<07:43,  3.03s/it] 81%|  | 628/780 [04:21<05:35,  2.21s/it] 81%|  | 629/780 [04:21<04:06,  1.63s/it] 81%|  | 630/780 [04:21<03:04,  1.23s/it] 81%|  | 631/780 [04:22<02:21,  1.05it/s] 81%|  | 632/780 [04:22<01:51,  1.33it/s] 81%|  | 633/780 [04:22<01:30,  1.62it/s] 81%| | 634/780 [04:22<01:15,  1.93it/s] 81%| | 635/780 [04:23<01:05,  2.22it/s] 82%| | 636/780 [04:23<00:58,  2.48it/s] 82%| | 637/780 [04:23<00:55,  2.58it/s] 82%| | 638/780 [04:24<00:54,  2.61it/s] 82%| | 639/780 [04:24<00:50,  2.81it/s] 82%| | 640/780 [04:24<00:47,  2.96it/s] 82%| | 641/780 [04:25<00:45,  3.09it/s] 82%| | 642/780 [04:25<00:43,  3.17it/s] 82%| | 643/780 [04:25<00:42,  3.24it/s] 83%| | 644/780 [04:26<00:41,  3.29it/s] 83%| | 645/780 [04:26<00:40,  3.33it/s] 83%| | 646/780 [04:26<00:40,  3.35it/s] 83%| | 647/780 [04:26<00:42,  3.16it/s] 83%| | 648/780 [04:27<00:40,  3.23it/s] 83%| | 649/780 [04:27<00:39,  3.28it/s] 83%| | 650/780 [04:27<00:39,  3.31it/s] 83%| | 651/780 [04:28<00:38,  3.35it/s] 84%| | 652/780 [04:28<00:38,  3.36it/s] 84%| | 653/780 [04:28<00:37,  3.38it/s] 84%| | 654/780 [04:29<00:37,  3.39it/s] 84%| | 655/780 [04:29<00:36,  3.40it/s] 84%| | 656/780 [04:29<00:36,  3.40it/s] 84%| | 657/780 [04:29<00:38,  3.23it/s] 84%| | 658/780 [04:30<00:37,  3.28it/s] 84%| | 659/780 [04:30<00:36,  3.32it/s] 85%| | 660/780 [04:30<00:35,  3.34it/s] 85%| | 661/780 [04:31<00:35,  3.36it/s] 85%| | 662/780 [04:31<00:34,  3.38it/s] 85%| | 663/780 [04:31<00:34,  3.39it/s] 85%| | 664/780 [04:32<00:34,  3.39it/s] 85%| | 665/780 [04:32<00:33,  3.39it/s] 85%| | 666/780 [04:32<00:33,  3.39it/s] 86%| | 667/780 [04:32<00:33,  3.39it/s] 86%| | 668/780 [04:33<00:33,  3.39it/s] 86%| | 669/780 [04:33<00:32,  3.40it/s] 86%| | 670/780 [04:33<00:32,  3.40it/s] 86%| | 671/780 [04:34<00:32,  3.40it/s] 86%| | 672/780 [04:34<00:31,  3.40it/s] 86%| | 673/780 [04:34<00:31,  3.40it/s] 86%| | 674/780 [04:35<00:32,  3.24it/s] 87%| | 675/780 [04:35<00:31,  3.29it/s] 87%| | 676/780 [04:35<00:31,  3.32it/s] 87%| | 677/780 [04:35<00:30,  3.35it/s] 87%| | 678/780 [04:36<00:30,  3.36it/s] 87%| | 679/780 [04:36<00:29,  3.37it/s] 87%| | 680/780 [04:36<00:29,  3.38it/s] 87%| | 681/780 [04:37<00:29,  3.39it/s] 87%| | 682/780 [04:37<00:28,  3.39it/s] 88%| | 683/780 [04:37<00:28,  3.39it/s] 88%| | 684/780 [04:37<00:28,  3.40it/s] 88%| | 685/780 [04:38<00:28,  3.30it/s] 88%| | 686/780 [04:38<00:28,  3.33it/s] 88%| | 687/780 [04:38<00:27,  3.35it/s] 88%| | 688/780 [04:39<00:27,  3.37it/s] 88%| | 689/780 [04:39<00:26,  3.38it/s] 88%| | 690/780 [04:39<00:26,  3.38it/s] 89%| | 691/780 [04:40<00:26,  3.39it/s] 89%| | 692/780 [04:40<00:25,  3.39it/s] 89%| | 693/780 [04:40<00:25,  3.40it/s] 89%| | 694/780 [04:40<00:25,  3.40it/s] 89%| | 695/780 [04:41<00:25,  3.40it/s] 89%| | 696/780 [04:41<00:25,  3.34it/s] 89%| | 697/780 [04:41<00:24,  3.35it/s] 89%| | 698/780 [04:42<00:24,  3.37it/s] 90%| | 699/780 [04:42<00:23,  3.38it/s] 90%| | 700/780 [04:42<00:23,  3.39it/s] 90%| | 701/780 [04:42<00:23,  3.39it/s] 90%| | 702/780 [04:43<00:22,  3.40it/s] 90%| | 703/780 [04:43<00:22,  3.40it/s] 90%| | 704/780 [04:43<00:22,  3.40it/s] 90%| | 705/780 [04:44<00:22,  3.40it/s] 91%| | 706/780 [04:44<00:21,  3.40it/s] 91%| | 707/780 [04:44<00:21,  3.32it/s] 91%| | 708/780 [04:45<00:21,  3.34it/s] 91%| | 709/780 [04:45<00:21,  3.36it/s] 91%| | 710/780 [04:45<00:20,  3.37it/s] 91%| | 711/780 [04:45<00:20,  3.37it/s] 91%|| 712/780 [04:46<00:20,  3.38it/s] 91%|| 713/780 [04:46<00:19,  3.39it/s] 92%|| 714/780 [04:46<00:19,  3.41it/s] 92%|| 715/780 [04:47<00:18,  3.43it/s] 92%|| 716/780 [04:47<00:18,  3.44it/s] 92%|| 717/780 [04:47<00:18,  3.44it/s] 92%|| 718/780 [04:47<00:17,  3.45it/s] 92%|| 719/780 [04:48<00:17,  3.45it/s] 92%|| 720/780 [04:48<00:17,  3.45it/s] 92%|| 721/780 [04:48<00:17,  3.45it/s] 93%|| 722/780 [04:49<00:16,  3.46it/s] 93%|| 723/780 [04:49<00:16,  3.46it/s] 93%|| 724/780 [04:49<00:16,  3.46it/s] 93%|| 725/780 [04:50<00:15,  3.46it/s] 93%|| 726/780 [04:50<00:16,  3.34it/s] 93%|| 727/780 [04:50<00:15,  3.37it/s] 93%|| 728/780 [04:50<00:15,  3.39it/s] 93%|| 729/780 [04:51<00:14,  3.41it/s] 94%|| 730/780 [04:51<00:14,  3.43it/s] 94%|| 731/780 [04:51<00:14,  3.44it/s] 94%|| 732/780 [04:52<00:13,  3.45it/s] 94%|| 733/780 [04:52<00:13,  3.46it/s] 94%|| 734/780 [04:52<00:13,  3.46it/s] 94%|| 735/780 [04:52<00:12,  3.46it/s] 94%|| 736/780 [04:53<00:12,  3.47it/s] 94%|| 737/780 [04:53<00:13,  3.25it/s] 95%|| 738/780 [04:53<00:12,  3.32it/s] 95%|| 739/780 [04:54<00:12,  3.36it/s] 95%|| 740/780 [04:54<00:11,  3.39it/s] 95%|| 741/780 [04:54<00:11,  3.41it/s] 95%|| 742/780 [04:55<00:11,  3.43it/s] 95%|| 743/780 [04:55<00:10,  3.43it/s] 95%|| 744/780 [04:55<00:10,  3.44it/s] 96%|| 745/780 [04:55<00:10,  3.44it/s] 96%|| 746/780 [04:56<00:09,  3.44it/s] 96%|| 747/780 [04:56<00:09,  3.44it/s] 96%|| 748/780 [04:56<00:09,  3.23it/s] 96%|| 749/780 [04:57<00:09,  3.29it/s] 96%|| 750/780 [04:57<00:08,  3.34it/s] 96%|| 751/780 [04:57<00:08,  3.38it/s] 96%|| 752/780 [04:57<00:08,  3.40it/s] 97%|| 753/780 [04:58<00:07,  3.42it/s] 97%|| 754/780 [04:58<00:07,  3.43it/s] 97%|| 755/780 [04:58<00:07,  3.44it/s] 97%|| 756/780 [04:59<00:06,  3.45it/s] 97%|| 757/780 [04:59<00:06,  3.45it/s] 97%|| 758/780 [04:59<00:06,  3.45it/s] 97%|| 759/780 [05:00<00:06,  3.39it/s] 97%|| 760/780 [05:00<00:05,  3.41it/s] 98%|| 761/780 [05:00<00:05,  3.43it/s] 98%|| 762/780 [05:00<00:05,  3.44it/s] 98%|| 763/780 [05:01<00:04,  3.44it/s] 98%|| 764/780 [05:01<00:04,  3.44it/s] 98%|| 765/780 [05:01<00:04,  3.45it/s] 98%|| 766/780 [05:02<00:04,  3.45it/s] 98%|| 767/780 [05:02<00:03,  3.46it/s] 98%|| 768/780 [05:02<00:03,  3.46it/s] 99%|| 769/780 [05:02<00:03,  3.46it/s] 99%|| 770/780 [05:03<00:02,  3.40it/s] 99%|| 771/780 [05:03<00:02,  3.41it/s] 99%|| 772/780 [05:03<00:02,  3.42it/s] 99%|| 773/780 [05:04<00:02,  3.42it/s] 99%|| 774/780 [05:04<00:01,  3.44it/s] 99%|| 775/780 [05:04<00:01,  3.44it/s] 99%|| 776/780 [05:04<00:01,  3.45it/s]100%|| 777/780 [05:05<00:00,  3.45it/s]100%|| 778/780 [05:05<00:00,  3.45it/s]100%|| 779/780 [05:05<00:00,  3.45it/s]100%|| 780/780 [05:06<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 07:24:33,592 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:24:33,592 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 07:24:33,592 >>   Batch size = 8
{'eval_loss': 0.9993681907653809, 'eval_runtime': 9.762, 'eval_samples_per_second': 356.175, 'eval_steps_per_second': 44.56, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 56.70it/s][A
  3%|         | 12/435 [00:00<00:08, 49.18it/s][A
  4%|         | 17/435 [00:00<00:08, 47.52it/s][A
  5%|         | 22/435 [00:00<00:08, 46.76it/s][A
  6%|         | 27/435 [00:00<00:08, 46.18it/s][A
  7%|         | 32/435 [00:00<00:08, 45.88it/s][A
  9%|         | 37/435 [00:00<00:08, 45.35it/s][A
 10%|         | 42/435 [00:00<00:08, 45.16it/s][A
 11%|         | 47/435 [00:01<00:08, 44.94it/s][A
 12%|        | 52/435 [00:01<00:08, 44.86it/s][A
 13%|        | 57/435 [00:01<00:08, 45.00it/s][A
 14%|        | 62/435 [00:01<00:08, 44.96it/s][A
 15%|        | 67/435 [00:01<00:08, 45.03it/s][A
 17%|        | 72/435 [00:01<00:08, 45.14it/s][A
 18%|        | 77/435 [00:01<00:07, 45.16it/s][A
 19%|        | 82/435 [00:01<00:07, 45.10it/s][A
 20%|        | 87/435 [00:01<00:07, 44.82it/s][A
 21%|        | 92/435 [00:02<00:07, 44.69it/s][A
 22%|       | 97/435 [00:02<00:07, 44.80it/s][A
 23%|       | 102/435 [00:02<00:07, 44.92it/s][A
 25%|       | 107/435 [00:02<00:07, 45.00it/s][A
 26%|       | 112/435 [00:02<00:07, 44.91it/s][A
 27%|       | 117/435 [00:02<00:07, 45.02it/s][A
 28%|       | 122/435 [00:02<00:06, 45.01it/s][A
 29%|       | 127/435 [00:02<00:06, 45.04it/s][A
 30%|       | 132/435 [00:02<00:06, 44.93it/s][A
 31%|      | 137/435 [00:03<00:06, 44.89it/s][A
 33%|      | 142/435 [00:03<00:06, 44.31it/s][A
 34%|      | 147/435 [00:03<00:06, 44.52it/s][A
 35%|      | 152/435 [00:03<00:06, 44.69it/s][A
 36%|      | 157/435 [00:03<00:06, 44.85it/s][A
 37%|      | 162/435 [00:03<00:06, 44.92it/s][A
 38%|      | 167/435 [00:03<00:05, 44.90it/s][A
 40%|      | 172/435 [00:03<00:05, 45.04it/s][A
 41%|      | 177/435 [00:03<00:05, 44.90it/s][A
 42%|     | 182/435 [00:04<00:05, 44.83it/s][A
 43%|     | 187/435 [00:04<00:05, 44.84it/s][A
 44%|     | 192/435 [00:04<00:05, 44.80it/s][A
 45%|     | 197/435 [00:04<00:05, 44.95it/s][A
 46%|     | 202/435 [00:04<00:05, 44.92it/s][A
 48%|     | 207/435 [00:04<00:05, 44.96it/s][A
 49%|     | 212/435 [00:04<00:04, 44.98it/s][A
 50%|     | 217/435 [00:04<00:04, 45.03it/s][A
 51%|     | 222/435 [00:04<00:04, 45.02it/s][A
 52%|    | 227/435 [00:05<00:04, 44.94it/s][A
 53%|    | 232/435 [00:05<00:04, 44.88it/s][A
 54%|    | 237/435 [00:05<00:04, 44.91it/s][A
 56%|    | 242/435 [00:05<00:04, 44.95it/s][A
 57%|    | 247/435 [00:05<00:04, 44.92it/s][A
 58%|    | 252/435 [00:05<00:04, 44.86it/s][A
 59%|    | 257/435 [00:05<00:03, 44.93it/s][A
 60%|    | 262/435 [00:05<00:03, 44.93it/s][A
 61%|   | 267/435 [00:05<00:03, 44.93it/s][A
 63%|   | 272/435 [00:06<00:03, 44.86it/s][A
 64%|   | 277/435 [00:06<00:03, 44.57it/s][A
 65%|   | 282/435 [00:06<00:03, 44.75it/s][A
 66%|   | 287/435 [00:06<00:03, 44.81it/s][A
 67%|   | 292/435 [00:06<00:03, 44.75it/s][A
 68%|   | 297/435 [00:06<00:03, 44.85it/s][A
 69%|   | 302/435 [00:06<00:02, 44.97it/s][A
 71%|   | 307/435 [00:06<00:02, 45.03it/s][A
 72%|  | 312/435 [00:06<00:02, 44.96it/s][A
 73%|  | 317/435 [00:07<00:02, 44.85it/s][A
 74%|  | 322/435 [00:07<00:02, 44.89it/s][A
 75%|  | 327/435 [00:07<00:02, 44.98it/s][A
 76%|  | 332/435 [00:07<00:02, 44.92it/s][A
 77%|  | 337/435 [00:07<00:02, 44.82it/s][A
 79%|  | 342/435 [00:07<00:02, 44.88it/s][A
 80%|  | 347/435 [00:07<00:01, 44.88it/s][A
 81%|  | 352/435 [00:07<00:01, 44.95it/s][A
 82%| | 357/435 [00:07<00:01, 44.98it/s][A
 83%| | 362/435 [00:08<00:01, 44.92it/s][A
 84%| | 367/435 [00:08<00:01, 44.81it/s][A
 86%| | 372/435 [00:08<00:01, 44.81it/s][A
 87%| | 377/435 [00:08<00:01, 44.83it/s][A
 88%| | 382/435 [00:08<00:01, 44.91it/s][A
 89%| | 387/435 [00:08<00:01, 44.93it/s][A
 90%| | 392/435 [00:08<00:00, 44.94it/s][A
 91%|| 397/435 [00:08<00:00, 44.99it/s][A
 92%|| 402/435 [00:08<00:00, 45.04it/s][A
 94%|| 407/435 [00:09<00:00, 45.04it/s][A
 95%|| 412/435 [00:09<00:00, 43.64it/s][A
 96%|| 417/435 [00:09<00:00, 44.01it/s][A
 97%|| 422/435 [00:09<00:00, 44.25it/s][A
 98%|| 427/435 [00:09<00:00, 44.43it/s][A
 99%|| 432/435 [00:09<00:00, 44.60it/s][A                                                 
                                                 [A100%|| 780/780 [05:15<00:00,  3.46it/s]
100%|| 435/435 [00:09<00:00, 44.60it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:24:43,415 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 07:24:43,548 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:24:45,950 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:24:46,015 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:24:46,054 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 07:24:51,394 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 07:24:51,418 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156 (score: 0.9695549607276917).
                                                 100%|| 780/780 [05:35<00:00,  3.46it/s]100%|| 780/780 [05:35<00:00,  2.33it/s]
[INFO|trainer.py:1894] 2023-08-29 07:25:02,800 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 07:25:03,061 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:25:06,203 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:25:06,339 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:25:06,410 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:25:06,860 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:06,860 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:06,860 >>   train_loss               =      0.561
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:06,860 >>   train_runtime            = 0:05:35.26
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:06,860 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:06,860 >>   train_samples_per_second =     149.12
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:06,860 >>   train_steps_per_second   =      2.327
{'eval_loss': 1.0030102729797363, 'eval_runtime': 9.6901, 'eval_samples_per_second': 358.82, 'eval_steps_per_second': 44.891, 'epoch': 5.0}
{'train_runtime': 335.2668, 'train_samples_per_second': 149.12, 'train_steps_per_second': 2.327, 'train_loss': 0.5609754513471554, 'epoch': 5.0}
08/29/2023 07:25:07 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 07:25:07,042 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:25:07,042 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 07:25:07,042 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 55.67it/s]  3%|         | 12/435 [00:00<00:08, 49.00it/s]  4%|         | 17/435 [00:00<00:08, 47.57it/s]  5%|         | 22/435 [00:00<00:08, 46.50it/s]  6%|         | 27/435 [00:00<00:08, 46.03it/s]  7%|         | 32/435 [00:00<00:08, 45.94it/s]  9%|         | 37/435 [00:00<00:08, 45.60it/s] 10%|         | 42/435 [00:00<00:08, 45.38it/s] 11%|         | 47/435 [00:01<00:08, 45.07it/s] 12%|        | 52/435 [00:01<00:08, 45.00it/s] 13%|        | 57/435 [00:01<00:08, 45.04it/s] 14%|        | 62/435 [00:01<00:08, 45.14it/s] 15%|        | 67/435 [00:01<00:08, 45.20it/s] 17%|        | 72/435 [00:01<00:08, 45.14it/s] 18%|        | 77/435 [00:01<00:07, 45.19it/s] 19%|        | 82/435 [00:01<00:07, 45.18it/s] 20%|        | 87/435 [00:01<00:07, 45.02it/s] 21%|        | 92/435 [00:02<00:07, 44.65it/s] 22%|       | 97/435 [00:02<00:07, 44.69it/s] 23%|       | 102/435 [00:02<00:07, 44.74it/s] 25%|       | 107/435 [00:02<00:07, 44.03it/s] 26%|       | 112/435 [00:02<00:07, 44.54it/s] 27%|       | 117/435 [00:02<00:07, 44.74it/s] 28%|       | 122/435 [00:02<00:06, 44.91it/s] 29%|       | 127/435 [00:02<00:06, 44.98it/s] 30%|       | 132/435 [00:02<00:06, 44.99it/s] 31%|      | 137/435 [00:03<00:06, 44.86it/s] 33%|      | 142/435 [00:03<00:06, 44.82it/s] 34%|      | 147/435 [00:03<00:06, 44.79it/s] 35%|      | 152/435 [00:03<00:06, 44.83it/s] 36%|      | 157/435 [00:03<00:06, 45.01it/s] 37%|      | 162/435 [00:03<00:06, 44.89it/s] 38%|      | 167/435 [00:03<00:05, 45.11it/s] 40%|      | 172/435 [00:03<00:05, 45.08it/s] 41%|      | 177/435 [00:03<00:05, 45.07it/s] 42%|     | 182/435 [00:04<00:05, 45.03it/s] 43%|     | 187/435 [00:04<00:05, 44.90it/s] 44%|     | 192/435 [00:04<00:05, 44.99it/s] 45%|     | 197/435 [00:04<00:05, 44.94it/s] 46%|     | 202/435 [00:04<00:05, 44.78it/s] 48%|     | 207/435 [00:04<00:05, 45.05it/s] 49%|     | 212/435 [00:04<00:04, 44.99it/s] 50%|     | 217/435 [00:04<00:04, 45.03it/s] 51%|     | 222/435 [00:04<00:04, 45.02it/s] 52%|    | 227/435 [00:05<00:04, 44.98it/s] 53%|    | 232/435 [00:05<00:04, 44.91it/s] 54%|    | 237/435 [00:05<00:04, 44.90it/s] 56%|    | 242/435 [00:05<00:04, 43.59it/s] 57%|    | 247/435 [00:05<00:04, 44.06it/s] 58%|    | 252/435 [00:05<00:04, 44.50it/s] 59%|    | 257/435 [00:05<00:03, 44.70it/s] 60%|    | 262/435 [00:05<00:03, 44.75it/s] 61%|   | 267/435 [00:05<00:03, 44.91it/s] 63%|   | 272/435 [00:06<00:03, 44.94it/s] 64%|   | 277/435 [00:06<00:03, 44.92it/s] 65%|   | 282/435 [00:06<00:03, 44.88it/s] 66%|   | 287/435 [00:06<00:03, 44.90it/s] 67%|   | 292/435 [00:06<00:03, 45.12it/s] 68%|   | 297/435 [00:06<00:03, 45.18it/s] 69%|   | 302/435 [00:06<00:02, 45.16it/s] 71%|   | 307/435 [00:06<00:02, 45.25it/s] 72%|  | 312/435 [00:06<00:02, 45.25it/s] 73%|  | 317/435 [00:07<00:02, 45.14it/s] 74%|  | 322/435 [00:07<00:02, 44.89it/s] 75%|  | 327/435 [00:07<00:02, 44.81it/s] 76%|  | 332/435 [00:07<00:02, 44.93it/s] 77%|  | 337/435 [00:07<00:02, 45.06it/s] 79%|  | 342/435 [00:07<00:02, 45.18it/s] 80%|  | 347/435 [00:07<00:01, 45.31it/s] 81%|  | 352/435 [00:07<00:01, 45.24it/s] 82%| | 357/435 [00:07<00:01, 45.25it/s] 83%| | 362/435 [00:08<00:01, 45.23it/s] 84%| | 367/435 [00:08<00:01, 45.08it/s] 86%| | 372/435 [00:08<00:01, 45.02it/s] 87%| | 377/435 [00:08<00:01, 44.16it/s] 88%| | 382/435 [00:08<00:01, 44.42it/s] 89%| | 387/435 [00:08<00:01, 44.82it/s] 90%| | 392/435 [00:08<00:00, 44.95it/s] 91%|| 397/435 [00:08<00:00, 45.13it/s] 92%|| 402/435 [00:08<00:00, 45.10it/s] 94%|| 407/435 [00:09<00:00, 45.20it/s] 95%|| 412/435 [00:09<00:00, 45.18it/s] 96%|| 417/435 [00:09<00:00, 45.10it/s] 97%|| 422/435 [00:09<00:00, 44.98it/s] 98%|| 427/435 [00:09<00:00, 44.97it/s] 99%|| 432/435 [00:09<00:00, 45.09it/s]100%|| 435/435 [00:09<00:00, 45.04it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:25:16,722 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:16,722 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:16,722 >>   eval_loss               =     0.9696
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:16,722 >>   eval_runtime            = 0:00:09.67
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:16,722 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:16,722 >>   eval_samples_per_second =     359.22
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:16,722 >>   eval_steps_per_second   =     44.941
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:16,722 >>   perplexity              =     2.6368
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:27,899 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:27,921 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:27,921 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:27,921 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:27,921 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:25:28,566 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:25:28,567 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:25:29,195 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:25:30,203 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:25:30,203 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:33,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:33,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:33,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:33,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:25:33,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:25:33,883 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:25:33,884 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:25:34,477 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:25:34,643 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:25:34,643 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:06,  1.50it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.53it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:13,  1.61it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:15,  1.63it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:17,  1.55it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.56it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.60it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:24,  1.58it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:27,  1.58it/s]Extractor Predicting: 44it [00:27,  1.58it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.57it/s]Extractor Predicting: 47it [00:29,  1.59it/s]Extractor Predicting: 48it [00:30,  1.59it/s]Extractor Predicting: 49it [00:31,  1.62it/s]Extractor Predicting: 50it [00:31,  1.64it/s]Extractor Predicting: 51it [00:32,  1.62it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:33,  1.58it/s]Extractor Predicting: 54it [00:34,  1.61it/s]Extractor Predicting: 55it [00:34,  1.58it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.57it/s]Extractor Predicting: 58it [00:36,  1.59it/s]Extractor Predicting: 59it [00:37,  1.63it/s]Extractor Predicting: 60it [00:37,  1.63it/s]Extractor Predicting: 61it [00:38,  1.64it/s]Extractor Predicting: 62it [00:39,  1.65it/s]Extractor Predicting: 63it [00:39,  1.65it/s]Extractor Predicting: 64it [00:40,  1.65it/s]Extractor Predicting: 65it [00:40,  1.65it/s]Extractor Predicting: 66it [00:41,  1.62it/s]Extractor Predicting: 67it [00:42,  1.63it/s]Extractor Predicting: 68it [00:42,  1.64it/s]Extractor Predicting: 69it [00:43,  1.64it/s]Extractor Predicting: 70it [00:44,  1.63it/s]Extractor Predicting: 71it [00:44,  1.62it/s]Extractor Predicting: 72it [00:45,  1.65it/s]Extractor Predicting: 73it [00:45,  1.64it/s]Extractor Predicting: 74it [00:46,  1.65it/s]Extractor Predicting: 75it [00:47,  1.67it/s]Extractor Predicting: 76it [00:47,  1.66it/s]Extractor Predicting: 77it [00:48,  1.62it/s]Extractor Predicting: 78it [00:48,  1.62it/s]Extractor Predicting: 79it [00:49,  1.59it/s]Extractor Predicting: 80it [00:50,  1.59it/s]Extractor Predicting: 81it [00:50,  1.57it/s]Extractor Predicting: 82it [00:51,  1.60it/s]Extractor Predicting: 83it [00:52,  1.61it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:53,  1.62it/s]Extractor Predicting: 87it [00:54,  1.61it/s]Extractor Predicting: 88it [00:55,  1.60it/s]Extractor Predicting: 89it [00:55,  1.61it/s]Extractor Predicting: 90it [00:56,  1.62it/s]Extractor Predicting: 91it [00:56,  1.67it/s]Extractor Predicting: 92it [00:57,  1.71it/s]Extractor Predicting: 93it [00:58,  1.72it/s]Extractor Predicting: 94it [00:58,  1.68it/s]Extractor Predicting: 95it [00:59,  1.70it/s]Extractor Predicting: 96it [00:59,  1.68it/s]Extractor Predicting: 97it [01:00,  1.68it/s]Extractor Predicting: 98it [01:01,  1.68it/s]Extractor Predicting: 99it [01:01,  1.63it/s]Extractor Predicting: 100it [01:02,  1.59it/s]Extractor Predicting: 101it [01:03,  1.46it/s]Extractor Predicting: 102it [01:03,  1.56it/s]Extractor Predicting: 103it [01:04,  1.61it/s]Extractor Predicting: 104it [01:04,  1.61it/s]Extractor Predicting: 105it [01:05,  1.61it/s]Extractor Predicting: 106it [01:06,  1.62it/s]Extractor Predicting: 107it [01:06,  1.64it/s]Extractor Predicting: 108it [01:07,  1.64it/s]Extractor Predicting: 109it [01:08,  1.64it/s]Extractor Predicting: 110it [01:08,  1.63it/s]Extractor Predicting: 111it [01:09,  1.65it/s]Extractor Predicting: 112it [01:09,  1.64it/s]Extractor Predicting: 113it [01:10,  1.69it/s]Extractor Predicting: 114it [01:10,  1.73it/s]Extractor Predicting: 115it [01:11,  1.71it/s]Extractor Predicting: 116it [01:12,  1.71it/s]Extractor Predicting: 117it [01:12,  1.72it/s]Extractor Predicting: 118it [01:13,  1.69it/s]Extractor Predicting: 119it [01:13,  1.69it/s]Extractor Predicting: 120it [01:14,  1.64it/s]Extractor Predicting: 121it [01:15,  1.61it/s]Extractor Predicting: 122it [01:15,  1.62it/s]Extractor Predicting: 123it [01:16,  1.62it/s]Extractor Predicting: 124it [01:17,  1.59it/s]Extractor Predicting: 125it [01:17,  1.60it/s]Extractor Predicting: 126it [01:18,  1.63it/s]Extractor Predicting: 127it [01:18,  1.62it/s]Extractor Predicting: 128it [01:19,  1.63it/s]Extractor Predicting: 129it [01:20,  1.60it/s]Extractor Predicting: 130it [01:20,  1.59it/s]Extractor Predicting: 131it [01:21,  1.61it/s]Extractor Predicting: 132it [01:22,  1.59it/s]Extractor Predicting: 133it [01:22,  1.62it/s]Extractor Predicting: 134it [01:23,  1.63it/s]Extractor Predicting: 135it [01:23,  1.62it/s]Extractor Predicting: 136it [01:24,  1.60it/s]Extractor Predicting: 137it [01:25,  1.64it/s]Extractor Predicting: 138it [01:25,  1.63it/s]Extractor Predicting: 139it [01:26,  1.61it/s]Extractor Predicting: 140it [01:26,  1.63it/s]Extractor Predicting: 141it [01:27,  1.64it/s]Extractor Predicting: 142it [01:28,  1.62it/s]Extractor Predicting: 143it [01:28,  1.65it/s]Extractor Predicting: 144it [01:29,  1.65it/s]Extractor Predicting: 144it [01:29,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:16,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:16,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:16,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:16,150 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:16,150 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:27:16,755 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:27:16,756 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:27:17,355 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:27:18,398 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:27:18,398 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:21,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:21,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:21,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:21,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:21,384 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:27:22,070 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:27:22,071 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:27:22,676 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:27:22,837 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:27:22,837 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3706233988044407,
  "recall": 0.12482024733966063,
  "score": 0.18674698795180725,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.70it/s]Extractor Predicting: 8it [00:04,  1.74it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:10,  1.67it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.69it/s]Extractor Predicting: 23it [00:13,  1.69it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:14,  1.66it/s]Extractor Predicting: 26it [00:15,  1.66it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:16,  1.65it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:17,  1.69it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.74it/s]Extractor Predicting: 33it [00:19,  1.70it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:20,  1.64it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.68it/s]Extractor Predicting: 38it [00:22,  1.70it/s]Extractor Predicting: 39it [00:23,  1.70it/s]Extractor Predicting: 40it [00:23,  1.69it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.72it/s]Extractor Predicting: 43it [00:25,  1.67it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:26,  1.65it/s]Extractor Predicting: 46it [00:27,  1.63it/s]Extractor Predicting: 47it [00:28,  1.66it/s]Extractor Predicting: 48it [00:28,  1.64it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:29,  1.69it/s]Extractor Predicting: 51it [00:30,  1.67it/s]Extractor Predicting: 52it [00:31,  1.65it/s]Extractor Predicting: 53it [00:31,  1.66it/s]Extractor Predicting: 54it [00:32,  1.63it/s]Extractor Predicting: 55it [00:32,  1.68it/s]Extractor Predicting: 56it [00:33,  1.68it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:34,  1.61it/s]Extractor Predicting: 59it [00:35,  1.59it/s]Extractor Predicting: 60it [00:36,  1.63it/s]Extractor Predicting: 61it [00:36,  1.62it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:37,  1.62it/s]Extractor Predicting: 64it [00:38,  1.62it/s]Extractor Predicting: 65it [00:39,  1.67it/s]Extractor Predicting: 66it [00:39,  1.64it/s]Extractor Predicting: 67it [00:40,  1.62it/s]Extractor Predicting: 68it [00:40,  1.58it/s]Extractor Predicting: 69it [00:41,  1.60it/s]Extractor Predicting: 70it [00:42,  1.57it/s]Extractor Predicting: 71it [00:42,  1.60it/s]Extractor Predicting: 72it [00:43,  1.60it/s]Extractor Predicting: 73it [00:44,  1.58it/s]Extractor Predicting: 74it [00:44,  1.61it/s]Extractor Predicting: 75it [00:45,  1.61it/s]Extractor Predicting: 76it [00:45,  1.63it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:47,  1.65it/s]Extractor Predicting: 79it [00:47,  1.65it/s]Extractor Predicting: 80it [00:48,  1.69it/s]Extractor Predicting: 81it [00:48,  1.70it/s]Extractor Predicting: 82it [00:49,  1.70it/s]Extractor Predicting: 83it [00:50,  1.69it/s]Extractor Predicting: 84it [00:50,  1.68it/s]Extractor Predicting: 85it [00:51,  1.66it/s]Extractor Predicting: 86it [00:51,  1.59it/s]Extractor Predicting: 87it [00:52,  1.56it/s]Extractor Predicting: 88it [00:53,  1.60it/s]Extractor Predicting: 89it [00:53,  1.58it/s]Extractor Predicting: 90it [00:54,  1.56it/s]Extractor Predicting: 91it [00:55,  1.55it/s]Extractor Predicting: 92it [00:56,  1.41it/s]Extractor Predicting: 93it [00:56,  1.45it/s]Extractor Predicting: 94it [00:57,  1.50it/s]Extractor Predicting: 95it [00:57,  1.52it/s]Extractor Predicting: 96it [00:58,  1.53it/s]Extractor Predicting: 97it [00:59,  1.52it/s]Extractor Predicting: 98it [00:59,  1.50it/s]Extractor Predicting: 99it [01:00,  1.51it/s]Extractor Predicting: 100it [01:01,  1.55it/s]Extractor Predicting: 101it [01:01,  1.55it/s]Extractor Predicting: 102it [01:02,  1.57it/s]Extractor Predicting: 103it [01:03,  1.55it/s]Extractor Predicting: 104it [01:03,  1.54it/s]Extractor Predicting: 105it [01:04,  1.57it/s]Extractor Predicting: 106it [01:05,  1.56it/s]Extractor Predicting: 107it [01:05,  1.57it/s]Extractor Predicting: 108it [01:06,  1.59it/s]Extractor Predicting: 109it [01:06,  1.57it/s]Extractor Predicting: 110it [01:07,  1.56it/s]Extractor Predicting: 111it [01:08,  1.58it/s]Extractor Predicting: 112it [01:08,  1.57it/s]Extractor Predicting: 113it [01:09,  1.58it/s]Extractor Predicting: 114it [01:10,  1.60it/s]Extractor Predicting: 115it [01:10,  1.59it/s]Extractor Predicting: 116it [01:11,  1.54it/s]Extractor Predicting: 117it [01:12,  1.52it/s]Extractor Predicting: 118it [01:12,  1.55it/s]Extractor Predicting: 119it [01:13,  1.54it/s]Extractor Predicting: 120it [01:14,  1.54it/s]Extractor Predicting: 121it [01:14,  1.56it/s]Extractor Predicting: 122it [01:15,  1.60it/s]Extractor Predicting: 123it [01:15,  1.60it/s]Extractor Predicting: 124it [01:16,  1.61it/s]Extractor Predicting: 125it [01:17,  1.59it/s]Extractor Predicting: 126it [01:17,  1.66it/s]Extractor Predicting: 127it [01:18,  1.66it/s]Extractor Predicting: 128it [01:18,  1.62it/s]Extractor Predicting: 129it [01:19,  1.67it/s]Extractor Predicting: 130it [01:20,  1.66it/s]Extractor Predicting: 131it [01:20,  1.64it/s]Extractor Predicting: 132it [01:21,  1.65it/s]Extractor Predicting: 133it [01:21,  1.70it/s]Extractor Predicting: 134it [01:22,  1.71it/s]Extractor Predicting: 135it [01:23,  1.65it/s]Extractor Predicting: 136it [01:23,  1.66it/s]Extractor Predicting: 137it [01:24,  1.68it/s]Extractor Predicting: 138it [01:24,  1.73it/s]Extractor Predicting: 139it [01:25,  1.70it/s]Extractor Predicting: 140it [01:26,  1.69it/s]Extractor Predicting: 141it [01:26,  1.67it/s]Extractor Predicting: 142it [01:27,  1.67it/s]Extractor Predicting: 143it [01:27,  1.64it/s]Extractor Predicting: 144it [01:28,  1.63it/s]Extractor Predicting: 145it [01:29,  1.63it/s]Extractor Predicting: 146it [01:29,  1.66it/s]Extractor Predicting: 147it [01:30,  1.72it/s]Extractor Predicting: 148it [01:30,  1.73it/s]Extractor Predicting: 149it [01:31,  1.73it/s]Extractor Predicting: 150it [01:31,  1.77it/s]Extractor Predicting: 151it [01:32,  1.75it/s]Extractor Predicting: 152it [01:33,  1.76it/s]Extractor Predicting: 153it [01:33,  1.76it/s]Extractor Predicting: 154it [01:34,  1.79it/s]Extractor Predicting: 155it [01:34,  1.77it/s]Extractor Predicting: 156it [01:35,  1.79it/s]Extractor Predicting: 157it [01:35,  1.82it/s]Extractor Predicting: 158it [01:36,  1.80it/s]Extractor Predicting: 159it [01:36,  1.86it/s]Extractor Predicting: 160it [01:37,  1.89it/s]Extractor Predicting: 161it [01:37,  1.83it/s]Extractor Predicting: 162it [01:38,  1.78it/s]Extractor Predicting: 163it [01:39,  1.78it/s]Extractor Predicting: 164it [01:39,  1.78it/s]Extractor Predicting: 165it [01:40,  1.82it/s]Extractor Predicting: 166it [01:40,  1.83it/s]Extractor Predicting: 167it [01:41,  1.82it/s]Extractor Predicting: 168it [01:41,  1.79it/s]Extractor Predicting: 169it [01:42,  1.81it/s]Extractor Predicting: 170it [01:42,  1.78it/s]Extractor Predicting: 171it [01:43,  1.83it/s]Extractor Predicting: 172it [01:44,  1.83it/s]Extractor Predicting: 173it [01:44,  1.81it/s]Extractor Predicting: 174it [01:45,  1.72it/s]Extractor Predicting: 175it [01:45,  1.71it/s]Extractor Predicting: 176it [01:46,  1.69it/s]Extractor Predicting: 177it [01:47,  1.67it/s]Extractor Predicting: 178it [01:47,  1.62it/s]Extractor Predicting: 179it [01:48,  1.61it/s]Extractor Predicting: 180it [01:48,  1.64it/s]Extractor Predicting: 181it [01:49,  1.62it/s]Extractor Predicting: 182it [01:50,  1.63it/s]Extractor Predicting: 183it [01:50,  1.65it/s]Extractor Predicting: 184it [01:51,  1.64it/s]Extractor Predicting: 185it [01:52,  1.64it/s]Extractor Predicting: 186it [01:52,  1.66it/s]Extractor Predicting: 187it [01:53,  1.64it/s]Extractor Predicting: 188it [01:53,  1.62it/s]Extractor Predicting: 189it [01:54,  1.63it/s]Extractor Predicting: 190it [01:55,  1.62it/s]Extractor Predicting: 191it [01:55,  1.59it/s]Extractor Predicting: 192it [01:56,  1.59it/s]Extractor Predicting: 193it [01:57,  1.57it/s]Extractor Predicting: 194it [01:57,  1.56it/s]Extractor Predicting: 195it [01:58,  1.55it/s]Extractor Predicting: 196it [01:58,  1.59it/s]Extractor Predicting: 197it [01:59,  1.62it/s]Extractor Predicting: 198it [02:00,  1.61it/s]Extractor Predicting: 199it [02:00,  1.63it/s]Extractor Predicting: 200it [02:01,  1.59it/s]Extractor Predicting: 201it [02:02,  1.58it/s]Extractor Predicting: 202it [02:02,  1.55it/s]Extractor Predicting: 203it [02:03,  1.52it/s]Extractor Predicting: 204it [02:04,  1.51it/s]Extractor Predicting: 205it [02:04,  1.52it/s]Extractor Predicting: 206it [02:05,  1.53it/s]Extractor Predicting: 207it [02:06,  1.55it/s]Extractor Predicting: 208it [02:06,  1.54it/s]Extractor Predicting: 209it [02:07,  1.50it/s]Extractor Predicting: 210it [02:08,  1.50it/s]Extractor Predicting: 211it [02:08,  1.49it/s]Extractor Predicting: 212it [02:09,  1.52it/s]Extractor Predicting: 213it [02:10,  1.35it/s]Extractor Predicting: 214it [02:10,  1.42it/s]Extractor Predicting: 215it [02:11,  1.42it/s]Extractor Predicting: 216it [02:12,  1.44it/s]Extractor Predicting: 217it [02:12,  1.47it/s]Extractor Predicting: 218it [02:13,  1.47it/s]Extractor Predicting: 219it [02:14,  1.47it/s]Extractor Predicting: 220it [02:14,  1.45it/s]Extractor Predicting: 221it [02:15,  1.45it/s]Extractor Predicting: 222it [02:16,  1.45it/s]Extractor Predicting: 223it [02:16,  1.49it/s]Extractor Predicting: 224it [02:17,  1.51it/s]Extractor Predicting: 225it [02:18,  1.51it/s]Extractor Predicting: 226it [02:19,  1.48it/s]Extractor Predicting: 227it [02:19,  1.49it/s]Extractor Predicting: 228it [02:20,  1.51it/s]Extractor Predicting: 229it [02:20,  1.53it/s]Extractor Predicting: 230it [02:21,  1.56it/s]Extractor Predicting: 231it [02:22,  1.60it/s]Extractor Predicting: 232it [02:22,  1.64it/s]Extractor Predicting: 233it [02:23,  1.66it/s]Extractor Predicting: 234it [02:23,  1.59it/s]Extractor Predicting: 235it [02:24,  1.61it/s]Extractor Predicting: 236it [02:25,  1.62it/s]Extractor Predicting: 237it [02:25,  1.62it/s]Extractor Predicting: 238it [02:26,  1.62it/s]Extractor Predicting: 239it [02:27,  1.59it/s]Extractor Predicting: 240it [02:27,  1.63it/s]Extractor Predicting: 241it [02:28,  1.65it/s]Extractor Predicting: 242it [02:28,  1.66it/s]Extractor Predicting: 243it [02:29,  1.66it/s]Extractor Predicting: 244it [02:29,  1.72it/s]Extractor Predicting: 245it [02:30,  1.67it/s]Extractor Predicting: 246it [02:31,  1.64it/s]Extractor Predicting: 247it [02:31,  1.62it/s]Extractor Predicting: 248it [02:32,  1.60it/s]Extractor Predicting: 249it [02:33,  1.63it/s]Extractor Predicting: 250it [02:33,  1.60it/s]Extractor Predicting: 251it [02:34,  1.64it/s]Extractor Predicting: 252it [02:34,  1.68it/s]Extractor Predicting: 253it [02:35,  1.66it/s]Extractor Predicting: 254it [02:36,  1.62it/s]Extractor Predicting: 255it [02:36,  1.63it/s]Extractor Predicting: 256it [02:37,  1.62it/s]Extractor Predicting: 257it [02:38,  1.58it/s]Extractor Predicting: 258it [02:38,  1.59it/s]Extractor Predicting: 259it [02:39,  1.57it/s]Extractor Predicting: 260it [02:40,  1.55it/s]Extractor Predicting: 261it [02:40,  1.55it/s]Extractor Predicting: 262it [02:41,  1.57it/s]Extractor Predicting: 263it [02:41,  1.60it/s]Extractor Predicting: 264it [02:42,  1.59it/s]Extractor Predicting: 265it [02:43,  1.59it/s]Extractor Predicting: 266it [02:43,  1.58it/s]Extractor Predicting: 267it [02:44,  1.55it/s]Extractor Predicting: 268it [02:45,  1.54it/s]Extractor Predicting: 269it [02:45,  1.56it/s]Extractor Predicting: 270it [02:46,  1.57it/s]Extractor Predicting: 271it [02:47,  1.58it/s]Extractor Predicting: 272it [02:47,  1.61it/s]Extractor Predicting: 273it [02:48,  1.56it/s]Extractor Predicting: 274it [02:48,  1.54it/s]Extractor Predicting: 275it [02:49,  1.54it/s]Extractor Predicting: 276it [02:50,  1.55it/s]Extractor Predicting: 277it [02:50,  1.56it/s]Extractor Predicting: 278it [02:51,  1.58it/s]Extractor Predicting: 279it [02:52,  1.56it/s]Extractor Predicting: 280it [02:52,  1.57it/s]Extractor Predicting: 281it [02:53,  1.61it/s]Extractor Predicting: 282it [02:54,  1.59it/s]Extractor Predicting: 283it [02:54,  1.54it/s]Extractor Predicting: 284it [02:55,  1.61it/s]Extractor Predicting: 285it [02:55,  1.59it/s]Extractor Predicting: 286it [02:56,  1.63it/s]Extractor Predicting: 287it [02:57,  1.62it/s]Extractor Predicting: 288it [02:57,  1.61it/s]Extractor Predicting: 289it [02:58,  1.60it/s]Extractor Predicting: 290it [02:59,  1.60it/s]Extractor Predicting: 291it [02:59,  1.51it/s]Extractor Predicting: 292it [03:00,  1.51it/s]Extractor Predicting: 293it [03:01,  1.55it/s]Extractor Predicting: 294it [03:01,  1.53it/s]Extractor Predicting: 295it [03:02,  1.56it/s]Extractor Predicting: 296it [03:02,  1.55it/s]Extractor Predicting: 297it [03:03,  1.58it/s]Extractor Predicting: 298it [03:04,  1.56it/s]Extractor Predicting: 299it [03:04,  1.53it/s]Extractor Predicting: 300it [03:05,  1.56it/s]Extractor Predicting: 301it [03:06,  1.55it/s]Extractor Predicting: 302it [03:06,  1.59it/s]Extractor Predicting: 303it [03:07,  1.55it/s]Extractor Predicting: 304it [03:08,  1.55it/s]Extractor Predicting: 305it [03:08,  1.55it/s]Extractor Predicting: 306it [03:09,  1.59it/s]Extractor Predicting: 307it [03:09,  1.57it/s]Extractor Predicting: 308it [03:10,  1.59it/s]Extractor Predicting: 309it [03:11,  1.60it/s]Extractor Predicting: 310it [03:11,  1.62it/s]Extractor Predicting: 311it [03:12,  1.59it/s]Extractor Predicting: 312it [03:13,  1.65it/s]Extractor Predicting: 313it [03:13,  1.69it/s]Extractor Predicting: 314it [03:14,  1.71it/s]Extractor Predicting: 315it [03:14,  1.71it/s]Extractor Predicting: 316it [03:15,  1.67it/s]Extractor Predicting: 317it [03:15,  1.65it/s]Extractor Predicting: 318it [03:16,  1.64it/s]Extractor Predicting: 319it [03:17,  1.63it/s]Extractor Predicting: 320it [03:17,  1.62it/s]Extractor Predicting: 321it [03:18,  1.42it/s]Extractor Predicting: 322it [03:19,  1.47it/s]Extractor Predicting: 323it [03:20,  1.46it/s]Extractor Predicting: 324it [03:20,  1.49it/s]Extractor Predicting: 325it [03:21,  1.51it/s]Extractor Predicting: 326it [03:21,  1.54it/s]Extractor Predicting: 327it [03:22,  1.58it/s]Extractor Predicting: 328it [03:23,  1.57it/s]Extractor Predicting: 329it [03:23,  1.59it/s]Extractor Predicting: 330it [03:24,  1.59it/s]Extractor Predicting: 331it [03:25,  1.56it/s]Extractor Predicting: 332it [03:25,  1.57it/s]Extractor Predicting: 333it [03:26,  1.60it/s]Extractor Predicting: 334it [03:26,  1.59it/s]Extractor Predicting: 335it [03:27,  1.61it/s]Extractor Predicting: 336it [03:28,  1.59it/s]Extractor Predicting: 337it [03:28,  1.60it/s]Extractor Predicting: 338it [03:29,  1.61it/s]Extractor Predicting: 339it [03:30,  1.65it/s]Extractor Predicting: 340it [03:30,  1.66it/s]Extractor Predicting: 341it [03:31,  1.65it/s]Extractor Predicting: 342it [03:31,  1.63it/s]Extractor Predicting: 343it [03:32,  1.62it/s]Extractor Predicting: 344it [03:33,  1.65it/s]Extractor Predicting: 345it [03:33,  1.60it/s]Extractor Predicting: 346it [03:34,  1.61it/s]Extractor Predicting: 347it [03:35,  1.60it/s]Extractor Predicting: 348it [03:35,  1.59it/s]Extractor Predicting: 349it [03:36,  1.62it/s]Extractor Predicting: 350it [03:36,  1.63it/s]Extractor Predicting: 351it [03:37,  1.64it/s]Extractor Predicting: 352it [03:38,  1.61it/s]Extractor Predicting: 353it [03:38,  1.63it/s]Extractor Predicting: 354it [03:39,  1.66it/s]Extractor Predicting: 355it [03:39,  1.69it/s]Extractor Predicting: 356it [03:40,  1.65it/s]Extractor Predicting: 357it [03:41,  1.65it/s]Extractor Predicting: 358it [03:41,  1.66it/s]Extractor Predicting: 359it [03:42,  1.64it/s]Extractor Predicting: 360it [03:42,  1.64it/s]Extractor Predicting: 361it [03:43,  1.65it/s]Extractor Predicting: 362it [03:44,  1.66it/s]Extractor Predicting: 363it [03:44,  1.68it/s]Extractor Predicting: 364it [03:45,  1.68it/s]Extractor Predicting: 365it [03:45,  1.67it/s]Extractor Predicting: 366it [03:46,  1.66it/s]Extractor Predicting: 367it [03:47,  1.61it/s]Extractor Predicting: 368it [03:47,  1.61it/s]Extractor Predicting: 369it [03:48,  1.62it/s]Extractor Predicting: 370it [03:48,  1.67it/s]Extractor Predicting: 371it [03:49,  1.69it/s]Extractor Predicting: 372it [03:50,  1.69it/s]Extractor Predicting: 373it [03:50,  1.64it/s]Extractor Predicting: 374it [03:51,  1.66it/s]Extractor Predicting: 375it [03:51,  1.67it/s]Extractor Predicting: 376it [03:52,  1.68it/s]Extractor Predicting: 377it [03:53,  1.69it/s]Extractor Predicting: 378it [03:53,  1.72it/s]Extractor Predicting: 379it [03:54,  1.67it/s]Extractor Predicting: 380it [03:54,  1.67it/s]Extractor Predicting: 381it [03:55,  1.68it/s]Extractor Predicting: 382it [03:56,  1.66it/s]Extractor Predicting: 383it [03:56,  1.70it/s]Extractor Predicting: 384it [03:57,  1.72it/s]Extractor Predicting: 385it [03:57,  1.69it/s]Extractor Predicting: 386it [03:58,  1.69it/s]Extractor Predicting: 387it [03:59,  1.68it/s]Extractor Predicting: 388it [03:59,  1.67it/s]Extractor Predicting: 389it [04:00,  1.68it/s]Extractor Predicting: 390it [04:00,  1.63it/s]Extractor Predicting: 391it [04:01,  1.63it/s]Extractor Predicting: 392it [04:02,  1.64it/s]Extractor Predicting: 393it [04:02,  1.68it/s]Extractor Predicting: 394it [04:03,  1.60it/s]Extractor Predicting: 395it [04:04,  1.56it/s]Extractor Predicting: 396it [04:04,  1.55it/s]Extractor Predicting: 397it [04:05,  1.54it/s]Extractor Predicting: 398it [04:05,  1.54it/s]Extractor Predicting: 399it [04:06,  1.53it/s]Extractor Predicting: 400it [04:07,  1.59it/s]Extractor Predicting: 401it [04:07,  1.56it/s]Extractor Predicting: 402it [04:08,  1.54it/s]Extractor Predicting: 403it [04:09,  1.54it/s]Extractor Predicting: 404it [04:09,  1.49it/s]Extractor Predicting: 405it [04:10,  1.50it/s]Extractor Predicting: 406it [04:11,  1.50it/s]Extractor Predicting: 407it [04:11,  1.48it/s]Extractor Predicting: 408it [04:12,  1.52it/s]Extractor Predicting: 409it [04:13,  1.54it/s]Extractor Predicting: 410it [04:13,  1.51it/s]Extractor Predicting: 411it [04:14,  1.53it/s]Extractor Predicting: 412it [04:15,  1.54it/s]Extractor Predicting: 413it [04:15,  1.56it/s]Extractor Predicting: 414it [04:16,  1.58it/s]Extractor Predicting: 415it [04:16,  1.62it/s]Extractor Predicting: 416it [04:17,  1.42it/s]Extractor Predicting: 417it [04:18,  1.47it/s]Extractor Predicting: 418it [04:19,  1.49it/s]Extractor Predicting: 419it [04:19,  1.47it/s]Extractor Predicting: 420it [04:20,  1.49it/s]Extractor Predicting: 421it [04:21,  1.59it/s]Extractor Predicting: 421it [04:21,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:55,556 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:55,594 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:55,594 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:55,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:55,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:31:56,188 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:31:56,189 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:31:56,619 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:31:57,725 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:31:57,725 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:59,764 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:59,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:59,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:59,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:31:59,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:32:00,232 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:32:00,234 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:32:00,517 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:32:00,732 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:32:00,732 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.20853080568720378,
  "recall": 0.052303120356612186,
  "score": 0.08363031598954622,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.50it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.97it/s]Extractor Predicting: 9it [00:05,  1.64it/s]
[INFO|configuration_utils.py:515] 2023-08-29 07:32:08,114 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:32:08,115 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:32:08,198 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:32:08,199 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 07:32:08,225 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:32:19,224 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 07:32:19,240 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 07:32:19,370 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:32:19,371 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:32:19,438 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:19,480 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:19,480 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:19,480 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:19,480 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:19,480 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:19,480 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2619047619047619,
  "recall": 0.027160493827160494,
  "score": 0.04921700223713646,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 07:32:19,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:20,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:20,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:21,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:22,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:22,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:23,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:23,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:24,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:25,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:26,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:26,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:27,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:27,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:28,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:29,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:29,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:30,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:31,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:31,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:32,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:33,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:33,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:14<04:40, 14.77s/it][WARNING|generation_utils.py:914] 2023-08-29 07:32:34,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:35,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:35,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:36,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:37,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:37,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:38,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:38,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:39,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:40,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:40,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:41,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:41,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:42,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:42,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:43,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:43,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:44,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:45,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:45,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:46,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:47,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:47,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:48,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:48,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:49,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:30<04:32, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-29 07:32:49,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:50,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:51,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:51,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:52,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:52,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:53,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:54,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:54,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:55,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:55,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:56,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:56,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:57,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:58,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:58,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:59,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:59,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:00,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:00,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:41<03:49, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:01,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:02,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:02,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:03,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:03,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:04,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:05,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:05,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:06,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:06,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:07,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:08,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:08,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:09,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:10,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:10,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:11,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:12,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:12,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:13,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:14,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [00:54<03:34, 13.38s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:14,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:15,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:15,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:16,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:17,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:17,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:18,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:19,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:19,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:20,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:20,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:21,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:22,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:22,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:23,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:23,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:24,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:24,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:25,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:26,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:26,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:07<03:15, 13.03s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:27,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:27,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:28,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:28,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:29,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:30,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:30,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:31,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:31,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:32,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:33,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:33,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:34,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:35,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:35,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:36,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:36,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:37,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:38,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:38,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:19<02:58, 12.77s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:39,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:40,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:40,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:41,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:41,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:42,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:43,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:43,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:44,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:44,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:45,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:46,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:46,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:47,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:47,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:48,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:49,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:49,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:50,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:51,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:51,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:52,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:33<02:49, 13.05s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:53,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:53,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:54,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:55,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:55,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:56,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:56,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:57,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:58,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:58,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:59,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:59,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:00,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:01,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:01,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:02,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:02,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:03,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:04,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:04,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:05,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [01:46<02:36, 13.00s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:05,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:06,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:07,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:07,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:08,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:08,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:09,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:10,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:10,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:11,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:12,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:12,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:13,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:13,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:14,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:15,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:16,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:16,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:17,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:17,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:18,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [01:59<02:23, 13.07s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:19,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:19,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:20,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:21,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:21,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:22,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:22,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:23,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:24,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:24,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:25,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:26,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:26,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:27,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:27,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:28,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:29,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:29,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:30,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:30,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:31,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:12<02:10, 13.06s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:32,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:32,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:33,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:33,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:34,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:35,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:35,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:36,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:37,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:37,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:38,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:39,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:39,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:40,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:40,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:41,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:42,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:42,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:43,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:43,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:24<01:55, 12.82s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:44,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:45,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:45,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:46,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:46,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:47,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:48,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:48,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:49,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:50,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:50,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:51,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:51,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:52,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:53,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:53,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:54,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:55,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:55,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:56,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:56,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [02:37<01:43, 12.97s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:57,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:58,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:59,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:59,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:00,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:00,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:01,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:02,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:02,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:03,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:03,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:04,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:05,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:05,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:06,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:06,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:07,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:08,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:09,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:09,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:10,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [02:51<01:31, 13.11s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:11,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:11,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:12,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:12,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:13,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:13,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:14,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:14,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:15,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:15,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:16,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:17,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:17,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:18,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:18,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:19,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:19,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:20,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:20,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:21,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:02<01:14, 12.36s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:21,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:22,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:22,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:23,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:24,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:24,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:25,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:25,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:26,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:26,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:27,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:28,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:28,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:29,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:29,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:30,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:31,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:31,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:32,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:32,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:33,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [03:14<01:01, 12.30s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:33,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:34,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:35,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:35,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:36,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:36,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:37,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:38,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:38,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:39,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:39,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:40,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:41,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:41,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:42,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:42,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:43,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:44,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:44,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:45,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [03:26<00:48, 12.16s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:45,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:46,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:46,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:47,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:48,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:48,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:49,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:49,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:50,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:50,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:51,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:52,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:52,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:53,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:53,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:54,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:55,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:55,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:56,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:56,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:57,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [03:38<00:36, 12.25s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:58,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:58,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:59,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:59,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:00,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:01,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:01,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:02,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:02,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:03,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:03,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:04,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:05,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:05,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:06,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:06,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:07,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:08,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:08,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:09,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:09,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [03:50<00:24, 12.19s/it][WARNING|generation_utils.py:914] 2023-08-29 07:36:10,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:10,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:11,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:12,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:13,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:13,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:14,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:14,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:15,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:16,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:16,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:17,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:17,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:18,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:19,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:19,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:20,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:20,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:21,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:22,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:22,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [04:03<00:12, 12.43s/it][WARNING|generation_utils.py:914] 2023-08-29 07:36:23,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:23,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:24,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:25,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:26,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:26,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:27,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:28,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:28,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:29,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:30,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:31,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:31,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:32,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:33,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:33,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:34,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:35,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:35,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:36,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:37,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:37,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [04:18<00:00, 13.29s/it]Generating: 100%|| 20/20 [04:18<00:00, 12.94s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:45,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:45,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:45,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:45,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:45,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:36:46,483 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:36:46,484 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:36:47,104 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:36:48,173 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:36:48,173 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:51,027 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:51,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:51,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:51,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:36:51,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:36:51,669 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:36:51,671 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:36:52,235 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:36:52,411 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:36:52,411 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that he would be given an automatic leave to appeal the decision . Head Entity : CJ , Tail Entity : CJ .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 192, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 380, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 426, 'raw': 576}
{'target': 600, 'success': 451, 'raw': 608}
{'target': 600, 'success': 478, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 576, 'raw': 768}
{'target': 600, 'success': 596, 'raw': 800}
{'target': 600, 'success': 621, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7463942307692307, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8934659090909091, 'errors': {''}}
['Relation : crosses . Context : On 31 March 2014 , the Armenian national team won a bronze medal match against the Czech Republic in the Eurovision Song Contest at the Arena in Krasnodar , Russia , where the song " Let \'s Get " was played . Head Entity : Let \'s Get , Tail Entity : Kazakh national team .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : crosses .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9671875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : participant .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : participating team .', 'success_rate': 0.95, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : platform . Context : The game is a spin on " The Legend of Zelda: Breath of the Wild " . Head Entity : The Legend of Zelda: Breath of the Wild , Tail Entity : Nintendo .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 11139
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11239, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.57it/s]Extractor Estimating: 3it [00:01,  1.68it/s]Extractor Estimating: 4it [00:02,  1.69it/s]Extractor Estimating: 5it [00:02,  1.70it/s]Extractor Estimating: 6it [00:03,  1.67it/s]Extractor Estimating: 7it [00:04,  1.73it/s]Extractor Estimating: 8it [00:04,  1.54it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:06,  1.62it/s]Extractor Estimating: 12it [00:07,  1.65it/s]Extractor Estimating: 13it [00:07,  1.75it/s]Extractor Estimating: 14it [00:08,  1.83it/s]Extractor Estimating: 15it [00:08,  1.80it/s]Extractor Estimating: 16it [00:09,  1.76it/s]Extractor Estimating: 17it [00:10,  1.75it/s]Extractor Estimating: 18it [00:10,  1.76it/s]Extractor Estimating: 19it [00:11,  1.73it/s]Extractor Estimating: 20it [00:11,  1.68it/s]Extractor Estimating: 21it [00:12,  1.70it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:13,  1.68it/s]Extractor Estimating: 24it [00:14,  1.64it/s]Extractor Estimating: 25it [00:14,  1.62it/s]Extractor Estimating: 26it [00:15,  1.56it/s]Extractor Estimating: 27it [00:16,  1.55it/s]Extractor Estimating: 28it [00:17,  1.50it/s]Extractor Estimating: 29it [00:17,  1.45it/s]Extractor Estimating: 30it [00:18,  1.49it/s]Extractor Estimating: 31it [00:19,  1.51it/s]Extractor Estimating: 32it [00:19,  1.51it/s]Extractor Estimating: 33it [00:20,  1.50it/s]Extractor Estimating: 34it [00:21,  1.46it/s]Extractor Estimating: 35it [00:21,  1.45it/s]Extractor Estimating: 36it [00:22,  1.45it/s]Extractor Estimating: 37it [00:23,  1.49it/s]Extractor Estimating: 38it [00:23,  1.51it/s]Extractor Estimating: 39it [00:24,  1.50it/s]Extractor Estimating: 40it [00:25,  1.51it/s]Extractor Estimating: 41it [00:25,  1.52it/s]Extractor Estimating: 42it [00:26,  1.51it/s]Extractor Estimating: 43it [00:27,  1.47it/s]Extractor Estimating: 44it [00:27,  1.47it/s]Extractor Estimating: 45it [00:28,  1.51it/s]Extractor Estimating: 46it [00:29,  1.55it/s]Extractor Estimating: 47it [00:29,  1.59it/s]Extractor Estimating: 48it [00:30,  1.52it/s]Extractor Estimating: 49it [00:31,  1.49it/s]Extractor Estimating: 50it [00:31,  1.54it/s]Extractor Estimating: 51it [00:32,  1.63it/s]Extractor Estimating: 52it [00:32,  1.63it/s]Extractor Estimating: 53it [00:33,  1.62it/s]Extractor Estimating: 54it [00:33,  1.67it/s]Extractor Estimating: 55it [00:34,  1.70it/s]Extractor Estimating: 56it [00:35,  1.73it/s]Extractor Estimating: 57it [00:35,  1.68it/s]Extractor Estimating: 58it [00:36,  1.76it/s]Extractor Estimating: 59it [00:36,  1.76it/s]Extractor Estimating: 60it [00:37,  1.75it/s]Extractor Estimating: 61it [00:38,  1.67it/s]Extractor Estimating: 62it [00:38,  1.69it/s]Extractor Estimating: 63it [00:39,  1.67it/s]Extractor Estimating: 64it [00:39,  1.75it/s]Extractor Estimating: 65it [00:40,  1.71it/s]Extractor Estimating: 66it [00:40,  1.71it/s]Extractor Estimating: 67it [00:41,  1.72it/s]Extractor Estimating: 68it [00:42,  1.65it/s]Extractor Estimating: 69it [00:42,  1.64it/s]Extractor Estimating: 70it [00:43,  1.69it/s]Extractor Estimating: 71it [00:43,  1.69it/s]Extractor Estimating: 72it [00:44,  1.70it/s]Extractor Estimating: 73it [00:45,  1.69it/s]Extractor Estimating: 74it [00:45,  1.71it/s]Extractor Estimating: 75it [00:46,  1.76it/s]Extractor Estimating: 76it [00:46,  1.70it/s]Extractor Estimating: 77it [00:47,  1.60it/s]Extractor Estimating: 78it [00:48,  1.58it/s]Extractor Estimating: 79it [00:48,  1.60it/s]Extractor Estimating: 80it [00:49,  1.58it/s]Extractor Estimating: 81it [00:50,  1.60it/s]Extractor Estimating: 82it [00:50,  1.55it/s]Extractor Estimating: 83it [00:51,  1.57it/s]Extractor Estimating: 84it [00:52,  1.58it/s]Extractor Estimating: 85it [00:52,  1.61it/s]Extractor Estimating: 86it [00:53,  1.41it/s]Extractor Estimating: 87it [00:54,  1.44it/s]Extractor Estimating: 88it [00:54,  1.43it/s]Extractor Estimating: 89it [00:55,  1.42it/s]Extractor Estimating: 90it [00:56,  1.46it/s]Extractor Estimating: 91it [00:56,  1.45it/s]Extractor Estimating: 92it [00:57,  1.51it/s]Extractor Estimating: 93it [00:58,  1.54it/s]Extractor Estimating: 94it [00:58,  1.47it/s]Extractor Estimating: 95it [00:59,  1.52it/s]Extractor Estimating: 96it [01:00,  1.47it/s]Extractor Estimating: 97it [01:00,  1.49it/s]Extractor Estimating: 98it [01:01,  1.46it/s]Extractor Estimating: 99it [01:02,  1.48it/s]Extractor Estimating: 100it [01:02,  1.49it/s]Extractor Estimating: 101it [01:03,  1.56it/s]Extractor Estimating: 102it [01:04,  1.60it/s]Extractor Estimating: 103it [01:04,  1.63it/s]Extractor Estimating: 104it [01:05,  1.65it/s]Extractor Estimating: 105it [01:05,  1.66it/s]Extractor Estimating: 106it [01:06,  1.70it/s]Extractor Estimating: 107it [01:06,  1.74it/s]Extractor Estimating: 108it [01:07,  1.71it/s]Extractor Estimating: 109it [01:08,  1.75it/s]Extractor Estimating: 110it [01:08,  1.67it/s]Extractor Estimating: 111it [01:09,  1.73it/s]Extractor Estimating: 112it [01:09,  1.72it/s]Extractor Estimating: 113it [01:10,  1.78it/s]Extractor Estimating: 114it [01:11,  1.74it/s]Extractor Estimating: 115it [01:11,  1.79it/s]Extractor Estimating: 116it [01:12,  1.79it/s]Extractor Estimating: 117it [01:12,  1.75it/s]Extractor Estimating: 118it [01:13,  1.76it/s]Extractor Estimating: 119it [01:13,  1.67it/s]Extractor Estimating: 120it [01:14,  1.68it/s]Extractor Estimating: 121it [01:15,  1.67it/s]Extractor Estimating: 122it [01:15,  1.74it/s]Extractor Estimating: 123it [01:16,  1.81it/s]Extractor Estimating: 124it [01:16,  1.81it/s]Extractor Estimating: 125it [01:17,  1.75it/s]Extractor Estimating: 126it [01:17,  1.66it/s]Extractor Estimating: 127it [01:18,  1.54it/s]Extractor Estimating: 128it [01:19,  1.52it/s]Extractor Estimating: 129it [01:20,  1.56it/s]Extractor Estimating: 130it [01:20,  1.52it/s]Extractor Estimating: 131it [01:21,  1.51it/s]Extractor Estimating: 132it [01:22,  1.49it/s]Extractor Estimating: 133it [01:22,  1.46it/s]Extractor Estimating: 134it [01:23,  1.43it/s]Extractor Estimating: 135it [01:24,  1.45it/s]Extractor Estimating: 136it [01:24,  1.38it/s]Extractor Estimating: 137it [01:25,  1.44it/s]Extractor Estimating: 138it [01:26,  1.49it/s]Extractor Estimating: 139it [01:26,  1.44it/s]Extractor Estimating: 140it [01:27,  1.45it/s]Extractor Estimating: 141it [01:28,  1.44it/s]Extractor Estimating: 142it [01:29,  1.42it/s]Extractor Estimating: 143it [01:29,  1.44it/s]Extractor Estimating: 144it [01:30,  1.42it/s]Extractor Estimating: 145it [01:31,  1.44it/s]Extractor Estimating: 146it [01:31,  1.47it/s]Extractor Estimating: 147it [01:32,  1.46it/s]Extractor Estimating: 148it [01:33,  1.32it/s]Extractor Estimating: 149it [01:34,  1.35it/s]Extractor Estimating: 150it [01:34,  1.37it/s]Extractor Estimating: 151it [01:35,  1.44it/s]Extractor Estimating: 152it [01:36,  1.51it/s]Extractor Estimating: 153it [01:36,  1.57it/s]Extractor Estimating: 154it [01:37,  1.57it/s]Extractor Estimating: 155it [01:37,  1.60it/s]Extractor Estimating: 156it [01:38,  1.68it/s]Extractor Estimating: 157it [01:38,  1.68it/s]Extractor Estimating: 158it [01:39,  1.75it/s]Extractor Estimating: 159it [01:40,  1.80it/s]Extractor Estimating: 160it [01:40,  1.79it/s]Extractor Estimating: 161it [01:41,  1.80it/s]Extractor Estimating: 162it [01:41,  1.80it/s]Extractor Estimating: 163it [01:42,  1.73it/s]Extractor Estimating: 164it [01:42,  1.66it/s]Extractor Estimating: 165it [01:43,  1.68it/s]Extractor Estimating: 166it [01:44,  1.63it/s]Extractor Estimating: 167it [01:44,  1.69it/s]Extractor Estimating: 168it [01:45,  1.63it/s]Extractor Estimating: 169it [01:45,  1.66it/s]Extractor Estimating: 170it [01:46,  1.68it/s]Extractor Estimating: 171it [01:47,  1.64it/s]Extractor Estimating: 172it [01:47,  1.64it/s]Extractor Estimating: 173it [01:48,  1.66it/s]Extractor Estimating: 174it [01:49,  1.64it/s]Extractor Estimating: 175it [01:49,  1.65it/s]Extractor Estimating: 176it [01:50,  1.68it/s]Extractor Estimating: 177it [01:50,  1.65it/s]Extractor Estimating: 178it [01:51,  1.62it/s]Extractor Estimating: 179it [01:52,  1.67it/s]Extractor Estimating: 180it [01:52,  1.71it/s]Extractor Estimating: 181it [01:53,  1.69it/s]Extractor Estimating: 182it [01:53,  1.72it/s]Extractor Estimating: 183it [01:54,  1.71it/s]Extractor Estimating: 184it [01:54,  1.68it/s]Extractor Estimating: 185it [01:55,  1.72it/s]Extractor Estimating: 186it [01:56,  1.74it/s]Extractor Estimating: 187it [01:56,  1.71it/s]Extractor Estimating: 188it [01:57,  1.74it/s]Extractor Estimating: 189it [01:57,  1.75it/s]Extractor Estimating: 190it [01:58,  1.72it/s]Extractor Estimating: 191it [01:58,  1.70it/s]Extractor Estimating: 192it [01:59,  1.70it/s]Extractor Estimating: 193it [02:00,  1.68it/s]Extractor Estimating: 194it [02:00,  1.70it/s]Extractor Estimating: 195it [02:01,  1.71it/s]Extractor Estimating: 196it [02:01,  1.74it/s]Extractor Estimating: 197it [02:02,  1.68it/s]Extractor Estimating: 198it [02:03,  1.75it/s]Extractor Estimating: 199it [02:03,  1.71it/s]Extractor Estimating: 200it [02:04,  1.70it/s]Extractor Estimating: 201it [02:04,  1.65it/s]Extractor Estimating: 202it [02:05,  1.61it/s]Extractor Estimating: 203it [02:06,  1.63it/s]Extractor Estimating: 204it [02:06,  1.57it/s]Extractor Estimating: 205it [02:07,  1.55it/s]Extractor Estimating: 206it [02:08,  1.59it/s]Extractor Estimating: 207it [02:08,  1.55it/s]Extractor Estimating: 208it [02:09,  1.52it/s]Extractor Estimating: 209it [02:10,  1.48it/s]Extractor Estimating: 210it [02:10,  1.52it/s]Extractor Estimating: 211it [02:11,  1.50it/s]Extractor Estimating: 212it [02:12,  1.52it/s]Extractor Estimating: 213it [02:12,  1.53it/s]Extractor Estimating: 214it [02:13,  1.54it/s]Extractor Estimating: 215it [02:14,  1.51it/s]Extractor Estimating: 216it [02:14,  1.56it/s]Extractor Estimating: 217it [02:15,  1.53it/s]Extractor Estimating: 218it [02:15,  1.59it/s]Extractor Estimating: 219it [02:16,  1.44it/s]Extractor Estimating: 220it [02:17,  1.46it/s]Extractor Estimating: 221it [02:18,  1.48it/s]Extractor Estimating: 222it [02:18,  1.54it/s]Extractor Estimating: 223it [02:19,  1.55it/s]Extractor Estimating: 224it [02:20,  1.46it/s]Extractor Estimating: 225it [02:20,  1.50it/s]Extractor Estimating: 226it [02:21,  1.50it/s]Extractor Estimating: 227it [02:22,  1.50it/s]Extractor Estimating: 228it [02:22,  1.52it/s]Extractor Estimating: 229it [02:23,  1.63it/s]Extractor Estimating: 230it [02:23,  1.65it/s]Extractor Estimating: 231it [02:24,  1.63it/s]Extractor Estimating: 232it [02:25,  1.42it/s]Extractor Estimating: 233it [02:25,  1.48it/s]Extractor Estimating: 234it [02:26,  1.52it/s]Extractor Estimating: 235it [02:27,  1.55it/s]Extractor Estimating: 236it [02:27,  1.54it/s]Extractor Estimating: 237it [02:28,  1.55it/s]Extractor Estimating: 238it [02:29,  1.57it/s]Extractor Estimating: 239it [02:29,  1.54it/s]Extractor Estimating: 240it [02:30,  1.50it/s]Extractor Estimating: 241it [02:31,  1.56it/s]Extractor Estimating: 242it [02:31,  1.61it/s]Extractor Estimating: 243it [02:32,  1.62it/s]Extractor Estimating: 244it [02:32,  1.63it/s]Extractor Estimating: 245it [02:33,  1.62it/s]Extractor Estimating: 246it [02:34,  1.59it/s]Extractor Estimating: 247it [02:34,  1.60it/s]Extractor Estimating: 248it [02:35,  1.62it/s]Extractor Estimating: 249it [02:36,  1.56it/s]Extractor Estimating: 250it [02:36,  1.58it/s]Extractor Estimating: 251it [02:37,  1.61it/s]Extractor Estimating: 252it [02:37,  1.57it/s]Extractor Estimating: 253it [02:38,  1.60it/s]Extractor Estimating: 254it [02:39,  1.52it/s]Extractor Estimating: 255it [02:39,  1.55it/s]Extractor Estimating: 256it [02:40,  1.56it/s]Extractor Estimating: 257it [02:41,  1.52it/s]Extractor Estimating: 258it [02:41,  1.49it/s]Extractor Estimating: 259it [02:42,  1.45it/s]Extractor Estimating: 260it [02:43,  1.46it/s]Extractor Estimating: 261it [02:43,  1.49it/s]Extractor Estimating: 262it [02:44,  1.50it/s]Extractor Estimating: 263it [02:45,  1.52it/s]Extractor Estimating: 264it [02:45,  1.48it/s]Extractor Estimating: 265it [02:46,  1.50it/s]Extractor Estimating: 266it [02:47,  1.52it/s]Extractor Estimating: 267it [02:47,  1.52it/s]Extractor Estimating: 268it [02:48,  1.56it/s]Extractor Estimating: 269it [02:49,  1.58it/s]Extractor Estimating: 270it [02:49,  1.58it/s]Extractor Estimating: 271it [02:50,  1.52it/s]Extractor Estimating: 272it [02:51,  1.49it/s]Extractor Estimating: 273it [02:51,  1.49it/s]Extractor Estimating: 274it [02:52,  1.53it/s]Extractor Estimating: 275it [02:53,  1.55it/s]Extractor Estimating: 276it [02:53,  1.59it/s]Extractor Estimating: 277it [02:54,  1.56it/s]Extractor Estimating: 278it [02:54,  1.58it/s]Extractor Estimating: 279it [02:55,  1.59it/s]Extractor Estimating: 280it [02:56,  1.55it/s]Extractor Estimating: 281it [02:56,  1.56it/s]Extractor Estimating: 282it [02:57,  1.66it/s]Extractor Estimating: 283it [02:58,  1.59it/s]Extractor Estimating: 284it [02:58,  1.60it/s]Extractor Estimating: 285it [02:59,  1.60it/s]Extractor Estimating: 286it [02:59,  1.61it/s]Extractor Estimating: 287it [03:00,  1.58it/s]Extractor Estimating: 288it [03:01,  1.60it/s]Extractor Estimating: 289it [03:01,  1.59it/s]Extractor Estimating: 290it [03:02,  1.54it/s]Extractor Estimating: 291it [03:03,  1.59it/s]Extractor Estimating: 292it [03:03,  1.60it/s]Extractor Estimating: 293it [03:04,  1.62it/s]Extractor Estimating: 294it [03:04,  1.62it/s]Extractor Estimating: 295it [03:05,  1.64it/s]Extractor Estimating: 296it [03:06,  1.64it/s]Extractor Estimating: 297it [03:06,  1.63it/s]Extractor Estimating: 298it [03:07,  1.65it/s]Extractor Estimating: 299it [03:08,  1.64it/s]Extractor Estimating: 300it [03:08,  1.64it/s]Extractor Estimating: 301it [03:09,  1.66it/s]Extractor Estimating: 302it [03:09,  1.58it/s]Extractor Estimating: 303it [03:10,  1.63it/s]Extractor Estimating: 304it [03:11,  1.57it/s]Extractor Estimating: 305it [03:11,  1.58it/s]Extractor Estimating: 306it [03:12,  1.64it/s]Extractor Estimating: 307it [03:12,  1.64it/s]Extractor Estimating: 308it [03:13,  1.58it/s]Extractor Estimating: 309it [03:14,  1.54it/s]Extractor Estimating: 310it [03:14,  1.59it/s]Extractor Estimating: 311it [03:15,  1.60it/s]Extractor Estimating: 312it [03:16,  1.43it/s]Extractor Estimating: 313it [03:17,  1.46it/s]Extractor Estimating: 314it [03:17,  1.47it/s]Extractor Estimating: 315it [03:18,  1.47it/s]Extractor Estimating: 316it [03:19,  1.53it/s]Extractor Estimating: 317it [03:19,  1.53it/s]Extractor Estimating: 318it [03:20,  1.54it/s]Extractor Estimating: 319it [03:20,  1.55it/s]Extractor Estimating: 320it [03:21,  1.49it/s]Extractor Estimating: 321it [03:22,  1.45it/s]Extractor Estimating: 322it [03:23,  1.43it/s]Extractor Estimating: 323it [03:23,  1.47it/s]Extractor Estimating: 324it [03:24,  1.52it/s]Extractor Estimating: 325it [03:25,  1.49it/s]Extractor Estimating: 326it [03:25,  1.67it/s]Extractor Estimating: 327it [03:25,  1.80it/s]Extractor Estimating: 328it [03:26,  1.83it/s]Extractor Estimating: 329it [03:27,  1.79it/s]Extractor Estimating: 330it [03:27,  1.70it/s]Extractor Estimating: 331it [03:28,  1.76it/s]Extractor Estimating: 332it [03:28,  1.88it/s]Extractor Estimating: 333it [03:29,  1.90it/s]Extractor Estimating: 334it [03:29,  1.87it/s]Extractor Estimating: 335it [03:30,  1.98it/s]Extractor Estimating: 336it [03:30,  1.84it/s]Extractor Estimating: 337it [03:31,  1.91it/s]Extractor Estimating: 338it [03:31,  1.87it/s]Extractor Estimating: 339it [03:32,  1.89it/s]Extractor Estimating: 340it [03:32,  1.99it/s]Extractor Estimating: 341it [03:33,  2.01it/s]Extractor Estimating: 342it [03:33,  2.05it/s]Extractor Estimating: 343it [03:34,  2.04it/s]Extractor Estimating: 344it [03:34,  2.00it/s]Extractor Estimating: 345it [03:35,  2.03it/s]Extractor Estimating: 346it [03:35,  1.89it/s]Extractor Estimating: 347it [03:36,  2.04it/s]Extractor Estimating: 348it [03:36,  2.04it/s]Extractor Estimating: 349it [03:37,  2.02it/s]Extractor Estimating: 350it [03:37,  1.99it/s]Extractor Estimating: 351it [03:38,  1.94it/s]Extractor Estimating: 352it [03:38,  1.89it/s]Extractor Estimating: 353it [03:39,  1.85it/s]Extractor Estimating: 354it [03:40,  1.80it/s]Extractor Estimating: 355it [03:40,  1.79it/s]Extractor Estimating: 356it [03:41,  1.78it/s]Extractor Estimating: 357it [03:41,  1.80it/s]Extractor Estimating: 358it [03:42,  1.78it/s]Extractor Estimating: 359it [03:42,  1.81it/s]Extractor Estimating: 360it [03:43,  1.76it/s]Extractor Estimating: 361it [03:44,  1.75it/s]Extractor Estimating: 362it [03:44,  1.76it/s]Extractor Estimating: 363it [03:45,  1.79it/s]Extractor Estimating: 364it [03:45,  1.82it/s]Extractor Estimating: 365it [03:46,  1.80it/s]Extractor Estimating: 366it [03:46,  1.79it/s]Extractor Estimating: 367it [03:47,  1.77it/s]Extractor Estimating: 368it [03:47,  1.82it/s]Extractor Estimating: 369it [03:48,  1.80it/s]Extractor Estimating: 370it [03:49,  1.73it/s]Extractor Estimating: 371it [03:49,  1.76it/s]Extractor Estimating: 372it [03:50,  1.81it/s]Extractor Estimating: 373it [03:50,  1.79it/s]Extractor Estimating: 374it [03:51,  1.79it/s]Extractor Estimating: 375it [03:51,  1.82it/s]Extractor Estimating: 376it [03:52,  1.76it/s]Extractor Estimating: 377it [03:52,  1.75it/s]Extractor Estimating: 378it [03:53,  1.77it/s]Extractor Estimating: 379it [03:54,  1.79it/s]Extractor Estimating: 380it [03:54,  1.78it/s]Extractor Estimating: 381it [03:55,  1.82it/s]Extractor Estimating: 382it [03:55,  1.81it/s]Extractor Estimating: 383it [03:56,  1.79it/s]Extractor Estimating: 384it [03:56,  1.76it/s]Extractor Estimating: 385it [03:57,  1.73it/s]Extractor Estimating: 386it [03:58,  1.79it/s]Extractor Estimating: 387it [03:58,  1.76it/s]Extractor Estimating: 388it [03:59,  1.83it/s]Extractor Estimating: 389it [03:59,  1.83it/s]Extractor Estimating: 390it [04:00,  1.78it/s]Extractor Estimating: 391it [04:00,  1.76it/s]Extractor Estimating: 392it [04:01,  1.78it/s]Extractor Estimating: 393it [04:01,  1.79it/s]Extractor Estimating: 394it [04:02,  1.71it/s]Extractor Estimating: 395it [04:03,  1.77it/s]Extractor Estimating: 396it [04:03,  1.69it/s]Extractor Estimating: 397it [04:04,  1.70it/s]Extractor Estimating: 398it [04:04,  1.76it/s]Extractor Estimating: 399it [04:05,  1.77it/s]Extractor Estimating: 400it [04:05,  1.77it/s]Extractor Estimating: 401it [04:06,  1.75it/s]Extractor Estimating: 402it [04:07,  1.56it/s]Extractor Estimating: 403it [04:08,  1.55it/s]Extractor Estimating: 404it [04:08,  1.57it/s]Extractor Estimating: 405it [04:09,  1.69it/s]Extractor Estimating: 406it [04:09,  1.75it/s]Extractor Estimating: 407it [04:10,  1.72it/s]Extractor Estimating: 408it [04:10,  1.78it/s]Extractor Estimating: 409it [04:11,  1.79it/s]Extractor Estimating: 410it [04:11,  1.71it/s]Extractor Estimating: 411it [04:12,  1.70it/s]Extractor Estimating: 412it [04:13,  1.68it/s]Extractor Estimating: 413it [04:13,  1.72it/s]Extractor Estimating: 414it [04:14,  1.81it/s]Extractor Estimating: 415it [04:14,  1.72it/s]Extractor Estimating: 416it [04:15,  1.58it/s]Extractor Estimating: 417it [04:16,  1.63it/s]Extractor Estimating: 418it [04:16,  1.73it/s]Extractor Estimating: 419it [04:17,  1.66it/s]Extractor Estimating: 420it [04:17,  1.64it/s]Extractor Estimating: 421it [04:18,  1.70it/s]Extractor Estimating: 422it [04:19,  1.75it/s]Extractor Estimating: 423it [04:19,  1.76it/s]Extractor Estimating: 424it [04:20,  1.72it/s]Extractor Estimating: 425it [04:20,  1.73it/s]Extractor Estimating: 426it [04:21,  1.75it/s]Extractor Estimating: 427it [04:21,  1.81it/s]Extractor Estimating: 428it [04:22,  1.78it/s]Extractor Estimating: 429it [04:23,  1.74it/s]Extractor Estimating: 430it [04:23,  1.77it/s]Extractor Estimating: 431it [04:24,  1.71it/s]Extractor Estimating: 432it [04:24,  1.70it/s]Extractor Estimating: 433it [04:25,  1.73it/s]Extractor Estimating: 434it [04:25,  1.70it/s]Extractor Estimating: 435it [04:26,  1.74it/s]Extractor Estimating: 436it [04:27,  1.74it/s]Extractor Estimating: 437it [04:27,  1.77it/s]Extractor Estimating: 438it [04:28,  1.76it/s]Extractor Estimating: 439it [04:28,  1.76it/s]Extractor Estimating: 440it [04:29,  1.75it/s]Extractor Estimating: 441it [04:29,  1.70it/s]Extractor Estimating: 442it [04:30,  1.72it/s]Extractor Estimating: 443it [04:31,  1.77it/s]Extractor Estimating: 444it [04:31,  1.77it/s]Extractor Estimating: 445it [04:32,  1.70it/s]Extractor Estimating: 446it [04:32,  1.69it/s]Extractor Estimating: 447it [04:33,  1.64it/s]Extractor Estimating: 448it [04:34,  1.70it/s]Extractor Estimating: 449it [04:34,  1.73it/s]Extractor Estimating: 450it [04:35,  1.74it/s]Extractor Estimating: 451it [04:35,  1.73it/s]Extractor Estimating: 452it [04:36,  1.71it/s]Extractor Estimating: 453it [04:37,  1.46it/s]Extractor Estimating: 454it [04:37,  1.47it/s]Extractor Estimating: 455it [04:38,  1.44it/s]Extractor Estimating: 456it [04:39,  1.50it/s]Extractor Estimating: 457it [04:39,  1.56it/s]Extractor Estimating: 458it [04:40,  1.49it/s]Extractor Estimating: 459it [04:41,  1.57it/s]Extractor Estimating: 460it [04:41,  1.57it/s]Extractor Estimating: 461it [04:42,  1.59it/s]Extractor Estimating: 462it [04:42,  1.62it/s]Extractor Estimating: 463it [04:43,  1.58it/s]Extractor Estimating: 464it [04:44,  1.55it/s]Extractor Estimating: 465it [04:44,  1.58it/s]Extractor Estimating: 466it [04:45,  1.55it/s]Extractor Estimating: 467it [04:46,  1.54it/s]Extractor Estimating: 468it [04:46,  1.56it/s]Extractor Estimating: 469it [04:47,  1.60it/s]Extractor Estimating: 470it [04:48,  1.57it/s]Extractor Estimating: 471it [04:48,  1.55it/s]Extractor Estimating: 472it [04:49,  1.50it/s]Extractor Estimating: 473it [04:50,  1.53it/s]Extractor Estimating: 474it [04:50,  1.55it/s]Extractor Estimating: 475it [04:51,  1.61it/s]Extractor Estimating: 476it [04:51,  1.58it/s]Extractor Estimating: 477it [04:52,  1.64it/s]Extractor Estimating: 478it [04:53,  1.66it/s]Extractor Estimating: 479it [04:53,  1.66it/s]Extractor Estimating: 480it [04:54,  1.68it/s]Extractor Estimating: 481it [04:54,  1.66it/s]Extractor Estimating: 482it [04:55,  1.66it/s]Extractor Estimating: 483it [04:56,  1.70it/s]Extractor Estimating: 484it [04:56,  1.69it/s]Extractor Estimating: 485it [04:57,  1.72it/s]Extractor Estimating: 486it [04:57,  1.69it/s]Extractor Estimating: 487it [04:58,  1.64it/s]Extractor Estimating: 488it [04:59,  1.55it/s]Extractor Estimating: 489it [04:59,  1.50it/s]Extractor Estimating: 490it [05:00,  1.52it/s]Extractor Estimating: 491it [05:01,  1.55it/s]Extractor Estimating: 492it [05:01,  1.57it/s]Extractor Estimating: 493it [05:02,  1.57it/s]Extractor Estimating: 494it [05:03,  1.59it/s]Extractor Estimating: 495it [05:03,  1.40it/s]Extractor Estimating: 496it [05:04,  1.45it/s]Extractor Estimating: 497it [05:05,  1.52it/s]Extractor Estimating: 498it [05:05,  1.58it/s]Extractor Estimating: 499it [05:06,  1.56it/s]Extractor Estimating: 500it [05:06,  1.66it/s]Extractor Estimating: 500it [05:06,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:14,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:14,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:14,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:14,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:14,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:42:15,524 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:42:15,526 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:42:16,122 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:42:17,232 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:42:17,232 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:20,344 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:20,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:20,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:20,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:20,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:42:21,307 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:42:21,308 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:42:21,952 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:42:22,200 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:42:22,200 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 10:45:02,533 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 10:45:02,776 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9992 mean pseudo reward: 0.9314271654011511
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 17764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17864, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.031, loss:706.9992
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.039, loss:685.8533
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.031, loss:656.2224
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.034, loss:647.9923
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.092, loss:664.8353
>> valid entity prec:0.5946, rec:0.6258, f1:0.6098
>> valid relation prec:0.2421, rec:0.1251, f1:0.1650
>> valid relation with NER prec:0.2421, rec:0.1251, f1:0.1650
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.282, loss:634.4433
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.070, loss:635.8446
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.081, loss:644.4540
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.069, loss:575.7590
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.086, loss:605.6837
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5581, rec:0.6467, f1:0.5991
>> valid relation prec:0.2033, rec:0.1102, f1:0.1429
>> valid relation with NER prec:0.2033, rec:0.1102, f1:0.1429
g_step 1100, step 266, avg_time 2.323, loss:646.2164
g_step 1200, step 366, avg_time 1.054, loss:622.6785
g_step 1300, step 49, avg_time 1.069, loss:597.5590
g_step 1400, step 149, avg_time 1.047, loss:573.5636
g_step 1500, step 249, avg_time 1.093, loss:587.8023
>> valid entity prec:0.5913, rec:0.6032, f1:0.5972
>> valid relation prec:0.2445, rec:0.1245, f1:0.1650
>> valid relation with NER prec:0.2445, rec:0.1245, f1:0.1650
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.326, loss:598.9621
g_step 1700, step 32, avg_time 1.061, loss:561.8534
g_step 1800, step 132, avg_time 1.057, loss:552.3927
g_step 1900, step 232, avg_time 1.099, loss:565.1131
g_step 2000, step 332, avg_time 1.074, loss:567.1969
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6276, rec:0.5861, f1:0.6061
>> valid relation prec:0.2375, rec:0.1323, f1:0.1699
>> valid relation with NER prec:0.2375, rec:0.1323, f1:0.1699
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 15, avg_time 2.319, loss:540.2238
g_step 2200, step 115, avg_time 1.085, loss:494.9834
g_step 2300, step 215, avg_time 1.071, loss:523.3172
g_step 2400, step 315, avg_time 1.095, loss:543.0060
g_step 2500, step 415, avg_time 1.069, loss:565.2166
>> valid entity prec:0.6082, rec:0.6015, f1:0.6048
>> valid relation prec:0.2032, rec:0.1214, f1:0.1520
>> valid relation with NER prec:0.2032, rec:0.1214, f1:0.1520
g_step 2600, step 98, avg_time 2.302, loss:482.0478
g_step 2700, step 198, avg_time 1.112, loss:524.8620
g_step 2800, step 298, avg_time 1.086, loss:506.6094
g_step 2900, step 398, avg_time 1.073, loss:518.8408
g_step 3000, step 81, avg_time 1.044, loss:479.7713
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5987, rec:0.6113, f1:0.6049
>> valid relation prec:0.1755, rec:0.1168, f1:0.1402
>> valid relation with NER prec:0.1755, rec:0.1168, f1:0.1402
g_step 3100, step 181, avg_time 2.332, loss:485.9290
g_step 3200, step 281, avg_time 1.076, loss:493.3800
g_step 3300, step 381, avg_time 1.063, loss:493.0696
g_step 3400, step 64, avg_time 1.091, loss:469.0356
g_step 3500, step 164, avg_time 1.059, loss:449.3703
>> valid entity prec:0.5891, rec:0.5850, f1:0.5871
>> valid relation prec:0.2106, rec:0.1102, f1:0.1446
>> valid relation with NER prec:0.2106, rec:0.1102, f1:0.1446
g_step 3600, step 264, avg_time 2.356, loss:457.9003
g_step 3700, step 364, avg_time 1.070, loss:502.4275
g_step 3800, step 47, avg_time 1.062, loss:440.7435
g_step 3900, step 147, avg_time 1.072, loss:443.9232
g_step 4000, step 247, avg_time 1.085, loss:463.1350
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5654, rec:0.5901, f1:0.5775
>> valid relation prec:0.1411, rec:0.0848, f1:0.1060
>> valid relation with NER prec:0.1411, rec:0.0848, f1:0.1060
g_step 4100, step 347, avg_time 2.331, loss:464.0703
g_step 4200, step 30, avg_time 1.086, loss:447.6520
g_step 4300, step 130, avg_time 1.075, loss:436.6610
g_step 4400, step 230, avg_time 1.067, loss:406.2959
g_step 4500, step 330, avg_time 1.093, loss:452.9791
>> valid entity prec:0.6186, rec:0.5576, f1:0.5865
>> valid relation prec:0.2150, rec:0.1079, f1:0.1437
>> valid relation with NER prec:0.2150, rec:0.1079, f1:0.1437
g_step 4600, step 13, avg_time 2.313, loss:433.6883
g_step 4700, step 113, avg_time 1.093, loss:417.3633
g_step 4800, step 213, avg_time 1.059, loss:415.4543
g_step 4900, step 313, avg_time 1.084, loss:424.8809
g_step 5000, step 413, avg_time 1.083, loss:431.0193
learning rate was adjusted to 0.0008
>> valid entity prec:0.6154, rec:0.5694, f1:0.5915
>> valid relation prec:0.1780, rec:0.1110, f1:0.1367
>> valid relation with NER prec:0.1780, rec:0.1110, f1:0.1367
g_step 5100, step 96, avg_time 2.306, loss:387.2125
g_step 5200, step 196, avg_time 1.072, loss:396.7489
g_step 5300, step 296, avg_time 1.080, loss:414.1267
g_step 5400, step 396, avg_time 1.091, loss:410.5222
g_step 5500, step 79, avg_time 1.096, loss:359.6419
>> valid entity prec:0.5948, rec:0.6079, f1:0.6013
>> valid relation prec:0.1770, rec:0.1240, f1:0.1458
>> valid relation with NER prec:0.1770, rec:0.1240, f1:0.1458
g_step 5600, step 179, avg_time 2.310, loss:404.3535
g_step 5700, step 279, avg_time 1.105, loss:399.3052
g_step 5800, step 379, avg_time 1.077, loss:402.9094
g_step 5900, step 62, avg_time 1.086, loss:370.2708
g_step 6000, step 162, avg_time 1.077, loss:370.6910
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5865, rec:0.6151, f1:0.6004
>> valid relation prec:0.1838, rec:0.1404, f1:0.1592
>> valid relation with NER prec:0.1838, rec:0.1404, f1:0.1592
g_step 6100, step 262, avg_time 2.312, loss:390.1455
g_step 6200, step 362, avg_time 1.076, loss:392.3999
g_step 6300, step 45, avg_time 1.051, loss:367.5213
g_step 6400, step 145, avg_time 1.100, loss:349.1319
g_step 6500, step 245, avg_time 1.071, loss:372.2309
>> valid entity prec:0.6219, rec:0.5615, f1:0.5902
>> valid relation prec:0.1752, rec:0.1004, f1:0.1276
>> valid relation with NER prec:0.1752, rec:0.1004, f1:0.1276
g_step 6600, step 345, avg_time 2.322, loss:382.2928
g_step 6700, step 28, avg_time 1.099, loss:375.8195
g_step 6800, step 128, avg_time 1.093, loss:349.0810
g_step 6900, step 228, avg_time 1.077, loss:359.5977
g_step 7000, step 328, avg_time 1.072, loss:364.7196
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6038, rec:0.5954, f1:0.5996
>> valid relation prec:0.1692, rec:0.1119, f1:0.1347
>> valid relation with NER prec:0.1692, rec:0.1119, f1:0.1347
g_step 7100, step 11, avg_time 2.289, loss:351.0639
g_step 7200, step 111, avg_time 1.044, loss:331.3578
g_step 7300, step 211, avg_time 1.103, loss:343.7826
g_step 7400, step 311, avg_time 1.070, loss:346.4476
g_step 7500, step 411, avg_time 1.074, loss:367.9528
>> valid entity prec:0.5914, rec:0.6089, f1:0.6000
>> valid relation prec:0.1811, rec:0.1274, f1:0.1496
>> valid relation with NER prec:0.1811, rec:0.1274, f1:0.1496
g_step 7600, step 94, avg_time 2.332, loss:332.1684
g_step 7700, step 194, avg_time 1.053, loss:332.1083
g_step 7800, step 294, avg_time 1.073, loss:353.8851
g_step 7900, step 394, avg_time 1.127, loss:334.3215
g_step 8000, step 77, avg_time 1.080, loss:320.3959
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6125, rec:0.5888, f1:0.6004
>> valid relation prec:0.1564, rec:0.1099, f1:0.1291
>> valid relation with NER prec:0.1564, rec:0.1099, f1:0.1291
g_step 8100, step 177, avg_time 2.298, loss:309.5039
g_step 8200, step 277, avg_time 1.080, loss:340.7824
g_step 8300, step 377, avg_time 1.082, loss:339.8647
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 10:45:02 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 10:45:02 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_10-45-02_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 10:45:03 - WARNING - datasets.builder -   Using custom data configuration default-e8cf4c7c76c6e726
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e8cf4c7c76c6e726/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 10:45:06,379 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:45:06,380 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 10:45:06,381 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:45:06,382 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 10:45:06,499 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:45:06,555 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:45:06,555 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:45:06,555 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:45:06,555 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:45:06,556 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:45:06,556 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 10:45:07,009 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 10:45:10,136 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 10:45:10,205 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e8cf4c7c76c6e726/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:03,  2.89ba/s] 20%|        | 2/10 [00:00<00:02,  3.80ba/s] 30%|       | 3/10 [00:00<00:01,  4.21ba/s] 40%|      | 4/10 [00:00<00:01,  4.42ba/s] 50%|     | 5/10 [00:01<00:01,  4.57ba/s] 60%|    | 6/10 [00:01<00:00,  4.65ba/s] 70%|   | 7/10 [00:01<00:00,  4.71ba/s] 80%|  | 8/10 [00:01<00:00,  3.92ba/s] 90%| | 9/10 [00:02<00:00,  4.14ba/s]100%|| 10/10 [00:02<00:00,  4.31ba/s]100%|| 10/10 [00:02<00:00,  4.24ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:01,  2.88ba/s] 50%|     | 2/4 [00:00<00:00,  3.30ba/s] 75%|  | 3/4 [00:00<00:00,  3.74ba/s]100%|| 4/4 [00:00<00:00,  4.88ba/s]100%|| 4/4 [00:00<00:00,  4.20ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  5.00ba/s] 30%|       | 3/10 [00:00<00:00,  8.16ba/s] 50%|     | 5/10 [00:00<00:00,  9.38ba/s] 70%|   | 7/10 [00:00<00:00,  9.86ba/s] 90%| | 9/10 [00:00<00:00, 10.08ba/s]100%|| 10/10 [00:01<00:00,  9.56ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.53ba/s] 75%|  | 3/4 [00:00<00:00,  8.69ba/s]100%|| 4/4 [00:00<00:00,  9.70ba/s]
[INFO|trainer.py:414] 2023-08-29 10:45:16,580 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 10:45:16,710 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 10:45:16,710 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 10:45:16,710 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 10:45:16,710 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 10:45:16,710 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 10:45:16,710 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 10:45:16,710 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:53,  3.33it/s]  0%|          | 2/780 [00:00<03:48,  3.40it/s]  0%|          | 3/780 [00:00<03:46,  3.42it/s]  1%|          | 4/780 [00:01<03:45,  3.44it/s]  1%|          | 5/780 [00:01<03:45,  3.44it/s]  1%|          | 6/780 [00:01<03:45,  3.43it/s]  1%|          | 7/780 [00:02<03:55,  3.28it/s]  1%|          | 8/780 [00:02<03:52,  3.32it/s]  1%|          | 9/780 [00:02<03:50,  3.34it/s]  1%|         | 10/780 [00:02<03:49,  3.36it/s]  1%|         | 11/780 [00:03<03:48,  3.37it/s]  2%|         | 12/780 [00:03<03:47,  3.38it/s]  2%|         | 13/780 [00:03<03:46,  3.39it/s]  2%|         | 14/780 [00:04<03:46,  3.39it/s]  2%|         | 15/780 [00:04<03:45,  3.39it/s]  2%|         | 16/780 [00:04<03:44,  3.40it/s]  2%|         | 17/780 [00:05<03:44,  3.40it/s]  2%|         | 18/780 [00:05<03:44,  3.40it/s]  2%|         | 19/780 [00:05<03:43,  3.40it/s]  3%|         | 20/780 [00:05<03:43,  3.40it/s]  3%|         | 21/780 [00:06<03:43,  3.40it/s]  3%|         | 22/780 [00:06<03:43,  3.40it/s]  3%|         | 23/780 [00:06<03:42,  3.40it/s]  3%|         | 24/780 [00:07<03:52,  3.25it/s]  3%|         | 25/780 [00:07<03:49,  3.29it/s]  3%|         | 26/780 [00:07<03:47,  3.32it/s]  3%|         | 27/780 [00:08<03:45,  3.34it/s]  4%|         | 28/780 [00:08<03:43,  3.36it/s]  4%|         | 29/780 [00:08<03:42,  3.37it/s]  4%|         | 30/780 [00:08<03:42,  3.38it/s]  4%|         | 31/780 [00:09<03:41,  3.39it/s]  4%|         | 32/780 [00:09<03:40,  3.39it/s]  4%|         | 33/780 [00:09<03:40,  3.39it/s]  4%|         | 34/780 [00:10<03:40,  3.39it/s]  4%|         | 35/780 [00:10<03:39,  3.39it/s]  5%|         | 36/780 [00:10<03:39,  3.39it/s]  5%|         | 37/780 [00:10<03:38,  3.39it/s]  5%|         | 38/780 [00:11<03:38,  3.40it/s]  5%|         | 39/780 [00:11<03:38,  3.40it/s]  5%|         | 40/780 [00:11<03:37,  3.40it/s]  5%|         | 41/780 [00:12<03:44,  3.29it/s]  5%|         | 42/780 [00:12<03:42,  3.32it/s]  6%|         | 43/780 [00:12<03:40,  3.34it/s]  6%|         | 44/780 [00:13<03:38,  3.36it/s]  6%|         | 45/780 [00:13<03:38,  3.37it/s]  6%|         | 46/780 [00:13<03:37,  3.38it/s]  6%|         | 47/780 [00:13<03:36,  3.38it/s]  6%|         | 48/780 [00:14<03:36,  3.39it/s]  6%|         | 49/780 [00:14<03:35,  3.39it/s]  6%|         | 50/780 [00:14<03:35,  3.39it/s]  7%|         | 51/780 [00:15<03:34,  3.39it/s]  7%|         | 52/780 [00:15<03:38,  3.33it/s]  7%|         | 53/780 [00:15<03:36,  3.35it/s]  7%|         | 54/780 [00:16<03:35,  3.37it/s]  7%|         | 55/780 [00:16<03:34,  3.38it/s]  7%|         | 56/780 [00:16<03:33,  3.38it/s]  7%|         | 57/780 [00:16<03:33,  3.39it/s]  7%|         | 58/780 [00:17<03:32,  3.40it/s]  8%|         | 59/780 [00:17<03:31,  3.42it/s]  8%|         | 60/780 [00:17<03:30,  3.43it/s]  8%|         | 61/780 [00:18<03:29,  3.43it/s]  8%|         | 62/780 [00:18<03:28,  3.44it/s]  8%|         | 63/780 [00:18<03:39,  3.27it/s]  8%|         | 64/780 [00:18<03:35,  3.32it/s]  8%|         | 65/780 [00:19<03:33,  3.36it/s]  8%|         | 66/780 [00:19<03:31,  3.37it/s]  9%|         | 67/780 [00:19<03:30,  3.39it/s]  9%|         | 68/780 [00:20<03:28,  3.41it/s]  9%|         | 69/780 [00:20<03:28,  3.42it/s]  9%|         | 70/780 [00:20<03:27,  3.42it/s]  9%|         | 71/780 [00:21<03:26,  3.43it/s]  9%|         | 72/780 [00:21<03:26,  3.43it/s]  9%|         | 73/780 [00:21<03:25,  3.44it/s]  9%|         | 74/780 [00:21<03:48,  3.09it/s] 10%|         | 75/780 [00:22<03:40,  3.19it/s] 10%|         | 76/780 [00:22<03:35,  3.26it/s] 10%|         | 77/780 [00:22<03:31,  3.32it/s] 10%|         | 78/780 [00:23<03:29,  3.35it/s] 10%|         | 79/780 [00:23<03:27,  3.38it/s] 10%|         | 80/780 [00:23<03:25,  3.40it/s] 10%|         | 81/780 [00:24<03:24,  3.41it/s] 11%|         | 82/780 [00:24<03:23,  3.42it/s] 11%|         | 83/780 [00:24<03:23,  3.43it/s] 11%|         | 84/780 [00:24<03:41,  3.14it/s] 11%|         | 85/780 [00:25<03:35,  3.23it/s] 11%|         | 86/780 [00:25<03:30,  3.29it/s] 11%|         | 87/780 [00:25<03:27,  3.33it/s] 11%|        | 88/780 [00:26<03:25,  3.37it/s] 11%|        | 89/780 [00:26<03:23,  3.39it/s] 12%|        | 90/780 [00:26<03:22,  3.41it/s] 12%|        | 91/780 [00:27<03:21,  3.41it/s] 12%|        | 92/780 [00:27<03:20,  3.42it/s] 12%|        | 93/780 [00:27<03:20,  3.43it/s] 12%|        | 94/780 [00:27<03:19,  3.44it/s] 12%|        | 95/780 [00:28<03:32,  3.22it/s] 12%|        | 96/780 [00:28<03:28,  3.28it/s] 12%|        | 97/780 [00:28<03:25,  3.33it/s] 13%|        | 98/780 [00:29<03:22,  3.36it/s] 13%|        | 99/780 [00:29<03:20,  3.39it/s] 13%|        | 100/780 [00:29<03:27,  3.28it/s] 13%|        | 101/780 [00:30<03:23,  3.33it/s] 13%|        | 102/780 [00:30<03:21,  3.37it/s] 13%|        | 103/780 [00:30<03:19,  3.39it/s] 13%|        | 104/780 [00:30<03:18,  3.41it/s] 13%|        | 105/780 [00:31<03:17,  3.42it/s] 14%|        | 106/780 [00:31<03:16,  3.43it/s] 14%|        | 107/780 [00:31<03:15,  3.43it/s] 14%|        | 108/780 [00:32<03:15,  3.44it/s] 14%|        | 109/780 [00:32<03:14,  3.44it/s] 14%|        | 110/780 [00:32<03:28,  3.22it/s] 14%|        | 111/780 [00:32<03:23,  3.28it/s] 14%|        | 112/780 [00:33<03:20,  3.33it/s] 14%|        | 113/780 [00:33<03:18,  3.36it/s] 15%|        | 114/780 [00:33<03:16,  3.39it/s] 15%|        | 115/780 [00:34<03:15,  3.40it/s] 15%|        | 116/780 [00:34<03:14,  3.41it/s] 15%|        | 117/780 [00:34<03:23,  3.25it/s] 15%|        | 118/780 [00:35<03:20,  3.31it/s] 15%|        | 119/780 [00:35<03:17,  3.35it/s] 15%|        | 120/780 [00:35<03:15,  3.38it/s] 16%|        | 121/780 [00:35<03:13,  3.40it/s] 16%|        | 122/780 [00:36<03:12,  3.41it/s] 16%|        | 123/780 [00:36<03:11,  3.42it/s] 16%|        | 124/780 [00:36<03:11,  3.43it/s] 16%|        | 125/780 [00:37<03:10,  3.43it/s] 16%|        | 126/780 [00:37<03:10,  3.43it/s] 16%|        | 127/780 [00:37<03:16,  3.32it/s] 16%|        | 128/780 [00:38<03:14,  3.35it/s] 17%|        | 129/780 [00:38<03:12,  3.37it/s] 17%|        | 130/780 [00:38<03:11,  3.39it/s] 17%|        | 131/780 [00:38<03:10,  3.40it/s] 17%|        | 132/780 [00:39<03:09,  3.41it/s] 17%|        | 133/780 [00:39<03:09,  3.42it/s] 17%|        | 134/780 [00:39<03:08,  3.43it/s] 17%|        | 135/780 [00:40<03:08,  3.43it/s] 17%|        | 136/780 [00:40<03:07,  3.43it/s] 18%|        | 137/780 [00:40<03:07,  3.43it/s] 18%|        | 138/780 [00:40<03:07,  3.43it/s] 18%|        | 139/780 [00:41<03:06,  3.44it/s] 18%|        | 140/780 [00:41<03:06,  3.43it/s] 18%|        | 141/780 [00:41<03:05,  3.44it/s] 18%|        | 142/780 [00:42<03:05,  3.44it/s] 18%|        | 143/780 [00:42<03:05,  3.44it/s] 18%|        | 144/780 [00:42<03:05,  3.44it/s] 19%|        | 145/780 [00:42<03:11,  3.32it/s] 19%|        | 146/780 [00:43<03:09,  3.35it/s] 19%|        | 147/780 [00:43<03:07,  3.38it/s] 19%|        | 148/780 [00:43<03:16,  3.22it/s] 19%|        | 149/780 [00:44<03:12,  3.28it/s] 19%|        | 150/780 [00:44<03:09,  3.33it/s] 19%|        | 151/780 [00:44<03:07,  3.35it/s] 19%|        | 152/780 [00:45<03:06,  3.38it/s] 20%|        | 153/780 [00:45<03:04,  3.39it/s] 20%|        | 154/780 [00:45<03:03,  3.41it/s] 20%|        | 155/780 [00:46<04:39,  2.24it/s] 20%|        | 156/780 [00:46<04:09,  2.50it/s][INFO|trainer.py:2140] 2023-08-29 10:46:03,519 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:46:03,519 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 10:46:03,519 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.96it/s][A
  3%|         | 12/435 [00:00<00:08, 48.92it/s][A
  4%|         | 17/435 [00:00<00:08, 47.15it/s][A
  5%|         | 22/435 [00:00<00:08, 46.48it/s][A
  6%|         | 27/435 [00:00<00:08, 45.59it/s][A
  7%|         | 32/435 [00:00<00:08, 44.98it/s][A
  9%|         | 37/435 [00:00<00:08, 44.74it/s][A
 10%|         | 42/435 [00:00<00:09, 40.93it/s][A
 11%|         | 47/435 [00:01<00:09, 42.23it/s][A
 12%|        | 52/435 [00:01<00:08, 42.97it/s][A
 13%|        | 57/435 [00:01<00:08, 43.63it/s][A
 14%|        | 62/435 [00:01<00:08, 44.00it/s][A
 15%|        | 67/435 [00:01<00:08, 44.36it/s][A
 17%|        | 72/435 [00:01<00:08, 44.52it/s][A
 18%|        | 77/435 [00:01<00:08, 44.64it/s][A
 19%|        | 82/435 [00:01<00:07, 44.25it/s][A
 20%|        | 87/435 [00:01<00:07, 44.26it/s][A
 21%|        | 92/435 [00:02<00:07, 44.39it/s][A
 22%|       | 97/435 [00:02<00:07, 44.55it/s][A
 23%|       | 102/435 [00:02<00:07, 44.73it/s][A
 25%|       | 107/435 [00:02<00:07, 44.68it/s][A
 26%|       | 112/435 [00:02<00:07, 44.82it/s][A
 27%|       | 117/435 [00:02<00:07, 44.90it/s][A
 28%|       | 122/435 [00:02<00:06, 44.78it/s][A
 29%|       | 127/435 [00:02<00:06, 44.60it/s][A
 30%|       | 132/435 [00:02<00:06, 44.57it/s][A
 31%|      | 137/435 [00:03<00:06, 44.44it/s][A
 33%|      | 142/435 [00:03<00:06, 44.65it/s][A
 34%|      | 147/435 [00:03<00:06, 44.68it/s][A
 35%|      | 152/435 [00:03<00:06, 44.75it/s][A
 36%|      | 157/435 [00:03<00:06, 44.70it/s][A
 37%|      | 162/435 [00:03<00:06, 44.70it/s][A
 38%|      | 167/435 [00:03<00:05, 44.73it/s][A
 40%|      | 172/435 [00:03<00:05, 44.61it/s][A
 41%|      | 177/435 [00:03<00:05, 44.46it/s][A
 42%|     | 182/435 [00:04<00:05, 44.52it/s][A
 43%|     | 187/435 [00:04<00:05, 44.57it/s][A
 44%|     | 192/435 [00:04<00:05, 44.68it/s][A
 45%|     | 197/435 [00:04<00:05, 44.74it/s][A
 46%|     | 202/435 [00:04<00:05, 44.85it/s][A
 48%|     | 207/435 [00:04<00:05, 44.76it/s][A
 49%|     | 212/435 [00:04<00:04, 44.73it/s][A
 50%|     | 217/435 [00:04<00:04, 44.66it/s][A
 51%|     | 222/435 [00:04<00:04, 44.64it/s][A
 52%|    | 227/435 [00:05<00:04, 44.58it/s][A
 53%|    | 232/435 [00:05<00:04, 44.49it/s][A
 54%|    | 237/435 [00:05<00:04, 44.60it/s][A
 56%|    | 242/435 [00:05<00:04, 44.57it/s][A
 57%|    | 247/435 [00:05<00:04, 44.69it/s][A
 58%|    | 252/435 [00:05<00:04, 44.70it/s][A
 59%|    | 257/435 [00:05<00:03, 44.67it/s][A
 60%|    | 262/435 [00:05<00:03, 44.76it/s][A
 61%|   | 267/435 [00:06<00:03, 44.73it/s][A
 63%|   | 272/435 [00:06<00:03, 42.05it/s][A
 64%|   | 277/435 [00:06<00:03, 42.91it/s][A
 65%|   | 282/435 [00:06<00:03, 43.45it/s][A
 66%|   | 287/435 [00:06<00:03, 43.84it/s][A
 67%|   | 292/435 [00:06<00:03, 44.17it/s][A
 68%|   | 297/435 [00:06<00:03, 44.36it/s][A
 69%|   | 302/435 [00:06<00:02, 44.39it/s][A
 71%|   | 307/435 [00:06<00:02, 44.48it/s][A
 72%|  | 312/435 [00:07<00:02, 44.31it/s][A
 73%|  | 317/435 [00:07<00:02, 44.26it/s][A
 74%|  | 322/435 [00:07<00:02, 44.43it/s][A
 75%|  | 327/435 [00:07<00:02, 44.47it/s][A
 76%|  | 332/435 [00:07<00:02, 44.56it/s][A
 77%|  | 337/435 [00:07<00:02, 44.68it/s][A
 79%|  | 342/435 [00:07<00:02, 44.72it/s][A
 80%|  | 347/435 [00:07<00:01, 44.77it/s][A
 81%|  | 352/435 [00:07<00:01, 44.66it/s][A
 82%| | 357/435 [00:08<00:01, 44.46it/s][A
 83%| | 362/435 [00:08<00:01, 38.32it/s][A
 84%| | 367/435 [00:08<00:01, 40.09it/s][A
 86%| | 372/435 [00:08<00:01, 41.50it/s][A
 87%| | 377/435 [00:08<00:01, 42.50it/s][A
 88%| | 382/435 [00:08<00:01, 43.22it/s][A
 89%| | 387/435 [00:08<00:01, 43.81it/s][A
 90%| | 392/435 [00:08<00:00, 44.11it/s][A
 91%|| 397/435 [00:08<00:00, 44.31it/s][A
 92%|| 402/435 [00:09<00:00, 44.08it/s][A
 94%|| 407/435 [00:09<00:00, 44.09it/s][A
 95%|| 412/435 [00:09<00:00, 44.22it/s][A
 96%|| 417/435 [00:09<00:00, 44.45it/s][A
 97%|| 422/435 [00:09<00:00, 44.59it/s][A
 98%|| 427/435 [00:09<00:00, 44.67it/s][A
 99%|| 432/435 [00:09<00:00, 44.84it/s][A                                                 
                                                 [A 20%|        | 156/780 [00:56<04:09,  2.50it/s]
100%|| 435/435 [00:09<00:00, 44.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:46:13,828 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 10:46:14,166 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:46:18,021 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:46:18,179 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:46:18,248 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:10<1:17:24,  7.45s/it] 20%|        | 158/780 [01:11<55:06,  5.32s/it]   20%|        | 159/780 [01:11<39:26,  3.81s/it] 21%|        | 160/780 [01:11<28:28,  2.76s/it] 21%|        | 161/780 [01:11<20:48,  2.02s/it] 21%|        | 162/780 [01:12<15:27,  1.50s/it] 21%|        | 163/780 [01:12<11:42,  1.14s/it] 21%|        | 164/780 [01:12<09:05,  1.13it/s] 21%|        | 165/780 [01:13<07:14,  1.42it/s] 21%|       | 166/780 [01:13<05:57,  1.72it/s] 21%|       | 167/780 [01:13<05:02,  2.02it/s] 22%|       | 168/780 [01:13<04:24,  2.31it/s] 22%|       | 169/780 [01:14<04:03,  2.51it/s] 22%|       | 170/780 [01:14<03:43,  2.73it/s] 22%|       | 171/780 [01:14<03:29,  2.91it/s] 22%|       | 172/780 [01:15<03:18,  3.06it/s] 22%|       | 173/780 [01:15<03:11,  3.16it/s] 22%|       | 174/780 [01:15<03:06,  3.24it/s] 22%|       | 175/780 [01:15<03:03,  3.30it/s] 23%|       | 176/780 [01:16<03:00,  3.35it/s] 23%|       | 177/780 [01:16<02:58,  3.37it/s] 23%|       | 178/780 [01:16<02:57,  3.40it/s] 23%|       | 179/780 [01:17<02:56,  3.40it/s] 23%|       | 180/780 [01:17<03:01,  3.31it/s] 23%|       | 181/780 [01:17<02:58,  3.35it/s] 23%|       | 182/780 [01:18<02:57,  3.38it/s] 23%|       | 183/780 [01:18<02:55,  3.40it/s] 24%|       | 184/780 [01:18<02:54,  3.41it/s] 24%|       | 185/780 [01:18<02:53,  3.42it/s] 24%|       | 186/780 [01:19<02:53,  3.43it/s] 24%|       | 187/780 [01:19<02:52,  3.43it/s] 24%|       | 188/780 [01:19<02:52,  3.44it/s] 24%|       | 189/780 [01:20<02:51,  3.44it/s] 24%|       | 190/780 [01:20<02:51,  3.44it/s] 24%|       | 191/780 [01:20<03:05,  3.18it/s] 25%|       | 192/780 [01:21<03:00,  3.26it/s] 25%|       | 193/780 [01:21<02:57,  3.31it/s] 25%|       | 194/780 [01:21<02:54,  3.35it/s] 25%|       | 195/780 [01:21<02:53,  3.38it/s] 25%|       | 196/780 [01:22<02:51,  3.40it/s] 25%|       | 197/780 [01:22<02:50,  3.41it/s] 25%|       | 198/780 [01:22<02:50,  3.42it/s] 26%|       | 199/780 [01:23<02:49,  3.43it/s] 26%|       | 200/780 [01:23<02:48,  3.43it/s] 26%|       | 201/780 [01:23<02:48,  3.44it/s] 26%|       | 202/780 [01:23<02:56,  3.28it/s] 26%|       | 203/780 [01:24<02:53,  3.33it/s] 26%|       | 204/780 [01:24<02:51,  3.36it/s] 26%|       | 205/780 [01:24<02:49,  3.39it/s] 26%|       | 206/780 [01:25<02:48,  3.40it/s] 27%|       | 207/780 [01:25<02:47,  3.41it/s] 27%|       | 208/780 [01:25<02:47,  3.42it/s] 27%|       | 209/780 [01:26<02:51,  3.33it/s] 27%|       | 210/780 [01:26<02:49,  3.36it/s] 27%|       | 211/780 [01:26<02:48,  3.39it/s] 27%|       | 212/780 [01:26<02:46,  3.40it/s] 27%|       | 213/780 [01:27<02:46,  3.41it/s] 27%|       | 214/780 [01:27<02:45,  3.42it/s] 28%|       | 215/780 [01:27<02:44,  3.43it/s] 28%|       | 216/780 [01:28<02:44,  3.43it/s] 28%|       | 217/780 [01:28<02:43,  3.43it/s] 28%|       | 218/780 [01:28<02:43,  3.44it/s] 28%|       | 219/780 [01:29<02:51,  3.27it/s] 28%|       | 220/780 [01:29<02:48,  3.32it/s] 28%|       | 221/780 [01:29<02:46,  3.36it/s] 28%|       | 222/780 [01:29<02:44,  3.38it/s] 29%|       | 223/780 [01:30<02:43,  3.40it/s] 29%|       | 224/780 [01:30<02:42,  3.41it/s] 29%|       | 225/780 [01:30<02:42,  3.42it/s] 29%|       | 226/780 [01:31<02:41,  3.43it/s] 29%|       | 227/780 [01:31<02:41,  3.43it/s] 29%|       | 228/780 [01:31<02:40,  3.43it/s] 29%|       | 229/780 [01:31<02:40,  3.43it/s] 29%|       | 230/780 [01:32<02:52,  3.19it/s] 30%|       | 231/780 [01:32<02:48,  3.26it/s] 30%|       | 232/780 [01:32<02:45,  3.31it/s] 30%|       | 233/780 [01:33<02:43,  3.35it/s] 30%|       | 234/780 [01:33<02:41,  3.38it/s] 30%|       | 235/780 [01:33<02:40,  3.40it/s] 30%|       | 236/780 [01:34<02:39,  3.41it/s] 30%|       | 237/780 [01:34<02:38,  3.42it/s] 31%|       | 238/780 [01:34<02:38,  3.43it/s] 31%|       | 239/780 [01:34<02:37,  3.43it/s] 31%|       | 240/780 [01:35<02:37,  3.43it/s] 31%|       | 241/780 [01:35<02:40,  3.37it/s] 31%|       | 242/780 [01:35<02:38,  3.39it/s] 31%|       | 243/780 [01:36<02:37,  3.40it/s] 31%|      | 244/780 [01:36<02:36,  3.42it/s] 31%|      | 245/780 [01:36<02:36,  3.42it/s] 32%|      | 246/780 [01:36<02:35,  3.43it/s] 32%|      | 247/780 [01:37<02:35,  3.43it/s] 32%|      | 248/780 [01:37<02:34,  3.43it/s] 32%|      | 249/780 [01:37<02:34,  3.44it/s] 32%|      | 250/780 [01:38<02:34,  3.44it/s] 32%|      | 251/780 [01:38<02:33,  3.44it/s] 32%|      | 252/780 [01:38<02:33,  3.44it/s] 32%|      | 253/780 [01:38<02:33,  3.44it/s] 33%|      | 254/780 [01:39<02:39,  3.30it/s] 33%|      | 255/780 [01:39<02:37,  3.34it/s] 33%|      | 256/780 [01:39<02:35,  3.37it/s] 33%|      | 257/780 [01:40<02:34,  3.38it/s] 33%|      | 258/780 [01:40<02:33,  3.40it/s] 33%|      | 259/780 [01:40<02:32,  3.41it/s] 33%|      | 260/780 [01:41<02:32,  3.42it/s] 33%|      | 261/780 [01:41<02:31,  3.42it/s] 34%|      | 262/780 [01:41<02:31,  3.43it/s] 34%|      | 263/780 [01:41<02:30,  3.43it/s] 34%|      | 264/780 [01:42<02:30,  3.43it/s] 34%|      | 265/780 [01:42<02:30,  3.43it/s] 34%|      | 266/780 [01:42<02:30,  3.43it/s] 34%|      | 267/780 [01:43<02:29,  3.43it/s] 34%|      | 268/780 [01:43<02:29,  3.43it/s] 34%|      | 269/780 [01:43<02:28,  3.43it/s] 35%|      | 270/780 [01:44<02:36,  3.27it/s] 35%|      | 271/780 [01:44<02:44,  3.09it/s] 35%|      | 272/780 [01:44<02:39,  3.19it/s] 35%|      | 273/780 [01:44<02:35,  3.26it/s] 35%|      | 274/780 [01:45<02:33,  3.30it/s] 35%|      | 275/780 [01:45<02:31,  3.34it/s] 35%|      | 276/780 [01:45<02:29,  3.37it/s] 36%|      | 277/780 [01:46<03:36,  2.32it/s] 36%|      | 278/780 [01:46<03:15,  2.57it/s] 36%|      | 279/780 [01:47<03:00,  2.78it/s] 36%|      | 280/780 [01:47<02:49,  2.95it/s] 36%|      | 281/780 [01:47<02:42,  3.08it/s] 36%|      | 282/780 [01:48<02:36,  3.18it/s] 36%|      | 283/780 [01:48<02:32,  3.25it/s] 36%|      | 284/780 [01:48<02:30,  3.30it/s] 37%|      | 285/780 [01:48<02:28,  3.34it/s] 37%|      | 286/780 [01:49<02:26,  3.37it/s] 37%|      | 287/780 [01:49<02:32,  3.24it/s] 37%|      | 288/780 [01:49<02:29,  3.29it/s] 37%|      | 289/780 [01:50<02:27,  3.33it/s] 37%|      | 290/780 [01:50<02:25,  3.36it/s] 37%|      | 291/780 [01:50<02:24,  3.38it/s] 37%|      | 292/780 [01:51<02:23,  3.40it/s] 38%|      | 293/780 [01:51<02:22,  3.41it/s] 38%|      | 294/780 [01:51<02:22,  3.41it/s] 38%|      | 295/780 [01:51<02:21,  3.42it/s] 38%|      | 296/780 [01:52<02:21,  3.42it/s] 38%|      | 297/780 [01:52<02:21,  3.42it/s] 38%|      | 298/780 [01:52<02:20,  3.43it/s] 38%|      | 299/780 [01:53<02:20,  3.43it/s] 38%|      | 300/780 [01:53<02:19,  3.43it/s] 39%|      | 301/780 [01:53<02:19,  3.43it/s] 39%|      | 302/780 [01:53<02:19,  3.44it/s] 39%|      | 303/780 [01:54<02:18,  3.43it/s] 39%|      | 304/780 [01:54<02:24,  3.29it/s] 39%|      | 305/780 [01:54<02:22,  3.33it/s] 39%|      | 306/780 [01:55<02:21,  3.35it/s] 39%|      | 307/780 [01:55<02:19,  3.38it/s] 39%|      | 308/780 [01:55<02:19,  3.40it/s] 40%|      | 309/780 [01:56<02:18,  3.40it/s] 40%|      | 310/780 [01:56<02:17,  3.41it/s] 40%|      | 311/780 [01:56<02:17,  3.42it/s] 40%|      | 312/780 [01:56<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 10:47:13,631 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:47:13,631 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 10:47:13,632 >>   Batch size = 8
{'eval_loss': 1.0221641063690186, 'eval_runtime': 9.8743, 'eval_samples_per_second': 352.127, 'eval_steps_per_second': 44.054, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.93it/s][A
  3%|         | 12/435 [00:00<00:08, 48.73it/s][A
  4%|         | 17/435 [00:00<00:08, 47.12it/s][A
  5%|         | 22/435 [00:00<00:08, 46.21it/s][A
  6%|         | 27/435 [00:00<00:08, 45.50it/s][A
  7%|         | 32/435 [00:00<00:08, 45.00it/s][A
  9%|         | 37/435 [00:00<00:08, 44.69it/s][A
 10%|         | 42/435 [00:00<00:08, 44.47it/s][A
 11%|         | 47/435 [00:01<00:08, 44.60it/s][A
 12%|        | 52/435 [00:01<00:08, 44.77it/s][A
 13%|        | 57/435 [00:01<00:08, 44.81it/s][A
 14%|        | 62/435 [00:01<00:08, 44.97it/s][A
 15%|        | 67/435 [00:01<00:08, 44.83it/s][A
 17%|        | 72/435 [00:01<00:08, 44.73it/s][A
 18%|        | 77/435 [00:01<00:08, 44.58it/s][A
 19%|        | 82/435 [00:01<00:07, 44.39it/s][A
 20%|        | 87/435 [00:01<00:07, 44.42it/s][A
 21%|        | 92/435 [00:02<00:07, 44.46it/s][A
 22%|       | 97/435 [00:02<00:07, 44.61it/s][A
 23%|       | 102/435 [00:02<00:07, 44.77it/s][A
 25%|       | 107/435 [00:02<00:07, 44.86it/s][A
 26%|       | 112/435 [00:02<00:08, 40.23it/s][A
 27%|       | 117/435 [00:02<00:07, 41.26it/s][A
 28%|       | 122/435 [00:02<00:07, 42.62it/s][A
 29%|       | 127/435 [00:02<00:07, 43.26it/s][A
 30%|       | 132/435 [00:02<00:06, 43.78it/s][A
 31%|      | 137/435 [00:03<00:06, 44.18it/s][A
 33%|      | 142/435 [00:03<00:06, 44.26it/s][A
 34%|      | 147/435 [00:03<00:06, 44.49it/s][A
 35%|      | 152/435 [00:03<00:06, 44.16it/s][A
 36%|      | 157/435 [00:03<00:06, 44.17it/s][A
 37%|      | 162/435 [00:03<00:06, 44.41it/s][A
 38%|      | 167/435 [00:03<00:06, 44.50it/s][A
 40%|      | 172/435 [00:03<00:05, 44.63it/s][A
 41%|      | 177/435 [00:03<00:05, 44.75it/s][A
 42%|     | 182/435 [00:04<00:05, 44.84it/s][A
 43%|     | 187/435 [00:04<00:05, 44.88it/s][A
 44%|     | 192/435 [00:04<00:05, 44.66it/s][A
 45%|     | 197/435 [00:04<00:05, 44.29it/s][A
 46%|     | 202/435 [00:04<00:05, 44.34it/s][A
 48%|     | 207/435 [00:04<00:05, 44.40it/s][A
 49%|     | 212/435 [00:04<00:05, 44.56it/s][A
 50%|     | 217/435 [00:04<00:04, 44.62it/s][A
 51%|     | 222/435 [00:04<00:04, 44.70it/s][A
 52%|    | 227/435 [00:05<00:04, 44.83it/s][A
 53%|    | 232/435 [00:05<00:04, 44.85it/s][A
 54%|    | 237/435 [00:05<00:04, 44.63it/s][A
 56%|    | 242/435 [00:05<00:04, 44.35it/s][A
 57%|    | 247/435 [00:05<00:04, 44.29it/s][A
 58%|    | 252/435 [00:05<00:04, 44.41it/s][A
 59%|    | 257/435 [00:05<00:03, 44.59it/s][A
 60%|    | 262/435 [00:05<00:03, 44.71it/s][A
 61%|   | 267/435 [00:05<00:03, 44.81it/s][A
 63%|   | 272/435 [00:06<00:03, 44.80it/s][A
 64%|   | 277/435 [00:06<00:03, 44.81it/s][A
 65%|   | 282/435 [00:06<00:03, 44.58it/s][A
 66%|   | 287/435 [00:06<00:03, 44.47it/s][A
 67%|   | 292/435 [00:06<00:03, 44.36it/s][A
 68%|   | 297/435 [00:06<00:03, 44.39it/s][A
 69%|   | 302/435 [00:06<00:02, 44.57it/s][A
 71%|   | 307/435 [00:06<00:02, 44.68it/s][A
 72%|  | 312/435 [00:07<00:02, 44.85it/s][A
 73%|  | 317/435 [00:07<00:02, 44.91it/s][A
 74%|  | 322/435 [00:07<00:02, 44.92it/s][A
 75%|  | 327/435 [00:07<00:02, 44.72it/s][A
 76%|  | 332/435 [00:07<00:02, 44.45it/s][A
 77%|  | 337/435 [00:07<00:02, 44.50it/s][A
 79%|  | 342/435 [00:07<00:02, 43.52it/s][A
 80%|  | 347/435 [00:07<00:02, 44.00it/s][A
 81%|  | 352/435 [00:07<00:01, 44.15it/s][A
 82%| | 357/435 [00:08<00:01, 44.47it/s][A
 83%| | 362/435 [00:08<00:01, 44.66it/s][A
 84%| | 367/435 [00:08<00:01, 44.50it/s][A
 86%| | 372/435 [00:08<00:01, 44.54it/s][A
 87%| | 377/435 [00:08<00:01, 44.51it/s][A
 88%| | 382/435 [00:08<00:01, 44.23it/s][A
 89%| | 387/435 [00:08<00:01, 44.39it/s][A
 90%| | 392/435 [00:08<00:00, 44.54it/s][A
 91%|| 397/435 [00:08<00:00, 44.63it/s][A
 92%|| 402/435 [00:09<00:00, 44.75it/s][A
 94%|| 407/435 [00:09<00:00, 44.71it/s][A
 95%|| 412/435 [00:09<00:00, 44.75it/s][A
 96%|| 417/435 [00:09<00:00, 44.69it/s][A
 97%|| 422/435 [00:09<00:00, 44.47it/s][A
 98%|| 427/435 [00:09<00:00, 44.48it/s][A
 99%|| 432/435 [00:09<00:00, 40.45it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 40.45it/s][A 40%|      | 312/780 [02:06<02:16,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:47:23,798 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 10:47:24,032 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:47:27,834 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:47:28,014 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:47:28,095 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:19<53:54,  6.93s/it] 40%|      | 314/780 [02:19<38:26,  4.95s/it] 40%|      | 315/780 [02:19<27:31,  3.55s/it] 41%|      | 316/780 [02:20<19:54,  2.57s/it] 41%|      | 317/780 [02:20<14:35,  1.89s/it] 41%|      | 318/780 [02:20<10:52,  1.41s/it] 41%|      | 319/780 [02:21<08:16,  1.08s/it] 41%|      | 320/780 [02:21<06:27,  1.19it/s] 41%|      | 321/780 [02:21<05:10,  1.48it/s] 41%|     | 322/780 [02:21<04:17,  1.78it/s] 41%|     | 323/780 [02:22<03:40,  2.07it/s] 42%|     | 324/780 [02:22<03:14,  2.35it/s] 42%|     | 325/780 [02:22<03:05,  2.45it/s] 42%|     | 326/780 [02:23<02:49,  2.67it/s] 42%|     | 327/780 [02:23<02:38,  2.86it/s] 42%|     | 328/780 [02:23<02:30,  3.00it/s] 42%|     | 329/780 [02:24<02:25,  3.11it/s] 42%|     | 330/780 [02:24<02:21,  3.19it/s] 42%|     | 331/780 [02:24<02:18,  3.25it/s] 43%|     | 332/780 [02:24<02:16,  3.29it/s] 43%|     | 333/780 [02:25<02:14,  3.32it/s] 43%|     | 334/780 [02:25<02:13,  3.35it/s] 43%|     | 335/780 [02:25<02:21,  3.15it/s] 43%|     | 336/780 [02:26<02:17,  3.22it/s] 43%|     | 337/780 [02:26<02:15,  3.27it/s] 43%|     | 338/780 [02:26<02:13,  3.31it/s] 43%|     | 339/780 [02:27<02:12,  3.34it/s] 44%|     | 340/780 [02:27<02:11,  3.36it/s] 44%|     | 341/780 [02:27<02:10,  3.37it/s] 44%|     | 342/780 [02:27<02:09,  3.38it/s] 44%|     | 343/780 [02:28<02:09,  3.39it/s] 44%|     | 344/780 [02:28<02:08,  3.39it/s] 44%|     | 345/780 [02:28<02:15,  3.20it/s] 44%|     | 346/780 [02:29<02:13,  3.26it/s] 44%|     | 347/780 [02:29<02:11,  3.30it/s] 45%|     | 348/780 [02:29<02:09,  3.33it/s] 45%|     | 349/780 [02:30<02:08,  3.35it/s] 45%|     | 350/780 [02:30<02:07,  3.36it/s] 45%|     | 351/780 [02:30<02:07,  3.37it/s] 45%|     | 352/780 [02:30<02:06,  3.38it/s] 45%|     | 353/780 [02:31<02:05,  3.39it/s] 45%|     | 354/780 [02:31<02:05,  3.39it/s] 46%|     | 355/780 [02:31<02:10,  3.25it/s] 46%|     | 356/780 [02:32<02:08,  3.30it/s] 46%|     | 357/780 [02:32<02:07,  3.33it/s] 46%|     | 358/780 [02:32<02:06,  3.35it/s] 46%|     | 359/780 [02:33<02:05,  3.36it/s] 46%|     | 360/780 [02:33<02:04,  3.37it/s] 46%|     | 361/780 [02:33<02:03,  3.38it/s] 46%|     | 362/780 [02:33<02:03,  3.39it/s] 47%|     | 363/780 [02:34<02:03,  3.39it/s] 47%|     | 364/780 [02:34<02:02,  3.39it/s] 47%|     | 365/780 [02:34<02:02,  3.39it/s] 47%|     | 366/780 [02:35<02:07,  3.25it/s] 47%|     | 367/780 [02:35<02:05,  3.29it/s] 47%|     | 368/780 [02:35<02:04,  3.32it/s] 47%|     | 369/780 [02:36<02:02,  3.35it/s] 47%|     | 370/780 [02:36<02:02,  3.36it/s] 48%|     | 371/780 [02:36<02:01,  3.37it/s] 48%|     | 372/780 [02:36<02:00,  3.38it/s] 48%|     | 373/780 [02:37<02:00,  3.39it/s] 48%|     | 374/780 [02:37<01:59,  3.39it/s] 48%|     | 375/780 [02:37<01:59,  3.39it/s] 48%|     | 376/780 [02:38<01:59,  3.39it/s] 48%|     | 377/780 [02:38<01:58,  3.41it/s] 48%|     | 378/780 [02:38<01:57,  3.41it/s] 49%|     | 379/780 [02:39<01:57,  3.42it/s] 49%|     | 380/780 [02:39<01:56,  3.43it/s] 49%|     | 381/780 [02:39<01:56,  3.43it/s] 49%|     | 382/780 [02:39<01:55,  3.44it/s] 49%|     | 383/780 [02:40<01:55,  3.44it/s] 49%|     | 384/780 [02:40<01:54,  3.44it/s] 49%|     | 385/780 [02:40<01:59,  3.30it/s] 49%|     | 386/780 [02:41<01:57,  3.34it/s] 50%|     | 387/780 [02:41<01:56,  3.37it/s] 50%|     | 388/780 [02:41<01:55,  3.39it/s] 50%|     | 389/780 [02:41<01:54,  3.41it/s] 50%|     | 390/780 [02:42<01:54,  3.42it/s] 50%|     | 391/780 [02:42<01:53,  3.43it/s] 50%|     | 392/780 [02:42<01:52,  3.44it/s] 50%|     | 393/780 [02:43<01:52,  3.44it/s] 51%|     | 394/780 [02:43<01:52,  3.44it/s] 51%|     | 395/780 [02:43<01:51,  3.44it/s] 51%|     | 396/780 [02:44<02:00,  3.18it/s] 51%|     | 397/780 [02:44<01:57,  3.26it/s] 51%|     | 398/780 [02:44<01:55,  3.31it/s] 51%|     | 399/780 [02:44<01:53,  3.35it/s] 51%|    | 400/780 [02:45<01:52,  3.38it/s] 51%|    | 401/780 [02:45<01:51,  3.40it/s] 52%|    | 402/780 [02:45<01:50,  3.41it/s] 52%|    | 403/780 [02:46<01:50,  3.42it/s] 52%|    | 404/780 [02:46<02:31,  2.48it/s] 52%|    | 405/780 [02:47<02:23,  2.61it/s] 52%|    | 406/780 [02:47<02:13,  2.81it/s] 52%|    | 407/780 [02:47<02:05,  2.98it/s] 52%|    | 408/780 [02:47<01:59,  3.10it/s] 52%|    | 409/780 [02:48<01:55,  3.20it/s] 53%|    | 410/780 [02:48<01:53,  3.27it/s] 53%|    | 411/780 [02:48<01:51,  3.32it/s] 53%|    | 412/780 [02:49<01:49,  3.35it/s] 53%|    | 413/780 [02:49<01:48,  3.38it/s] 53%|    | 414/780 [02:49<01:47,  3.40it/s] 53%|    | 415/780 [02:50<01:47,  3.41it/s] 53%|    | 416/780 [02:50<01:51,  3.26it/s] 53%|    | 417/780 [02:50<01:49,  3.31it/s] 54%|    | 418/780 [02:50<01:48,  3.35it/s] 54%|    | 419/780 [02:51<01:46,  3.37it/s] 54%|    | 420/780 [02:51<01:46,  3.39it/s] 54%|    | 421/780 [02:51<01:45,  3.40it/s] 54%|    | 422/780 [02:52<01:44,  3.41it/s] 54%|    | 423/780 [02:52<01:44,  3.42it/s] 54%|    | 424/780 [02:52<01:43,  3.42it/s] 54%|    | 425/780 [02:52<01:43,  3.43it/s] 55%|    | 426/780 [02:53<01:43,  3.43it/s] 55%|    | 427/780 [02:53<01:45,  3.34it/s] 55%|    | 428/780 [02:53<01:44,  3.37it/s] 55%|    | 429/780 [02:54<01:43,  3.39it/s] 55%|    | 430/780 [02:54<01:42,  3.40it/s] 55%|    | 431/780 [02:54<01:42,  3.41it/s] 55%|    | 432/780 [02:55<01:41,  3.42it/s] 56%|    | 433/780 [02:55<01:41,  3.43it/s] 56%|    | 434/780 [02:55<01:40,  3.43it/s] 56%|    | 435/780 [02:55<01:40,  3.43it/s] 56%|    | 436/780 [02:56<01:40,  3.43it/s] 56%|    | 437/780 [02:56<01:39,  3.44it/s] 56%|    | 438/780 [02:56<01:42,  3.33it/s] 56%|    | 439/780 [02:57<01:41,  3.36it/s] 56%|    | 440/780 [02:57<01:40,  3.39it/s] 57%|    | 441/780 [02:57<01:39,  3.40it/s] 57%|    | 442/780 [02:57<01:39,  3.41it/s] 57%|    | 443/780 [02:58<01:38,  3.42it/s] 57%|    | 444/780 [02:58<01:38,  3.42it/s] 57%|    | 445/780 [02:58<01:37,  3.43it/s] 57%|    | 446/780 [02:59<01:37,  3.43it/s] 57%|    | 447/780 [02:59<01:37,  3.43it/s] 57%|    | 448/780 [02:59<01:36,  3.43it/s] 58%|    | 449/780 [03:00<01:40,  3.30it/s] 58%|    | 450/780 [03:00<01:38,  3.34it/s] 58%|    | 451/780 [03:00<01:37,  3.37it/s] 58%|    | 452/780 [03:00<01:36,  3.39it/s] 58%|    | 453/780 [03:01<01:36,  3.41it/s] 58%|    | 454/780 [03:01<01:35,  3.41it/s] 58%|    | 455/780 [03:01<01:34,  3.42it/s] 58%|    | 456/780 [03:02<01:34,  3.42it/s] 59%|    | 457/780 [03:02<01:34,  3.42it/s] 59%|    | 458/780 [03:02<01:33,  3.43it/s] 59%|    | 459/780 [03:02<01:33,  3.43it/s] 59%|    | 460/780 [03:03<01:36,  3.31it/s] 59%|    | 461/780 [03:03<01:35,  3.35it/s] 59%|    | 462/780 [03:03<01:34,  3.38it/s] 59%|    | 463/780 [03:04<01:33,  3.40it/s] 59%|    | 464/780 [03:04<01:32,  3.41it/s] 60%|    | 465/780 [03:04<01:32,  3.42it/s] 60%|    | 466/780 [03:05<01:31,  3.42it/s] 60%|    | 467/780 [03:05<01:31,  3.43it/s] 60%|    | 468/780 [03:05<01:30,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 10:48:22,354 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:48:22,354 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 10:48:22,354 >>   Batch size = 8
{'eval_loss': 1.0409157276153564, 'eval_runtime': 9.8585, 'eval_samples_per_second': 352.689, 'eval_steps_per_second': 44.124, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.62it/s][A
  3%|         | 12/435 [00:00<00:08, 48.74it/s][A
  4%|         | 17/435 [00:00<00:08, 47.10it/s][A
  5%|         | 22/435 [00:00<00:09, 44.63it/s][A
  6%|         | 27/435 [00:00<00:09, 44.65it/s][A
  7%|         | 32/435 [00:00<00:09, 44.55it/s][A
  9%|         | 37/435 [00:00<00:08, 44.48it/s][A
 10%|         | 42/435 [00:00<00:08, 44.38it/s][A
 11%|         | 47/435 [00:01<00:08, 44.57it/s][A
 12%|        | 52/435 [00:01<00:08, 44.71it/s][A
 13%|        | 57/435 [00:01<00:08, 44.85it/s][A
 14%|        | 62/435 [00:01<00:08, 44.70it/s][A
 15%|        | 67/435 [00:01<00:08, 44.69it/s][A
 17%|        | 72/435 [00:01<00:08, 44.56it/s][A
 18%|        | 77/435 [00:01<00:08, 44.50it/s][A
 19%|        | 82/435 [00:01<00:07, 44.45it/s][A
 20%|        | 87/435 [00:01<00:07, 44.52it/s][A
 21%|        | 92/435 [00:02<00:07, 44.72it/s][A
 22%|       | 97/435 [00:02<00:07, 44.76it/s][A
 23%|       | 102/435 [00:02<00:07, 44.82it/s][A
 25%|       | 107/435 [00:02<00:07, 44.81it/s][A
 26%|       | 112/435 [00:02<00:07, 44.65it/s][A
 27%|       | 117/435 [00:02<00:07, 44.54it/s][A
 28%|       | 122/435 [00:02<00:07, 44.41it/s][A
 29%|       | 127/435 [00:02<00:06, 44.38it/s][A
 30%|       | 132/435 [00:02<00:06, 44.50it/s][A
 31%|      | 137/435 [00:03<00:06, 44.66it/s][A
 33%|      | 142/435 [00:03<00:06, 44.74it/s][A
 34%|      | 147/435 [00:03<00:06, 44.81it/s][A
 35%|      | 152/435 [00:03<00:06, 44.84it/s][A
 36%|      | 157/435 [00:03<00:06, 44.68it/s][A
 37%|      | 162/435 [00:03<00:06, 44.60it/s][A
 38%|      | 167/435 [00:03<00:06, 44.60it/s][A
 40%|      | 172/435 [00:03<00:05, 44.50it/s][A
 41%|      | 177/435 [00:03<00:05, 44.46it/s][A
 42%|     | 182/435 [00:04<00:05, 44.61it/s][A
 43%|     | 187/435 [00:04<00:05, 44.75it/s][A
 44%|     | 192/435 [00:04<00:05, 44.77it/s][A
 45%|     | 197/435 [00:04<00:05, 44.67it/s][A
 46%|     | 202/435 [00:04<00:05, 44.65it/s][A
 48%|     | 207/435 [00:04<00:05, 44.73it/s][A
 49%|     | 212/435 [00:04<00:04, 44.60it/s][A
 50%|     | 217/435 [00:04<00:04, 44.62it/s][A
 51%|     | 222/435 [00:04<00:04, 44.57it/s][A
 52%|    | 227/435 [00:05<00:04, 44.62it/s][A
 53%|    | 232/435 [00:05<00:04, 44.75it/s][A
 54%|    | 237/435 [00:05<00:04, 44.73it/s][A
 56%|    | 242/435 [00:05<00:04, 44.73it/s][A
 57%|    | 247/435 [00:05<00:04, 42.27it/s][A
 58%|    | 252/435 [00:05<00:04, 43.13it/s][A
 59%|    | 257/435 [00:05<00:04, 43.63it/s][A
 60%|    | 262/435 [00:05<00:03, 43.97it/s][A
 61%|   | 267/435 [00:05<00:03, 44.20it/s][A
 63%|   | 272/435 [00:06<00:03, 44.34it/s][A
 64%|   | 277/435 [00:06<00:03, 44.48it/s][A
 65%|   | 282/435 [00:06<00:03, 44.51it/s][A
 66%|   | 287/435 [00:06<00:03, 44.28it/s][A
 67%|   | 292/435 [00:06<00:03, 44.41it/s][A
 68%|   | 297/435 [00:06<00:03, 44.56it/s][A
 69%|   | 302/435 [00:06<00:02, 44.64it/s][A
 71%|   | 307/435 [00:06<00:02, 44.63it/s][A
 72%|  | 312/435 [00:06<00:02, 44.66it/s][A
 73%|  | 317/435 [00:07<00:02, 44.72it/s][A
 74%|  | 322/435 [00:07<00:02, 44.65it/s][A
 75%|  | 327/435 [00:07<00:02, 44.53it/s][A
 76%|  | 332/435 [00:07<00:02, 44.44it/s][A
 77%|  | 337/435 [00:07<00:02, 44.39it/s][A
 79%|  | 342/435 [00:07<00:02, 44.54it/s][A
 80%|  | 347/435 [00:07<00:01, 44.61it/s][A
 81%|  | 352/435 [00:07<00:01, 44.70it/s][A
 82%| | 357/435 [00:08<00:01, 44.66it/s][A
 83%| | 362/435 [00:08<00:01, 44.67it/s][A
 84%| | 367/435 [00:08<00:01, 44.66it/s][A
 86%| | 372/435 [00:08<00:01, 44.52it/s][A
 87%| | 377/435 [00:08<00:01, 44.44it/s][A
 88%| | 382/435 [00:08<00:01, 43.43it/s][A
 89%| | 387/435 [00:08<00:01, 41.21it/s][A
 90%| | 392/435 [00:08<00:01, 42.67it/s][A
 91%|| 397/435 [00:08<00:00, 43.38it/s][A
 92%|| 402/435 [00:09<00:00, 43.90it/s][A
 94%|| 407/435 [00:09<00:00, 44.21it/s][A
 95%|| 412/435 [00:09<00:00, 44.44it/s][A
 96%|| 417/435 [00:09<00:00, 44.54it/s][A
 97%|| 422/435 [00:09<00:00, 44.46it/s][A
 98%|| 427/435 [00:09<00:00, 44.47it/s][A
 99%|| 432/435 [00:09<00:00, 44.21it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.21it/s][A 60%|    | 468/780 [03:15<01:30,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:48:32,468 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 10:48:32,709 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:48:36,508 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:48:36,706 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:48:36,787 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:33<43:48,  8.45s/it] 60%|    | 470/780 [03:33<31:07,  6.02s/it] 60%|    | 471/780 [03:33<22:10,  4.30s/it] 61%|    | 472/780 [03:34<15:55,  3.10s/it] 61%|    | 473/780 [03:34<11:33,  2.26s/it] 61%|    | 474/780 [03:34<08:30,  1.67s/it] 61%|    | 475/780 [03:34<06:23,  1.26s/it] 61%|    | 476/780 [03:35<04:54,  1.03it/s] 61%|    | 477/780 [03:35<03:52,  1.31it/s] 61%|   | 478/780 [03:35<03:08,  1.60it/s] 61%|   | 479/780 [03:36<02:38,  1.90it/s] 62%|   | 480/780 [03:36<02:22,  2.11it/s] 62%|   | 481/780 [03:36<02:05,  2.38it/s] 62%|   | 482/780 [03:37<01:53,  2.62it/s] 62%|   | 483/780 [03:37<01:45,  2.81it/s] 62%|   | 484/780 [03:37<01:39,  2.96it/s] 62%|   | 485/780 [03:37<01:35,  3.08it/s] 62%|   | 486/780 [03:38<01:32,  3.17it/s] 62%|   | 487/780 [03:38<01:30,  3.23it/s] 63%|   | 488/780 [03:38<01:28,  3.28it/s] 63%|   | 489/780 [03:39<01:30,  3.20it/s] 63%|   | 490/780 [03:39<01:29,  3.26it/s] 63%|   | 491/780 [03:39<01:27,  3.30it/s] 63%|   | 492/780 [03:40<01:26,  3.33it/s] 63%|   | 493/780 [03:40<01:25,  3.35it/s] 63%|   | 494/780 [03:40<01:25,  3.36it/s] 63%|   | 495/780 [03:40<01:24,  3.37it/s] 64%|   | 496/780 [03:41<01:24,  3.38it/s] 64%|   | 497/780 [03:41<01:23,  3.39it/s] 64%|   | 498/780 [03:41<01:23,  3.39it/s] 64%|   | 499/780 [03:42<01:28,  3.18it/s] 64%|   | 500/780 [03:42<01:26,  3.24it/s]                                                  64%|   | 500/780 [03:42<01:26,  3.24it/s] 64%|   | 501/780 [03:42<01:24,  3.28it/s] 64%|   | 502/780 [03:43<01:23,  3.31it/s] 64%|   | 503/780 [03:43<01:22,  3.34it/s] 65%|   | 504/780 [03:43<01:22,  3.35it/s] 65%|   | 505/780 [03:43<01:21,  3.36it/s] 65%|   | 506/780 [03:44<01:25,  3.22it/s] 65%|   | 507/780 [03:44<01:23,  3.27it/s] 65%|   | 508/780 [03:44<01:22,  3.31it/s] 65%|   | 509/780 [03:45<01:24,  3.22it/s] 65%|   | 510/780 [03:45<01:22,  3.27it/s] 66%|   | 511/780 [03:45<01:21,  3.31it/s] 66%|   | 512/780 [03:46<01:20,  3.33it/s] 66%|   | 513/780 [03:46<01:19,  3.35it/s] 66%|   | 514/780 [03:46<01:41,  2.62it/s] 66%|   | 515/780 [03:47<01:34,  2.82it/s] 66%|   | 516/780 [03:47<01:28,  2.97it/s] 66%|   | 517/780 [03:47<01:25,  3.08it/s] 66%|   | 518/780 [03:48<01:22,  3.17it/s] 67%|   | 519/780 [03:48<01:23,  3.11it/s] 67%|   | 520/780 [03:48<01:21,  3.19it/s] 67%|   | 521/780 [03:49<01:19,  3.25it/s] 67%|   | 522/780 [03:49<01:18,  3.29it/s] 67%|   | 523/780 [03:49<01:17,  3.32it/s] 67%|   | 524/780 [03:49<01:16,  3.34it/s] 67%|   | 525/780 [03:50<01:15,  3.36it/s] 67%|   | 526/780 [03:50<01:15,  3.37it/s] 68%|   | 527/780 [03:50<01:14,  3.38it/s] 68%|   | 528/780 [03:51<01:14,  3.38it/s] 68%|   | 529/780 [03:51<01:14,  3.38it/s] 68%|   | 530/780 [03:51<01:17,  3.22it/s] 68%|   | 531/780 [03:52<01:16,  3.27it/s] 68%|   | 532/780 [03:52<01:14,  3.31it/s] 68%|   | 533/780 [03:52<01:14,  3.33it/s] 68%|   | 534/780 [03:52<01:13,  3.35it/s] 69%|   | 535/780 [03:53<01:12,  3.36it/s] 69%|   | 536/780 [03:53<01:12,  3.37it/s] 69%|   | 537/780 [03:53<01:11,  3.38it/s] 69%|   | 538/780 [03:54<01:11,  3.38it/s] 69%|   | 539/780 [03:54<01:11,  3.38it/s] 69%|   | 540/780 [03:54<01:10,  3.39it/s] 69%|   | 541/780 [03:55<01:13,  3.26it/s] 69%|   | 542/780 [03:55<01:12,  3.30it/s] 70%|   | 543/780 [03:55<01:11,  3.32it/s] 70%|   | 544/780 [03:55<01:10,  3.34it/s] 70%|   | 545/780 [03:56<01:09,  3.36it/s] 70%|   | 546/780 [03:56<01:09,  3.37it/s] 70%|   | 547/780 [03:56<01:09,  3.37it/s] 70%|   | 548/780 [03:57<01:08,  3.38it/s] 70%|   | 549/780 [03:57<01:08,  3.38it/s] 71%|   | 550/780 [03:57<01:07,  3.38it/s] 71%|   | 551/780 [03:57<01:07,  3.39it/s] 71%|   | 552/780 [03:58<01:09,  3.30it/s] 71%|   | 553/780 [03:58<01:08,  3.33it/s] 71%|   | 554/780 [03:58<01:07,  3.35it/s] 71%|   | 555/780 [03:59<01:06,  3.36it/s] 71%|  | 556/780 [03:59<01:06,  3.37it/s] 71%|  | 557/780 [03:59<01:06,  3.38it/s] 72%|  | 558/780 [04:00<01:05,  3.38it/s] 72%|  | 559/780 [04:00<01:05,  3.38it/s] 72%|  | 560/780 [04:00<01:05,  3.38it/s] 72%|  | 561/780 [04:00<01:04,  3.39it/s] 72%|  | 562/780 [04:01<01:04,  3.41it/s] 72%|  | 563/780 [04:01<01:06,  3.24it/s] 72%|  | 564/780 [04:01<01:05,  3.30it/s] 72%|  | 565/780 [04:02<01:04,  3.34it/s] 73%|  | 566/780 [04:02<01:03,  3.37it/s] 73%|  | 567/780 [04:02<01:02,  3.39it/s] 73%|  | 568/780 [04:03<01:02,  3.41it/s] 73%|  | 569/780 [04:03<01:01,  3.42it/s] 73%|  | 570/780 [04:03<01:01,  3.42it/s] 73%|  | 571/780 [04:03<01:00,  3.43it/s] 73%|  | 572/780 [04:04<01:00,  3.43it/s] 73%|  | 573/780 [04:04<01:00,  3.44it/s] 74%|  | 574/780 [04:04<01:03,  3.25it/s] 74%|  | 575/780 [04:05<01:01,  3.31it/s] 74%|  | 576/780 [04:05<01:00,  3.35it/s] 74%|  | 577/780 [04:05<01:00,  3.37it/s] 74%|  | 578/780 [04:05<00:59,  3.39it/s] 74%|  | 579/780 [04:06<00:58,  3.41it/s] 74%|  | 580/780 [04:06<00:58,  3.42it/s] 74%|  | 581/780 [04:06<00:58,  3.43it/s] 75%|  | 582/780 [04:07<00:57,  3.43it/s] 75%|  | 583/780 [04:07<00:57,  3.43it/s] 75%|  | 584/780 [04:07<00:57,  3.44it/s] 75%|  | 585/780 [04:08<00:58,  3.34it/s] 75%|  | 586/780 [04:08<00:57,  3.37it/s] 75%|  | 587/780 [04:08<00:56,  3.39it/s] 75%|  | 588/780 [04:08<00:56,  3.41it/s] 76%|  | 589/780 [04:09<00:55,  3.42it/s] 76%|  | 590/780 [04:09<00:55,  3.42it/s] 76%|  | 591/780 [04:10<01:12,  2.61it/s] 76%|  | 592/780 [04:10<01:06,  2.81it/s] 76%|  | 593/780 [04:10<01:02,  2.97it/s] 76%|  | 594/780 [04:10<01:00,  3.10it/s] 76%|  | 595/780 [04:11<00:57,  3.19it/s] 76%|  | 596/780 [04:11<00:56,  3.26it/s] 77%|  | 597/780 [04:11<00:55,  3.31it/s] 77%|  | 598/780 [04:12<00:54,  3.35it/s] 77%|  | 599/780 [04:12<00:53,  3.38it/s] 77%|  | 600/780 [04:12<00:54,  3.28it/s] 77%|  | 601/780 [04:13<00:53,  3.32it/s] 77%|  | 602/780 [04:13<00:53,  3.36it/s] 77%|  | 603/780 [04:13<00:52,  3.38it/s] 77%|  | 604/780 [04:13<00:51,  3.39it/s] 78%|  | 605/780 [04:14<00:51,  3.41it/s] 78%|  | 606/780 [04:14<00:50,  3.41it/s] 78%|  | 607/780 [04:14<00:50,  3.42it/s] 78%|  | 608/780 [04:15<00:50,  3.43it/s] 78%|  | 609/780 [04:15<00:49,  3.43it/s] 78%|  | 610/780 [04:15<00:49,  3.43it/s] 78%|  | 611/780 [04:16<00:51,  3.28it/s] 78%|  | 612/780 [04:16<00:50,  3.33it/s] 79%|  | 613/780 [04:16<00:49,  3.36it/s] 79%|  | 614/780 [04:16<00:49,  3.38it/s] 79%|  | 615/780 [04:17<00:48,  3.40it/s] 79%|  | 616/780 [04:17<00:48,  3.41it/s] 79%|  | 617/780 [04:17<00:47,  3.42it/s] 79%|  | 618/780 [04:18<00:47,  3.42it/s] 79%|  | 619/780 [04:18<00:47,  3.43it/s] 79%|  | 620/780 [04:18<00:46,  3.43it/s] 80%|  | 621/780 [04:18<00:46,  3.43it/s] 80%|  | 622/780 [04:19<00:48,  3.25it/s] 80%|  | 623/780 [04:19<00:47,  3.30it/s] 80%|  | 624/780 [04:19<00:46,  3.34it/s][INFO|trainer.py:2140] 2023-08-29 10:49:36,592 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:49:36,592 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 10:49:36,592 >>   Batch size = 8
{'eval_loss': 1.0531671047210693, 'eval_runtime': 9.8288, 'eval_samples_per_second': 353.755, 'eval_steps_per_second': 44.258, 'epoch': 3.0}
{'loss': 0.4461, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.68it/s][A
  3%|         | 12/435 [00:00<00:08, 48.85it/s][A
  4%|         | 17/435 [00:00<00:08, 47.14it/s][A
  5%|         | 22/435 [00:00<00:08, 46.25it/s][A
  6%|         | 27/435 [00:00<00:08, 45.64it/s][A
  7%|         | 32/435 [00:00<00:08, 45.22it/s][A
  9%|         | 37/435 [00:00<00:08, 44.86it/s][A
 10%|         | 42/435 [00:00<00:08, 44.55it/s][A
 11%|         | 47/435 [00:01<00:08, 44.57it/s][A
 12%|        | 52/435 [00:01<00:08, 44.83it/s][A
 13%|        | 57/435 [00:01<00:08, 44.93it/s][A
 14%|        | 62/435 [00:01<00:08, 44.95it/s][A
 15%|        | 67/435 [00:01<00:08, 44.80it/s][A
 17%|        | 72/435 [00:01<00:08, 44.77it/s][A
 18%|        | 77/435 [00:01<00:08, 44.71it/s][A
 19%|        | 82/435 [00:01<00:07, 44.46it/s][A
 20%|        | 87/435 [00:01<00:07, 44.29it/s][A
 21%|        | 92/435 [00:02<00:07, 44.37it/s][A
 22%|       | 97/435 [00:02<00:07, 42.34it/s][A
 23%|       | 102/435 [00:02<00:07, 43.17it/s][A
 25%|       | 107/435 [00:02<00:07, 43.70it/s][A
 26%|       | 112/435 [00:02<00:07, 44.04it/s][A
 27%|       | 117/435 [00:02<00:07, 44.11it/s][A
 28%|       | 122/435 [00:02<00:07, 44.40it/s][A
 29%|       | 127/435 [00:02<00:06, 44.58it/s][A
 30%|       | 132/435 [00:02<00:06, 44.46it/s][A
 31%|      | 137/435 [00:03<00:06, 44.23it/s][A
 33%|      | 142/435 [00:03<00:06, 44.40it/s][A
 34%|      | 147/435 [00:03<00:06, 44.51it/s][A
 35%|      | 152/435 [00:03<00:06, 44.60it/s][A
 36%|      | 157/435 [00:03<00:06, 44.56it/s][A
 37%|      | 162/435 [00:03<00:06, 44.69it/s][A
 38%|      | 167/435 [00:03<00:05, 44.76it/s][A
 40%|      | 172/435 [00:03<00:05, 44.64it/s][A
 41%|      | 177/435 [00:03<00:05, 44.55it/s][A
 42%|     | 182/435 [00:04<00:05, 44.41it/s][A
 43%|     | 187/435 [00:04<00:05, 44.50it/s][A
 44%|     | 192/435 [00:04<00:05, 44.60it/s][A
 45%|     | 197/435 [00:04<00:05, 44.59it/s][A
 46%|     | 202/435 [00:04<00:05, 44.63it/s][A
 48%|     | 207/435 [00:04<00:05, 44.74it/s][A
 49%|     | 212/435 [00:04<00:04, 44.86it/s][A
 50%|     | 217/435 [00:04<00:04, 44.83it/s][A
 51%|     | 222/435 [00:04<00:04, 44.73it/s][A
 52%|    | 227/435 [00:05<00:04, 44.56it/s][A
 53%|    | 232/435 [00:05<00:05, 39.43it/s][A
 54%|    | 237/435 [00:05<00:04, 41.03it/s][A
 56%|    | 242/435 [00:05<00:04, 42.11it/s][A
 57%|    | 247/435 [00:05<00:04, 42.93it/s][A
 58%|    | 252/435 [00:05<00:04, 43.60it/s][A
 59%|    | 257/435 [00:05<00:04, 43.98it/s][A
 60%|    | 262/435 [00:05<00:03, 44.32it/s][A
 61%|   | 267/435 [00:06<00:03, 44.44it/s][A
 63%|   | 272/435 [00:06<00:03, 44.00it/s][A
 64%|   | 277/435 [00:06<00:03, 44.00it/s][A
 65%|   | 282/435 [00:06<00:03, 44.22it/s][A
 66%|   | 287/435 [00:06<00:03, 44.36it/s][A
 67%|   | 292/435 [00:06<00:03, 44.58it/s][A
 68%|   | 297/435 [00:06<00:03, 44.79it/s][A
 69%|   | 302/435 [00:06<00:02, 44.84it/s][A
 71%|   | 307/435 [00:06<00:02, 44.92it/s][A
 72%|  | 312/435 [00:07<00:02, 44.68it/s][A
 73%|  | 317/435 [00:07<00:02, 44.54it/s][A
 74%|  | 322/435 [00:07<00:02, 44.47it/s][A
 75%|  | 327/435 [00:07<00:02, 44.51it/s][A
 76%|  | 332/435 [00:07<00:02, 44.62it/s][A
 77%|  | 337/435 [00:07<00:02, 44.66it/s][A
 79%|  | 342/435 [00:07<00:02, 44.76it/s][A
 80%|  | 347/435 [00:07<00:01, 44.84it/s][A
 81%|  | 352/435 [00:07<00:01, 44.86it/s][A
 82%| | 357/435 [00:08<00:01, 44.76it/s][A
 83%| | 362/435 [00:08<00:01, 44.55it/s][A
 84%| | 367/435 [00:08<00:01, 37.78it/s][A
 86%| | 372/435 [00:08<00:01, 39.74it/s][A
 87%| | 377/435 [00:08<00:01, 41.17it/s][A
 88%| | 382/435 [00:08<00:01, 42.31it/s][A
 89%| | 387/435 [00:08<00:01, 43.13it/s][A
 90%| | 392/435 [00:08<00:00, 43.68it/s][A
 91%|| 397/435 [00:08<00:00, 43.95it/s][A
 92%|| 402/435 [00:09<00:00, 44.23it/s][A
 94%|| 407/435 [00:09<00:00, 44.00it/s][A
 95%|| 412/435 [00:09<00:00, 44.06it/s][A
 96%|| 417/435 [00:09<00:00, 43.99it/s][A
 97%|| 422/435 [00:09<00:00, 44.50it/s][A
 98%|| 427/435 [00:09<00:00, 44.67it/s][A
 99%|| 432/435 [00:09<00:00, 44.76it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.76it/s][A 80%|  | 624/780 [04:29<00:46,  3.34it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:49:47,072 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 10:49:47,464 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:49:52,751 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:49:53,155 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:49:53,332 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:44<19:58,  7.73s/it] 80%|  | 626/780 [04:45<14:09,  5.51s/it] 80%|  | 627/780 [04:45<10:04,  3.95s/it] 81%|  | 628/780 [04:45<07:13,  2.85s/it] 81%|  | 629/780 [04:46<05:14,  2.08s/it] 81%|  | 630/780 [04:46<03:51,  1.55s/it] 81%|  | 631/780 [04:47<03:09,  1.27s/it] 81%|  | 632/780 [04:47<02:24,  1.02it/s] 81%|  | 633/780 [04:47<01:53,  1.30it/s] 81%| | 634/780 [04:47<01:31,  1.59it/s] 81%| | 635/780 [04:48<01:18,  1.85it/s] 82%| | 636/780 [04:48<01:07,  2.15it/s] 82%| | 637/780 [04:48<00:59,  2.42it/s] 82%| | 638/780 [04:49<00:53,  2.66it/s] 82%| | 639/780 [04:49<00:49,  2.86it/s] 82%| | 640/780 [04:49<00:46,  3.01it/s] 82%| | 641/780 [04:50<00:44,  3.13it/s] 82%| | 642/780 [04:50<00:42,  3.22it/s] 82%| | 643/780 [04:50<00:41,  3.29it/s] 83%| | 644/780 [04:50<00:40,  3.33it/s] 83%| | 645/780 [04:51<00:40,  3.37it/s] 83%| | 646/780 [04:51<00:43,  3.09it/s] 83%| | 647/780 [04:51<00:41,  3.19it/s] 83%| | 648/780 [04:52<00:40,  3.27it/s] 83%| | 649/780 [04:52<00:39,  3.32it/s] 83%| | 650/780 [04:52<00:38,  3.36it/s] 83%| | 651/780 [04:53<00:38,  3.38it/s] 84%| | 652/780 [04:53<00:37,  3.40it/s] 84%| | 653/780 [04:53<00:37,  3.42it/s] 84%| | 654/780 [04:53<00:36,  3.43it/s] 84%| | 655/780 [04:54<00:36,  3.43it/s] 84%| | 656/780 [04:54<00:36,  3.44it/s] 84%| | 657/780 [04:54<00:37,  3.31it/s] 84%| | 658/780 [04:55<00:36,  3.35it/s] 84%| | 659/780 [04:55<00:35,  3.38it/s] 85%| | 660/780 [04:55<00:35,  3.40it/s] 85%| | 661/780 [04:55<00:34,  3.42it/s] 85%| | 662/780 [04:56<00:34,  3.43it/s] 85%| | 663/780 [04:56<00:34,  3.43it/s] 85%| | 664/780 [04:56<00:33,  3.44it/s] 85%| | 665/780 [04:57<00:33,  3.44it/s] 85%| | 666/780 [04:57<00:33,  3.45it/s] 86%| | 667/780 [04:57<00:32,  3.45it/s] 86%| | 668/780 [04:58<00:34,  3.26it/s] 86%| | 669/780 [04:58<00:33,  3.32it/s] 86%| | 670/780 [04:58<00:32,  3.36it/s] 86%| | 671/780 [04:58<00:32,  3.38it/s] 86%| | 672/780 [04:59<00:31,  3.40it/s] 86%| | 673/780 [04:59<00:31,  3.42it/s] 86%| | 674/780 [04:59<00:30,  3.43it/s] 87%| | 675/780 [05:00<00:30,  3.44it/s] 87%| | 676/780 [05:00<00:30,  3.44it/s] 87%| | 677/780 [05:00<00:29,  3.44it/s] 87%| | 678/780 [05:00<00:29,  3.44it/s] 87%| | 679/780 [05:01<00:30,  3.28it/s] 87%| | 680/780 [05:01<00:30,  3.33it/s] 87%| | 681/780 [05:01<00:29,  3.36it/s] 87%| | 682/780 [05:02<00:28,  3.39it/s] 88%| | 683/780 [05:02<00:28,  3.41it/s] 88%| | 684/780 [05:02<00:28,  3.42it/s] 88%| | 685/780 [05:03<00:27,  3.43it/s] 88%| | 686/780 [05:03<00:27,  3.44it/s] 88%| | 687/780 [05:03<00:27,  3.44it/s] 88%| | 688/780 [05:03<00:26,  3.44it/s] 88%| | 689/780 [05:04<00:26,  3.44it/s] 88%| | 690/780 [05:04<00:26,  3.37it/s] 89%| | 691/780 [05:04<00:26,  3.39it/s] 89%| | 692/780 [05:05<00:25,  3.41it/s] 89%| | 693/780 [05:05<00:25,  3.42it/s] 89%| | 694/780 [05:05<00:25,  3.43it/s] 89%| | 695/780 [05:05<00:24,  3.44it/s] 89%| | 696/780 [05:06<00:24,  3.44it/s] 89%| | 697/780 [05:06<00:24,  3.44it/s] 89%| | 698/780 [05:06<00:23,  3.45it/s] 90%| | 699/780 [05:07<00:23,  3.45it/s] 90%| | 700/780 [05:07<00:23,  3.45it/s] 90%| | 701/780 [05:07<00:23,  3.32it/s] 90%| | 702/780 [05:07<00:23,  3.35it/s] 90%| | 703/780 [05:08<00:22,  3.38it/s] 90%| | 704/780 [05:08<00:22,  3.40it/s] 90%| | 705/780 [05:08<00:21,  3.41it/s] 91%| | 706/780 [05:09<00:21,  3.43it/s] 91%| | 707/780 [05:09<00:21,  3.43it/s] 91%| | 708/780 [05:09<00:20,  3.44it/s] 91%| | 709/780 [05:10<00:20,  3.44it/s] 91%| | 710/780 [05:10<00:20,  3.44it/s] 91%| | 711/780 [05:10<00:20,  3.44it/s] 91%|| 712/780 [05:10<00:20,  3.33it/s] 91%|| 713/780 [05:11<00:19,  3.37it/s] 92%|| 714/780 [05:11<00:19,  3.39it/s] 92%|| 715/780 [05:11<00:19,  3.41it/s] 92%|| 716/780 [05:12<00:18,  3.42it/s] 92%|| 717/780 [05:12<00:18,  3.43it/s] 92%|| 718/780 [05:12<00:18,  3.43it/s] 92%|| 719/780 [05:12<00:17,  3.44it/s] 92%|| 720/780 [05:13<00:17,  3.44it/s] 92%|| 721/780 [05:13<00:17,  3.44it/s] 93%|| 722/780 [05:13<00:16,  3.44it/s] 93%|| 723/780 [05:14<00:16,  3.45it/s] 93%|| 724/780 [05:14<00:16,  3.45it/s] 93%|| 725/780 [05:14<00:16,  3.38it/s] 93%|| 726/780 [05:15<00:15,  3.40it/s] 93%|| 727/780 [05:15<00:15,  3.42it/s] 93%|| 728/780 [05:15<00:15,  3.42it/s] 93%|| 729/780 [05:15<00:14,  3.43it/s] 94%|| 730/780 [05:16<00:14,  3.43it/s] 94%|| 731/780 [05:16<00:14,  3.44it/s] 94%|| 732/780 [05:16<00:13,  3.44it/s] 94%|| 733/780 [05:17<00:13,  3.44it/s] 94%|| 734/780 [05:17<00:13,  3.44it/s] 94%|| 735/780 [05:17<00:13,  3.44it/s] 94%|| 736/780 [05:17<00:13,  3.30it/s] 94%|| 737/780 [05:18<00:12,  3.34it/s] 95%|| 738/780 [05:18<00:12,  3.37it/s] 95%|| 739/780 [05:18<00:12,  3.39it/s] 95%|| 740/780 [05:19<00:11,  3.41it/s] 95%|| 741/780 [05:19<00:11,  3.42it/s] 95%|| 742/780 [05:19<00:11,  3.42it/s] 95%|| 743/780 [05:19<00:10,  3.43it/s] 95%|| 744/780 [05:20<00:10,  3.43it/s] 96%|| 745/780 [05:20<00:10,  3.43it/s] 96%|| 746/780 [05:20<00:09,  3.43it/s] 96%|| 747/780 [05:21<00:09,  3.35it/s] 96%|| 748/780 [05:21<00:09,  3.38it/s] 96%|| 749/780 [05:21<00:09,  3.39it/s] 96%|| 750/780 [05:22<00:08,  3.40it/s] 96%|| 751/780 [05:22<00:08,  3.41it/s] 96%|| 752/780 [05:22<00:08,  3.42it/s] 97%|| 753/780 [05:22<00:07,  3.43it/s] 97%|| 754/780 [05:23<00:07,  3.43it/s] 97%|| 755/780 [05:23<00:07,  3.44it/s] 97%|| 756/780 [05:23<00:06,  3.44it/s] 97%|| 757/780 [05:24<00:06,  3.44it/s] 97%|| 758/780 [05:24<00:06,  3.26it/s] 97%|| 759/780 [05:24<00:06,  3.31it/s] 97%|| 760/780 [05:25<00:05,  3.35it/s] 98%|| 761/780 [05:25<00:05,  3.38it/s] 98%|| 762/780 [05:25<00:05,  3.40it/s] 98%|| 763/780 [05:25<00:04,  3.41it/s] 98%|| 764/780 [05:26<00:04,  3.42it/s] 98%|| 765/780 [05:26<00:04,  3.42it/s] 98%|| 766/780 [05:26<00:04,  3.43it/s] 98%|| 767/780 [05:27<00:03,  3.43it/s] 98%|| 768/780 [05:27<00:03,  3.43it/s] 99%|| 769/780 [05:27<00:03,  3.23it/s] 99%|| 770/780 [05:27<00:03,  3.29it/s] 99%|| 771/780 [05:28<00:02,  3.33it/s] 99%|| 772/780 [05:28<00:02,  3.36it/s] 99%|| 773/780 [05:28<00:02,  3.39it/s] 99%|| 774/780 [05:29<00:01,  3.40it/s] 99%|| 775/780 [05:29<00:01,  3.41it/s] 99%|| 776/780 [05:29<00:01,  3.42it/s]100%|| 777/780 [05:30<00:00,  3.42it/s]100%|| 778/780 [05:30<00:00,  3.43it/s]100%|| 779/780 [05:30<00:00,  3.43it/s]100%|| 780/780 [05:30<00:00,  3.26it/s][INFO|trainer.py:2140] 2023-08-29 10:50:47,656 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:50:47,656 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 10:50:47,656 >>   Batch size = 8
{'eval_loss': 1.0626311302185059, 'eval_runtime': 9.8975, 'eval_samples_per_second': 351.302, 'eval_steps_per_second': 43.951, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|         | 6/435 [00:00<00:07, 55.90it/s][A
  3%|         | 12/435 [00:00<00:08, 48.87it/s][A
  4%|         | 17/435 [00:00<00:08, 47.30it/s][A
  5%|         | 22/435 [00:00<00:08, 46.55it/s][A
  6%|         | 27/435 [00:00<00:08, 45.97it/s][A
  7%|         | 32/435 [00:00<00:08, 45.30it/s][A
  9%|         | 37/435 [00:00<00:08, 44.82it/s][A
 10%|         | 42/435 [00:00<00:08, 44.53it/s][A
 11%|         | 47/435 [00:01<00:08, 44.50it/s][A
 12%|        | 52/435 [00:01<00:08, 44.70it/s][A
 13%|        | 57/435 [00:01<00:08, 44.81it/s][A
 14%|        | 62/435 [00:01<00:08, 44.91it/s][A
 15%|        | 67/435 [00:01<00:08, 44.90it/s][A
 17%|        | 72/435 [00:01<00:08, 44.87it/s][A
 18%|        | 77/435 [00:01<00:08, 44.60it/s][A
 19%|        | 82/435 [00:01<00:07, 44.39it/s][A
 20%|        | 87/435 [00:01<00:07, 44.36it/s][A
 21%|        | 92/435 [00:02<00:07, 44.28it/s][A
 22%|       | 97/435 [00:02<00:07, 44.50it/s][A
 23%|       | 102/435 [00:02<00:07, 44.69it/s][A
 25%|       | 107/435 [00:02<00:07, 44.86it/s][A
 26%|       | 112/435 [00:02<00:07, 44.92it/s][A
 27%|       | 117/435 [00:02<00:07, 44.92it/s][A
 28%|       | 122/435 [00:02<00:06, 44.79it/s][A
 29%|       | 127/435 [00:02<00:08, 38.39it/s][A
 30%|       | 132/435 [00:02<00:07, 40.16it/s][A
 31%|      | 137/435 [00:03<00:07, 41.57it/s][A
 33%|      | 142/435 [00:03<00:06, 42.52it/s][A
 34%|      | 147/435 [00:03<00:06, 43.21it/s][A
 35%|      | 152/435 [00:03<00:06, 43.81it/s][A
 36%|      | 157/435 [00:03<00:06, 44.15it/s][A
 37%|      | 162/435 [00:03<00:06, 44.38it/s][A
 38%|      | 167/435 [00:03<00:06, 44.05it/s][A
 40%|      | 172/435 [00:03<00:05, 44.11it/s][A
 41%|      | 177/435 [00:03<00:05, 44.27it/s][A
 42%|     | 182/435 [00:04<00:05, 44.47it/s][A
 43%|     | 187/435 [00:04<00:05, 44.62it/s][A
 44%|     | 192/435 [00:04<00:05, 44.77it/s][A
 45%|     | 197/435 [00:04<00:05, 44.81it/s][A
 46%|     | 202/435 [00:04<00:05, 44.93it/s][A
 48%|     | 207/435 [00:04<00:05, 44.84it/s][A
 49%|     | 212/435 [00:04<00:05, 44.53it/s][A
 50%|     | 217/435 [00:04<00:04, 44.38it/s][A
 51%|     | 222/435 [00:05<00:04, 44.49it/s][A
 52%|    | 227/435 [00:05<00:04, 44.70it/s][A
 53%|    | 232/435 [00:05<00:04, 44.82it/s][A
 54%|    | 237/435 [00:05<00:04, 44.74it/s][A
 56%|    | 242/435 [00:05<00:04, 44.88it/s][A
 57%|    | 247/435 [00:05<00:04, 44.87it/s][A
 58%|    | 252/435 [00:05<00:04, 44.80it/s][A
 59%|    | 257/435 [00:05<00:03, 44.58it/s][A
 60%|    | 262/435 [00:05<00:04, 36.51it/s][A
 61%|   | 267/435 [00:06<00:04, 38.80it/s][A
 63%|   | 272/435 [00:06<00:04, 40.43it/s][A
 64%|   | 277/435 [00:06<00:03, 41.76it/s][A
 65%|   | 282/435 [00:06<00:03, 42.73it/s][A
 66%|   | 287/435 [00:06<00:03, 43.29it/s][A
 67%|   | 292/435 [00:06<00:03, 43.82it/s][A
 68%|   | 297/435 [00:06<00:03, 44.13it/s][A
 69%|   | 302/435 [00:06<00:03, 43.91it/s][A
 71%|   | 307/435 [00:06<00:02, 44.01it/s][A
 72%|  | 312/435 [00:07<00:02, 44.15it/s][A
 73%|  | 317/435 [00:07<00:02, 44.47it/s][A
 74%|  | 322/435 [00:07<00:02, 44.59it/s][A
 75%|  | 327/435 [00:07<00:02, 44.75it/s][A
 76%|  | 332/435 [00:07<00:02, 44.81it/s][A
 77%|  | 337/435 [00:07<00:02, 44.91it/s][A
 79%|  | 342/435 [00:07<00:02, 44.80it/s][A
 80%|  | 347/435 [00:07<00:01, 44.48it/s][A
 81%|  | 352/435 [00:07<00:01, 44.37it/s][A
 82%| | 357/435 [00:08<00:01, 44.48it/s][A
 83%| | 362/435 [00:08<00:01, 44.59it/s][A
 84%| | 367/435 [00:08<00:01, 44.75it/s][A
 86%| | 372/435 [00:08<00:01, 44.78it/s][A
 87%| | 377/435 [00:08<00:01, 44.91it/s][A
 88%| | 382/435 [00:08<00:01, 44.89it/s][A
 89%| | 387/435 [00:08<00:01, 44.87it/s][A
 90%| | 392/435 [00:08<00:00, 44.59it/s][A
 91%|| 397/435 [00:09<00:00, 41.99it/s][A
 92%|| 402/435 [00:09<00:00, 42.90it/s][A
 94%|| 407/435 [00:09<00:00, 43.44it/s][A
 95%|| 412/435 [00:09<00:00, 43.98it/s][A
 96%|| 417/435 [00:09<00:00, 44.31it/s][A
 97%|| 422/435 [00:09<00:00, 44.53it/s][A
 98%|| 427/435 [00:09<00:00, 44.71it/s][A
 99%|| 432/435 [00:09<00:00, 44.57it/s][A
                                                 [A                                                 
100%|| 435/435 [00:09<00:00, 44.57it/s][A100%|| 780/780 [05:40<00:00,  3.26it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:50:57,813 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 10:50:58,112 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:51:01,216 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:51:01,355 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:51:01,423 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 10:51:11,904 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 10:51:11,958 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156 (score: 1.0221641063690186).
                                                 100%|| 780/780 [06:06<00:00,  3.26it/s]100%|| 780/780 [06:06<00:00,  2.13it/s]
[INFO|trainer.py:1894] 2023-08-29 10:51:23,235 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 10:51:23,357 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:51:26,465 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:51:26,664 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:51:26,739 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:51:27,461 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:27,462 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:27,462 >>   train_loss               =     0.4378
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:27,462 >>   train_runtime            = 0:06:06.47
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:27,462 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:27,462 >>   train_samples_per_second =    136.436
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:27,462 >>   train_steps_per_second   =      2.128
{'eval_loss': 1.0668494701385498, 'eval_runtime': 9.8844, 'eval_samples_per_second': 351.767, 'eval_steps_per_second': 44.009, 'epoch': 5.0}
{'train_runtime': 366.4722, 'train_samples_per_second': 136.436, 'train_steps_per_second': 2.128, 'train_loss': 0.4378256382086338, 'epoch': 5.0}
08/29/2023 10:51:27 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 10:51:28,972 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:51:28,972 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 10:51:28,972 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|         | 6/435 [00:00<00:07, 59.95it/s]  3%|         | 12/435 [00:00<00:10, 40.85it/s]  4%|         | 17/435 [00:00<00:09, 42.64it/s]  5%|         | 22/435 [00:00<00:09, 43.66it/s]  6%|         | 27/435 [00:00<00:09, 44.19it/s]  7%|         | 32/435 [00:00<00:09, 44.60it/s]  9%|         | 37/435 [00:00<00:08, 44.77it/s] 10%|         | 42/435 [00:00<00:08, 44.99it/s] 11%|         | 47/435 [00:01<00:08, 44.99it/s] 12%|        | 52/435 [00:01<00:08, 44.64it/s] 13%|        | 57/435 [00:01<00:08, 44.68it/s] 14%|        | 62/435 [00:01<00:08, 44.81it/s] 15%|        | 67/435 [00:01<00:08, 44.99it/s] 17%|        | 72/435 [00:01<00:08, 45.09it/s] 18%|        | 77/435 [00:01<00:07, 45.18it/s] 19%|        | 82/435 [00:01<00:07, 45.29it/s] 20%|        | 87/435 [00:01<00:07, 45.28it/s] 21%|        | 92/435 [00:02<00:07, 43.69it/s] 22%|       | 97/435 [00:02<00:07, 44.11it/s] 23%|       | 102/435 [00:02<00:07, 44.28it/s] 25%|       | 107/435 [00:02<00:07, 44.50it/s] 26%|       | 112/435 [00:02<00:07, 44.75it/s] 27%|       | 117/435 [00:02<00:07, 44.82it/s] 28%|       | 122/435 [00:02<00:06, 45.02it/s] 29%|       | 127/435 [00:02<00:06, 45.05it/s] 30%|       | 132/435 [00:02<00:06, 45.02it/s] 31%|      | 137/435 [00:03<00:06, 44.92it/s] 33%|      | 142/435 [00:03<00:06, 44.85it/s] 34%|      | 147/435 [00:03<00:06, 43.47it/s] 35%|      | 152/435 [00:03<00:06, 43.98it/s] 36%|      | 157/435 [00:03<00:06, 44.38it/s] 37%|      | 162/435 [00:03<00:06, 44.62it/s] 38%|      | 167/435 [00:03<00:05, 44.68it/s] 40%|      | 172/435 [00:03<00:05, 44.81it/s] 41%|      | 177/435 [00:03<00:05, 44.86it/s] 42%|     | 182/435 [00:04<00:05, 44.84it/s] 43%|     | 187/435 [00:04<00:05, 44.56it/s] 44%|     | 192/435 [00:04<00:05, 44.63it/s] 45%|     | 197/435 [00:04<00:05, 44.86it/s] 46%|     | 202/435 [00:04<00:05, 44.92it/s] 48%|     | 207/435 [00:04<00:05, 45.02it/s] 49%|     | 212/435 [00:04<00:04, 44.97it/s] 50%|     | 217/435 [00:04<00:04, 44.95it/s] 51%|     | 222/435 [00:04<00:04, 45.04it/s] 52%|    | 227/435 [00:05<00:04, 44.88it/s] 53%|    | 232/435 [00:05<00:04, 44.68it/s] 54%|    | 237/435 [00:05<00:04, 44.71it/s] 56%|    | 242/435 [00:05<00:04, 44.88it/s] 57%|    | 247/435 [00:05<00:04, 44.92it/s] 58%|    | 252/435 [00:05<00:04, 45.01it/s] 59%|    | 257/435 [00:05<00:03, 45.11it/s] 60%|    | 262/435 [00:05<00:03, 45.20it/s] 61%|   | 267/435 [00:05<00:03, 45.25it/s] 63%|   | 272/435 [00:06<00:03, 45.03it/s] 64%|   | 277/435 [00:06<00:03, 44.92it/s] 65%|   | 282/435 [00:06<00:03, 42.95it/s] 66%|   | 287/435 [00:06<00:03, 43.66it/s] 67%|   | 292/435 [00:06<00:03, 44.19it/s] 68%|   | 297/435 [00:06<00:03, 44.46it/s] 69%|   | 302/435 [00:06<00:02, 44.84it/s] 71%|   | 307/435 [00:06<00:02, 45.00it/s] 72%|  | 312/435 [00:06<00:02, 45.12it/s] 73%|  | 317/435 [00:07<00:02, 45.11it/s] 74%|  | 322/435 [00:07<00:02, 44.80it/s] 75%|  | 327/435 [00:07<00:02, 44.73it/s] 76%|  | 332/435 [00:07<00:02, 44.84it/s] 77%|  | 337/435 [00:07<00:02, 44.96it/s] 79%|  | 342/435 [00:07<00:02, 45.10it/s] 80%|  | 347/435 [00:07<00:01, 45.12it/s] 81%|  | 352/435 [00:07<00:01, 45.27it/s] 82%| | 357/435 [00:07<00:01, 45.23it/s] 83%| | 362/435 [00:08<00:01, 45.29it/s] 84%| | 367/435 [00:08<00:01, 45.17it/s] 86%| | 372/435 [00:08<00:01, 45.05it/s] 87%| | 377/435 [00:08<00:01, 45.05it/s] 88%| | 382/435 [00:08<00:01, 45.03it/s] 89%| | 387/435 [00:08<00:01, 45.14it/s] 90%| | 392/435 [00:08<00:00, 45.15it/s] 91%|| 397/435 [00:08<00:00, 45.17it/s] 92%|| 402/435 [00:08<00:00, 45.22it/s] 94%|| 407/435 [00:09<00:00, 45.27it/s] 95%|| 412/435 [00:09<00:00, 45.11it/s] 96%|| 417/435 [00:09<00:00, 42.74it/s] 97%|| 422/435 [00:09<00:00, 43.26it/s] 98%|| 427/435 [00:09<00:00, 43.96it/s] 99%|| 432/435 [00:09<00:00, 44.32it/s]100%|| 435/435 [00:09<00:00, 44.61it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:51:38,930 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:38,930 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:38,930 >>   eval_loss               =     1.0222
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:38,930 >>   eval_runtime            = 0:00:09.95
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:38,930 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:38,930 >>   eval_samples_per_second =     349.17
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:38,930 >>   eval_steps_per_second   =     43.684
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:51:38,930 >>   perplexity              =     2.7792
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:52:36,230 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:52:36,275 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:52:36,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:52:36,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:52:36,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:52:37,445 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:52:37,446 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:52:38,106 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:53:33,330 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:53:33,360 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:53:36,139 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:53:36,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:53:36,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:53:36,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:53:36,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:53:37,523 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:53:37,524 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:53:37,903 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:53:39,446 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:53:39,446 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.11it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:04,  1.51it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.52it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.52it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:12,  1.55it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:13,  1.56it/s]Extractor Predicting: 21it [00:14,  1.52it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.57it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:21,  1.49it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.56it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:26,  1.55it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:27,  1.59it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:30,  1.58it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:32,  1.59it/s]Extractor Predicting: 50it [00:32,  1.64it/s]Extractor Predicting: 51it [00:33,  1.63it/s]Extractor Predicting: 52it [00:33,  1.58it/s]Extractor Predicting: 53it [00:34,  1.55it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.54it/s]Extractor Predicting: 57it [00:37,  1.55it/s]Extractor Predicting: 58it [00:37,  1.56it/s]Extractor Predicting: 59it [00:38,  1.60it/s]Extractor Predicting: 60it [00:39,  1.62it/s]Extractor Predicting: 61it [00:39,  1.62it/s]Extractor Predicting: 62it [00:40,  1.64it/s]Extractor Predicting: 63it [00:40,  1.59it/s]Extractor Predicting: 64it [00:41,  1.61it/s]Extractor Predicting: 65it [00:42,  1.49it/s]Extractor Predicting: 66it [00:42,  1.50it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.54it/s]Extractor Predicting: 69it [00:44,  1.56it/s]Extractor Predicting: 70it [00:45,  1.56it/s]Extractor Predicting: 71it [00:46,  1.56it/s]Extractor Predicting: 72it [00:46,  1.58it/s]Extractor Predicting: 73it [00:47,  1.56it/s]Extractor Predicting: 74it [00:47,  1.59it/s]Extractor Predicting: 75it [00:48,  1.61it/s]Extractor Predicting: 76it [00:49,  1.59it/s]Extractor Predicting: 77it [00:49,  1.57it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:51,  1.53it/s]Extractor Predicting: 80it [00:51,  1.53it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:53,  1.55it/s]Extractor Predicting: 83it [00:53,  1.54it/s]Extractor Predicting: 84it [00:54,  1.56it/s]Extractor Predicting: 85it [00:55,  1.55it/s]Extractor Predicting: 86it [00:55,  1.56it/s]Extractor Predicting: 87it [00:56,  1.55it/s]Extractor Predicting: 88it [00:57,  1.55it/s]Extractor Predicting: 89it [00:57,  1.58it/s]Extractor Predicting: 90it [00:58,  1.58it/s]Extractor Predicting: 91it [00:58,  1.60it/s]Extractor Predicting: 92it [00:59,  1.64it/s]Extractor Predicting: 93it [01:00,  1.65it/s]Extractor Predicting: 94it [01:00,  1.62it/s]Extractor Predicting: 95it [01:01,  1.66it/s]Extractor Predicting: 96it [01:01,  1.65it/s]Extractor Predicting: 97it [01:02,  1.63it/s]Extractor Predicting: 98it [01:03,  1.62it/s]Extractor Predicting: 99it [01:03,  1.58it/s]Extractor Predicting: 100it [01:04,  1.56it/s]Extractor Predicting: 101it [01:05,  1.56it/s]Extractor Predicting: 102it [01:05,  1.60it/s]Extractor Predicting: 103it [01:06,  1.64it/s]Extractor Predicting: 104it [01:06,  1.64it/s]Extractor Predicting: 105it [01:07,  1.63it/s]Extractor Predicting: 106it [01:08,  1.63it/s]Extractor Predicting: 107it [01:08,  1.61it/s]Extractor Predicting: 108it [01:09,  1.61it/s]Extractor Predicting: 109it [01:09,  1.61it/s]Extractor Predicting: 110it [01:10,  1.63it/s]Extractor Predicting: 111it [01:11,  1.64it/s]Extractor Predicting: 112it [01:11,  1.59it/s]Extractor Predicting: 113it [01:12,  1.64it/s]Extractor Predicting: 114it [01:12,  1.69it/s]Extractor Predicting: 115it [01:13,  1.67it/s]Extractor Predicting: 116it [01:14,  1.68it/s]Extractor Predicting: 117it [01:14,  1.69it/s]Extractor Predicting: 118it [01:15,  1.64it/s]Extractor Predicting: 119it [01:16,  1.66it/s]Extractor Predicting: 120it [01:16,  1.62it/s]Extractor Predicting: 121it [01:17,  1.60it/s]Extractor Predicting: 122it [01:17,  1.61it/s]Extractor Predicting: 123it [01:18,  1.59it/s]Extractor Predicting: 124it [01:19,  1.58it/s]Extractor Predicting: 125it [01:19,  1.59it/s]Extractor Predicting: 126it [01:20,  1.61it/s]Extractor Predicting: 127it [01:21,  1.60it/s]Extractor Predicting: 128it [01:21,  1.59it/s]Extractor Predicting: 129it [01:22,  1.60it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.59it/s]Extractor Predicting: 132it [01:24,  1.57it/s]Extractor Predicting: 133it [01:24,  1.55it/s]Extractor Predicting: 134it [01:25,  1.60it/s]Extractor Predicting: 135it [01:26,  1.58it/s]Extractor Predicting: 136it [01:26,  1.57it/s]Extractor Predicting: 137it [01:27,  1.61it/s]Extractor Predicting: 138it [01:27,  1.60it/s]Extractor Predicting: 139it [01:28,  1.58it/s]Extractor Predicting: 140it [01:29,  1.46it/s]Extractor Predicting: 141it [01:30,  1.49it/s]Extractor Predicting: 142it [01:30,  1.51it/s]Extractor Predicting: 143it [01:31,  1.56it/s]Extractor Predicting: 144it [01:31,  1.59it/s]Extractor Predicting: 144it [01:31,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:27,784 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:27,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:27,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:27,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:27,837 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:55:28,758 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:55:28,759 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:55:29,422 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:55:30,575 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:55:30,576 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:33,609 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:33,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:33,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:33,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:55:33,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:55:34,604 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:55:34,605 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:55:35,427 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:55:35,676 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:55:35,676 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.35666912306558585,
  "recall": 0.13920046016681048,
  "score": 0.2002482416218453,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:14,  1.62it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:19,  1.68it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:20,  1.62it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:22,  1.63it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:24,  1.50it/s]Extractor Predicting: 41it [00:25,  1.55it/s]Extractor Predicting: 42it [00:25,  1.60it/s]Extractor Predicting: 43it [00:26,  1.57it/s]Extractor Predicting: 44it [00:27,  1.59it/s]Extractor Predicting: 45it [00:27,  1.57it/s]Extractor Predicting: 46it [00:28,  1.54it/s]Extractor Predicting: 47it [00:29,  1.59it/s]Extractor Predicting: 48it [00:29,  1.58it/s]Extractor Predicting: 49it [00:30,  1.61it/s]Extractor Predicting: 50it [00:31,  1.61it/s]Extractor Predicting: 51it [00:31,  1.60it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:32,  1.60it/s]Extractor Predicting: 54it [00:33,  1.54it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:34,  1.59it/s]Extractor Predicting: 57it [00:35,  1.58it/s]Extractor Predicting: 58it [00:36,  1.56it/s]Extractor Predicting: 59it [00:36,  1.55it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.57it/s]Extractor Predicting: 62it [00:38,  1.56it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:39,  1.58it/s]Extractor Predicting: 65it [00:40,  1.63it/s]Extractor Predicting: 66it [00:41,  1.60it/s]Extractor Predicting: 67it [00:41,  1.58it/s]Extractor Predicting: 68it [00:42,  1.57it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:43,  1.52it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:45,  1.55it/s]Extractor Predicting: 73it [00:45,  1.53it/s]Extractor Predicting: 74it [00:46,  1.56it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:47,  1.58it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:48,  1.58it/s]Extractor Predicting: 79it [00:49,  1.59it/s]Extractor Predicting: 80it [00:50,  1.64it/s]Extractor Predicting: 81it [00:50,  1.65it/s]Extractor Predicting: 82it [00:51,  1.65it/s]Extractor Predicting: 83it [00:51,  1.64it/s]Extractor Predicting: 84it [00:52,  1.63it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:53,  1.52it/s]Extractor Predicting: 87it [00:54,  1.51it/s]Extractor Predicting: 88it [00:55,  1.55it/s]Extractor Predicting: 89it [00:55,  1.54it/s]Extractor Predicting: 90it [00:56,  1.52it/s]Extractor Predicting: 91it [00:57,  1.52it/s]Extractor Predicting: 92it [00:57,  1.54it/s]Extractor Predicting: 93it [00:58,  1.56it/s]Extractor Predicting: 94it [00:59,  1.56it/s]Extractor Predicting: 95it [00:59,  1.57it/s]Extractor Predicting: 96it [01:00,  1.58it/s]Extractor Predicting: 97it [01:00,  1.56it/s]Extractor Predicting: 98it [01:01,  1.52it/s]Extractor Predicting: 99it [01:02,  1.51it/s]Extractor Predicting: 100it [01:02,  1.54it/s]Extractor Predicting: 101it [01:03,  1.56it/s]Extractor Predicting: 102it [01:04,  1.57it/s]Extractor Predicting: 103it [01:04,  1.55it/s]Extractor Predicting: 104it [01:05,  1.52it/s]Extractor Predicting: 105it [01:06,  1.56it/s]Extractor Predicting: 106it [01:06,  1.54it/s]Extractor Predicting: 107it [01:07,  1.56it/s]Extractor Predicting: 108it [01:08,  1.57it/s]Extractor Predicting: 109it [01:08,  1.56it/s]Extractor Predicting: 110it [01:09,  1.54it/s]Extractor Predicting: 111it [01:10,  1.56it/s]Extractor Predicting: 112it [01:10,  1.56it/s]Extractor Predicting: 113it [01:11,  1.57it/s]Extractor Predicting: 114it [01:11,  1.59it/s]Extractor Predicting: 115it [01:12,  1.58it/s]Extractor Predicting: 116it [01:13,  1.54it/s]Extractor Predicting: 117it [01:13,  1.54it/s]Extractor Predicting: 118it [01:14,  1.52it/s]Extractor Predicting: 119it [01:15,  1.52it/s]Extractor Predicting: 120it [01:15,  1.54it/s]Extractor Predicting: 121it [01:16,  1.56it/s]Extractor Predicting: 122it [01:17,  1.60it/s]Extractor Predicting: 123it [01:17,  1.59it/s]Extractor Predicting: 124it [01:18,  1.60it/s]Extractor Predicting: 125it [01:18,  1.59it/s]Extractor Predicting: 126it [01:19,  1.59it/s]Extractor Predicting: 127it [01:20,  1.61it/s]Extractor Predicting: 128it [01:20,  1.59it/s]Extractor Predicting: 129it [01:21,  1.64it/s]Extractor Predicting: 130it [01:22,  1.63it/s]Extractor Predicting: 131it [01:22,  1.62it/s]Extractor Predicting: 132it [01:23,  1.63it/s]Extractor Predicting: 133it [01:23,  1.68it/s]Extractor Predicting: 134it [01:24,  1.67it/s]Extractor Predicting: 135it [01:25,  1.63it/s]Extractor Predicting: 136it [01:25,  1.64it/s]Extractor Predicting: 137it [01:26,  1.66it/s]Extractor Predicting: 138it [01:26,  1.70it/s]Extractor Predicting: 139it [01:27,  1.68it/s]Extractor Predicting: 140it [01:28,  1.66it/s]Extractor Predicting: 141it [01:28,  1.64it/s]Extractor Predicting: 142it [01:29,  1.66it/s]Extractor Predicting: 143it [01:29,  1.59it/s]Extractor Predicting: 144it [01:30,  1.58it/s]Extractor Predicting: 145it [01:31,  1.59it/s]Extractor Predicting: 146it [01:31,  1.62it/s]Extractor Predicting: 147it [01:32,  1.67it/s]Extractor Predicting: 148it [01:32,  1.69it/s]Extractor Predicting: 149it [01:33,  1.70it/s]Extractor Predicting: 150it [01:34,  1.74it/s]Extractor Predicting: 151it [01:34,  1.71it/s]Extractor Predicting: 152it [01:35,  1.72it/s]Extractor Predicting: 153it [01:35,  1.73it/s]Extractor Predicting: 154it [01:36,  1.75it/s]Extractor Predicting: 155it [01:36,  1.73it/s]Extractor Predicting: 156it [01:37,  1.53it/s]Extractor Predicting: 157it [01:38,  1.61it/s]Extractor Predicting: 158it [01:38,  1.64it/s]Extractor Predicting: 159it [01:39,  1.73it/s]Extractor Predicting: 160it [01:39,  1.77it/s]Extractor Predicting: 161it [01:40,  1.73it/s]Extractor Predicting: 162it [01:41,  1.69it/s]Extractor Predicting: 163it [01:41,  1.68it/s]Extractor Predicting: 164it [01:42,  1.70it/s]Extractor Predicting: 165it [01:42,  1.73it/s]Extractor Predicting: 166it [01:43,  1.75it/s]Extractor Predicting: 167it [01:44,  1.75it/s]Extractor Predicting: 168it [01:44,  1.72it/s]Extractor Predicting: 169it [01:45,  1.74it/s]Extractor Predicting: 170it [01:45,  1.73it/s]Extractor Predicting: 171it [01:46,  1.77it/s]Extractor Predicting: 172it [01:46,  1.77it/s]Extractor Predicting: 173it [01:47,  1.77it/s]Extractor Predicting: 174it [01:48,  1.67it/s]Extractor Predicting: 175it [01:48,  1.67it/s]Extractor Predicting: 176it [01:49,  1.64it/s]Extractor Predicting: 177it [01:50,  1.59it/s]Extractor Predicting: 178it [01:50,  1.56it/s]Extractor Predicting: 179it [01:51,  1.55it/s]Extractor Predicting: 180it [01:52,  1.54it/s]Extractor Predicting: 181it [01:52,  1.54it/s]Extractor Predicting: 182it [01:53,  1.56it/s]Extractor Predicting: 183it [01:53,  1.58it/s]Extractor Predicting: 184it [01:54,  1.58it/s]Extractor Predicting: 185it [01:55,  1.56it/s]Extractor Predicting: 186it [01:55,  1.59it/s]Extractor Predicting: 187it [01:56,  1.58it/s]Extractor Predicting: 188it [01:57,  1.56it/s]Extractor Predicting: 189it [01:57,  1.58it/s]Extractor Predicting: 190it [01:58,  1.56it/s]Extractor Predicting: 191it [01:59,  1.54it/s]Extractor Predicting: 192it [01:59,  1.53it/s]Extractor Predicting: 193it [02:00,  1.49it/s]Extractor Predicting: 194it [02:01,  1.49it/s]Extractor Predicting: 195it [02:01,  1.52it/s]Extractor Predicting: 196it [02:02,  1.56it/s]Extractor Predicting: 197it [02:02,  1.57it/s]Extractor Predicting: 198it [02:03,  1.57it/s]Extractor Predicting: 199it [02:04,  1.58it/s]Extractor Predicting: 200it [02:04,  1.55it/s]Extractor Predicting: 201it [02:05,  1.51it/s]Extractor Predicting: 202it [02:06,  1.50it/s]Extractor Predicting: 203it [02:06,  1.48it/s]Extractor Predicting: 204it [02:07,  1.48it/s]Extractor Predicting: 205it [02:08,  1.48it/s]Extractor Predicting: 206it [02:08,  1.49it/s]Extractor Predicting: 207it [02:09,  1.51it/s]Extractor Predicting: 208it [02:10,  1.50it/s]Extractor Predicting: 209it [02:11,  1.44it/s]Extractor Predicting: 210it [02:11,  1.45it/s]Extractor Predicting: 211it [02:12,  1.45it/s]Extractor Predicting: 212it [02:13,  1.48it/s]Extractor Predicting: 213it [02:13,  1.49it/s]Extractor Predicting: 214it [02:14,  1.47it/s]Extractor Predicting: 215it [02:15,  1.46it/s]Extractor Predicting: 216it [02:15,  1.47it/s]Extractor Predicting: 217it [02:16,  1.50it/s]Extractor Predicting: 218it [02:17,  1.49it/s]Extractor Predicting: 219it [02:17,  1.45it/s]Extractor Predicting: 220it [02:18,  1.43it/s]Extractor Predicting: 221it [02:19,  1.43it/s]Extractor Predicting: 222it [02:19,  1.43it/s]Extractor Predicting: 223it [02:20,  1.48it/s]Extractor Predicting: 224it [02:21,  1.44it/s]Extractor Predicting: 225it [02:21,  1.46it/s]Extractor Predicting: 226it [02:22,  1.46it/s]Extractor Predicting: 227it [02:23,  1.48it/s]Extractor Predicting: 228it [02:23,  1.49it/s]Extractor Predicting: 229it [02:24,  1.46it/s]Extractor Predicting: 230it [02:25,  1.50it/s]Extractor Predicting: 231it [02:25,  1.56it/s]Extractor Predicting: 232it [02:26,  1.60it/s]Extractor Predicting: 233it [02:27,  1.62it/s]Extractor Predicting: 234it [02:27,  1.52it/s]Extractor Predicting: 235it [02:28,  1.57it/s]Extractor Predicting: 236it [02:29,  1.59it/s]Extractor Predicting: 237it [02:29,  1.60it/s]Extractor Predicting: 238it [02:30,  1.62it/s]Extractor Predicting: 239it [02:31,  1.53it/s]Extractor Predicting: 240it [02:31,  1.58it/s]Extractor Predicting: 241it [02:32,  1.60it/s]Extractor Predicting: 242it [02:32,  1.64it/s]Extractor Predicting: 243it [02:33,  1.64it/s]Extractor Predicting: 244it [02:33,  1.66it/s]Extractor Predicting: 245it [02:34,  1.63it/s]Extractor Predicting: 246it [02:35,  1.61it/s]Extractor Predicting: 247it [02:35,  1.59it/s]Extractor Predicting: 248it [02:36,  1.58it/s]Extractor Predicting: 249it [02:37,  1.59it/s]Extractor Predicting: 250it [02:37,  1.60it/s]Extractor Predicting: 251it [02:38,  1.62it/s]Extractor Predicting: 252it [02:38,  1.66it/s]Extractor Predicting: 253it [02:39,  1.64it/s]Extractor Predicting: 254it [02:40,  1.60it/s]Extractor Predicting: 255it [02:40,  1.61it/s]Extractor Predicting: 256it [02:41,  1.56it/s]Extractor Predicting: 257it [02:42,  1.53it/s]Extractor Predicting: 258it [02:42,  1.54it/s]Extractor Predicting: 259it [02:43,  1.56it/s]Extractor Predicting: 260it [02:44,  1.55it/s]Extractor Predicting: 261it [02:44,  1.53it/s]Extractor Predicting: 262it [02:45,  1.56it/s]Extractor Predicting: 263it [02:46,  1.59it/s]Extractor Predicting: 264it [02:46,  1.57it/s]Extractor Predicting: 265it [02:47,  1.58it/s]Extractor Predicting: 266it [02:47,  1.53it/s]Extractor Predicting: 267it [02:48,  1.54it/s]Extractor Predicting: 268it [02:49,  1.53it/s]Extractor Predicting: 269it [02:50,  1.36it/s]Extractor Predicting: 270it [02:50,  1.41it/s]Extractor Predicting: 271it [02:51,  1.43it/s]Extractor Predicting: 272it [02:52,  1.49it/s]Extractor Predicting: 273it [02:52,  1.46it/s]Extractor Predicting: 274it [02:53,  1.46it/s]Extractor Predicting: 275it [02:54,  1.48it/s]Extractor Predicting: 276it [02:54,  1.47it/s]Extractor Predicting: 277it [02:55,  1.49it/s]Extractor Predicting: 278it [02:56,  1.51it/s]Extractor Predicting: 279it [02:56,  1.50it/s]Extractor Predicting: 280it [02:57,  1.51it/s]Extractor Predicting: 281it [02:58,  1.52it/s]Extractor Predicting: 282it [02:58,  1.51it/s]Extractor Predicting: 283it [02:59,  1.49it/s]Extractor Predicting: 284it [03:00,  1.56it/s]Extractor Predicting: 285it [03:00,  1.54it/s]Extractor Predicting: 286it [03:01,  1.55it/s]Extractor Predicting: 287it [03:02,  1.56it/s]Extractor Predicting: 288it [03:02,  1.55it/s]Extractor Predicting: 289it [03:03,  1.54it/s]Extractor Predicting: 290it [03:04,  1.53it/s]Extractor Predicting: 291it [03:04,  1.46it/s]Extractor Predicting: 292it [03:05,  1.46it/s]Extractor Predicting: 293it [03:06,  1.50it/s]Extractor Predicting: 294it [03:06,  1.47it/s]Extractor Predicting: 295it [03:07,  1.50it/s]Extractor Predicting: 296it [03:08,  1.45it/s]Extractor Predicting: 297it [03:08,  1.46it/s]Extractor Predicting: 298it [03:09,  1.46it/s]Extractor Predicting: 299it [03:10,  1.46it/s]Extractor Predicting: 300it [03:10,  1.50it/s]Extractor Predicting: 301it [03:11,  1.48it/s]Extractor Predicting: 302it [03:12,  1.50it/s]Extractor Predicting: 303it [03:12,  1.47it/s]Extractor Predicting: 304it [03:13,  1.47it/s]Extractor Predicting: 305it [03:14,  1.48it/s]Extractor Predicting: 306it [03:14,  1.52it/s]Extractor Predicting: 307it [03:15,  1.46it/s]Extractor Predicting: 308it [03:16,  1.50it/s]Extractor Predicting: 309it [03:16,  1.52it/s]Extractor Predicting: 310it [03:17,  1.54it/s]Extractor Predicting: 311it [03:18,  1.53it/s]Extractor Predicting: 312it [03:18,  1.54it/s]Extractor Predicting: 313it [03:19,  1.60it/s]Extractor Predicting: 314it [03:19,  1.63it/s]Extractor Predicting: 315it [03:20,  1.65it/s]Extractor Predicting: 316it [03:21,  1.62it/s]Extractor Predicting: 317it [03:21,  1.54it/s]Extractor Predicting: 318it [03:22,  1.55it/s]Extractor Predicting: 319it [03:23,  1.56it/s]Extractor Predicting: 320it [03:23,  1.56it/s]Extractor Predicting: 321it [03:24,  1.57it/s]Extractor Predicting: 322it [03:25,  1.54it/s]Extractor Predicting: 323it [03:25,  1.51it/s]Extractor Predicting: 324it [03:26,  1.53it/s]Extractor Predicting: 325it [03:27,  1.54it/s]Extractor Predicting: 326it [03:27,  1.55it/s]Extractor Predicting: 327it [03:28,  1.56it/s]Extractor Predicting: 328it [03:29,  1.56it/s]Extractor Predicting: 329it [03:29,  1.58it/s]Extractor Predicting: 330it [03:30,  1.57it/s]Extractor Predicting: 331it [03:30,  1.57it/s]Extractor Predicting: 332it [03:31,  1.53it/s]Extractor Predicting: 333it [03:32,  1.58it/s]Extractor Predicting: 334it [03:32,  1.58it/s]Extractor Predicting: 335it [03:33,  1.60it/s]Extractor Predicting: 336it [03:34,  1.58it/s]Extractor Predicting: 337it [03:34,  1.55it/s]Extractor Predicting: 338it [03:35,  1.57it/s]Extractor Predicting: 339it [03:35,  1.62it/s]Extractor Predicting: 340it [03:36,  1.64it/s]Extractor Predicting: 341it [03:37,  1.64it/s]Extractor Predicting: 342it [03:37,  1.59it/s]Extractor Predicting: 343it [03:38,  1.58it/s]Extractor Predicting: 344it [03:39,  1.61it/s]Extractor Predicting: 345it [03:39,  1.57it/s]Extractor Predicting: 346it [03:40,  1.58it/s]Extractor Predicting: 347it [03:40,  1.57it/s]Extractor Predicting: 348it [03:41,  1.58it/s]Extractor Predicting: 349it [03:42,  1.61it/s]Extractor Predicting: 350it [03:42,  1.57it/s]Extractor Predicting: 351it [03:43,  1.59it/s]Extractor Predicting: 352it [03:44,  1.56it/s]Extractor Predicting: 353it [03:44,  1.58it/s]Extractor Predicting: 354it [03:45,  1.62it/s]Extractor Predicting: 355it [03:45,  1.62it/s]Extractor Predicting: 356it [03:46,  1.60it/s]Extractor Predicting: 357it [03:47,  1.61it/s]Extractor Predicting: 358it [03:47,  1.62it/s]Extractor Predicting: 359it [03:48,  1.61it/s]Extractor Predicting: 360it [03:49,  1.58it/s]Extractor Predicting: 361it [03:49,  1.60it/s]Extractor Predicting: 362it [03:50,  1.61it/s]Extractor Predicting: 363it [03:50,  1.63it/s]Extractor Predicting: 364it [03:51,  1.65it/s]Extractor Predicting: 365it [03:52,  1.65it/s]Extractor Predicting: 366it [03:53,  1.43it/s]Extractor Predicting: 367it [03:53,  1.44it/s]Extractor Predicting: 368it [03:54,  1.48it/s]Extractor Predicting: 369it [03:54,  1.51it/s]Extractor Predicting: 370it [03:55,  1.56it/s]Extractor Predicting: 371it [03:56,  1.59it/s]Extractor Predicting: 372it [03:56,  1.60it/s]Extractor Predicting: 373it [03:57,  1.58it/s]Extractor Predicting: 374it [03:58,  1.60it/s]Extractor Predicting: 375it [03:58,  1.58it/s]Extractor Predicting: 376it [03:59,  1.60it/s]Extractor Predicting: 377it [03:59,  1.61it/s]Extractor Predicting: 378it [04:00,  1.64it/s]Extractor Predicting: 379it [04:01,  1.60it/s]Extractor Predicting: 380it [04:01,  1.58it/s]Extractor Predicting: 381it [04:02,  1.60it/s]Extractor Predicting: 382it [04:03,  1.60it/s]Extractor Predicting: 383it [04:03,  1.63it/s]Extractor Predicting: 384it [04:04,  1.67it/s]Extractor Predicting: 385it [04:04,  1.60it/s]Extractor Predicting: 386it [04:05,  1.62it/s]Extractor Predicting: 387it [04:06,  1.62it/s]Extractor Predicting: 388it [04:06,  1.60it/s]Extractor Predicting: 389it [04:07,  1.60it/s]Extractor Predicting: 390it [04:08,  1.59it/s]Extractor Predicting: 391it [04:08,  1.55it/s]Extractor Predicting: 392it [04:09,  1.57it/s]Extractor Predicting: 393it [04:09,  1.61it/s]Extractor Predicting: 394it [04:10,  1.54it/s]Extractor Predicting: 395it [04:11,  1.50it/s]Extractor Predicting: 396it [04:11,  1.50it/s]Extractor Predicting: 397it [04:12,  1.49it/s]Extractor Predicting: 398it [04:13,  1.48it/s]Extractor Predicting: 399it [04:14,  1.48it/s]Extractor Predicting: 400it [04:14,  1.54it/s]Extractor Predicting: 401it [04:15,  1.51it/s]Extractor Predicting: 402it [04:16,  1.49it/s]Extractor Predicting: 403it [04:16,  1.50it/s]Extractor Predicting: 404it [04:17,  1.45it/s]Extractor Predicting: 405it [04:18,  1.46it/s]Extractor Predicting: 406it [04:18,  1.42it/s]Extractor Predicting: 407it [04:19,  1.42it/s]Extractor Predicting: 408it [04:20,  1.46it/s]Extractor Predicting: 409it [04:20,  1.49it/s]Extractor Predicting: 410it [04:21,  1.46it/s]Extractor Predicting: 411it [04:22,  1.48it/s]Extractor Predicting: 412it [04:22,  1.50it/s]Extractor Predicting: 413it [04:23,  1.52it/s]Extractor Predicting: 414it [04:24,  1.49it/s]Extractor Predicting: 415it [04:24,  1.54it/s]Extractor Predicting: 416it [04:25,  1.55it/s]Extractor Predicting: 417it [04:26,  1.56it/s]Extractor Predicting: 418it [04:26,  1.56it/s]Extractor Predicting: 419it [04:27,  1.50it/s]Extractor Predicting: 420it [04:28,  1.51it/s]Extractor Predicting: 421it [04:28,  1.57it/s]Extractor Predicting: 421it [04:28,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:19,660 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:19,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:19,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:19,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:19,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:00:20,819 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:00:20,820 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:00:21,533 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:00:22,747 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:00:22,820 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:26,119 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:26,180 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:26,181 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:26,181 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:00:26,181 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:00:27,083 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:00:27,084 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:00:27,760 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:00:28,059 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:00:28,059 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.21086408434563375,
  "recall": 0.09113422486379395,
  "score": 0.12726518190621108,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 9it [00:05,  1.56it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.30434782608695654,
  "recall": 0.05185185185185185,
  "score": 0.08860759493670885,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
