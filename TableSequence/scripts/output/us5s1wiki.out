Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:22, 15.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:34<02:20, 17.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:50<01:56, 16.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:04<01:34, 15.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:19<01:17, 15.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:34<01:00, 15.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:50<00:46, 15.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:04<00:30, 15.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:19<00:14, 15.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:35<00:00, 15.39s/it]Generating: 100%|██████████| 10/10 [02:35<00:00, 15.57s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('', 'composer', 'Jérémie Léger', 'The title track of the album was a track Iced Tea made by Jérémie Léger and written by Pierre Bousquet and Bernard Barrou , among others .')"}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 397, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 443, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 489, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 555, 'raw': 800}
{'target': 600, 'success': 574, 'raw': 832}
{'target': 600, 'success': 593, 'raw': 864}
{'target': 600, 'success': 614, 'raw': 896}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.6852678571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : opposite of . Context : Later in the year ( 1143 ) , Ptolemaeus , the founder of the Macedonian Republic , married Ariadne , daughter of Ariadne , founder of Macedonian kings , Thebes and Ileani . Head Entity : Ileani , Tail Entity : Ileanius .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8491847826086957, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : creator .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.75375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Lothar', 'twinned administrative body', '', 'He died in the Battle of Mervyn at Rheinmetall in 645 along with his daughter Lothar , D.')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 12521
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12621, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_5_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:14, 14.82s/it]Extractor Estimating: 2it [00:17,  7.81s/it]Extractor Estimating: 3it [00:18,  4.52s/it]Extractor Estimating: 4it [00:19,  3.01s/it]Extractor Estimating: 5it [00:19,  2.14s/it]Extractor Estimating: 6it [00:20,  1.62s/it]Extractor Estimating: 7it [00:20,  1.28s/it]Extractor Estimating: 8it [00:21,  1.10s/it]Extractor Estimating: 9it [00:22,  1.03it/s]Extractor Estimating: 10it [00:22,  1.16it/s]Extractor Estimating: 11it [00:23,  1.04it/s]Extractor Estimating: 12it [00:24,  1.16it/s]Extractor Estimating: 13it [00:25,  1.26it/s]Extractor Estimating: 14it [00:25,  1.35it/s]Extractor Estimating: 15it [00:26,  1.42it/s]Extractor Estimating: 16it [00:27,  1.44it/s]Extractor Estimating: 17it [00:29,  1.19s/it]Extractor Estimating: 18it [00:30,  1.03s/it]Extractor Estimating: 19it [00:30,  1.10it/s]Extractor Estimating: 20it [00:31,  1.21it/s]Extractor Estimating: 21it [00:32,  1.29it/s]Extractor Estimating: 22it [00:32,  1.38it/s]Extractor Estimating: 23it [00:33,  1.45it/s]Extractor Estimating: 24it [00:34,  1.44it/s]Extractor Estimating: 25it [00:34,  1.48it/s]Extractor Estimating: 26it [00:35,  1.51it/s]Extractor Estimating: 27it [00:35,  1.54it/s]Extractor Estimating: 28it [00:36,  1.51it/s]Extractor Estimating: 29it [00:37,  1.56it/s]Extractor Estimating: 30it [00:37,  1.56it/s]Extractor Estimating: 31it [00:38,  1.58it/s]Extractor Estimating: 32it [00:39,  1.56it/s]Extractor Estimating: 33it [00:39,  1.51it/s]Extractor Estimating: 34it [00:40,  1.54it/s]Extractor Estimating: 35it [00:41,  1.54it/s]Extractor Estimating: 36it [00:41,  1.54it/s]Extractor Estimating: 37it [00:42,  1.50it/s]Extractor Estimating: 38it [00:43,  1.52it/s]Extractor Estimating: 39it [00:43,  1.50it/s]Extractor Estimating: 40it [00:44,  1.51it/s]Extractor Estimating: 41it [00:45,  1.54it/s]Extractor Estimating: 42it [00:45,  1.52it/s]Extractor Estimating: 43it [00:46,  1.55it/s]Extractor Estimating: 44it [00:46,  1.55it/s]Extractor Estimating: 45it [00:47,  1.54it/s]Extractor Estimating: 46it [00:48,  1.53it/s]Extractor Estimating: 47it [00:48,  1.53it/s]Extractor Estimating: 48it [00:49,  1.52it/s]Extractor Estimating: 49it [00:50,  1.46it/s]Extractor Estimating: 50it [00:51,  1.49it/s]Extractor Estimating: 51it [00:51,  1.51it/s]Extractor Estimating: 52it [00:52,  1.53it/s]Extractor Estimating: 53it [00:52,  1.53it/s]Extractor Estimating: 54it [00:53,  1.53it/s]Extractor Estimating: 55it [00:54,  1.50it/s]Extractor Estimating: 56it [00:54,  1.53it/s]Extractor Estimating: 57it [00:55,  1.51it/s]Extractor Estimating: 58it [00:56,  1.50it/s]Extractor Estimating: 59it [00:56,  1.50it/s]Extractor Estimating: 60it [00:57,  1.54it/s]Extractor Estimating: 61it [00:58,  1.59it/s]Extractor Estimating: 62it [00:58,  1.57it/s]Extractor Estimating: 63it [00:59,  1.55it/s]Extractor Estimating: 64it [01:00,  1.54it/s]Extractor Estimating: 65it [01:00,  1.54it/s]Extractor Estimating: 66it [01:01,  1.61it/s]Extractor Estimating: 67it [01:02,  1.57it/s]Extractor Estimating: 68it [01:02,  1.64it/s]Extractor Estimating: 69it [01:03,  1.67it/s]Extractor Estimating: 70it [01:03,  1.70it/s]Extractor Estimating: 71it [01:04,  1.69it/s]Extractor Estimating: 72it [01:04,  1.64it/s]Extractor Estimating: 73it [01:05,  1.67it/s]Extractor Estimating: 74it [01:06,  1.65it/s]Extractor Estimating: 75it [01:06,  1.51it/s]Extractor Estimating: 76it [01:07,  1.55it/s]Extractor Estimating: 77it [01:08,  1.57it/s]Extractor Estimating: 78it [01:08,  1.57it/s]Extractor Estimating: 79it [01:09,  1.61it/s]Extractor Estimating: 80it [01:10,  1.57it/s]Extractor Estimating: 81it [01:10,  1.56it/s]Extractor Estimating: 82it [01:11,  1.58it/s]Extractor Estimating: 83it [01:11,  1.56it/s]Extractor Estimating: 84it [01:12,  1.62it/s]Extractor Estimating: 85it [01:13,  1.67it/s]Extractor Estimating: 86it [01:13,  1.67it/s]Extractor Estimating: 87it [01:14,  1.67it/s]Extractor Estimating: 88it [01:14,  1.70it/s]Extractor Estimating: 89it [01:15,  1.66it/s]Extractor Estimating: 90it [01:16,  1.65it/s]Extractor Estimating: 91it [01:16,  1.65it/s]Extractor Estimating: 92it [01:17,  1.63it/s]Extractor Estimating: 93it [01:17,  1.64it/s]Extractor Estimating: 94it [01:18,  1.64it/s]Extractor Estimating: 95it [01:19,  1.67it/s]Extractor Estimating: 96it [01:19,  1.68it/s]Extractor Estimating: 97it [01:20,  1.71it/s]Extractor Estimating: 98it [01:20,  1.72it/s]Extractor Estimating: 99it [01:21,  1.73it/s]Extractor Estimating: 100it [01:22,  1.71it/s]Extractor Estimating: 101it [01:22,  1.64it/s]Extractor Estimating: 102it [01:23,  1.63it/s]Extractor Estimating: 103it [01:23,  1.58it/s]Extractor Estimating: 104it [01:24,  1.54it/s]Extractor Estimating: 105it [01:25,  1.59it/s]Extractor Estimating: 106it [01:25,  1.57it/s]Extractor Estimating: 107it [01:26,  1.58it/s]Extractor Estimating: 108it [01:27,  1.58it/s]Extractor Estimating: 109it [01:27,  1.57it/s]Extractor Estimating: 110it [01:28,  1.62it/s]Extractor Estimating: 111it [01:29,  1.58it/s]Extractor Estimating: 112it [01:29,  1.62it/s]Extractor Estimating: 113it [01:30,  1.61it/s]Extractor Estimating: 114it [01:30,  1.59it/s]Extractor Estimating: 115it [01:31,  1.59it/s]Extractor Estimating: 116it [01:32,  1.62it/s]Extractor Estimating: 117it [01:32,  1.60it/s]Extractor Estimating: 118it [01:33,  1.55it/s]Extractor Estimating: 119it [01:34,  1.52it/s]Extractor Estimating: 120it [01:34,  1.54it/s]Extractor Estimating: 121it [01:35,  1.58it/s]Extractor Estimating: 122it [01:35,  1.60it/s]Extractor Estimating: 123it [01:36,  1.60it/s]Extractor Estimating: 124it [01:37,  1.60it/s]Extractor Estimating: 125it [01:37,  1.58it/s]Extractor Estimating: 126it [01:38,  1.55it/s]Extractor Estimating: 127it [01:39,  1.59it/s]Extractor Estimating: 128it [01:39,  1.62it/s]Extractor Estimating: 129it [01:40,  1.62it/s]Extractor Estimating: 130it [01:40,  1.67it/s]Extractor Estimating: 131it [01:41,  1.58it/s]Extractor Estimating: 132it [01:42,  1.56it/s]Extractor Estimating: 133it [01:42,  1.58it/s]Extractor Estimating: 134it [01:43,  1.62it/s]Extractor Estimating: 135it [01:44,  1.62it/s]Extractor Estimating: 136it [01:44,  1.59it/s]Extractor Estimating: 137it [01:45,  1.60it/s]Extractor Estimating: 138it [01:45,  1.61it/s]Extractor Estimating: 139it [01:46,  1.62it/s]Extractor Estimating: 140it [01:47,  1.61it/s]Extractor Estimating: 141it [01:47,  1.59it/s]Extractor Estimating: 142it [01:48,  1.57it/s]Extractor Estimating: 143it [01:49,  1.46it/s]Extractor Estimating: 144it [01:49,  1.47it/s]Extractor Estimating: 145it [01:50,  1.51it/s]Extractor Estimating: 146it [01:51,  1.52it/s]Extractor Estimating: 147it [01:51,  1.54it/s]Extractor Estimating: 148it [01:52,  1.55it/s]Extractor Estimating: 149it [01:53,  1.60it/s]Extractor Estimating: 150it [01:53,  1.62it/s]Extractor Estimating: 151it [01:54,  1.65it/s]Extractor Estimating: 152it [01:54,  1.58it/s]Extractor Estimating: 153it [01:55,  1.59it/s]Extractor Estimating: 154it [01:56,  1.56it/s]Extractor Estimating: 155it [01:56,  1.57it/s]Extractor Estimating: 156it [01:57,  1.57it/s]Extractor Estimating: 157it [01:58,  1.56it/s]Extractor Estimating: 158it [01:58,  1.59it/s]Extractor Estimating: 159it [01:59,  1.57it/s]Extractor Estimating: 160it [02:00,  1.59it/s]Extractor Estimating: 161it [02:00,  1.58it/s]Extractor Estimating: 162it [02:01,  1.59it/s]Extractor Estimating: 163it [02:01,  1.56it/s]Extractor Estimating: 164it [02:02,  1.53it/s]Extractor Estimating: 165it [02:03,  1.55it/s]Extractor Estimating: 166it [02:03,  1.57it/s]Extractor Estimating: 167it [02:04,  1.56it/s]Extractor Estimating: 168it [02:05,  1.63it/s]Extractor Estimating: 169it [02:05,  1.64it/s]Extractor Estimating: 170it [02:06,  1.59it/s]Extractor Estimating: 171it [02:07,  1.56it/s]Extractor Estimating: 172it [02:07,  1.57it/s]Extractor Estimating: 173it [02:08,  1.58it/s]Extractor Estimating: 174it [02:08,  1.61it/s]Extractor Estimating: 175it [02:09,  1.60it/s]Extractor Estimating: 176it [02:10,  1.62it/s]Extractor Estimating: 177it [02:10,  1.51it/s]Extractor Estimating: 178it [02:11,  1.53it/s]Extractor Estimating: 179it [02:12,  1.54it/s]Extractor Estimating: 180it [02:12,  1.56it/s]Extractor Estimating: 181it [02:13,  1.61it/s]Extractor Estimating: 182it [02:13,  1.60it/s]Extractor Estimating: 183it [02:14,  1.62it/s]Extractor Estimating: 184it [02:15,  1.63it/s]Extractor Estimating: 185it [02:15,  1.59it/s]Extractor Estimating: 186it [02:16,  1.57it/s]Extractor Estimating: 187it [02:17,  1.59it/s]Extractor Estimating: 188it [02:17,  1.57it/s]Extractor Estimating: 189it [02:18,  1.55it/s]Extractor Estimating: 190it [02:19,  1.58it/s]Extractor Estimating: 191it [02:19,  1.61it/s]Extractor Estimating: 192it [02:20,  1.53it/s]Extractor Estimating: 193it [02:21,  1.53it/s]Extractor Estimating: 194it [02:21,  1.52it/s]Extractor Estimating: 195it [02:22,  1.54it/s]Extractor Estimating: 196it [02:22,  1.54it/s]Extractor Estimating: 197it [02:23,  1.51it/s]Extractor Estimating: 198it [02:24,  1.53it/s]Extractor Estimating: 199it [02:24,  1.55it/s]Extractor Estimating: 200it [02:25,  1.55it/s]Extractor Estimating: 201it [02:26,  1.59it/s]Extractor Estimating: 202it [02:26,  1.58it/s]Extractor Estimating: 203it [02:27,  1.59it/s]Extractor Estimating: 204it [02:28,  1.60it/s]Extractor Estimating: 205it [02:28,  1.61it/s]Extractor Estimating: 206it [02:29,  1.63it/s]Extractor Estimating: 207it [02:29,  1.63it/s]Extractor Estimating: 208it [02:30,  1.63it/s]Extractor Estimating: 209it [02:31,  1.63it/s]Extractor Estimating: 210it [02:31,  1.64it/s]Extractor Estimating: 211it [02:32,  1.65it/s]Extractor Estimating: 212it [02:32,  1.63it/s]Extractor Estimating: 213it [02:33,  1.67it/s]Extractor Estimating: 214it [02:34,  1.71it/s]Extractor Estimating: 215it [02:34,  1.72it/s]Extractor Estimating: 216it [02:35,  1.71it/s]Extractor Estimating: 217it [02:35,  1.68it/s]Extractor Estimating: 218it [02:36,  1.70it/s]Extractor Estimating: 219it [02:36,  1.71it/s]Extractor Estimating: 220it [02:37,  1.70it/s]Extractor Estimating: 221it [02:38,  1.54it/s]Extractor Estimating: 222it [02:39,  1.53it/s]Extractor Estimating: 223it [02:39,  1.54it/s]Extractor Estimating: 224it [02:40,  1.56it/s]Extractor Estimating: 225it [02:40,  1.57it/s]Extractor Estimating: 226it [02:41,  1.59it/s]Extractor Estimating: 227it [02:42,  1.58it/s]Extractor Estimating: 228it [02:42,  1.57it/s]Extractor Estimating: 229it [02:43,  1.55it/s]Extractor Estimating: 230it [02:44,  1.63it/s]Extractor Estimating: 231it [02:44,  1.58it/s]Extractor Estimating: 232it [02:45,  1.61it/s]Extractor Estimating: 233it [02:45,  1.63it/s]Extractor Estimating: 234it [02:46,  1.68it/s]Extractor Estimating: 235it [02:47,  1.64it/s]Extractor Estimating: 236it [02:47,  1.61it/s]Extractor Estimating: 237it [02:48,  1.58it/s]Extractor Estimating: 238it [02:48,  1.64it/s]Extractor Estimating: 239it [02:49,  1.60it/s]Extractor Estimating: 240it [02:50,  1.57it/s]Extractor Estimating: 241it [02:50,  1.61it/s]Extractor Estimating: 242it [02:51,  1.63it/s]Extractor Estimating: 243it [02:52,  1.62it/s]Extractor Estimating: 244it [02:55,  1.51s/it]Extractor Estimating: 245it [02:56,  1.24s/it]Extractor Estimating: 246it [02:56,  1.07s/it]Extractor Estimating: 247it [02:57,  1.08it/s]Extractor Estimating: 248it [02:58,  1.22it/s]Extractor Estimating: 249it [02:58,  1.29it/s]Extractor Estimating: 250it [02:59,  1.46it/s]Extractor Estimating: 250it [02:59,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4996 mean pseudo reward: 0.9650600055364144
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 26853
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26953, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_5_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26953, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.268, loss:2851.1972
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.006, loss:1951.5259
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.995, loss:1690.7607
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.984, loss:1562.8653
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.983, loss:1480.7400
>> valid entity prec:0.5078, rec:0.4170, f1:0.4579
>> valid relation prec:0.3659, rec:0.0174, f1:0.0333
>> valid relation with NER prec:0.3659, rec:0.0174, f1:0.0333
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.122, loss:1484.3911
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.003, loss:1333.2042
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.996, loss:1360.4963
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.983, loss:1216.9832
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.995, loss:1229.9660
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4960, rec:0.3356, f1:0.4003
>> valid relation prec:0.3730, rec:0.0132, f1:0.0255
>> valid relation with NER prec:0.3730, rec:0.0132, f1:0.0255
g_step 1100, step 55, avg_time 3.097, loss:1151.7923
g_step 1200, step 155, avg_time 0.989, loss:1142.6113
g_step 1300, step 46, avg_time 0.998, loss:1070.9933
g_step 1400, step 146, avg_time 0.999, loss:1050.2072
g_step 1500, step 37, avg_time 0.989, loss:1034.7766
>> valid entity prec:0.4520, rec:0.4162, f1:0.4334
>> valid relation prec:0.2651, rec:0.0185, f1:0.0345
>> valid relation with NER prec:0.2651, rec:0.0185, f1:0.0345
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 137, avg_time 3.116, loss:1007.2378
g_step 1700, step 28, avg_time 0.991, loss:954.7693
g_step 1800, step 128, avg_time 0.992, loss:961.0089
g_step 1900, step 19, avg_time 0.988, loss:924.3642
g_step 2000, step 119, avg_time 0.989, loss:897.9545
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4461, rec:0.3283, f1:0.3783
>> valid relation prec:0.3212, rec:0.0218, f1:0.0408
>> valid relation with NER prec:0.3212, rec:0.0218, f1:0.0408
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 10, avg_time 3.108, loss:915.4400
g_step 2200, step 110, avg_time 0.992, loss:856.5118
g_step 2300, step 1, avg_time 0.989, loss:857.6984
g_step 2400, step 101, avg_time 0.987, loss:829.0922
g_step 2500, step 201, avg_time 0.997, loss:817.4776
>> valid entity prec:0.4969, rec:0.4320, f1:0.4622
>> valid relation prec:0.2617, rec:0.0390, f1:0.0678
>> valid relation with NER prec:0.2617, rec:0.0390, f1:0.0678
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 92, avg_time 3.118, loss:756.2578
g_step 2700, step 192, avg_time 0.991, loss:829.1453
g_step 2800, step 83, avg_time 0.990, loss:743.8854
g_step 2900, step 183, avg_time 0.994, loss:782.3929
g_step 3000, step 74, avg_time 0.997, loss:754.7905
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4355, rec:0.4747, f1:0.4543
>> valid relation prec:0.2029, rec:0.0221, f1:0.0398
>> valid relation with NER prec:0.2029, rec:0.0221, f1:0.0398
g_step 3100, step 174, avg_time 3.114, loss:726.3658
g_step 3200, step 65, avg_time 0.993, loss:684.7040
g_step 3300, step 165, avg_time 0.999, loss:725.3979
g_step 3400, step 56, avg_time 0.987, loss:664.1443
g_step 3500, step 156, avg_time 1.000, loss:667.7403
>> valid entity prec:0.4583, rec:0.3103, f1:0.3701
>> valid relation prec:0.2304, rec:0.0145, f1:0.0273
>> valid relation with NER prec:0.2304, rec:0.0145, f1:0.0273
g_step 3600, step 47, avg_time 3.100, loss:668.9786
g_step 3700, step 147, avg_time 0.986, loss:671.9506
g_step 3800, step 38, avg_time 0.996, loss:617.6295
g_step 3900, step 138, avg_time 0.989, loss:624.2844
g_step 4000, step 29, avg_time 0.996, loss:636.0433
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4625, rec:0.3241, f1:0.3811
>> valid relation prec:0.2479, rec:0.0305, f1:0.0544
>> valid relation with NER prec:0.2479, rec:0.0305, f1:0.0544
g_step 4100, step 129, avg_time 3.101, loss:604.3452
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 22:47:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 22:47:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_22-47-12_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 22:47:14 - WARNING - datasets.builder -   Using custom data configuration default-bb856b8952d0a127
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-bb856b8952d0a127/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  4.78 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 22:47:15,678 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:47:15,679 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 22:47:15,679 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:47:15,680 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 22:47:15,706 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:15,724 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:15,724 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:15,724 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:15,724 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:15,724 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:15,724 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 22:47:15,990 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 22:47:19,172 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 22:47:19,172 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-bb856b8952d0a127/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 22:47:19 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x147c0111a050> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:04,  1.16ba/s] 33%|███▎      | 2/6 [00:01<00:02,  1.80ba/s] 50%|█████     | 3/6 [00:01<00:01,  2.49ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.03ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.44ba/s]100%|██████████| 6/6 [00:01<00:00,  3.19ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.15ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.10ba/s] 43%|████▎     | 3/7 [00:00<00:01,  3.50ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  3.84ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.07ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.21ba/s]100%|██████████| 7/7 [00:01<00:00,  4.45ba/s]100%|██████████| 7/7 [00:01<00:00,  3.91ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.30ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.53ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.67ba/s]100%|██████████| 6/6 [00:00<00:00, 10.70ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.95ba/s] 43%|████▎     | 3/7 [00:00<00:00,  7.58ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  8.96ba/s]100%|██████████| 7/7 [00:00<00:00,  9.88ba/s]100%|██████████| 7/7 [00:00<00:00,  8.85ba/s]
[INFO|trainer.py:414] 2023-08-27 22:47:25,751 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 22:47:25,910 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 22:47:25,910 >>   Num examples = 5052
[INFO|trainer.py:1149] 2023-08-27 22:47:25,910 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 22:47:25,910 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 22:47:25,911 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 22:47:25,911 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 22:47:25,911 >>   Total optimization steps = 395
  0%|          | 0/395 [00:00<?, ?it/s]  0%|          | 1/395 [00:01<11:27,  1.74s/it]  1%|          | 2/395 [00:02<05:50,  1.12it/s]  1%|          | 3/395 [00:02<04:02,  1.62it/s]  1%|          | 4/395 [00:02<03:11,  2.04it/s]  1%|▏         | 5/395 [00:02<02:43,  2.38it/s]  2%|▏         | 6/395 [00:03<02:26,  2.65it/s]  2%|▏         | 7/395 [00:03<02:15,  2.86it/s]  2%|▏         | 8/395 [00:03<02:08,  3.01it/s]  2%|▏         | 9/395 [00:04<02:03,  3.12it/s]  3%|▎         | 10/395 [00:04<02:00,  3.20it/s]  3%|▎         | 11/395 [00:04<01:57,  3.25it/s]  3%|▎         | 12/395 [00:04<01:56,  3.30it/s]  3%|▎         | 13/395 [00:05<01:54,  3.33it/s]  4%|▎         | 14/395 [00:05<01:53,  3.35it/s]  4%|▍         | 15/395 [00:05<01:53,  3.36it/s]  4%|▍         | 16/395 [00:06<01:52,  3.37it/s]  4%|▍         | 17/395 [00:06<01:51,  3.38it/s]  5%|▍         | 18/395 [00:06<01:51,  3.38it/s]  5%|▍         | 19/395 [00:07<01:51,  3.39it/s]  5%|▌         | 20/395 [00:07<01:50,  3.39it/s]  5%|▌         | 21/395 [00:07<01:50,  3.39it/s]  6%|▌         | 22/395 [00:07<01:50,  3.39it/s]  6%|▌         | 23/395 [00:08<01:49,  3.39it/s]  6%|▌         | 24/395 [00:08<01:54,  3.25it/s]  6%|▋         | 25/395 [00:08<01:52,  3.29it/s]  7%|▋         | 26/395 [00:09<01:51,  3.32it/s]  7%|▋         | 27/395 [00:09<01:50,  3.34it/s]  7%|▋         | 28/395 [00:09<01:49,  3.36it/s]  7%|▋         | 29/395 [00:10<01:48,  3.37it/s]  8%|▊         | 30/395 [00:10<01:48,  3.38it/s]  8%|▊         | 31/395 [00:10<01:47,  3.38it/s]  8%|▊         | 32/395 [00:10<01:47,  3.38it/s]  8%|▊         | 33/395 [00:11<01:46,  3.39it/s]  9%|▊         | 34/395 [00:11<01:46,  3.39it/s]  9%|▉         | 35/395 [00:11<01:46,  3.39it/s]  9%|▉         | 36/395 [00:12<01:45,  3.39it/s]  9%|▉         | 37/395 [00:12<01:45,  3.39it/s] 10%|▉         | 38/395 [00:12<01:45,  3.39it/s] 10%|▉         | 39/395 [00:12<01:45,  3.39it/s] 10%|█         | 40/395 [00:13<01:44,  3.39it/s] 10%|█         | 41/395 [00:13<01:44,  3.39it/s] 11%|█         | 42/395 [00:13<01:44,  3.39it/s] 11%|█         | 43/395 [00:14<01:43,  3.39it/s] 11%|█         | 44/395 [00:14<01:43,  3.39it/s] 11%|█▏        | 45/395 [00:14<01:43,  3.40it/s] 12%|█▏        | 46/395 [00:15<01:42,  3.39it/s] 12%|█▏        | 47/395 [00:15<01:42,  3.39it/s] 12%|█▏        | 48/395 [00:15<01:42,  3.39it/s] 12%|█▏        | 49/395 [00:15<01:41,  3.39it/s] 13%|█▎        | 50/395 [00:16<01:41,  3.39it/s] 13%|█▎        | 51/395 [00:16<01:41,  3.39it/s] 13%|█▎        | 52/395 [00:16<01:41,  3.39it/s] 13%|█▎        | 53/395 [00:17<01:40,  3.39it/s] 14%|█▎        | 54/395 [00:17<01:40,  3.39it/s] 14%|█▍        | 55/395 [00:17<01:40,  3.39it/s] 14%|█▍        | 56/395 [00:17<01:40,  3.39it/s] 14%|█▍        | 57/395 [00:18<01:39,  3.39it/s] 15%|█▍        | 58/395 [00:18<01:39,  3.39it/s] 15%|█▍        | 59/395 [00:18<01:39,  3.39it/s] 15%|█▌        | 60/395 [00:19<01:38,  3.39it/s] 15%|█▌        | 61/395 [00:19<01:38,  3.39it/s] 16%|█▌        | 62/395 [00:19<01:38,  3.39it/s] 16%|█▌        | 63/395 [00:20<01:38,  3.38it/s] 16%|█▌        | 64/395 [00:20<01:37,  3.38it/s] 16%|█▋        | 65/395 [00:20<01:37,  3.39it/s] 17%|█▋        | 66/395 [00:20<01:37,  3.39it/s] 17%|█▋        | 67/395 [00:21<01:36,  3.39it/s] 17%|█▋        | 68/395 [00:21<01:36,  3.39it/s] 17%|█▋        | 69/395 [00:21<01:36,  3.39it/s] 18%|█▊        | 70/395 [00:22<01:35,  3.39it/s] 18%|█▊        | 71/395 [00:22<01:35,  3.39it/s] 18%|█▊        | 72/395 [00:22<01:35,  3.39it/s] 18%|█▊        | 73/395 [00:23<01:34,  3.39it/s] 19%|█▊        | 74/395 [00:23<01:34,  3.39it/s] 19%|█▉        | 75/395 [00:23<01:34,  3.39it/s] 19%|█▉        | 76/395 [00:23<01:34,  3.38it/s] 19%|█▉        | 77/395 [00:24<01:34,  3.38it/s] 20%|█▉        | 78/395 [00:24<01:33,  3.39it/s] 20%|██        | 79/395 [00:24<01:34,  3.33it/s][INFO|trainer.py:2140] 2023-08-27 22:47:50,717 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:47:50,717 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-27 22:47:50,717 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.73it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.60it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.85it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.89it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.29it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.84it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.25it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.89it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.08it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.29it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.50it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.55it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.53it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.38it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.19it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.95it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.76it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.94it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.18it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.29it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.47it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.53it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.44it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.20it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.89it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.93it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.05it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.21it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.32it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.43it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.40it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.34it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.05it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.94it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.79it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.02it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.22it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.35it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.50it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.42it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.19it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.99it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.88it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 43.88it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.95it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.18it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.35it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.43it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.48it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.27it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.11it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.00it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.92it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.96it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.22it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.30it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.46it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.39it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.34it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.15it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 43.94it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.96it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.99it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.16it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.30it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.41it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.45it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.28it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.14it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.03it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.02it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.05it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.20it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.27it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.41it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.41it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.23it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.07it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.00it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.06it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.02it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.11it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.30it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.34it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.32it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.12it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.08it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.04it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.09it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.09it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.19it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.25it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.40it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.30it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.07it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.07it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.10it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.05it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.10it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.21it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.28it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.46it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.32it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.19it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.23it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.24it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.22it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.25it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.38it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.52it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.48it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.37it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.34it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.25it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.31it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.31it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.34it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.46it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.52it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.41it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.37it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.34it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.24it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.33it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.26it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.35it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.54it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.53it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.40it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.41it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.40it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.33it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.29it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.26it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.31it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.44it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.51it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.49it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.45it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.41it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.32it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.16it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.31it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.35it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.47it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.50it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.48it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.50it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.46it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 44.20it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.27it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.25it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.32it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.50it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.50it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.42it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.46it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.34it/s][A
 93%|█████████▎| 797/861 [00:17<00:01, 44.26it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.19it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.25it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.37it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.36it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.43it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.46it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.26it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.34it/s][A
 98%|█████████▊| 842/861 [00:18<00:00, 44.30it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.18it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.24it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.41it/s][A                                                
                                                 [A 20%|██        | 79/395 [00:44<01:34,  3.33it/s]
100%|██████████| 861/861 [00:19<00:00, 44.41it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:48:10,355 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79
[INFO|configuration_utils.py:351] 2023-08-27 22:48:10,583 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:48:13,829 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:48:13,909 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:48:13,940 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79/special_tokens_map.json
 20%|██        | 80/395 [00:53<46:34,  8.87s/it] 21%|██        | 81/395 [00:53<32:57,  6.30s/it] 21%|██        | 82/395 [00:54<23:27,  4.50s/it] 21%|██        | 83/395 [00:54<16:49,  3.24s/it] 21%|██▏       | 84/395 [00:54<12:11,  2.35s/it] 22%|██▏       | 85/395 [00:55<08:58,  1.74s/it] 22%|██▏       | 86/395 [00:55<06:42,  1.30s/it] 22%|██▏       | 87/395 [00:55<05:08,  1.00s/it] 22%|██▏       | 88/395 [00:56<04:03,  1.26it/s] 23%|██▎       | 89/395 [00:56<03:16,  1.56it/s] 23%|██▎       | 90/395 [00:56<02:44,  1.86it/s] 23%|██▎       | 91/395 [00:56<02:21,  2.15it/s] 23%|██▎       | 92/395 [00:57<02:05,  2.42it/s] 24%|██▎       | 93/395 [00:57<01:54,  2.64it/s] 24%|██▍       | 94/395 [00:57<01:46,  2.83it/s] 24%|██▍       | 95/395 [00:58<01:40,  2.98it/s] 24%|██▍       | 96/395 [00:58<01:36,  3.09it/s] 25%|██▍       | 97/395 [00:58<01:33,  3.18it/s] 25%|██▍       | 98/395 [00:58<01:31,  3.24it/s] 25%|██▌       | 99/395 [00:59<01:30,  3.25it/s] 25%|██▌       | 100/395 [00:59<01:29,  3.30it/s] 26%|██▌       | 101/395 [00:59<01:28,  3.33it/s] 26%|██▌       | 102/395 [01:00<01:27,  3.34it/s] 26%|██▌       | 103/395 [01:00<01:26,  3.36it/s] 26%|██▋       | 104/395 [01:00<01:25,  3.39it/s] 27%|██▋       | 105/395 [01:01<01:25,  3.40it/s] 27%|██▋       | 106/395 [01:01<01:24,  3.42it/s] 27%|██▋       | 107/395 [01:01<01:24,  3.43it/s] 27%|██▋       | 108/395 [01:01<01:23,  3.43it/s] 28%|██▊       | 109/395 [01:02<01:23,  3.44it/s] 28%|██▊       | 110/395 [01:02<01:26,  3.29it/s] 28%|██▊       | 111/395 [01:02<01:25,  3.34it/s] 28%|██▊       | 112/395 [01:03<01:23,  3.37it/s] 29%|██▊       | 113/395 [01:03<01:23,  3.39it/s] 29%|██▉       | 114/395 [01:03<01:22,  3.41it/s] 29%|██▉       | 115/395 [01:03<01:21,  3.42it/s] 29%|██▉       | 116/395 [01:04<01:21,  3.43it/s] 30%|██▉       | 117/395 [01:04<01:21,  3.43it/s] 30%|██▉       | 118/395 [01:04<01:20,  3.44it/s] 30%|███       | 119/395 [01:05<01:20,  3.44it/s] 30%|███       | 120/395 [01:05<01:19,  3.44it/s] 31%|███       | 121/395 [01:05<01:24,  3.24it/s] 31%|███       | 122/395 [01:06<01:22,  3.30it/s] 31%|███       | 123/395 [01:06<01:21,  3.34it/s] 31%|███▏      | 124/395 [01:06<01:20,  3.37it/s] 32%|███▏      | 125/395 [01:06<01:19,  3.39it/s] 32%|███▏      | 126/395 [01:07<01:18,  3.41it/s] 32%|███▏      | 127/395 [01:07<01:18,  3.42it/s] 32%|███▏      | 128/395 [01:07<01:18,  3.42it/s] 33%|███▎      | 129/395 [01:08<01:17,  3.43it/s] 33%|███▎      | 130/395 [01:08<01:17,  3.43it/s] 33%|███▎      | 131/395 [01:08<01:16,  3.43it/s] 33%|███▎      | 132/395 [01:09<01:18,  3.36it/s] 34%|███▎      | 133/395 [01:09<01:17,  3.38it/s] 34%|███▍      | 134/395 [01:09<01:16,  3.40it/s] 34%|███▍      | 135/395 [01:09<01:16,  3.41it/s] 34%|███▍      | 136/395 [01:10<01:15,  3.42it/s] 35%|███▍      | 137/395 [01:10<01:15,  3.43it/s] 35%|███▍      | 138/395 [01:10<01:14,  3.43it/s] 35%|███▌      | 139/395 [01:11<01:14,  3.43it/s] 35%|███▌      | 140/395 [01:11<01:14,  3.44it/s] 36%|███▌      | 141/395 [01:11<01:13,  3.43it/s] 36%|███▌      | 142/395 [01:11<01:13,  3.44it/s] 36%|███▌      | 143/395 [01:12<01:15,  3.35it/s] 36%|███▋      | 144/395 [01:12<01:14,  3.38it/s] 37%|███▋      | 145/395 [01:12<01:13,  3.40it/s] 37%|███▋      | 146/395 [01:13<01:13,  3.41it/s] 37%|███▋      | 147/395 [01:13<01:12,  3.42it/s] 37%|███▋      | 148/395 [01:13<01:12,  3.42it/s] 38%|███▊      | 149/395 [01:13<01:11,  3.43it/s] 38%|███▊      | 150/395 [01:14<01:11,  3.43it/s] 38%|███▊      | 151/395 [01:14<01:11,  3.43it/s] 38%|███▊      | 152/395 [01:14<01:10,  3.44it/s] 39%|███▊      | 153/395 [01:15<01:10,  3.44it/s] 39%|███▉      | 154/395 [01:15<01:11,  3.38it/s] 39%|███▉      | 155/395 [01:15<01:10,  3.39it/s] 39%|███▉      | 156/395 [01:16<01:10,  3.41it/s] 40%|███▉      | 157/395 [01:16<01:09,  3.42it/s] 40%|████      | 158/395 [01:16<01:08,  3.47it/s][INFO|trainer.py:2140] 2023-08-27 22:48:42,510 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:48:42,511 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-27 22:48:42,511 >>   Batch size = 8
{'eval_loss': 0.9813340902328491, 'eval_runtime': 19.4446, 'eval_samples_per_second': 354.032, 'eval_steps_per_second': 44.28, 'epoch': 1.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.58it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.19it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.49it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.61it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.15it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.81it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.70it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.51it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.65it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.71it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.58it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.55it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.34it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.26it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.75it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.96it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.10it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.20it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.32it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.44it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.29it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.36it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.20it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.22it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.34it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.38it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.38it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.48it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.47it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.43it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.37it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.18it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.26it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.29it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.47it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.43it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.56it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.53it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.38it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.37it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.23it/s][A
 25%|██▍       | 212/861 [00:04<00:15, 42.45it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.26it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 43.65it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.97it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.07it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.19it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.20it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.17it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.03it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.10it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.22it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.43it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.54it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.65it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.52it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.29it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.27it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.16it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.21it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.37it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.43it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.63it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.64it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.38it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.35it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.17it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.22it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.29it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.29it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.52it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.66it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.49it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.41it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.31it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.22it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.28it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.23it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.41it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.49it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.49it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.49it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.42it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.33it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.31it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.17it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.26it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 43.18it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.65it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.02it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.00it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.16it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.20it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.13it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.07it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.11it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.19it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.38it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.49it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.58it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.46it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.30it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.20it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.18it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.00it/s][A
 62%|██████▏   | 532/861 [00:11<00:07, 44.25it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.40it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.47it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.51it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.49it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.45it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.29it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.21it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.22it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 42.91it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.45it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.89it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.09it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.11it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.15it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.15it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.09it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 43.99it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.14it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.28it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.45it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.59it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.60it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.43it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.30it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.19it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.15it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.26it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.36it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.46it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.61it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.51it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.37it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.31it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.10it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.07it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.58it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.97it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.19it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.41it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.40it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.34it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.21it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.12it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 44.06it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.12it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.31it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.48it/s][A
 90%|████████▉ | 772/861 [00:17<00:01, 44.53it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.54it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.32it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.21it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.11it/s][A
 93%|█████████▎| 797/861 [00:17<00:01, 44.17it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.20it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.33it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.51it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.50it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.52it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.43it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.25it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.18it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.16it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.28it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.69it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.06it/s][A                                                 
                                                 [A 40%|████      | 158/395 [01:36<01:08,  3.47it/s]
100%|██████████| 861/861 [00:19<00:00, 44.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:49:02,030 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-27 22:49:02,105 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:49:05,834 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:49:05,916 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:49:05,937 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158/special_tokens_map.json
 40%|████      | 159/395 [01:45<35:25,  9.01s/it] 41%|████      | 160/395 [01:46<25:04,  6.40s/it] 41%|████      | 161/395 [01:46<17:49,  4.57s/it] 41%|████      | 162/395 [01:46<12:45,  3.29s/it] 41%|████▏     | 163/395 [01:47<09:14,  2.39s/it] 42%|████▏     | 164/395 [01:47<06:46,  1.76s/it] 42%|████▏     | 165/395 [01:47<05:03,  1.32s/it] 42%|████▏     | 166/395 [01:48<03:52,  1.01s/it] 42%|████▏     | 167/395 [01:48<03:01,  1.25it/s] 43%|████▎     | 168/395 [01:48<02:47,  1.36it/s] 43%|████▎     | 169/395 [01:49<02:16,  1.66it/s] 43%|████▎     | 170/395 [01:49<01:58,  1.90it/s] 43%|████▎     | 171/395 [01:49<01:42,  2.19it/s] 44%|████▎     | 172/395 [01:50<01:31,  2.45it/s] 44%|████▍     | 173/395 [01:50<01:23,  2.67it/s] 44%|████▍     | 174/395 [01:50<01:17,  2.85it/s] 44%|████▍     | 175/395 [01:51<01:13,  3.00it/s] 45%|████▍     | 176/395 [01:51<01:10,  3.10it/s] 45%|████▍     | 177/395 [01:51<01:08,  3.18it/s] 45%|████▌     | 178/395 [01:51<01:06,  3.24it/s] 45%|████▌     | 179/395 [01:52<01:05,  3.29it/s] 46%|████▌     | 180/395 [01:52<01:06,  3.23it/s] 46%|████▌     | 181/395 [01:52<01:05,  3.28it/s] 46%|████▌     | 182/395 [01:53<01:04,  3.31it/s] 46%|████▋     | 183/395 [01:53<01:03,  3.33it/s] 47%|████▋     | 184/395 [01:53<01:03,  3.34it/s] 47%|████▋     | 185/395 [01:54<01:02,  3.36it/s] 47%|████▋     | 186/395 [01:54<01:02,  3.37it/s] 47%|████▋     | 187/395 [01:54<01:01,  3.37it/s] 48%|████▊     | 188/395 [01:54<01:01,  3.37it/s] 48%|████▊     | 189/395 [01:55<01:00,  3.38it/s] 48%|████▊     | 190/395 [01:55<01:00,  3.38it/s] 48%|████▊     | 191/395 [01:55<01:00,  3.38it/s] 49%|████▊     | 192/395 [01:56<01:00,  3.38it/s] 49%|████▉     | 193/395 [01:56<00:59,  3.38it/s] 49%|████▉     | 194/395 [01:56<00:59,  3.38it/s] 49%|████▉     | 195/395 [01:56<00:59,  3.39it/s] 50%|████▉     | 196/395 [01:57<00:58,  3.38it/s] 50%|████▉     | 197/395 [01:57<00:58,  3.36it/s] 50%|█████     | 198/395 [01:57<00:58,  3.37it/s] 50%|█████     | 199/395 [01:58<00:58,  3.36it/s] 51%|█████     | 200/395 [01:58<00:57,  3.37it/s] 51%|█████     | 201/395 [01:58<00:57,  3.38it/s] 51%|█████     | 202/395 [01:59<00:57,  3.38it/s] 51%|█████▏    | 203/395 [01:59<00:56,  3.38it/s] 52%|█████▏    | 204/395 [01:59<00:56,  3.38it/s] 52%|█████▏    | 205/395 [01:59<00:56,  3.38it/s] 52%|█████▏    | 206/395 [02:00<00:55,  3.39it/s] 52%|█████▏    | 207/395 [02:00<00:55,  3.38it/s] 53%|█████▎    | 208/395 [02:00<00:57,  3.28it/s] 53%|█████▎    | 209/395 [02:01<00:56,  3.31it/s] 53%|█████▎    | 210/395 [02:01<00:55,  3.33it/s] 53%|█████▎    | 211/395 [02:01<00:54,  3.35it/s] 54%|█████▎    | 212/395 [02:02<00:54,  3.36it/s] 54%|█████▍    | 213/395 [02:02<00:54,  3.37it/s] 54%|█████▍    | 214/395 [02:02<00:53,  3.37it/s] 54%|█████▍    | 215/395 [02:02<00:53,  3.38it/s] 55%|█████▍    | 216/395 [02:03<00:52,  3.39it/s] 55%|█████▍    | 217/395 [02:03<00:52,  3.40it/s] 55%|█████▌    | 218/395 [02:03<00:51,  3.41it/s] 55%|█████▌    | 219/395 [02:04<00:53,  3.29it/s] 56%|█████▌    | 220/395 [02:04<00:52,  3.33it/s] 56%|█████▌    | 221/395 [02:04<00:51,  3.36it/s] 56%|█████▌    | 222/395 [02:05<00:51,  3.38it/s] 56%|█████▋    | 223/395 [02:05<00:50,  3.40it/s] 57%|█████▋    | 224/395 [02:05<00:50,  3.41it/s] 57%|█████▋    | 225/395 [02:05<00:49,  3.42it/s] 57%|█████▋    | 226/395 [02:06<00:49,  3.42it/s] 57%|█████▋    | 227/395 [02:06<00:49,  3.43it/s] 58%|█████▊    | 228/395 [02:06<00:48,  3.43it/s] 58%|█████▊    | 229/395 [02:07<00:48,  3.43it/s] 58%|█████▊    | 230/395 [02:07<00:52,  3.16it/s] 58%|█████▊    | 231/395 [02:07<00:50,  3.24it/s] 59%|█████▊    | 232/395 [02:07<00:49,  3.30it/s] 59%|█████▉    | 233/395 [02:08<00:48,  3.34it/s] 59%|█████▉    | 234/395 [02:08<00:47,  3.36it/s] 59%|█████▉    | 235/395 [02:08<00:47,  3.39it/s] 60%|█████▉    | 236/395 [02:09<00:46,  3.40it/s] 60%|██████    | 237/395 [02:09<00:45,  3.46it/s][INFO|trainer.py:2140] 2023-08-27 22:49:35,355 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:49:35,355 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-27 22:49:35,355 >>   Batch size = 8
{'eval_loss': 0.963557779788971, 'eval_runtime': 19.4522, 'eval_samples_per_second': 353.893, 'eval_steps_per_second': 44.262, 'epoch': 2.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.13it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.56it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.61it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.68it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.17it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.74it/s][A
  4%|▍         | 37/861 [00:00<00:19, 42.46it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.16it/s][A
  5%|▌         | 47/861 [00:01<00:18, 43.57it/s][A
  6%|▌         | 52/861 [00:01<00:18, 43.96it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.27it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.30it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.23it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.19it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.97it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.99it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.21it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.35it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.44it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.50it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.52it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.49it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.23it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.07it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.14it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.21it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.32it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.51it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.48it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.59it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.45it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.26it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.18it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.13it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.21it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.37it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.38it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.48it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.56it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.34it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.30it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.22it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.32it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.35it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.45it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.46it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.48it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.50it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.33it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.30it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.25it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.29it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.28it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.44it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.32it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.48it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.45it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.29it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.33it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.32it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.31it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.29it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.34it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.38it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.40it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.40it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.35it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.31it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.43it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.48it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.36it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.46it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.36it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.38it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.39it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.28it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.25it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.40it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.37it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.35it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.40it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.36it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.49it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.40it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.29it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.34it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.37it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.32it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.34it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.35it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.35it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.33it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.16it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.42it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.43it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.36it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.39it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.34it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.31it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.41it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.25it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.38it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.36it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.36it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.39it/s][A
 62%|██████▏   | 532/861 [00:11<00:07, 44.45it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.27it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.47it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.42it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.30it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.45it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.31it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.42it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.40it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 42.76it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.32it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.72it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 43.79it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.05it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.17it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.23it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.32it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.14it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.21it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.31it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.32it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.41it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.36it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.32it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.43it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.39it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.30it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.33it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.33it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.49it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.38it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.38it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.43it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.38it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.33it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.37it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.26it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.30it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.37it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.40it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.41it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.40it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.36it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.27it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 44.33it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.43it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.43it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.34it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.46it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.37it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.36it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.35it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.25it/s][A
 93%|█████████▎| 797/861 [00:17<00:01, 44.34it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.38it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.34it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.43it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.39it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.30it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.38it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.27it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.16it/s][A
 98%|█████████▊| 842/861 [00:18<00:00, 44.28it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.43it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.40it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.41it/s][A                                                 
                                                 [A 60%|██████    | 237/395 [02:28<00:45,  3.46it/s]
100%|██████████| 861/861 [00:19<00:00, 44.41it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:49:54,826 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237
[INFO|configuration_utils.py:351] 2023-08-27 22:49:54,922 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:49:57,379 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:49:57,476 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:49:57,529 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237/special_tokens_map.json
 60%|██████    | 238/395 [02:37<22:45,  8.70s/it] 61%|██████    | 239/395 [02:38<16:04,  6.18s/it] 61%|██████    | 240/395 [02:38<11:24,  4.42s/it] 61%|██████    | 241/395 [02:38<08:09,  3.18s/it] 61%|██████▏   | 242/395 [02:38<05:54,  2.31s/it] 62%|██████▏   | 243/395 [02:39<04:19,  1.71s/it] 62%|██████▏   | 244/395 [02:39<03:13,  1.28s/it] 62%|██████▏   | 245/395 [02:39<02:28,  1.01it/s] 62%|██████▏   | 246/395 [02:40<01:56,  1.28it/s] 63%|██████▎   | 247/395 [02:40<01:33,  1.58it/s] 63%|██████▎   | 248/395 [02:40<01:18,  1.88it/s] 63%|██████▎   | 249/395 [02:41<01:07,  2.17it/s] 63%|██████▎   | 250/395 [02:41<01:01,  2.36it/s] 64%|██████▎   | 251/395 [02:41<00:55,  2.60it/s] 64%|██████▍   | 252/395 [02:41<00:51,  2.80it/s] 64%|██████▍   | 253/395 [02:42<00:48,  2.95it/s] 64%|██████▍   | 254/395 [02:42<00:45,  3.07it/s] 65%|██████▍   | 255/395 [02:42<00:44,  3.16it/s] 65%|██████▍   | 256/395 [02:43<00:43,  3.23it/s] 65%|██████▌   | 257/395 [02:43<00:42,  3.27it/s] 65%|██████▌   | 258/395 [02:43<00:41,  3.31it/s] 66%|██████▌   | 259/395 [02:44<00:40,  3.33it/s] 66%|██████▌   | 260/395 [02:44<00:40,  3.35it/s] 66%|██████▌   | 261/395 [02:44<00:40,  3.28it/s] 66%|██████▋   | 262/395 [02:44<00:40,  3.32it/s] 67%|██████▋   | 263/395 [02:45<00:40,  3.25it/s] 67%|██████▋   | 264/395 [02:45<00:39,  3.29it/s] 67%|██████▋   | 265/395 [02:45<00:39,  3.32it/s] 67%|██████▋   | 266/395 [02:46<00:38,  3.35it/s] 68%|██████▊   | 267/395 [02:46<00:38,  3.36it/s] 68%|██████▊   | 268/395 [02:46<00:37,  3.37it/s] 68%|██████▊   | 269/395 [02:47<00:37,  3.38it/s] 68%|██████▊   | 270/395 [02:47<00:36,  3.38it/s] 69%|██████▊   | 271/395 [02:47<00:37,  3.33it/s] 69%|██████▉   | 272/395 [02:47<00:36,  3.35it/s] 69%|██████▉   | 273/395 [02:48<00:36,  3.37it/s] 69%|██████▉   | 274/395 [02:48<00:35,  3.37it/s] 70%|██████▉   | 275/395 [02:48<00:40,  2.97it/s] 70%|██████▉   | 276/395 [02:49<00:38,  3.09it/s] 70%|███████   | 277/395 [02:49<00:37,  3.17it/s] 70%|███████   | 278/395 [02:49<00:36,  3.24it/s] 71%|███████   | 279/395 [02:50<00:35,  3.29it/s] 71%|███████   | 280/395 [02:50<00:34,  3.32it/s] 71%|███████   | 281/395 [02:50<00:34,  3.28it/s] 71%|███████▏  | 282/395 [02:51<00:34,  3.31it/s] 72%|███████▏  | 283/395 [02:51<00:33,  3.34it/s] 72%|███████▏  | 284/395 [02:51<00:33,  3.35it/s] 72%|███████▏  | 285/395 [02:51<00:32,  3.37it/s] 72%|███████▏  | 286/395 [02:52<00:32,  3.38it/s] 73%|███████▎  | 287/395 [02:52<00:31,  3.38it/s] 73%|███████▎  | 288/395 [02:52<00:31,  3.39it/s] 73%|███████▎  | 289/395 [02:53<00:31,  3.39it/s] 73%|███████▎  | 290/395 [02:53<00:30,  3.39it/s] 74%|███████▎  | 291/395 [02:53<00:30,  3.39it/s] 74%|███████▍  | 292/395 [02:53<00:30,  3.32it/s] 74%|███████▍  | 293/395 [02:54<00:30,  3.35it/s] 74%|███████▍  | 294/395 [02:54<00:30,  3.36it/s] 75%|███████▍  | 295/395 [02:54<00:29,  3.37it/s] 75%|███████▍  | 296/395 [02:55<00:29,  3.38it/s] 75%|███████▌  | 297/395 [02:55<00:28,  3.38it/s] 75%|███████▌  | 298/395 [02:55<00:28,  3.38it/s] 76%|███████▌  | 299/395 [02:56<00:28,  3.39it/s] 76%|███████▌  | 300/395 [02:56<00:28,  3.39it/s] 76%|███████▌  | 301/395 [02:56<00:27,  3.39it/s] 76%|███████▋  | 302/395 [02:56<00:27,  3.39it/s] 77%|███████▋  | 303/395 [02:57<00:27,  3.39it/s] 77%|███████▋  | 304/395 [02:57<00:26,  3.39it/s] 77%|███████▋  | 305/395 [02:57<00:26,  3.39it/s] 77%|███████▋  | 306/395 [02:58<00:26,  3.39it/s] 78%|███████▊  | 307/395 [02:58<00:25,  3.39it/s] 78%|███████▊  | 308/395 [02:58<00:25,  3.39it/s] 78%|███████▊  | 309/395 [02:58<00:25,  3.36it/s] 78%|███████▊  | 310/395 [02:59<00:25,  3.37it/s] 79%|███████▊  | 311/395 [02:59<00:24,  3.38it/s] 79%|███████▉  | 312/395 [02:59<00:24,  3.38it/s] 79%|███████▉  | 313/395 [03:00<00:24,  3.38it/s] 79%|███████▉  | 314/395 [03:00<00:23,  3.39it/s] 80%|███████▉  | 315/395 [03:00<00:23,  3.39it/s] 80%|████████  | 316/395 [03:01<00:22,  3.44it/s][INFO|trainer.py:2140] 2023-08-27 22:50:26,954 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:50:26,954 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-27 22:50:26,954 >>   Batch size = 8
{'eval_loss': 0.9688735008239746, 'eval_runtime': 19.4298, 'eval_samples_per_second': 354.302, 'eval_steps_per_second': 44.313, 'epoch': 3.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.88it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.14it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.58it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.60it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.17it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.96it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.37it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.31it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.41it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.49it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.53it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.57it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.44it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.48it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.33it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.22it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.23it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.32it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.43it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.55it/s][A
 12%|█▏        | 107/861 [00:02<00:19, 39.30it/s][A
 13%|█▎        | 112/861 [00:02<00:18, 41.05it/s][A
 14%|█▎        | 117/861 [00:02<00:17, 42.13it/s][A
 14%|█▍        | 122/861 [00:02<00:17, 42.88it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.46it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.73it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.05it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.19it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.13it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 43.85it/s][A
 18%|█▊        | 157/861 [00:03<00:16, 43.82it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.05it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.20it/s][A
 20%|█▉        | 172/861 [00:03<00:16, 42.57it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.22it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.70it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.01it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 43.95it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 43.86it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 43.82it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 43.90it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.02it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.24it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.33it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.57it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.62it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.52it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.29it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.11it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.13it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.27it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.37it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.45it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.52it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.71it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.48it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.28it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.18it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.27it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.28it/s][A
 36%|███▌      | 307/861 [00:06<00:13, 42.52it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.21it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.67it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.00it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.22it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.28it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.21it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.09it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.99it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.08it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.31it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.43it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.59it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.50it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.46it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.36it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.22it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.13it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.22it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.34it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.37it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.53it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.62it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.47it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.40it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.17it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.22it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 41.97it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 42.87it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.41it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.90it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.17it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.26it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.23it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.10it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 43.86it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.96it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.18it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.42it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.49it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.62it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.53it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.37it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.25it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.07it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.96it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.19it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.31it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.42it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.72it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.48it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.31it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.30it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.18it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 42.88it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.45it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.91it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.10it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.29it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.31it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.23it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.20it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 43.91it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.12it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.39it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.51it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.53it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.57it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.44it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.48it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.12it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.98it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.15it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.36it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.49it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.59it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.56it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.46it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.34it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.13it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 44.02it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.16it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.50it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.10it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.34it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.47it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.46it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.35it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.12it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.93it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.05it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.25it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.40it/s][A
 90%|████████▉ | 772/861 [00:17<00:01, 44.50it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.57it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.49it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.34it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.11it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.05it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.14it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.18it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.39it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.55it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.66it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.52it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.30it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.16it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.09it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.15it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.19it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.33it/s][A                                                 
                                                 [A 80%|████████  | 316/395 [03:20<00:22,  3.44it/s]
100%|██████████| 861/861 [00:19<00:00, 44.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:50:46,535 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-27 22:50:46,666 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:50:49,621 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:50:49,697 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:50:49,762 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316/special_tokens_map.json
 80%|████████  | 317/395 [03:28<11:02,  8.50s/it] 81%|████████  | 318/395 [03:28<07:44,  6.04s/it] 81%|████████  | 319/395 [03:29<05:27,  4.31s/it] 81%|████████  | 320/395 [03:29<03:53,  3.11s/it] 81%|████████▏ | 321/395 [03:29<02:47,  2.27s/it] 82%|████████▏ | 322/395 [03:30<02:02,  1.67s/it] 82%|████████▏ | 323/395 [03:30<01:30,  1.26s/it] 82%|████████▏ | 324/395 [03:30<01:08,  1.03it/s] 82%|████████▏ | 325/395 [03:31<00:53,  1.30it/s] 83%|████████▎ | 326/395 [03:31<00:43,  1.60it/s] 83%|████████▎ | 327/395 [03:31<00:35,  1.90it/s] 83%|████████▎ | 328/395 [03:31<00:30,  2.19it/s] 83%|████████▎ | 329/395 [03:32<00:26,  2.45it/s] 84%|████████▎ | 330/395 [03:32<00:24,  2.67it/s] 84%|████████▍ | 331/395 [03:32<00:22,  2.84it/s] 84%|████████▍ | 332/395 [03:33<00:21,  2.99it/s] 84%|████████▍ | 333/395 [03:33<00:20,  3.10it/s] 85%|████████▍ | 334/395 [03:33<00:19,  3.18it/s] 85%|████████▍ | 335/395 [03:34<00:18,  3.24it/s] 85%|████████▌ | 336/395 [03:34<00:17,  3.28it/s] 85%|████████▌ | 337/395 [03:34<00:17,  3.32it/s] 86%|████████▌ | 338/395 [03:34<00:17,  3.34it/s] 86%|████████▌ | 339/395 [03:35<00:16,  3.36it/s] 86%|████████▌ | 340/395 [03:35<00:16,  3.36it/s] 86%|████████▋ | 341/395 [03:35<00:16,  3.37it/s] 87%|████████▋ | 342/395 [03:36<00:16,  3.30it/s] 87%|████████▋ | 343/395 [03:36<00:15,  3.33it/s] 87%|████████▋ | 344/395 [03:36<00:15,  3.35it/s] 87%|████████▋ | 345/395 [03:36<00:14,  3.36it/s] 88%|████████▊ | 346/395 [03:37<00:14,  3.37it/s] 88%|████████▊ | 347/395 [03:37<00:14,  3.38it/s] 88%|████████▊ | 348/395 [03:37<00:13,  3.38it/s] 88%|████████▊ | 349/395 [03:38<00:13,  3.38it/s] 89%|████████▊ | 350/395 [03:38<00:13,  3.38it/s] 89%|████████▉ | 351/395 [03:38<00:13,  3.38it/s] 89%|████████▉ | 352/395 [03:39<00:12,  3.38it/s] 89%|████████▉ | 353/395 [03:39<00:12,  3.30it/s] 90%|████████▉ | 354/395 [03:39<00:12,  3.33it/s] 90%|████████▉ | 355/395 [03:39<00:11,  3.34it/s] 90%|█████████ | 356/395 [03:40<00:11,  3.36it/s] 90%|█████████ | 357/395 [03:40<00:11,  3.37it/s] 91%|█████████ | 358/395 [03:40<00:10,  3.37it/s] 91%|█████████ | 359/395 [03:41<00:10,  3.38it/s] 91%|█████████ | 360/395 [03:41<00:10,  3.38it/s] 91%|█████████▏| 361/395 [03:41<00:10,  3.39it/s] 92%|█████████▏| 362/395 [03:42<00:09,  3.39it/s] 92%|█████████▏| 363/395 [03:42<00:09,  3.39it/s] 92%|█████████▏| 364/395 [03:42<00:09,  3.39it/s] 92%|█████████▏| 365/395 [03:42<00:08,  3.41it/s] 93%|█████████▎| 366/395 [03:43<00:08,  3.41it/s] 93%|█████████▎| 367/395 [03:43<00:08,  3.42it/s] 93%|█████████▎| 368/395 [03:43<00:07,  3.43it/s] 93%|█████████▎| 369/395 [03:44<00:07,  3.43it/s] 94%|█████████▎| 370/395 [03:44<00:07,  3.43it/s] 94%|█████████▍| 371/395 [03:44<00:06,  3.44it/s] 94%|█████████▍| 372/395 [03:44<00:06,  3.43it/s] 94%|█████████▍| 373/395 [03:45<00:06,  3.43it/s] 95%|█████████▍| 374/395 [03:45<00:06,  3.43it/s] 95%|█████████▍| 375/395 [03:45<00:05,  3.40it/s] 95%|█████████▌| 376/395 [03:46<00:05,  3.41it/s] 95%|█████████▌| 377/395 [03:46<00:05,  3.42it/s] 96%|█████████▌| 378/395 [03:46<00:04,  3.43it/s] 96%|█████████▌| 379/395 [03:46<00:04,  3.43it/s] 96%|█████████▌| 380/395 [03:47<00:04,  3.43it/s] 96%|█████████▋| 381/395 [03:47<00:04,  3.43it/s] 97%|█████████▋| 382/395 [03:47<00:03,  3.43it/s] 97%|█████████▋| 383/395 [03:48<00:03,  3.43it/s] 97%|█████████▋| 384/395 [03:48<00:03,  3.43it/s] 97%|█████████▋| 385/395 [03:48<00:02,  3.43it/s] 98%|█████████▊| 386/395 [03:49<00:02,  3.40it/s] 98%|█████████▊| 387/395 [03:49<00:02,  3.41it/s] 98%|█████████▊| 388/395 [03:49<00:02,  3.42it/s] 98%|█████████▊| 389/395 [03:49<00:01,  3.43it/s] 99%|█████████▊| 390/395 [03:50<00:01,  3.43it/s] 99%|█████████▉| 391/395 [03:50<00:01,  3.43it/s] 99%|█████████▉| 392/395 [03:50<00:00,  3.43it/s] 99%|█████████▉| 393/395 [03:51<00:00,  3.44it/s]100%|█████████▉| 394/395 [03:51<00:00,  3.44it/s]100%|██████████| 395/395 [03:51<00:00,  3.48it/s][INFO|trainer.py:2140] 2023-08-27 22:51:17,551 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:51:17,551 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-27 22:51:17,551 >>   Batch size = 8
{'eval_loss': 0.9726260900497437, 'eval_runtime': 19.5139, 'eval_samples_per_second': 352.774, 'eval_steps_per_second': 44.122, 'epoch': 4.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.00it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.34it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.58it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.67it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.22it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.86it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.64it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.38it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.49it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.57it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.65it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.58it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.49it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.37it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.29it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.25it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.18it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.35it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.40it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.60it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.56it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.49it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.34it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.23it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.09it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.16it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.37it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.49it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.60it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.46it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.37it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.28it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.22it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.11it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.32it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.36it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.44it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.57it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.49it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.37it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.32it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.17it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.18it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.29it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.43it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.45it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.45it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.39it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.41it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.29it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.17it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.28it/s][A
 31%|███       | 267/861 [00:05<00:13, 44.30it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.36it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.47it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.43it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.42it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.36it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.14it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.25it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.39it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.30it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.40it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.45it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.32it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.41it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.32it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.26it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.28it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.35it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.37it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.36it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.15it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.27it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.23it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.20it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.22it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.22it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.22it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.39it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.34it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.35it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.46it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.34it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.31it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.31it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.20it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.36it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.45it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.32it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.43it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.43it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.38it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.33it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.24it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.28it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.32it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.34it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.35it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.42it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.40it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.42it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.23it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.33it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.33it/s][A
 62%|██████▏   | 532/861 [00:11<00:07, 44.28it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.40it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.34it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.30it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.41it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.42it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.24it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.31it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.35it/s][A
 67%|██████▋   | 577/861 [00:12<00:06, 44.37it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.38it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.30it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.22it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.41it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.39it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.26it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.41it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.43it/s][A
 72%|███████▏  | 622/861 [00:13<00:05, 44.46it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.31it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.35it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.40it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.46it/s][A
 75%|███████▌  | 647/861 [00:14<00:05, 41.34it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 42.39it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 43.14it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 43.54it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 43.74it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 43.88it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 43.99it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.15it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 43.93it/s][A
 80%|████████  | 692/861 [00:15<00:03, 43.99it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.25it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.49it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.44it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.44it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.29it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.38it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.25it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.09it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.20it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.26it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.31it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 44.54it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.43it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.42it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.38it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.24it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.21it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 42.76it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 43.31it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 43.74it/s][A
 93%|█████████▎| 797/861 [00:17<00:01, 43.95it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.12it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.14it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.17it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.10it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 43.98it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.12it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.21it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.41it/s][A
 98%|█████████▊| 842/861 [00:18<00:00, 44.49it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.46it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.34it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.34it/s][A                                                 
                                                 [A100%|██████████| 395/395 [04:11<00:00,  3.48it/s]
100%|██████████| 861/861 [00:19<00:00, 44.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:51:37,265 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395
[INFO|configuration_utils.py:351] 2023-08-27 22:51:37,458 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:51:41,182 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:51:41,429 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:51:41,559 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 22:51:47,400 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 22:51:47,410 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158 (score: 0.963557779788971).
                                                 100%|██████████| 395/395 [04:29<00:00,  3.48it/s]100%|██████████| 395/395 [04:29<00:00,  1.47it/s]
[INFO|trainer.py:1894] 2023-08-27 22:51:55,020 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 22:51:55,380 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:51:59,940 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:52:00,046 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:52:00,087 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:52:00,507 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:00,507 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:00,507 >>   train_loss               =     0.7295
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:00,507 >>   train_runtime            = 0:04:29.08
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:00,507 >>   train_samples            =       5052
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:00,507 >>   train_samples_per_second =     93.874
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:00,507 >>   train_steps_per_second   =      1.468
{'eval_loss': 0.9747524261474609, 'eval_runtime': 19.446, 'eval_samples_per_second': 354.006, 'eval_steps_per_second': 44.276, 'epoch': 5.0}
{'train_runtime': 269.0827, 'train_samples_per_second': 93.874, 'train_steps_per_second': 1.468, 'train_loss': 0.7295443377917326, 'epoch': 5.0}
08/27/2023 22:52:00 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 22:52:00,757 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:52:00,758 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-27 22:52:00,758 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 54.84it/s]  1%|▏         | 12/861 [00:00<00:17, 48.71it/s]  2%|▏         | 17/861 [00:00<00:17, 47.00it/s]  3%|▎         | 22/861 [00:00<00:18, 46.13it/s]  3%|▎         | 27/861 [00:00<00:18, 45.69it/s]  4%|▎         | 32/861 [00:00<00:18, 45.38it/s]  4%|▍         | 37/861 [00:00<00:18, 45.20it/s]  5%|▍         | 42/861 [00:00<00:18, 44.75it/s]  5%|▌         | 47/861 [00:01<00:18, 44.26it/s]  6%|▌         | 52/861 [00:01<00:18, 44.04it/s]  7%|▋         | 57/861 [00:01<00:18, 44.21it/s]  7%|▋         | 62/861 [00:01<00:17, 44.41it/s]  8%|▊         | 67/861 [00:01<00:17, 44.37it/s]  8%|▊         | 72/861 [00:01<00:17, 44.62it/s]  9%|▉         | 77/861 [00:01<00:17, 44.65it/s] 10%|▉         | 82/861 [00:01<00:17, 44.73it/s] 10%|█         | 87/861 [00:01<00:17, 44.45it/s] 11%|█         | 92/861 [00:02<00:17, 44.26it/s] 11%|█▏        | 97/861 [00:02<00:17, 43.97it/s] 12%|█▏        | 102/861 [00:02<00:17, 44.21it/s] 12%|█▏        | 107/861 [00:02<00:18, 40.49it/s] 13%|█▎        | 112/861 [00:02<00:17, 41.73it/s] 14%|█▎        | 117/861 [00:02<00:17, 42.69it/s] 14%|█▍        | 122/861 [00:02<00:17, 43.41it/s] 15%|█▍        | 127/861 [00:02<00:16, 43.79it/s] 15%|█▌        | 132/861 [00:02<00:16, 44.13it/s] 16%|█▌        | 137/861 [00:03<00:16, 44.04it/s] 16%|█▋        | 142/861 [00:03<00:16, 44.03it/s] 17%|█▋        | 147/861 [00:03<00:16, 43.80it/s] 18%|█▊        | 152/861 [00:03<00:16, 43.98it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.14it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.45it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.57it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.75it/s] 21%|██        | 177/861 [00:03<00:15, 44.65it/s] 21%|██        | 182/861 [00:04<00:15, 44.52it/s] 22%|██▏       | 187/861 [00:04<00:15, 44.23it/s] 22%|██▏       | 192/861 [00:04<00:15, 44.12it/s] 23%|██▎       | 197/861 [00:04<00:15, 44.17it/s] 23%|██▎       | 202/861 [00:04<00:14, 44.28it/s] 24%|██▍       | 207/861 [00:04<00:14, 44.51it/s] 25%|██▍       | 212/861 [00:04<00:14, 44.62it/s] 25%|██▌       | 217/861 [00:04<00:14, 44.66it/s] 26%|██▌       | 222/861 [00:05<00:14, 44.68it/s] 26%|██▋       | 227/861 [00:05<00:14, 44.42it/s] 27%|██▋       | 232/861 [00:05<00:14, 44.22it/s] 28%|██▊       | 237/861 [00:05<00:14, 44.07it/s] 28%|██▊       | 242/861 [00:05<00:14, 43.60it/s] 29%|██▊       | 247/861 [00:05<00:13, 44.04it/s] 29%|██▉       | 252/861 [00:05<00:13, 44.24it/s] 30%|██▉       | 257/861 [00:05<00:13, 44.51it/s] 30%|███       | 262/861 [00:05<00:13, 44.44it/s] 31%|███       | 267/861 [00:06<00:13, 44.52it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.24it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.08it/s] 33%|███▎      | 282/861 [00:06<00:13, 43.81it/s] 33%|███▎      | 287/861 [00:06<00:13, 44.10it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.20it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.45it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.67it/s] 36%|███▌      | 307/861 [00:06<00:12, 44.64it/s] 36%|███▌      | 312/861 [00:07<00:12, 44.67it/s] 37%|███▋      | 317/861 [00:07<00:12, 44.37it/s] 37%|███▋      | 322/861 [00:07<00:12, 44.21it/s] 38%|███▊      | 327/861 [00:07<00:12, 44.03it/s] 39%|███▊      | 332/861 [00:07<00:11, 44.11it/s] 39%|███▉      | 337/861 [00:07<00:11, 44.31it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.46it/s] 40%|████      | 347/861 [00:07<00:11, 44.59it/s] 41%|████      | 352/861 [00:07<00:11, 44.62it/s] 41%|████▏     | 357/861 [00:08<00:11, 44.55it/s] 42%|████▏     | 362/861 [00:08<00:11, 44.49it/s] 43%|████▎     | 367/861 [00:08<00:11, 44.33it/s] 43%|████▎     | 372/861 [00:08<00:11, 43.93it/s] 44%|████▍     | 377/861 [00:08<00:11, 42.48it/s] 44%|████▍     | 382/861 [00:08<00:11, 43.27it/s] 45%|████▍     | 387/861 [00:08<00:10, 43.66it/s] 46%|████▌     | 392/861 [00:08<00:10, 44.09it/s] 46%|████▌     | 397/861 [00:08<00:10, 44.23it/s] 47%|████▋     | 402/861 [00:09<00:10, 44.30it/s] 47%|████▋     | 407/861 [00:09<00:10, 44.24it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.07it/s] 48%|████▊     | 417/861 [00:09<00:10, 43.85it/s] 49%|████▉     | 422/861 [00:09<00:09, 43.99it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.25it/s] 50%|█████     | 432/861 [00:09<00:09, 44.38it/s] 51%|█████     | 437/861 [00:09<00:09, 44.54it/s] 51%|█████▏    | 442/861 [00:09<00:09, 44.63it/s] 52%|█████▏    | 447/861 [00:10<00:09, 44.55it/s] 52%|█████▏    | 452/861 [00:10<00:09, 44.33it/s] 53%|█████▎    | 457/861 [00:10<00:09, 44.07it/s] 54%|█████▎    | 462/861 [00:10<00:09, 42.49it/s] 54%|█████▍    | 467/861 [00:10<00:09, 43.26it/s] 55%|█████▍    | 472/861 [00:10<00:08, 43.65it/s] 55%|█████▌    | 477/861 [00:10<00:08, 44.04it/s] 56%|█████▌    | 482/861 [00:10<00:08, 44.31it/s] 57%|█████▋    | 487/861 [00:11<00:08, 44.37it/s] 57%|█████▋    | 492/861 [00:11<00:08, 44.36it/s] 58%|█████▊    | 497/861 [00:11<00:08, 44.15it/s] 58%|█████▊    | 502/861 [00:11<00:08, 43.88it/s] 59%|█████▉    | 507/861 [00:11<00:08, 43.86it/s] 59%|█████▉    | 512/861 [00:11<00:08, 42.65it/s] 60%|██████    | 517/861 [00:11<00:07, 43.27it/s] 61%|██████    | 522/861 [00:11<00:07, 43.80it/s] 61%|██████    | 527/861 [00:11<00:07, 44.04it/s] 62%|██████▏   | 532/861 [00:12<00:07, 44.32it/s] 62%|██████▏   | 537/861 [00:12<00:07, 44.33it/s] 63%|██████▎   | 542/861 [00:12<00:07, 44.31it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.12it/s] 64%|██████▍   | 552/861 [00:12<00:07, 43.95it/s] 65%|██████▍   | 557/861 [00:12<00:06, 43.87it/s] 65%|██████▌   | 562/861 [00:12<00:06, 44.20it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.41it/s] 66%|██████▋   | 572/861 [00:12<00:06, 44.62it/s] 67%|██████▋   | 577/861 [00:13<00:06, 44.57it/s] 68%|██████▊   | 582/861 [00:13<00:06, 44.45it/s] 68%|██████▊   | 587/861 [00:13<00:06, 44.41it/s] 69%|██████▉   | 592/861 [00:13<00:06, 44.18it/s] 69%|██████▉   | 597/861 [00:13<00:06, 43.97it/s] 70%|██████▉   | 602/861 [00:13<00:05, 43.94it/s] 70%|███████   | 607/861 [00:13<00:05, 44.22it/s] 71%|███████   | 612/861 [00:13<00:05, 44.24it/s] 72%|███████▏  | 617/861 [00:13<00:05, 44.43it/s] 72%|███████▏  | 622/861 [00:14<00:07, 30.50it/s] 73%|███████▎  | 627/861 [00:14<00:06, 33.90it/s] 73%|███████▎  | 632/861 [00:14<00:06, 36.60it/s] 74%|███████▍  | 637/861 [00:14<00:05, 38.74it/s] 75%|███████▍  | 642/861 [00:14<00:05, 39.30it/s] 75%|███████▌  | 647/861 [00:14<00:05, 40.81it/s] 76%|███████▌  | 652/861 [00:14<00:04, 41.94it/s] 76%|███████▋  | 657/861 [00:15<00:04, 42.59it/s] 77%|███████▋  | 662/861 [00:15<00:04, 42.78it/s] 77%|███████▋  | 667/861 [00:15<00:04, 42.97it/s] 78%|███████▊  | 672/861 [00:15<00:04, 43.40it/s] 79%|███████▊  | 677/861 [00:15<00:04, 43.74it/s] 79%|███████▉  | 682/861 [00:15<00:04, 43.97it/s] 80%|███████▉  | 687/861 [00:15<00:03, 44.17it/s] 80%|████████  | 692/861 [00:15<00:03, 44.22it/s] 81%|████████  | 697/861 [00:15<00:03, 44.38it/s] 82%|████████▏ | 702/861 [00:16<00:03, 44.32it/s] 82%|████████▏ | 707/861 [00:16<00:03, 44.15it/s] 83%|████████▎ | 712/861 [00:16<00:03, 44.04it/s] 83%|████████▎ | 717/861 [00:16<00:03, 44.09it/s] 84%|████████▍ | 722/861 [00:16<00:03, 44.09it/s] 84%|████████▍ | 727/861 [00:16<00:03, 44.33it/s] 85%|████████▌ | 732/861 [00:16<00:02, 44.48it/s] 86%|████████▌ | 737/861 [00:16<00:02, 44.55it/s] 86%|████████▌ | 742/861 [00:16<00:02, 44.56it/s] 87%|████████▋ | 747/861 [00:17<00:02, 44.38it/s] 87%|████████▋ | 752/861 [00:17<00:02, 44.25it/s] 88%|████████▊ | 757/861 [00:17<00:02, 44.17it/s] 89%|████████▊ | 762/861 [00:17<00:02, 44.09it/s] 89%|████████▉ | 767/861 [00:17<00:02, 44.13it/s] 90%|████████▉ | 772/861 [00:17<00:02, 44.31it/s] 90%|█████████ | 777/861 [00:17<00:01, 42.73it/s] 91%|█████████ | 782/861 [00:17<00:01, 43.30it/s] 91%|█████████▏| 787/861 [00:17<00:01, 43.73it/s] 92%|█████████▏| 792/861 [00:18<00:01, 43.89it/s] 93%|█████████▎| 797/861 [00:18<00:01, 43.93it/s] 93%|█████████▎| 802/861 [00:18<00:01, 43.93it/s] 94%|█████████▎| 807/861 [00:18<00:01, 43.96it/s] 94%|█████████▍| 812/861 [00:18<00:01, 44.14it/s] 95%|█████████▍| 817/861 [00:18<00:00, 44.08it/s] 95%|█████████▌| 822/861 [00:18<00:00, 44.08it/s] 96%|█████████▌| 827/861 [00:18<00:00, 44.21it/s] 97%|█████████▋| 832/861 [00:18<00:00, 44.44it/s] 97%|█████████▋| 837/861 [00:19<00:00, 44.38it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.30it/s] 98%|█████████▊| 847/861 [00:19<00:00, 44.33it/s] 99%|█████████▉| 852/861 [00:19<00:00, 44.26it/s]100%|█████████▉| 857/861 [00:19<00:00, 44.12it/s]100%|██████████| 861/861 [00:19<00:00, 43.81it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:52:20,431 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:20,431 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:20,431 >>   eval_loss               =     0.9636
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:20,431 >>   eval_runtime            = 0:00:19.67
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:20,431 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:20,431 >>   eval_samples_per_second =    349.915
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:20,431 >>   eval_steps_per_second   =     43.765
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:52:20,431 >>   perplexity              =      2.621
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_1', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:19, 15.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:34<02:20, 17.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:50<01:57, 16.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:04<01:35, 15.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:19<01:17, 15.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:34<01:00, 15.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:50<00:46, 15.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:05<00:30, 15.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:19<00:15, 15.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:36<00:00, 15.44s/it]Generating: 100%|██████████| 10/10 [02:36<00:00, 15.63s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9166666666666666, 'errors': {'', "('', 'composer', 'Jérémie Léger', 'The title track of the album was a track Iced Tea made by Jérémie Léger and written by Pierre Bousquet and Bernard Barrou , among others .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 397, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 443, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 489, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 555, 'raw': 800}
{'target': 600, 'success': 574, 'raw': 832}
{'target': 600, 'success': 593, 'raw': 864}
{'target': 600, 'success': 614, 'raw': 896}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.6852678571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : opposite of . Context : Later in the year ( 1143 ) , Ptolemaeus , the founder of the Macedonian Republic , married Ariadne , daughter of Ariadne , founder of Macedonian kings , Thebes and Ileani . Head Entity : Ileani , Tail Entity : Ileanius .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8491847826086957, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : creator .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.75375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Lothar', 'twinned administrative body', '', 'He died in the Battle of Mervyn at Rheinmetall in 645 along with his daughter Lothar , D.')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 12521
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12621, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_5_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:15, 15.35s/it]Extractor Estimating: 2it [00:18,  8.02s/it]Extractor Estimating: 3it [00:18,  4.64s/it]Extractor Estimating: 4it [00:19,  3.07s/it]Extractor Estimating: 5it [00:20,  2.18s/it]Extractor Estimating: 6it [00:20,  1.65s/it]Extractor Estimating: 7it [00:21,  1.30s/it]Extractor Estimating: 8it [00:22,  1.11s/it]Extractor Estimating: 9it [00:22,  1.02it/s]Extractor Estimating: 10it [00:23,  1.15it/s]Extractor Estimating: 11it [00:24,  1.04it/s]Extractor Estimating: 12it [00:25,  1.15it/s]Extractor Estimating: 13it [00:25,  1.27it/s]Extractor Estimating: 14it [00:26,  1.36it/s]Extractor Estimating: 15it [00:27,  1.43it/s]Extractor Estimating: 16it [00:27,  1.44it/s]Extractor Estimating: 17it [00:30,  1.20s/it]Extractor Estimating: 18it [00:30,  1.04s/it]Extractor Estimating: 19it [00:31,  1.09it/s]Extractor Estimating: 20it [00:31,  1.21it/s]Extractor Estimating: 21it [00:32,  1.28it/s]Extractor Estimating: 22it [00:33,  1.38it/s]Extractor Estimating: 23it [00:33,  1.45it/s]Extractor Estimating: 24it [00:34,  1.43it/s]Extractor Estimating: 25it [00:35,  1.47it/s]Extractor Estimating: 26it [00:35,  1.50it/s]Extractor Estimating: 27it [00:36,  1.53it/s]Extractor Estimating: 28it [00:37,  1.50it/s]Extractor Estimating: 29it [00:37,  1.55it/s]Extractor Estimating: 30it [00:38,  1.55it/s]Extractor Estimating: 31it [00:39,  1.58it/s]Extractor Estimating: 32it [00:39,  1.55it/s]Extractor Estimating: 33it [00:40,  1.52it/s]Extractor Estimating: 34it [00:41,  1.53it/s]Extractor Estimating: 35it [00:41,  1.53it/s]Extractor Estimating: 36it [00:42,  1.53it/s]Extractor Estimating: 37it [00:43,  1.48it/s]Extractor Estimating: 38it [00:43,  1.50it/s]Extractor Estimating: 39it [00:44,  1.47it/s]Extractor Estimating: 40it [00:45,  1.49it/s]Extractor Estimating: 41it [00:45,  1.52it/s]Extractor Estimating: 42it [00:46,  1.51it/s]Extractor Estimating: 43it [00:46,  1.54it/s]Extractor Estimating: 44it [00:47,  1.52it/s]Extractor Estimating: 45it [00:48,  1.53it/s]Extractor Estimating: 46it [00:48,  1.52it/s]Extractor Estimating: 47it [00:49,  1.52it/s]Extractor Estimating: 48it [00:50,  1.51it/s]Extractor Estimating: 49it [00:51,  1.42it/s]Extractor Estimating: 50it [00:51,  1.48it/s]Extractor Estimating: 51it [00:52,  1.49it/s]Extractor Estimating: 52it [00:53,  1.52it/s]Extractor Estimating: 53it [00:53,  1.53it/s]Extractor Estimating: 54it [00:54,  1.49it/s]Extractor Estimating: 55it [00:55,  1.49it/s]Extractor Estimating: 56it [00:55,  1.52it/s]Extractor Estimating: 57it [00:56,  1.50it/s]Extractor Estimating: 58it [00:57,  1.49it/s]Extractor Estimating: 59it [00:57,  1.48it/s]Extractor Estimating: 60it [00:58,  1.54it/s]Extractor Estimating: 61it [00:58,  1.58it/s]Extractor Estimating: 62it [00:59,  1.56it/s]Extractor Estimating: 63it [01:00,  1.54it/s]Extractor Estimating: 64it [01:00,  1.53it/s]Extractor Estimating: 65it [01:01,  1.53it/s]Extractor Estimating: 66it [01:02,  1.60it/s]Extractor Estimating: 67it [01:02,  1.56it/s]Extractor Estimating: 68it [01:03,  1.63it/s]Extractor Estimating: 69it [01:03,  1.64it/s]Extractor Estimating: 70it [01:04,  1.70it/s]Extractor Estimating: 71it [01:05,  1.68it/s]Extractor Estimating: 72it [01:05,  1.63it/s]Extractor Estimating: 73it [01:06,  1.65it/s]Extractor Estimating: 74it [01:06,  1.63it/s]Extractor Estimating: 75it [01:07,  1.46it/s]Extractor Estimating: 76it [01:08,  1.51it/s]Extractor Estimating: 77it [01:09,  1.54it/s]Extractor Estimating: 78it [01:09,  1.54it/s]Extractor Estimating: 79it [01:10,  1.57it/s]Extractor Estimating: 80it [01:10,  1.54it/s]Extractor Estimating: 81it [01:11,  1.52it/s]Extractor Estimating: 82it [01:12,  1.55it/s]Extractor Estimating: 83it [01:12,  1.53it/s]Extractor Estimating: 84it [01:13,  1.59it/s]Extractor Estimating: 85it [01:14,  1.63it/s]Extractor Estimating: 86it [01:14,  1.63it/s]Extractor Estimating: 87it [01:15,  1.63it/s]Extractor Estimating: 88it [01:15,  1.66it/s]Extractor Estimating: 89it [01:16,  1.62it/s]Extractor Estimating: 90it [01:17,  1.62it/s]Extractor Estimating: 91it [01:17,  1.59it/s]Extractor Estimating: 92it [01:18,  1.59it/s]Extractor Estimating: 93it [01:19,  1.61it/s]Extractor Estimating: 94it [01:19,  1.62it/s]Extractor Estimating: 95it [01:20,  1.66it/s]Extractor Estimating: 96it [01:20,  1.65it/s]Extractor Estimating: 97it [01:21,  1.69it/s]Extractor Estimating: 98it [01:21,  1.69it/s]Extractor Estimating: 99it [01:22,  1.70it/s]Extractor Estimating: 100it [01:23,  1.69it/s]Extractor Estimating: 101it [01:23,  1.64it/s]Extractor Estimating: 102it [01:24,  1.62it/s]Extractor Estimating: 103it [01:25,  1.57it/s]Extractor Estimating: 104it [01:25,  1.57it/s]Extractor Estimating: 105it [01:26,  1.61it/s]Extractor Estimating: 106it [01:27,  1.59it/s]Extractor Estimating: 107it [01:27,  1.60it/s]Extractor Estimating: 108it [01:28,  1.60it/s]Extractor Estimating: 109it [01:28,  1.62it/s]Extractor Estimating: 110it [01:29,  1.65it/s]Extractor Estimating: 111it [01:30,  1.61it/s]Extractor Estimating: 112it [01:30,  1.63it/s]Extractor Estimating: 113it [01:31,  1.62it/s]Extractor Estimating: 114it [01:31,  1.59it/s]Extractor Estimating: 115it [01:32,  1.59it/s]Extractor Estimating: 116it [01:33,  1.62it/s]Extractor Estimating: 117it [01:33,  1.60it/s]Extractor Estimating: 118it [01:34,  1.55it/s]Extractor Estimating: 119it [01:35,  1.55it/s]Extractor Estimating: 120it [01:35,  1.57it/s]Extractor Estimating: 121it [01:36,  1.61it/s]Extractor Estimating: 122it [01:37,  1.59it/s]Extractor Estimating: 123it [01:37,  1.60it/s]Extractor Estimating: 124it [01:38,  1.60it/s]Extractor Estimating: 125it [01:38,  1.59it/s]Extractor Estimating: 126it [01:39,  1.55it/s]Extractor Estimating: 127it [01:40,  1.59it/s]Extractor Estimating: 128it [01:40,  1.61it/s]Extractor Estimating: 129it [01:41,  1.62it/s]Extractor Estimating: 130it [01:41,  1.67it/s]Extractor Estimating: 131it [01:42,  1.56it/s]Extractor Estimating: 132it [01:43,  1.54it/s]Extractor Estimating: 133it [01:43,  1.56it/s]Extractor Estimating: 134it [01:44,  1.61it/s]Extractor Estimating: 135it [01:45,  1.61it/s]Extractor Estimating: 136it [01:45,  1.55it/s]Extractor Estimating: 137it [01:46,  1.57it/s]Extractor Estimating: 138it [01:47,  1.58it/s]Extractor Estimating: 139it [01:47,  1.60it/s]Extractor Estimating: 140it [01:48,  1.60it/s]Extractor Estimating: 141it [01:49,  1.54it/s]Extractor Estimating: 142it [01:49,  1.54it/s]Extractor Estimating: 143it [01:50,  1.44it/s]Extractor Estimating: 144it [01:51,  1.45it/s]Extractor Estimating: 145it [01:51,  1.50it/s]Extractor Estimating: 146it [01:52,  1.47it/s]Extractor Estimating: 147it [01:53,  1.50it/s]Extractor Estimating: 148it [01:53,  1.52it/s]Extractor Estimating: 149it [01:54,  1.57it/s]Extractor Estimating: 150it [01:54,  1.60it/s]Extractor Estimating: 151it [01:55,  1.48it/s]Extractor Estimating: 152it [01:56,  1.48it/s]Extractor Estimating: 153it [01:57,  1.52it/s]Extractor Estimating: 154it [01:57,  1.51it/s]Extractor Estimating: 155it [01:58,  1.53it/s]Extractor Estimating: 156it [01:59,  1.51it/s]Extractor Estimating: 157it [01:59,  1.54it/s]Extractor Estimating: 158it [02:00,  1.56it/s]Extractor Estimating: 159it [02:00,  1.55it/s]Extractor Estimating: 160it [02:01,  1.58it/s]Extractor Estimating: 161it [02:02,  1.55it/s]Extractor Estimating: 162it [02:02,  1.59it/s]Extractor Estimating: 163it [02:03,  1.55it/s]Extractor Estimating: 164it [02:04,  1.52it/s]Extractor Estimating: 165it [02:04,  1.54it/s]Extractor Estimating: 166it [02:05,  1.55it/s]Extractor Estimating: 167it [02:06,  1.57it/s]Extractor Estimating: 168it [02:06,  1.63it/s]Extractor Estimating: 169it [02:07,  1.65it/s]Extractor Estimating: 170it [02:07,  1.59it/s]Extractor Estimating: 171it [02:08,  1.53it/s]Extractor Estimating: 172it [02:09,  1.56it/s]Extractor Estimating: 173it [02:09,  1.58it/s]Extractor Estimating: 174it [02:10,  1.61it/s]Extractor Estimating: 175it [02:11,  1.60it/s]Extractor Estimating: 176it [02:11,  1.61it/s]Extractor Estimating: 177it [02:12,  1.52it/s]Extractor Estimating: 178it [02:13,  1.54it/s]Extractor Estimating: 179it [02:13,  1.53it/s]Extractor Estimating: 180it [02:14,  1.56it/s]Extractor Estimating: 181it [02:14,  1.60it/s]Extractor Estimating: 182it [02:15,  1.60it/s]Extractor Estimating: 183it [02:16,  1.62it/s]Extractor Estimating: 184it [02:16,  1.61it/s]Extractor Estimating: 185it [02:17,  1.58it/s]Extractor Estimating: 186it [02:18,  1.56it/s]Extractor Estimating: 187it [02:18,  1.60it/s]Extractor Estimating: 188it [02:19,  1.57it/s]Extractor Estimating: 189it [02:19,  1.54it/s]Extractor Estimating: 190it [02:20,  1.57it/s]Extractor Estimating: 191it [02:21,  1.60it/s]Extractor Estimating: 192it [02:21,  1.58it/s]Extractor Estimating: 193it [02:22,  1.56it/s]Extractor Estimating: 194it [02:23,  1.53it/s]Extractor Estimating: 195it [02:23,  1.54it/s]Extractor Estimating: 196it [02:24,  1.54it/s]Extractor Estimating: 197it [02:25,  1.56it/s]Extractor Estimating: 198it [02:25,  1.56it/s]Extractor Estimating: 199it [02:26,  1.55it/s]Extractor Estimating: 200it [02:27,  1.55it/s]Extractor Estimating: 201it [02:27,  1.58it/s]Extractor Estimating: 202it [02:28,  1.60it/s]Extractor Estimating: 203it [02:28,  1.61it/s]Extractor Estimating: 204it [02:29,  1.59it/s]Extractor Estimating: 205it [02:30,  1.60it/s]Extractor Estimating: 206it [02:30,  1.61it/s]Extractor Estimating: 207it [02:31,  1.64it/s]Extractor Estimating: 208it [02:31,  1.64it/s]Extractor Estimating: 209it [02:32,  1.62it/s]Extractor Estimating: 210it [02:33,  1.63it/s]Extractor Estimating: 211it [02:33,  1.64it/s]Extractor Estimating: 212it [02:34,  1.65it/s]Extractor Estimating: 213it [02:34,  1.68it/s]Extractor Estimating: 214it [02:35,  1.70it/s]Extractor Estimating: 215it [02:36,  1.71it/s]Extractor Estimating: 216it [02:36,  1.70it/s]Extractor Estimating: 217it [02:37,  1.67it/s]Extractor Estimating: 218it [02:37,  1.70it/s]Extractor Estimating: 219it [02:38,  1.71it/s]Extractor Estimating: 220it [02:39,  1.68it/s]Extractor Estimating: 221it [02:39,  1.53it/s]Extractor Estimating: 222it [02:40,  1.51it/s]Extractor Estimating: 223it [02:41,  1.54it/s]Extractor Estimating: 224it [02:41,  1.56it/s]Extractor Estimating: 225it [02:42,  1.58it/s]Extractor Estimating: 226it [02:43,  1.59it/s]Extractor Estimating: 227it [02:43,  1.58it/s]Extractor Estimating: 228it [02:44,  1.57it/s]Extractor Estimating: 229it [02:44,  1.54it/s]Extractor Estimating: 230it [02:45,  1.62it/s]Extractor Estimating: 231it [02:46,  1.57it/s]Extractor Estimating: 232it [02:46,  1.60it/s]Extractor Estimating: 233it [02:47,  1.59it/s]Extractor Estimating: 234it [02:48,  1.64it/s]Extractor Estimating: 235it [02:48,  1.61it/s]Extractor Estimating: 236it [02:49,  1.60it/s]Extractor Estimating: 237it [02:49,  1.57it/s]Extractor Estimating: 238it [02:50,  1.59it/s]Extractor Estimating: 239it [02:51,  1.57it/s]Extractor Estimating: 240it [02:51,  1.55it/s]Extractor Estimating: 241it [02:52,  1.61it/s]Extractor Estimating: 242it [02:53,  1.62it/s]Extractor Estimating: 243it [02:53,  1.58it/s]Extractor Estimating: 244it [02:57,  1.56s/it]Extractor Estimating: 245it [02:58,  1.28s/it]Extractor Estimating: 246it [02:58,  1.10s/it]Extractor Estimating: 247it [02:59,  1.06it/s]Extractor Estimating: 248it [02:59,  1.20it/s]Extractor Estimating: 249it [03:00,  1.27it/s]Extractor Estimating: 250it [03:01,  1.44it/s]Extractor Estimating: 250it [03:01,  1.38it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5100 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 27061
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27161, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_train_large/unseen_5_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27161, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.298, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.028, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.021, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.011, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.012, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.806, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.022, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.010, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.019, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.021, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.785, loss:nan
g_step 1200, step 135, avg_time 1.022, loss:nan
g_step 1300, step 22, avg_time 0.996, loss:nan
g_step 1400, step 122, avg_time 1.016, loss:nan
g_step 1500, step 9, avg_time 1.010, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.802, loss:nan
g_step 1700, step 209, avg_time 1.008, loss:nan
g_step 1800, step 96, avg_time 1.004, loss:nan
g_step 1900, step 196, avg_time 1.018, loss:nan
g_step 2000, step 83, avg_time 1.010, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.801, loss:nan
g_step 2200, step 70, avg_time 1.018, loss:nan
g_step 2300, step 170, avg_time 1.016, loss:nan
g_step 2400, step 57, avg_time 1.011, loss:nan
g_step 2500, step 157, avg_time 1.005, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.791, loss:nan
g_step 2700, step 144, avg_time 0.994, loss:nan
g_step 2800, step 31, avg_time 0.996, loss:nan
g_step 2900, step 131, avg_time 0.981, loss:nan
g_step 3000, step 18, avg_time 1.021, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.793, loss:nan
g_step 3200, step 5, avg_time 1.008, loss:nan
g_step 3300, step 105, avg_time 0.970, loss:nan
g_step 3400, step 205, avg_time 0.992, loss:nan
g_step 3500, step 92, avg_time 1.012, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.799, loss:nan
g_step 3700, step 79, avg_time 1.002, loss:nan
g_step 3800, step 179, avg_time 1.012, loss:nan
g_step 3900, step 66, avg_time 1.021, loss:nan
g_step 4000, step 166, avg_time 1.005, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.788, loss:nan
g_step 4200, step 153, avg_time 1.012, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 00:39:29 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 00:39:29 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_00-39-28_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 00:39:30 - WARNING - datasets.builder -   Using custom data configuration default-b53015c25575424f
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b53015c25575424f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 00:39:32,926 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:39:32,927 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:39:32,928 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:39:32,929 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:39:33,003 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:39:33,051 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:39:33,051 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:39:33,051 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:39:33,052 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:39:33,052 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:39:33,052 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 00:39:33,311 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:39:36,407 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 00:39:36,407 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b53015c25575424f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 00:39:36 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14d842d32200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.30ba/s] 50%|█████     | 3/6 [00:01<00:01,  2.96ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.42ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.73ba/s]100%|██████████| 6/6 [00:01<00:00,  3.71ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.61ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.04ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.21ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.30ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.37ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.39ba/s]100%|██████████| 7/7 [00:01<00:00,  4.56ba/s]100%|██████████| 7/7 [00:01<00:00,  4.36ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.89ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.89ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.89ba/s]100%|██████████| 6/6 [00:00<00:00, 10.86ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.74ba/s] 43%|████▎     | 3/7 [00:00<00:00,  8.16ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.35ba/s]100%|██████████| 7/7 [00:00<00:00, 10.08ba/s]100%|██████████| 7/7 [00:00<00:00,  9.26ba/s]
[INFO|trainer.py:414] 2023-08-28 00:39:42,045 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 00:39:42,151 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 00:39:42,151 >>   Num examples = 5160
[INFO|trainer.py:1149] 2023-08-28 00:39:42,151 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 00:39:42,151 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 00:39:42,151 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 00:39:42,151 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 00:39:42,151 >>   Total optimization steps = 405
  0%|          | 0/405 [00:00<?, ?it/s]  0%|          | 1/405 [00:00<02:24,  2.80it/s]  0%|          | 2/405 [00:00<02:04,  3.23it/s]  1%|          | 3/405 [00:00<01:58,  3.40it/s]  1%|          | 4/405 [00:01<01:55,  3.48it/s]  1%|          | 5/405 [00:01<01:53,  3.52it/s]  1%|▏         | 6/405 [00:01<01:52,  3.54it/s]  2%|▏         | 7/405 [00:02<01:52,  3.55it/s]  2%|▏         | 8/405 [00:02<01:53,  3.50it/s]  2%|▏         | 9/405 [00:02<01:52,  3.52it/s]  2%|▏         | 10/405 [00:02<01:51,  3.54it/s]  3%|▎         | 11/405 [00:03<01:50,  3.55it/s]  3%|▎         | 12/405 [00:03<01:50,  3.56it/s]  3%|▎         | 13/405 [00:03<01:50,  3.56it/s]  3%|▎         | 14/405 [00:03<01:49,  3.56it/s]  4%|▎         | 15/405 [00:04<01:49,  3.57it/s]  4%|▍         | 16/405 [00:04<01:48,  3.57it/s]  4%|▍         | 17/405 [00:04<01:48,  3.57it/s]  4%|▍         | 18/405 [00:05<01:48,  3.57it/s]  5%|▍         | 19/405 [00:05<01:48,  3.57it/s]  5%|▍         | 20/405 [00:05<01:47,  3.57it/s]  5%|▌         | 21/405 [00:05<01:47,  3.57it/s]  5%|▌         | 22/405 [00:06<01:47,  3.57it/s]  6%|▌         | 23/405 [00:06<01:47,  3.56it/s]  6%|▌         | 24/405 [00:06<01:46,  3.57it/s]  6%|▌         | 25/405 [00:07<01:46,  3.57it/s]  6%|▋         | 26/405 [00:07<01:45,  3.59it/s]  7%|▋         | 27/405 [00:07<01:45,  3.60it/s]  7%|▋         | 28/405 [00:07<01:44,  3.60it/s]  7%|▋         | 29/405 [00:08<01:44,  3.61it/s]  7%|▋         | 30/405 [00:08<01:43,  3.62it/s]  8%|▊         | 31/405 [00:08<01:43,  3.62it/s]  8%|▊         | 32/405 [00:09<01:43,  3.62it/s]  8%|▊         | 33/405 [00:09<01:42,  3.62it/s]  8%|▊         | 34/405 [00:09<01:42,  3.62it/s]  9%|▊         | 35/405 [00:09<01:42,  3.62it/s]  9%|▉         | 36/405 [00:10<01:41,  3.62it/s]  9%|▉         | 37/405 [00:10<01:41,  3.62it/s]  9%|▉         | 38/405 [00:10<01:41,  3.62it/s] 10%|▉         | 39/405 [00:10<01:41,  3.62it/s] 10%|▉         | 40/405 [00:11<01:40,  3.62it/s] 10%|█         | 41/405 [00:11<01:40,  3.63it/s] 10%|█         | 42/405 [00:11<01:40,  3.62it/s] 11%|█         | 43/405 [00:12<01:39,  3.62it/s] 11%|█         | 44/405 [00:12<01:39,  3.62it/s] 11%|█         | 45/405 [00:12<01:39,  3.62it/s] 11%|█▏        | 46/405 [00:12<01:39,  3.62it/s] 12%|█▏        | 47/405 [00:13<01:38,  3.63it/s] 12%|█▏        | 48/405 [00:13<01:38,  3.62it/s] 12%|█▏        | 49/405 [00:13<01:38,  3.62it/s] 12%|█▏        | 50/405 [00:13<01:37,  3.62it/s] 13%|█▎        | 51/405 [00:14<01:37,  3.62it/s] 13%|█▎        | 52/405 [00:14<01:40,  3.50it/s] 13%|█▎        | 53/405 [00:14<01:39,  3.54it/s] 13%|█▎        | 54/405 [00:15<01:38,  3.56it/s] 14%|█▎        | 55/405 [00:15<01:37,  3.58it/s] 14%|█▍        | 56/405 [00:15<01:37,  3.59it/s] 14%|█▍        | 57/405 [00:15<01:36,  3.59it/s] 14%|█▍        | 58/405 [00:16<01:36,  3.60it/s] 15%|█▍        | 59/405 [00:16<01:35,  3.61it/s] 15%|█▍        | 60/405 [00:16<01:35,  3.61it/s] 15%|█▌        | 61/405 [00:17<01:35,  3.61it/s] 15%|█▌        | 62/405 [00:17<01:34,  3.61it/s] 16%|█▌        | 63/405 [00:17<01:34,  3.61it/s] 16%|█▌        | 64/405 [00:17<01:34,  3.61it/s] 16%|█▌        | 65/405 [00:18<01:34,  3.61it/s] 16%|█▋        | 66/405 [00:18<01:33,  3.62it/s] 17%|█▋        | 67/405 [00:18<01:33,  3.61it/s] 17%|█▋        | 68/405 [00:18<01:33,  3.61it/s] 17%|█▋        | 69/405 [00:19<01:33,  3.61it/s] 17%|█▋        | 70/405 [00:19<01:32,  3.61it/s] 18%|█▊        | 71/405 [00:19<01:32,  3.61it/s] 18%|█▊        | 72/405 [00:20<01:32,  3.61it/s] 18%|█▊        | 73/405 [00:20<01:31,  3.62it/s] 18%|█▊        | 74/405 [00:20<01:31,  3.61it/s] 19%|█▊        | 75/405 [00:20<01:31,  3.61it/s] 19%|█▉        | 76/405 [00:21<01:31,  3.61it/s] 19%|█▉        | 77/405 [00:21<01:30,  3.61it/s] 19%|█▉        | 78/405 [00:21<01:30,  3.61it/s] 20%|█▉        | 79/405 [00:22<01:30,  3.61it/s] 20%|█▉        | 80/405 [00:22<01:29,  3.62it/s] 20%|██        | 81/405 [00:22<01:20,  4.01it/s][INFO|trainer.py:2140] 2023-08-28 00:40:04,650 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:40:04,650 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 00:40:04,650 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.99it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.89it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.09it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.89it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.26it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.90it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.61it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.22it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.28it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.42it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.56it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.60it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.72it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.64it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.48it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.25it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.16it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.19it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.31it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.54it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.54it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.69it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.58it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.41it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.21it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.12it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.16it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.27it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.33it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.17it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.63it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.59it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.49it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.31it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.89it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.12it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.29it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.32it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.51it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.44it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.51it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.31it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.27it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.20it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.17it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.26it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.53it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.59it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.52it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.51it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.40it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.31it/s][A
 31%|███       | 267/861 [00:05<00:13, 44.28it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.21it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.22it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.45it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.50it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.59it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.51it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.44it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.37it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.29it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.29it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.41it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.34it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.51it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.60it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.54it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.45it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.44it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.34it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.38it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.39it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.52it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.49it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.60it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.47it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.49it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.38it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.45it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.40it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.41it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.46it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.54it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.58it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.50it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.50it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.45it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.41it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.39it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.33it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.44it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.55it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.55it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.50it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.50it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.43it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.44it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.31it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.27it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.51it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.56it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.54it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.53it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.54it/s][A
 62%|██████▏   | 532/861 [00:11<00:07, 44.50it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.35it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.31it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.43it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.47it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.54it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.46it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.49it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.56it/s][A
 67%|██████▋   | 577/861 [00:12<00:06, 44.40it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.38it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.39it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.38it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.55it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.52it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.49it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.56it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.43it/s][A
 72%|███████▏  | 622/861 [00:13<00:05, 44.33it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.31it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.37it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.41it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.47it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.54it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.56it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.55it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.42it/s][A
 77%|███████▋  | 667/861 [00:14<00:04, 44.48it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.45it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.32it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.37it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.40it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.55it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.61it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.61it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.51it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.46it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.43it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.44it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.46it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.48it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.43it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.60it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.62it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 44.43it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.40it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.42it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.39it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.44it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.45it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.45it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.59it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.55it/s][A
 93%|█████████▎| 797/861 [00:17<00:01, 44.39it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.35it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.44it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.50it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.40it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.41it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.48it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.42it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 42.87it/s][A
 98%|█████████▊| 842/861 [00:18<00:00, 43.31it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.76it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.91it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.00it/s][A                                                
                                                 [A 20%|██        | 81/405 [00:41<01:20,  4.01it/s]
100%|██████████| 861/861 [00:19<00:00, 44.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:40:24,249 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81
[INFO|configuration_utils.py:351] 2023-08-28 00:40:24,450 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:40:27,638 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:40:27,848 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:40:27,939 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81/special_tokens_map.json
 20%|██        | 82/405 [00:47<41:25,  7.70s/it] 20%|██        | 83/405 [00:47<29:25,  5.48s/it] 21%|██        | 84/405 [00:48<20:58,  3.92s/it] 21%|██        | 85/405 [00:48<15:04,  2.83s/it] 21%|██        | 86/405 [00:48<10:57,  2.06s/it] 21%|██▏       | 87/405 [00:48<08:05,  1.53s/it] 22%|██▏       | 88/405 [00:49<06:04,  1.15s/it] 22%|██▏       | 89/405 [00:49<04:40,  1.13it/s] 22%|██▏       | 90/405 [00:49<03:42,  1.42it/s] 22%|██▏       | 91/405 [00:50<03:01,  1.73it/s] 23%|██▎       | 92/405 [00:50<02:32,  2.06it/s] 23%|██▎       | 93/405 [00:50<02:12,  2.36it/s] 23%|██▎       | 94/405 [00:50<02:00,  2.58it/s] 23%|██▎       | 95/405 [00:51<01:49,  2.82it/s] 24%|██▎       | 96/405 [00:51<01:42,  3.02it/s] 24%|██▍       | 97/405 [00:51<01:36,  3.18it/s] 24%|██▍       | 98/405 [00:52<01:33,  3.30it/s] 24%|██▍       | 99/405 [00:52<01:30,  3.39it/s] 25%|██▍       | 100/405 [00:52<01:28,  3.45it/s] 25%|██▍       | 101/405 [00:52<01:26,  3.50it/s] 25%|██▌       | 102/405 [00:53<01:25,  3.54it/s] 25%|██▌       | 103/405 [00:53<01:24,  3.56it/s] 26%|██▌       | 104/405 [00:53<01:24,  3.58it/s] 26%|██▌       | 105/405 [00:54<01:26,  3.48it/s] 26%|██▌       | 106/405 [00:54<01:24,  3.52it/s] 26%|██▋       | 107/405 [00:54<01:23,  3.55it/s] 27%|██▋       | 108/405 [00:54<01:23,  3.57it/s] 27%|██▋       | 109/405 [00:55<01:22,  3.58it/s] 27%|██▋       | 110/405 [00:55<01:22,  3.59it/s] 27%|██▋       | 111/405 [00:55<01:21,  3.60it/s] 28%|██▊       | 112/405 [00:55<01:21,  3.60it/s] 28%|██▊       | 113/405 [00:56<01:20,  3.61it/s] 28%|██▊       | 114/405 [00:56<01:20,  3.61it/s] 28%|██▊       | 115/405 [00:56<01:20,  3.61it/s] 29%|██▊       | 116/405 [00:57<01:22,  3.50it/s] 29%|██▉       | 117/405 [00:57<01:21,  3.53it/s] 29%|██▉       | 118/405 [00:57<01:20,  3.55it/s] 29%|██▉       | 119/405 [00:57<01:20,  3.57it/s] 30%|██▉       | 120/405 [00:58<01:19,  3.58it/s] 30%|██▉       | 121/405 [00:58<01:19,  3.59it/s] 30%|███       | 122/405 [00:58<01:18,  3.59it/s] 30%|███       | 123/405 [00:59<01:18,  3.60it/s] 31%|███       | 124/405 [00:59<01:18,  3.60it/s] 31%|███       | 125/405 [00:59<01:17,  3.60it/s] 31%|███       | 126/405 [00:59<01:17,  3.60it/s] 31%|███▏      | 127/405 [01:00<01:18,  3.54it/s] 32%|███▏      | 128/405 [01:00<01:17,  3.56it/s] 32%|███▏      | 129/405 [01:00<01:17,  3.57it/s] 32%|███▏      | 130/405 [01:00<01:16,  3.58it/s] 32%|███▏      | 131/405 [01:01<01:16,  3.59it/s] 33%|███▎      | 132/405 [01:01<01:16,  3.59it/s] 33%|███▎      | 133/405 [01:01<01:15,  3.60it/s] 33%|███▎      | 134/405 [01:02<01:15,  3.60it/s] 33%|███▎      | 135/405 [01:02<01:14,  3.60it/s] 34%|███▎      | 136/405 [01:02<01:14,  3.60it/s] 34%|███▍      | 137/405 [01:02<01:14,  3.60it/s] 34%|███▍      | 138/405 [01:03<01:16,  3.51it/s] 34%|███▍      | 139/405 [01:03<01:15,  3.54it/s] 35%|███▍      | 140/405 [01:03<01:14,  3.56it/s] 35%|███▍      | 141/405 [01:04<01:13,  3.57it/s] 35%|███▌      | 142/405 [01:04<01:13,  3.58it/s] 35%|███▌      | 143/405 [01:04<01:12,  3.59it/s] 36%|███▌      | 144/405 [01:04<01:12,  3.60it/s] 36%|███▌      | 145/405 [01:05<01:12,  3.60it/s] 36%|███▌      | 146/405 [01:05<01:11,  3.60it/s] 36%|███▋      | 147/405 [01:05<01:11,  3.60it/s] 37%|███▋      | 148/405 [01:06<01:11,  3.60it/s] 37%|███▋      | 149/405 [01:06<01:13,  3.48it/s] 37%|███▋      | 150/405 [01:06<01:12,  3.52it/s] 37%|███▋      | 151/405 [01:06<01:11,  3.54it/s] 38%|███▊      | 152/405 [01:07<01:11,  3.56it/s] 38%|███▊      | 153/405 [01:07<01:10,  3.57it/s] 38%|███▊      | 154/405 [01:07<01:10,  3.58it/s] 38%|███▊      | 155/405 [01:07<01:09,  3.59it/s] 39%|███▊      | 156/405 [01:08<01:09,  3.59it/s] 39%|███▉      | 157/405 [01:08<01:08,  3.60it/s] 39%|███▉      | 158/405 [01:08<01:08,  3.60it/s] 39%|███▉      | 159/405 [01:09<01:08,  3.60it/s] 40%|███▉      | 160/405 [01:09<01:10,  3.48it/s] 40%|███▉      | 161/405 [01:09<01:09,  3.52it/s] 40%|████      | 162/405 [01:09<01:01,  3.93it/s][INFO|trainer.py:2140] 2023-08-28 00:40:52,024 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:40:52,024 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 00:40:52,024 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.3831, 'eval_samples_per_second': 355.154, 'eval_steps_per_second': 44.42, 'epoch': 1.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.65it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.69it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.10it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.15it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.33it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.88it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.62it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.24it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.14it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.42it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.53it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.76it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.74it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.48it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.37it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.28it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.09it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.05it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.09it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.24it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.20it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.47it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.56it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.57it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.36it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.19it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.09it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.24it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.26it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.40it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.56it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.58it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.55it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.38it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.16it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.21it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.27it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.37it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.37it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.47it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.55it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.43it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.34it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.21it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.18it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.37it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.35it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.39it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.45it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.48it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.45it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.27it/s][A
 31%|███       | 267/861 [00:05<00:13, 44.28it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.28it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.37it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.37it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.37it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.48it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.48it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 43.18it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 43.60it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.73it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.89it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.05it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.16it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.32it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.27it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.18it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.19it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.29it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.26it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.22it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.34it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.41it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.38it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.36it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.28it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.33it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.26it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.28it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.28it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.31it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.39it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.35it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.38it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.41it/s][A
 51%|█████     | 437/861 [00:09<00:10, 40.03it/s][A
 51%|█████▏    | 442/861 [00:09<00:10, 41.36it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 42.36it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.09it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.54it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 43.84it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 43.94it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.01it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.76it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 43.84it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.97it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.29it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.50it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.56it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.55it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.46it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.22it/s][A
 61%|██████    | 522/861 [00:11<00:07, 43.99it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.00it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.08it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.23it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.55it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.59it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.63it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.48it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.26it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.08it/s][A
 66%|██████▋   | 572/861 [00:12<00:07, 41.05it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 42.11it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 42.92it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.49it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 43.88it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.22it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.20it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.10it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.71it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 43.82it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.02it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.24it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.42it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.55it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.64it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.54it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.23it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.02it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.06it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.09it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.31it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.46it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.60it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.61it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.43it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.23it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.07it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 40.22it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 41.51it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 42.48it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.16it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.57it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 43.95it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.20it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.20it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.79it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.77it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.86it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.17it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.27it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.44it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.55it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.62it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.37it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.09it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.99it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.11it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.20it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.39it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.48it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.62it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.51it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.31it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 44.10it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 41.04it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 42.13it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 42.95it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.57it/s][A                                                 
                                                 [A 40%|████      | 162/405 [01:29<01:01,  3.93it/s]
100%|██████████| 861/861 [00:19<00:00, 43.57it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:41:12,089 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162
[INFO|configuration_utils.py:351] 2023-08-28 00:41:12,452 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:41:16,957 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:41:17,123 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:41:17,201 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162/special_tokens_map.json
 40%|████      | 163/405 [01:36<32:52,  8.15s/it] 40%|████      | 164/405 [01:36<23:15,  5.79s/it] 41%|████      | 165/405 [01:37<16:32,  4.14s/it] 41%|████      | 166/405 [01:37<11:52,  2.98s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/405 [01:37<08:44,  2.21s/it] 41%|████▏     | 168/405 [01:37<06:26,  1.63s/it] 42%|████▏     | 169/405 [01:38<04:51,  1.23s/it] 42%|████▏     | 170/405 [01:38<03:42,  1.05it/s] 42%|████▏     | 171/405 [01:38<02:55,  1.34it/s] 42%|████▏     | 172/405 [01:39<02:21,  1.64it/s] 43%|████▎     | 173/405 [01:39<01:58,  1.96it/s] 43%|████▎     | 174/405 [01:39<01:42,  2.26it/s] 43%|████▎     | 175/405 [01:39<01:30,  2.54it/s] 43%|████▎     | 176/405 [01:40<01:22,  2.78it/s] 44%|████▎     | 177/405 [01:40<01:16,  2.97it/s] 44%|████▍     | 178/405 [01:40<01:12,  3.13it/s] 44%|████▍     | 179/405 [01:41<01:09,  3.25it/s] 44%|████▍     | 180/405 [01:41<01:09,  3.25it/s] 45%|████▍     | 181/405 [01:41<01:07,  3.33it/s] 45%|████▍     | 182/405 [01:41<01:05,  3.39it/s] 45%|████▌     | 183/405 [01:42<01:04,  3.44it/s] 45%|████▌     | 184/405 [01:42<01:03,  3.47it/s] 46%|████▌     | 185/405 [01:42<01:02,  3.50it/s] 46%|████▌     | 186/405 [01:43<01:02,  3.51it/s] 46%|████▌     | 187/405 [01:43<01:01,  3.53it/s] 46%|████▋     | 188/405 [01:43<01:01,  3.54it/s] 47%|████▋     | 189/405 [01:43<01:01,  3.54it/s] 47%|████▋     | 190/405 [01:44<01:00,  3.55it/s] 47%|████▋     | 191/405 [01:44<01:07,  3.17it/s] 47%|████▋     | 192/405 [01:44<01:04,  3.28it/s] 48%|████▊     | 193/405 [01:45<01:03,  3.36it/s] 48%|████▊     | 194/405 [01:45<01:01,  3.41it/s] 48%|████▊     | 195/405 [01:45<01:01,  3.39it/s] 48%|████▊     | 196/405 [01:46<01:01,  3.42it/s] 49%|████▊     | 197/405 [01:46<01:00,  3.46it/s] 49%|████▉     | 198/405 [01:46<00:59,  3.48it/s] 49%|████▉     | 199/405 [01:47<01:11,  2.87it/s] 49%|████▉     | 200/405 [01:47<01:07,  3.03it/s] 50%|████▉     | 201/405 [01:47<01:05,  3.10it/s] 50%|████▉     | 202/405 [01:47<01:03,  3.22it/s] 50%|█████     | 203/405 [01:48<01:00,  3.32it/s] 50%|█████     | 204/405 [01:48<00:59,  3.38it/s] 51%|█████     | 205/405 [01:48<00:58,  3.43it/s] 51%|█████     | 206/405 [01:49<00:57,  3.47it/s] 51%|█████     | 207/405 [01:49<00:56,  3.49it/s] 51%|█████▏    | 208/405 [01:49<00:56,  3.50it/s] 52%|█████▏    | 209/405 [01:49<00:55,  3.51it/s] 52%|█████▏    | 210/405 [01:50<00:55,  3.52it/s] 52%|█████▏    | 211/405 [01:50<00:54,  3.53it/s] 52%|█████▏    | 212/405 [01:50<00:56,  3.42it/s] 53%|█████▎    | 213/405 [01:51<00:55,  3.46it/s] 53%|█████▎    | 214/405 [01:51<00:54,  3.48it/s] 53%|█████▎    | 215/405 [01:51<00:54,  3.50it/s] 53%|█████▎    | 216/405 [01:51<00:53,  3.52it/s] 54%|█████▎    | 217/405 [01:52<00:53,  3.53it/s] 54%|█████▍    | 218/405 [01:52<00:52,  3.54it/s] 54%|█████▍    | 219/405 [01:52<00:52,  3.54it/s] 54%|█████▍    | 220/405 [01:53<00:52,  3.54it/s] 55%|█████▍    | 221/405 [01:53<00:51,  3.54it/s] 55%|█████▍    | 222/405 [01:53<00:51,  3.54it/s] 55%|█████▌    | 223/405 [01:53<00:53,  3.41it/s] 55%|█████▌    | 224/405 [01:54<00:52,  3.45it/s] 56%|█████▌    | 225/405 [01:54<00:51,  3.48it/s] 56%|█████▌    | 226/405 [01:54<00:51,  3.50it/s] 56%|█████▌    | 227/405 [01:55<00:50,  3.51it/s] 56%|█████▋    | 228/405 [01:55<00:50,  3.53it/s] 57%|█████▋    | 229/405 [01:55<00:49,  3.53it/s] 57%|█████▋    | 230/405 [01:55<00:49,  3.54it/s] 57%|█████▋    | 231/405 [01:56<00:49,  3.54it/s] 57%|█████▋    | 232/405 [01:56<00:48,  3.54it/s] 58%|█████▊    | 233/405 [01:56<00:48,  3.54it/s] 58%|█████▊    | 234/405 [01:57<00:50,  3.38it/s] 58%|█████▊    | 235/405 [01:57<00:49,  3.43it/s] 58%|█████▊    | 236/405 [01:57<00:48,  3.47it/s] 59%|█████▊    | 237/405 [01:57<00:48,  3.49it/s] 59%|█████▉    | 238/405 [01:58<00:47,  3.51it/s] 59%|█████▉    | 239/405 [01:58<00:47,  3.53it/s] 59%|█████▉    | 240/405 [01:58<00:46,  3.53it/s] 60%|█████▉    | 241/405 [01:59<00:46,  3.54it/s] 60%|█████▉    | 242/405 [01:59<00:46,  3.54it/s] 60%|██████    | 243/405 [01:59<00:41,  3.92it/s][INFO|trainer.py:2140] 2023-08-28 00:41:41,702 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:41:41,702 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 00:41:41,702 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5407, 'eval_samples_per_second': 352.29, 'eval_steps_per_second': 44.062, 'epoch': 2.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.59it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.38it/s][A
  2%|▏         | 17/861 [00:00<00:18, 44.94it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.07it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.98it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.79it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.40it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.05it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.05it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.14it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.31it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.40it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.39it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.58it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.41it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.26it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.93it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.03it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.19it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.31it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.36it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.51it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.60it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.55it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.24it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.06it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.11it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.24it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.31it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 43.73it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.10it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.29it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.33it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.12it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.07it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.12it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.15it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.19it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.29it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.42it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.51it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.50it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.33it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.21it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.20it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.27it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.28it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.42it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.44it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.51it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.48it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.31it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.15it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.19it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.17it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.28it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 43.74it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.08it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.26it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.40it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.34it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.28it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.18it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.18it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.14it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.25it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.38it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.50it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.55it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.54it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.36it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.27it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.24it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.21it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.23it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.42it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.52it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.46it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.50it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.39it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.33it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.22it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.26it/s][A
 49%|████▉     | 422/861 [00:09<00:10, 42.44it/s][A
 50%|████▉     | 427/861 [00:09<00:10, 43.16it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.66it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.01it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.07it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.06it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.16it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.07it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 43.86it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 43.99it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.17it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.37it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.49it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.48it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.42it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.32it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.07it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.08it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.18it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.37it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.48it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.52it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.48it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.40it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.30it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.18it/s][A
 64%|██████▍   | 552/861 [00:12<00:07, 44.06it/s][A
 65%|██████▍   | 557/861 [00:12<00:07, 41.29it/s][A
 65%|██████▌   | 562/861 [00:12<00:07, 42.43it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 43.17it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 43.73it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 43.94it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.98it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.08it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.00it/s][A
 69%|██████▉   | 597/861 [00:13<00:06, 43.79it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 43.86it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.14it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.23it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.47it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.60it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.51it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.40it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.15it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 43.97it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.11it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.23it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.33it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.40it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.57it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.51it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.38it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.14it/s][A
 80%|███████▉  | 687/861 [00:15<00:04, 39.02it/s][A
 80%|████████  | 692/861 [00:15<00:04, 40.61it/s][A
 81%|████████  | 697/861 [00:15<00:03, 41.81it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 42.36it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.22it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.68it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.00it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.10it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.72it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 43.58it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 43.71it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.06it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.19it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.42it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.55it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.59it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.37it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.09it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 43.90it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.02it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.12it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.41it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.49it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.59it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.55it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.28it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.10it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 43.23it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 43.61it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 43.87it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.16it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.32it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.36it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.29it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.17it/s][A                                                 
                                                 [A 60%|██████    | 243/405 [02:19<00:41,  3.92it/s]
100%|██████████| 861/861 [00:19<00:00, 44.17it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:42:01,280 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243
[INFO|configuration_utils.py:351] 2023-08-28 00:42:01,419 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:42:05,840 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:42:06,109 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:42:06,251 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243/special_tokens_map.json
 60%|██████    | 244/405 [02:25<21:42,  8.09s/it] 60%|██████    | 245/405 [02:26<15:19,  5.75s/it] 61%|██████    | 246/405 [02:26<10:53,  4.11s/it] 61%|██████    | 247/405 [02:26<07:47,  2.96s/it] 61%|██████    | 248/405 [02:27<05:41,  2.17s/it] 61%|██████▏   | 249/405 [02:27<04:10,  1.60s/it] 62%|██████▏   | 250/405 [02:27<03:06,  1.21s/it] 62%|██████▏   | 251/405 [02:27<02:22,  1.08it/s] 62%|██████▏   | 252/405 [02:28<01:52,  1.37it/s] 62%|██████▏   | 253/405 [02:28<01:30,  1.68it/s] 63%|██████▎   | 254/405 [02:28<01:15,  2.00it/s] 63%|██████▎   | 255/405 [02:29<01:04,  2.31it/s] 63%|██████▎   | 256/405 [02:29<00:57,  2.59it/s] 63%|██████▎   | 257/405 [02:29<00:52,  2.83it/s] 64%|██████▎   | 258/405 [02:29<00:48,  3.02it/s] 64%|██████▍   | 259/405 [02:30<01:01,  2.38it/s] 64%|██████▍   | 260/405 [02:30<00:54,  2.65it/s] 64%|██████▍   | 261/405 [02:31<00:50,  2.88it/s] 65%|██████▍   | 262/405 [02:31<00:46,  3.06it/s] 65%|██████▍   | 263/405 [02:31<00:44,  3.20it/s] 65%|██████▌   | 264/405 [02:31<00:42,  3.31it/s] 65%|██████▌   | 265/405 [02:32<00:41,  3.39it/s] 66%|██████▌   | 266/405 [02:32<00:40,  3.45it/s] 66%|██████▌   | 267/405 [02:32<00:39,  3.50it/s] 66%|██████▌   | 268/405 [02:32<00:38,  3.53it/s] 66%|██████▋   | 269/405 [02:33<00:39,  3.44it/s] 67%|██████▋   | 270/405 [02:33<00:38,  3.48it/s] 67%|██████▋   | 271/405 [02:33<00:38,  3.52it/s] 67%|██████▋   | 272/405 [02:34<00:37,  3.54it/s] 67%|██████▋   | 273/405 [02:34<00:37,  3.56it/s] 68%|██████▊   | 274/405 [02:34<00:36,  3.58it/s] 68%|██████▊   | 275/405 [02:34<00:36,  3.58it/s] 68%|██████▊   | 276/405 [02:35<00:35,  3.59it/s] 68%|██████▊   | 277/405 [02:35<00:35,  3.59it/s] 69%|██████▊   | 278/405 [02:35<00:35,  3.59it/s] 69%|██████▉   | 279/405 [02:36<00:35,  3.59it/s] 69%|██████▉   | 280/405 [02:36<00:35,  3.54it/s] 69%|██████▉   | 281/405 [02:36<00:34,  3.56it/s] 70%|██████▉   | 282/405 [02:36<00:34,  3.57it/s] 70%|██████▉   | 283/405 [02:37<00:34,  3.58it/s] 70%|███████   | 284/405 [02:37<00:33,  3.59it/s] 70%|███████   | 285/405 [02:37<00:33,  3.59it/s] 71%|███████   | 286/405 [02:38<00:33,  3.59it/s] 71%|███████   | 287/405 [02:38<00:32,  3.59it/s] 71%|███████   | 288/405 [02:38<00:32,  3.60it/s] 71%|███████▏  | 289/405 [02:38<00:32,  3.60it/s] 72%|███████▏  | 290/405 [02:39<00:31,  3.60it/s] 72%|███████▏  | 291/405 [02:39<00:32,  3.49it/s] 72%|███████▏  | 292/405 [02:39<00:32,  3.53it/s] 72%|███████▏  | 293/405 [02:40<00:31,  3.55it/s] 73%|███████▎  | 294/405 [02:40<00:31,  3.57it/s] 73%|███████▎  | 295/405 [02:40<00:30,  3.58it/s] 73%|███████▎  | 296/405 [02:40<00:30,  3.58it/s] 73%|███████▎  | 297/405 [02:41<00:30,  3.59it/s] 74%|███████▎  | 298/405 [02:41<00:29,  3.58it/s] 74%|███████▍  | 299/405 [02:41<00:29,  3.59it/s] 74%|███████▍  | 300/405 [02:41<00:29,  3.59it/s] 74%|███████▍  | 301/405 [02:42<00:28,  3.60it/s] 75%|███████▍  | 302/405 [02:42<00:29,  3.53it/s] 75%|███████▍  | 303/405 [02:42<00:28,  3.55it/s] 75%|███████▌  | 304/405 [02:43<00:28,  3.57it/s] 75%|███████▌  | 305/405 [02:43<00:27,  3.58it/s] 76%|███████▌  | 306/405 [02:43<00:27,  3.58it/s] 76%|███████▌  | 307/405 [02:43<00:27,  3.59it/s] 76%|███████▌  | 308/405 [02:44<00:27,  3.59it/s] 76%|███████▋  | 309/405 [02:44<00:26,  3.59it/s] 77%|███████▋  | 310/405 [02:44<00:26,  3.60it/s] 77%|███████▋  | 311/405 [02:45<00:26,  3.60it/s] 77%|███████▋  | 312/405 [02:45<00:25,  3.60it/s] 77%|███████▋  | 313/405 [02:45<00:26,  3.52it/s] 78%|███████▊  | 314/405 [02:45<00:25,  3.55it/s] 78%|███████▊  | 315/405 [02:46<00:25,  3.56it/s] 78%|███████▊  | 316/405 [02:46<00:24,  3.57it/s] 78%|███████▊  | 317/405 [02:46<00:24,  3.58it/s] 79%|███████▊  | 318/405 [02:47<00:28,  3.01it/s] 79%|███████▉  | 319/405 [02:47<00:27,  3.16it/s] 79%|███████▉  | 320/405 [02:47<00:25,  3.29it/s] 79%|███████▉  | 321/405 [02:48<00:24,  3.38it/s] 80%|███████▉  | 322/405 [02:48<00:24,  3.44it/s] 80%|███████▉  | 323/405 [02:48<00:23,  3.49it/s] 80%|████████  | 324/405 [02:48<00:20,  3.89it/s][INFO|trainer.py:2140] 2023-08-28 00:42:30,965 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:42:30,966 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 00:42:30,966 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5316, 'eval_samples_per_second': 352.455, 'eval_steps_per_second': 44.082, 'epoch': 3.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.49it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.71it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.03it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.22it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.79it/s][A
  4%|▎         | 32/861 [00:00<00:18, 45.45it/s][A
  4%|▍         | 37/861 [00:00<00:18, 45.33it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.77it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.15it/s][A
  6%|▌         | 52/861 [00:01<00:18, 43.98it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.07it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.32it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.46it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.59it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.70it/s][A
 10%|▉         | 82/861 [00:01<00:18, 42.25it/s][A
 10%|█         | 87/861 [00:01<00:18, 42.95it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.16it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.29it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.63it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.03it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.28it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.42it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.34it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.37it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.22it/s][A
 16%|█▌        | 137/861 [00:03<00:17, 42.03it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 42.77it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 43.25it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 43.68it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.00it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.15it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.30it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.25it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.97it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.95it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.20it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.31it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.52it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.51it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.55it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.50it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.31it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.05it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.08it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.20it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.38it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.54it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.45it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.44it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.47it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.29it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.19it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 42.21it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.02it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 43.59it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 43.98it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.12it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.17it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.22it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.21it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.93it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.00it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.30it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.40it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.62it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.53it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.52it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.41it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.20it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.06it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.01it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.28it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.57it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.59it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.59it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.55it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.39it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.25it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.05it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 42.19it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 42.96it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 43.50it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 43.93it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.26it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.45it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.49it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.26it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.89it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.87it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.04it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.25it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.47it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.54it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.72it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.59it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.24it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.97it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.00it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.16it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.33it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.53it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.62it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.66it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.52it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.24it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 43.96it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 43.61it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 43.92it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.23it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.38it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.57it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.54it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.38it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.17it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.93it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.02it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.15it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.42it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.53it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.67it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.68it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.44it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.15it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.94it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 43.98it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.11it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.35it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.57it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.67it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.62it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.44it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.11it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 43.98it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 42.27it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 42.97it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 43.55it/s][A
 80%|████████  | 692/861 [00:15<00:03, 43.94it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.19it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.40it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.35it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.13it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.85it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.94it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.10it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.28it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.41it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.52it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.52it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.43it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.24it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.99it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.97it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.18it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.32it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.42it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.45it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.37it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.15it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.12it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 43.97it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 41.21it/s][A
 95%|█████████▍| 817/861 [00:18<00:01, 42.25it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 43.05it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 43.62it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 43.97it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.03it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.07it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.98it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.71it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.82it/s][A                                                 
                                                 [A 80%|████████  | 324/405 [03:08<00:20,  3.89it/s]
100%|██████████| 861/861 [00:19<00:00, 43.82it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:42:50,741 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324
[INFO|configuration_utils.py:351] 2023-08-28 00:42:51,050 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:42:54,640 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:42:54,783 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:42:54,847 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324/special_tokens_map.json
 80%|████████  | 325/405 [03:14<10:22,  7.78s/it] 80%|████████  | 326/405 [03:14<07:16,  5.53s/it] 81%|████████  | 327/405 [03:14<05:08,  3.95s/it] 81%|████████  | 328/405 [03:14<03:39,  2.85s/it] 81%|████████  | 329/405 [03:15<02:38,  2.08s/it] 81%|████████▏ | 330/405 [03:15<01:55,  1.54s/it] 82%|████████▏ | 331/405 [03:15<01:26,  1.16s/it] 82%|████████▏ | 332/405 [03:16<01:06,  1.10it/s] 82%|████████▏ | 333/405 [03:16<00:51,  1.39it/s] 82%|████████▏ | 334/405 [03:16<00:41,  1.71it/s] 83%|████████▎ | 335/405 [03:16<00:34,  2.02it/s] 83%|████████▎ | 336/405 [03:17<00:29,  2.32it/s] 83%|████████▎ | 337/405 [03:17<00:26,  2.59it/s] 83%|████████▎ | 338/405 [03:17<00:23,  2.82it/s] 84%|████████▎ | 339/405 [03:18<00:21,  3.01it/s] 84%|████████▍ | 340/405 [03:18<00:20,  3.16it/s] 84%|████████▍ | 341/405 [03:18<00:19,  3.27it/s] 84%|████████▍ | 342/405 [03:18<00:18,  3.35it/s] 85%|████████▍ | 343/405 [03:19<00:18,  3.41it/s] 85%|████████▍ | 344/405 [03:19<00:17,  3.45it/s] 85%|████████▌ | 345/405 [03:19<00:17,  3.39it/s] 85%|████████▌ | 346/405 [03:20<00:17,  3.44it/s] 86%|████████▌ | 347/405 [03:20<00:16,  3.48it/s] 86%|████████▌ | 348/405 [03:20<00:16,  3.50it/s] 86%|████████▌ | 349/405 [03:20<00:15,  3.53it/s] 86%|████████▋ | 350/405 [03:21<00:15,  3.56it/s] 87%|████████▋ | 351/405 [03:21<00:15,  3.57it/s] 87%|████████▋ | 352/405 [03:21<00:14,  3.58it/s] 87%|████████▋ | 353/405 [03:21<00:14,  3.59it/s] 87%|████████▋ | 354/405 [03:22<00:14,  3.60it/s] 88%|████████▊ | 355/405 [03:22<00:13,  3.60it/s] 88%|████████▊ | 356/405 [03:22<00:14,  3.39it/s] 88%|████████▊ | 357/405 [03:23<00:13,  3.45it/s] 88%|████████▊ | 358/405 [03:23<00:13,  3.49it/s] 89%|████████▊ | 359/405 [03:23<00:13,  3.53it/s] 89%|████████▉ | 360/405 [03:23<00:12,  3.55it/s] 89%|████████▉ | 361/405 [03:24<00:12,  3.57it/s] 89%|████████▉ | 362/405 [03:24<00:12,  3.58it/s] 90%|████████▉ | 363/405 [03:24<00:11,  3.59it/s] 90%|████████▉ | 364/405 [03:25<00:11,  3.59it/s] 90%|█████████ | 365/405 [03:25<00:11,  3.60it/s] 90%|█████████ | 366/405 [03:25<00:10,  3.60it/s] 91%|█████████ | 367/405 [03:25<00:11,  3.42it/s] 91%|█████████ | 368/405 [03:26<00:10,  3.47it/s] 91%|█████████ | 369/405 [03:26<00:10,  3.51it/s] 91%|█████████▏| 370/405 [03:26<00:09,  3.54it/s] 92%|█████████▏| 371/405 [03:27<00:09,  3.56it/s] 92%|█████████▏| 372/405 [03:27<00:09,  3.57it/s] 92%|█████████▏| 373/405 [03:27<00:08,  3.58it/s] 92%|█████████▏| 374/405 [03:27<00:08,  3.59it/s] 93%|█████████▎| 375/405 [03:28<00:08,  3.60it/s] 93%|█████████▎| 376/405 [03:28<00:08,  3.60it/s] 93%|█████████▎| 377/405 [03:28<00:07,  3.60it/s] 93%|█████████▎| 378/405 [03:29<00:08,  3.34it/s] 94%|█████████▎| 379/405 [03:29<00:07,  3.42it/s] 94%|█████████▍| 380/405 [03:29<00:07,  3.47it/s] 94%|█████████▍| 381/405 [03:29<00:06,  3.51it/s] 94%|█████████▍| 382/405 [03:30<00:06,  3.53it/s] 95%|█████████▍| 383/405 [03:30<00:06,  3.55it/s] 95%|█████████▍| 384/405 [03:30<00:05,  3.57it/s] 95%|█████████▌| 385/405 [03:31<00:05,  3.58it/s] 95%|█████████▌| 386/405 [03:31<00:05,  3.58it/s] 96%|█████████▌| 387/405 [03:31<00:05,  3.59it/s] 96%|█████████▌| 388/405 [03:31<00:04,  3.60it/s] 96%|█████████▌| 389/405 [03:32<00:04,  3.43it/s] 96%|█████████▋| 390/405 [03:32<00:04,  3.48it/s] 97%|█████████▋| 391/405 [03:32<00:03,  3.52it/s] 97%|█████████▋| 392/405 [03:33<00:03,  3.54it/s] 97%|█████████▋| 393/405 [03:33<00:03,  3.56it/s] 97%|█████████▋| 394/405 [03:33<00:03,  3.57it/s] 98%|█████████▊| 395/405 [03:33<00:02,  3.58it/s] 98%|█████████▊| 396/405 [03:34<00:02,  3.59it/s] 98%|█████████▊| 397/405 [03:34<00:02,  3.59it/s] 98%|█████████▊| 398/405 [03:34<00:01,  3.60it/s] 99%|█████████▊| 399/405 [03:34<00:01,  3.60it/s] 99%|█████████▉| 400/405 [03:35<00:01,  3.55it/s] 99%|█████████▉| 401/405 [03:35<00:01,  3.56it/s] 99%|█████████▉| 402/405 [03:35<00:00,  3.57it/s]100%|█████████▉| 403/405 [03:36<00:00,  3.58it/s]100%|█████████▉| 404/405 [03:36<00:00,  3.59it/s]100%|██████████| 405/405 [03:36<00:00,  3.99it/s][INFO|trainer.py:2140] 2023-08-28 00:43:18,693 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:43:18,694 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 00:43:18,694 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5213, 'eval_samples_per_second': 352.64, 'eval_steps_per_second': 44.106, 'epoch': 4.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.15it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.81it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.10it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.22it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.36it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.86it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.38it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.05it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.10it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.36it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.53it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.64it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.70it/s][A
  8%|▊         | 72/861 [00:01<00:18, 42.75it/s][A
  9%|▉         | 77/861 [00:01<00:18, 43.21it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.30it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.47it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.65it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.99it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.26it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.44it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.28it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.38it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.34it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.24it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.12it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.11it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.28it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.45it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.41it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.42it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.48it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.36it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.17it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.18it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.15it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.23it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.35it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.47it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.50it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.15it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.17it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.04it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.06it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.09it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.21it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.33it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.43it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.42it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.36it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.32it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.12it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.09it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.27it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.31it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.37it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.38it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.52it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.51it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.34it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.28it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.14it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.23it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.34it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.41it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.42it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.49it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.28it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.58it/s][A
 41%|████      | 352/861 [00:07<00:11, 43.73it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.77it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.09it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.22it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.41it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.42it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.32it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.25it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.25it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.15it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.17it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.26it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 42.41it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 43.02it/s][A
 49%|████▉     | 422/861 [00:09<00:10, 43.50it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 43.87it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.09it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.11it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.17it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.16it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.01it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.10it/s][A
 54%|█████▎    | 462/861 [00:10<00:12, 32.46it/s][A
 54%|█████▍    | 466/861 [00:10<00:12, 32.49it/s][A
 55%|█████▍    | 471/861 [00:10<00:10, 35.57it/s][A
 55%|█████▌    | 476/861 [00:10<00:10, 38.02it/s][A
 56%|█████▌    | 481/861 [00:11<00:09, 39.89it/s][A
 56%|█████▋    | 486/861 [00:11<00:09, 41.27it/s][A
 57%|█████▋    | 491/861 [00:11<00:08, 42.32it/s][A
 58%|█████▊    | 496/861 [00:11<00:08, 43.02it/s][A
 58%|█████▊    | 501/861 [00:11<00:08, 43.55it/s][A
 59%|█████▉    | 506/861 [00:11<00:08, 43.40it/s][A
 59%|█████▉    | 511/861 [00:11<00:08, 43.29it/s][A
 60%|█████▉    | 516/861 [00:11<00:07, 43.49it/s][A
 61%|██████    | 521/861 [00:11<00:07, 43.77it/s][A
 61%|██████    | 526/861 [00:12<00:07, 44.12it/s][A
 62%|██████▏   | 531/861 [00:12<00:07, 44.33it/s][A
 62%|██████▏   | 536/861 [00:12<00:07, 44.46it/s][A
 63%|██████▎   | 541/861 [00:12<00:07, 44.60it/s][A
 63%|██████▎   | 546/861 [00:12<00:07, 44.46it/s][A
 64%|██████▍   | 551/861 [00:12<00:07, 44.14it/s][A
 65%|██████▍   | 556/861 [00:12<00:06, 43.95it/s][A
 65%|██████▌   | 561/861 [00:12<00:06, 43.92it/s][A
 66%|██████▌   | 566/861 [00:12<00:06, 44.08it/s][A
 66%|██████▋   | 571/861 [00:13<00:06, 44.27it/s][A
 67%|██████▋   | 576/861 [00:13<00:06, 44.37it/s][A
 67%|██████▋   | 581/861 [00:13<00:06, 44.53it/s][A
 68%|██████▊   | 586/861 [00:13<00:06, 44.68it/s][A
 69%|██████▊   | 591/861 [00:13<00:06, 44.51it/s][A
 69%|██████▉   | 596/861 [00:13<00:05, 44.23it/s][A
 70%|██████▉   | 601/861 [00:13<00:06, 41.54it/s][A
 70%|███████   | 606/861 [00:13<00:06, 42.46it/s][A
 71%|███████   | 611/861 [00:13<00:05, 43.03it/s][A
 72%|███████▏  | 616/861 [00:14<00:05, 42.92it/s][A
 72%|███████▏  | 621/861 [00:14<00:05, 43.59it/s][A
 73%|███████▎  | 626/861 [00:14<00:05, 44.00it/s][A
 73%|███████▎  | 631/861 [00:14<00:05, 44.20it/s][A
 74%|███████▍  | 636/861 [00:14<00:05, 44.02it/s][A
 74%|███████▍  | 641/861 [00:14<00:05, 43.82it/s][A
 75%|███████▌  | 646/861 [00:14<00:04, 43.80it/s][A
 76%|███████▌  | 651/861 [00:14<00:04, 44.00it/s][A
 76%|███████▌  | 656/861 [00:15<00:04, 44.15it/s][A
 77%|███████▋  | 661/861 [00:15<00:04, 44.30it/s][A
 77%|███████▋  | 666/861 [00:15<00:04, 44.42it/s][A
 78%|███████▊  | 671/861 [00:15<00:04, 44.42it/s][A
 79%|███████▊  | 676/861 [00:15<00:04, 44.56it/s][A
 79%|███████▉  | 681/861 [00:15<00:04, 44.28it/s][A
 80%|███████▉  | 686/861 [00:15<00:03, 44.10it/s][A
 80%|████████  | 691/861 [00:15<00:03, 44.06it/s][A
 81%|████████  | 696/861 [00:15<00:03, 44.14it/s][A
 81%|████████▏ | 701/861 [00:16<00:03, 44.28it/s][A
 82%|████████▏ | 706/861 [00:16<00:03, 44.40it/s][A
 83%|████████▎ | 711/861 [00:16<00:03, 44.49it/s][A
 83%|████████▎ | 716/861 [00:16<00:03, 44.51it/s][A
 84%|████████▎ | 721/861 [00:16<00:03, 44.45it/s][A
 84%|████████▍ | 726/861 [00:16<00:03, 44.20it/s][A
 85%|████████▍ | 731/861 [00:16<00:02, 44.09it/s][A
 85%|████████▌ | 736/861 [00:16<00:02, 42.16it/s][A
 86%|████████▌ | 741/861 [00:16<00:02, 42.93it/s][A
 87%|████████▋ | 746/861 [00:17<00:02, 43.45it/s][A
 87%|████████▋ | 751/861 [00:17<00:02, 43.88it/s][A
 88%|████████▊ | 756/861 [00:17<00:02, 44.19it/s][A
 88%|████████▊ | 761/861 [00:17<00:02, 44.24it/s][A
 89%|████████▉ | 766/861 [00:17<00:02, 44.22it/s][A
 90%|████████▉ | 771/861 [00:17<00:02, 43.97it/s][A
 90%|█████████ | 776/861 [00:17<00:01, 43.74it/s][A
 91%|█████████ | 781/861 [00:17<00:01, 43.88it/s][A
 91%|█████████▏| 786/861 [00:17<00:01, 44.10it/s][A
 92%|█████████▏| 791/861 [00:18<00:01, 44.23it/s][A
 92%|█████████▏| 796/861 [00:18<00:01, 44.47it/s][A
 93%|█████████▎| 801/861 [00:18<00:01, 44.55it/s][A
 94%|█████████▎| 806/861 [00:18<00:01, 44.53it/s][A
 94%|█████████▍| 811/861 [00:18<00:01, 44.30it/s][A
 95%|█████████▍| 816/861 [00:18<00:01, 44.00it/s][A
 95%|█████████▌| 821/861 [00:18<00:00, 43.91it/s][A
 96%|█████████▌| 826/861 [00:18<00:00, 43.97it/s][A
 97%|█████████▋| 831/861 [00:18<00:00, 44.18it/s][A
 97%|█████████▋| 836/861 [00:19<00:00, 44.19it/s][A
 98%|█████████▊| 841/861 [00:19<00:00, 44.34it/s][A
 98%|█████████▊| 846/861 [00:19<00:00, 44.53it/s][A
 99%|█████████▉| 851/861 [00:19<00:00, 44.58it/s][A
 99%|█████████▉| 856/861 [00:19<00:00, 44.43it/s][A
100%|██████████| 861/861 [00:19<00:00, 44.23it/s][A                                                 
                                                 [A100%|██████████| 405/405 [03:56<00:00,  3.99it/s]
100%|██████████| 861/861 [00:19<00:00, 44.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:43:38,441 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405
[INFO|configuration_utils.py:351] 2023-08-28 00:43:38,606 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:43:41,773 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:43:41,910 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:43:42,006 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:43:43,451 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:43:43,451 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81 (score: 1.0493918657302856).
                                                 100%|██████████| 405/405 [04:09<00:00,  3.99it/s]100%|██████████| 405/405 [04:09<00:00,  1.62it/s]
[INFO|trainer.py:1894] 2023-08-28 00:43:52,267 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:43:52,389 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:43:55,278 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:43:55,416 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:43:55,478 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:43:55,898 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:43:55,899 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:43:55,899 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:43:55,899 >>   train_runtime            = 0:04:09.97
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:43:55,899 >>   train_samples            =       5160
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:43:55,899 >>   train_samples_per_second =     103.21
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:43:55,899 >>   train_steps_per_second   =       1.62
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.6679, 'eval_samples_per_second': 350.011, 'eval_steps_per_second': 43.777, 'epoch': 5.0}
{'train_runtime': 249.976, 'train_samples_per_second': 103.21, 'train_steps_per_second': 1.62, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 00:43:56 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:43:56,134 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:43:56,134 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 00:43:56,134 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 55.54it/s]  1%|▏         | 12/861 [00:00<00:17, 48.57it/s]  2%|▏         | 17/861 [00:00<00:17, 47.31it/s]  3%|▎         | 22/861 [00:00<00:18, 46.31it/s]  3%|▎         | 27/861 [00:00<00:18, 45.75it/s]  4%|▎         | 32/861 [00:00<00:18, 45.43it/s]  4%|▍         | 37/861 [00:00<00:18, 45.26it/s]  5%|▍         | 42/861 [00:00<00:18, 44.82it/s]  5%|▌         | 47/861 [00:01<00:18, 44.21it/s]  6%|▌         | 52/861 [00:01<00:18, 44.10it/s]  7%|▋         | 57/861 [00:01<00:18, 44.10it/s]  7%|▋         | 62/861 [00:01<00:18, 44.37it/s]  8%|▊         | 67/861 [00:01<00:17, 44.53it/s]  8%|▊         | 72/861 [00:01<00:17, 44.59it/s]  9%|▉         | 77/861 [00:01<00:17, 44.69it/s] 10%|▉         | 82/861 [00:01<00:17, 44.57it/s] 10%|█         | 87/861 [00:01<00:17, 44.31it/s] 11%|█         | 92/861 [00:02<00:17, 44.11it/s] 11%|█▏        | 97/861 [00:02<00:17, 43.97it/s] 12%|█▏        | 102/861 [00:02<00:17, 44.06it/s] 12%|█▏        | 107/861 [00:02<00:17, 44.26it/s] 13%|█▎        | 112/861 [00:02<00:16, 44.47it/s] 14%|█▎        | 117/861 [00:02<00:16, 44.48it/s] 14%|█▍        | 122/861 [00:02<00:16, 44.60it/s] 15%|█▍        | 127/861 [00:02<00:16, 44.62it/s] 15%|█▌        | 132/861 [00:02<00:16, 44.38it/s] 16%|█▌        | 137/861 [00:03<00:16, 44.16it/s] 16%|█▋        | 142/861 [00:03<00:16, 44.09it/s] 17%|█▋        | 147/861 [00:03<00:16, 44.12it/s] 18%|█▊        | 152/861 [00:03<00:16, 44.28it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.42it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.59it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.64it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.55it/s] 21%|██        | 177/861 [00:04<00:17, 39.28it/s] 21%|██        | 182/861 [00:04<00:16, 40.90it/s] 22%|██▏       | 187/861 [00:04<00:16, 41.87it/s] 22%|██▏       | 192/861 [00:04<00:15, 42.77it/s] 23%|██▎       | 197/861 [00:04<00:15, 43.35it/s] 23%|██▎       | 202/861 [00:04<00:15, 43.81it/s] 24%|██▍       | 207/861 [00:04<00:14, 44.11it/s] 25%|██▍       | 212/861 [00:04<00:14, 44.19it/s] 25%|██▌       | 217/861 [00:04<00:14, 43.94it/s] 26%|██▌       | 222/861 [00:05<00:14, 43.80it/s] 26%|██▋       | 227/861 [00:05<00:14, 43.82it/s] 27%|██▋       | 232/861 [00:05<00:14, 44.18it/s] 28%|██▊       | 237/861 [00:05<00:14, 44.29it/s] 28%|██▊       | 242/861 [00:05<00:13, 44.46it/s] 29%|██▊       | 247/861 [00:05<00:13, 44.55it/s] 29%|██▉       | 252/861 [00:05<00:13, 44.58it/s] 30%|██▉       | 257/861 [00:05<00:13, 44.44it/s] 30%|███       | 262/861 [00:05<00:13, 44.15it/s] 31%|███       | 267/861 [00:06<00:13, 44.06it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.02it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.16it/s] 33%|███▎      | 282/861 [00:06<00:13, 44.33it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.54it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.55it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.51it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.43it/s] 36%|███▌      | 307/861 [00:06<00:12, 44.13it/s] 36%|███▌      | 312/861 [00:07<00:12, 43.47it/s] 37%|███▋      | 317/861 [00:07<00:12, 43.67it/s] 37%|███▋      | 322/861 [00:07<00:12, 43.96it/s] 38%|███▊      | 327/861 [00:07<00:12, 44.21it/s] 39%|███▊      | 332/861 [00:07<00:11, 44.35it/s] 39%|███▉      | 337/861 [00:07<00:11, 44.45it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.37it/s] 40%|████      | 347/861 [00:07<00:11, 44.19it/s] 41%|████      | 352/861 [00:07<00:11, 44.01it/s] 41%|████▏     | 357/861 [00:08<00:11, 44.03it/s] 42%|████▏     | 362/861 [00:08<00:11, 44.10it/s] 43%|████▎     | 367/861 [00:08<00:11, 44.30it/s] 43%|████▎     | 372/861 [00:08<00:11, 44.42it/s] 44%|████▍     | 377/861 [00:08<00:10, 44.49it/s] 44%|████▍     | 382/861 [00:08<00:10, 44.52it/s] 45%|████▍     | 387/861 [00:08<00:10, 44.35it/s] 46%|████▌     | 392/861 [00:08<00:10, 44.18it/s] 46%|████▌     | 397/861 [00:08<00:10, 44.04it/s] 47%|████▋     | 402/861 [00:09<00:10, 44.15it/s] 47%|████▋     | 407/861 [00:09<00:10, 44.21it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.35it/s] 48%|████▊     | 417/861 [00:09<00:09, 44.48it/s] 49%|████▉     | 422/861 [00:09<00:09, 44.54it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.46it/s] 50%|█████     | 432/861 [00:09<00:09, 44.20it/s] 51%|█████     | 437/861 [00:09<00:09, 44.15it/s] 51%|█████▏    | 442/861 [00:09<00:09, 44.14it/s] 52%|█████▏    | 447/861 [00:10<00:10, 39.55it/s] 52%|█████▏    | 452/861 [00:10<00:09, 41.03it/s] 53%|█████▎    | 457/861 [00:10<00:09, 42.10it/s] 54%|█████▎    | 462/861 [00:10<00:09, 42.89it/s] 54%|█████▍    | 467/861 [00:10<00:09, 43.48it/s] 55%|█████▍    | 472/861 [00:10<00:08, 43.82it/s] 55%|█████▌    | 477/861 [00:10<00:08, 44.00it/s] 56%|█████▌    | 482/861 [00:10<00:08, 44.01it/s] 57%|█████▋    | 487/861 [00:11<00:08, 43.72it/s] 57%|█████▋    | 492/861 [00:11<00:08, 43.58it/s] 58%|█████▊    | 497/861 [00:11<00:08, 43.73it/s] 58%|█████▊    | 502/861 [00:11<00:08, 44.11it/s] 59%|█████▉    | 507/861 [00:11<00:07, 44.30it/s] 59%|█████▉    | 512/861 [00:11<00:07, 44.47it/s] 60%|██████    | 517/861 [00:11<00:07, 44.56it/s] 61%|██████    | 522/861 [00:11<00:07, 44.49it/s] 61%|██████    | 527/861 [00:11<00:07, 44.27it/s] 62%|██████▏   | 532/861 [00:12<00:07, 44.04it/s] 62%|██████▏   | 537/861 [00:12<00:07, 43.99it/s] 63%|██████▎   | 542/861 [00:12<00:07, 43.96it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.25it/s] 64%|██████▍   | 552/861 [00:12<00:06, 44.40it/s] 65%|██████▍   | 557/861 [00:12<00:06, 44.47it/s] 65%|██████▌   | 562/861 [00:12<00:06, 44.50it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.43it/s] 66%|██████▋   | 572/861 [00:12<00:06, 44.19it/s] 67%|██████▋   | 577/861 [00:13<00:06, 43.95it/s] 68%|██████▊   | 582/861 [00:13<00:07, 38.79it/s] 68%|██████▊   | 587/861 [00:13<00:06, 40.39it/s] 69%|██████▉   | 592/861 [00:13<00:06, 41.76it/s] 69%|██████▉   | 597/861 [00:13<00:06, 42.64it/s] 70%|██████▉   | 602/861 [00:13<00:05, 43.30it/s] 70%|███████   | 607/861 [00:13<00:05, 43.70it/s] 71%|███████   | 612/861 [00:13<00:05, 43.95it/s] 72%|███████▏  | 617/861 [00:14<00:05, 43.93it/s] 72%|███████▏  | 622/861 [00:14<00:05, 43.61it/s] 73%|███████▎  | 627/861 [00:14<00:05, 43.47it/s] 73%|███████▎  | 632/861 [00:14<00:05, 43.77it/s] 74%|███████▍  | 637/861 [00:14<00:05, 43.96it/s] 75%|███████▍  | 642/861 [00:14<00:04, 44.17it/s] 75%|███████▌  | 647/861 [00:14<00:04, 44.36it/s] 76%|███████▌  | 652/861 [00:14<00:04, 44.49it/s] 76%|███████▋  | 657/861 [00:14<00:04, 44.62it/s] 77%|███████▋  | 662/861 [00:15<00:04, 44.32it/s] 77%|███████▋  | 667/861 [00:15<00:04, 44.01it/s] 78%|███████▊  | 672/861 [00:15<00:04, 43.82it/s] 79%|███████▊  | 677/861 [00:15<00:04, 43.95it/s] 79%|███████▉  | 682/861 [00:15<00:04, 44.04it/s] 80%|███████▉  | 687/861 [00:15<00:03, 44.22it/s] 80%|████████  | 692/861 [00:15<00:03, 44.43it/s] 81%|████████  | 697/861 [00:15<00:03, 44.52it/s] 82%|████████▏ | 702/861 [00:15<00:03, 44.61it/s] 82%|████████▏ | 707/861 [00:16<00:03, 44.39it/s] 83%|████████▎ | 712/861 [00:16<00:03, 44.07it/s] 83%|████████▎ | 717/861 [00:16<00:03, 41.52it/s] 84%|████████▍ | 722/861 [00:16<00:03, 42.47it/s] 84%|████████▍ | 727/861 [00:16<00:03, 43.14it/s] 85%|████████▌ | 732/861 [00:16<00:02, 43.54it/s] 86%|████████▌ | 737/861 [00:16<00:02, 43.87it/s] 86%|████████▌ | 742/861 [00:16<00:02, 44.15it/s] 87%|████████▋ | 747/861 [00:16<00:02, 44.19it/s] 87%|████████▋ | 752/861 [00:17<00:02, 44.06it/s] 88%|████████▊ | 757/861 [00:17<00:02, 43.69it/s] 89%|████████▊ | 762/861 [00:17<00:02, 43.68it/s] 89%|████████▉ | 767/861 [00:17<00:02, 43.94it/s] 90%|████████▉ | 772/861 [00:17<00:02, 44.18it/s] 90%|█████████ | 777/861 [00:17<00:01, 44.37it/s] 91%|█████████ | 782/861 [00:17<00:01, 44.47it/s] 91%|█████████▏| 787/861 [00:17<00:01, 44.51it/s] 92%|█████████▏| 792/861 [00:18<00:01, 44.40it/s] 93%|█████████▎| 797/861 [00:18<00:01, 44.19it/s] 93%|█████████▎| 802/861 [00:18<00:01, 43.86it/s] 94%|█████████▎| 807/861 [00:18<00:01, 43.87it/s] 94%|█████████▍| 812/861 [00:18<00:01, 43.94it/s] 95%|█████████▍| 817/861 [00:18<00:00, 44.11it/s] 95%|█████████▌| 822/861 [00:18<00:00, 44.31it/s] 96%|█████████▌| 827/861 [00:18<00:00, 44.49it/s] 97%|█████████▋| 832/861 [00:18<00:00, 44.51it/s] 97%|█████████▋| 837/861 [00:19<00:00, 44.47it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.20it/s] 98%|█████████▊| 847/861 [00:19<00:00, 43.83it/s] 99%|█████████▉| 852/861 [00:19<00:00, 41.12it/s]100%|█████████▉| 857/861 [00:19<00:00, 42.21it/s]100%|██████████| 861/861 [00:19<00:00, 43.93it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:44:15,750 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:44:15,750 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:44:15,750 >>   eval_loss               =     1.0494
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:44:15,750 >>   eval_runtime            = 0:00:19.61
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:44:15,750 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:44:15,750 >>   eval_samples_per_second =     350.93
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:44:15,750 >>   eval_steps_per_second   =     43.892
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:44:15,751 >>   perplexity              =     2.8559
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:26,390 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:26,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:26,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:26,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:26,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:44:26,958 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:44:26,959 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:44:27,279 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:44:28,460 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:44:28,460 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:31,833 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:31,835 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:31,835 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:31,835 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:44:31,835 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:44:32,706 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:44:32,707 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:44:33,353 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:44:33,573 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:44:33,573 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.65it/s]Extractor Predicting: 14it [00:08,  1.72it/s]Extractor Predicting: 15it [00:09,  1.75it/s]Extractor Predicting: 16it [00:10,  1.74it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:11,  1.70it/s]Extractor Predicting: 19it [00:11,  1.72it/s]Extractor Predicting: 20it [00:12,  1.69it/s]Extractor Predicting: 21it [00:13,  1.65it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:14,  1.66it/s]Extractor Predicting: 24it [00:14,  1.70it/s]Extractor Predicting: 25it [00:15,  1.71it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:17,  1.68it/s]Extractor Predicting: 29it [00:17,  1.68it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:20,  1.66it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:21,  1.66it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.68it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:26,  1.71it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:27,  1.73it/s]Extractor Predicting: 46it [00:27,  1.75it/s]Extractor Predicting: 47it [00:28,  1.71it/s]Extractor Predicting: 48it [00:29,  1.71it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.64it/s]Extractor Predicting: 51it [00:30,  1.63it/s]Extractor Predicting: 52it [00:31,  1.63it/s]Extractor Predicting: 53it [00:32,  1.62it/s]Extractor Predicting: 54it [00:32,  1.64it/s]Extractor Predicting: 55it [00:33,  1.59it/s]Extractor Predicting: 56it [00:34,  1.62it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:35,  1.68it/s]Extractor Predicting: 59it [00:35,  1.64it/s]Extractor Predicting: 60it [00:36,  1.59it/s]Extractor Predicting: 61it [00:37,  1.64it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:38,  1.62it/s]Extractor Predicting: 64it [00:38,  1.63it/s]Extractor Predicting: 65it [00:39,  1.59it/s]Extractor Predicting: 66it [00:40,  1.61it/s]Extractor Predicting: 67it [00:40,  1.66it/s]Extractor Predicting: 68it [00:41,  1.67it/s]Extractor Predicting: 69it [00:41,  1.65it/s]Extractor Predicting: 70it [00:42,  1.62it/s]Extractor Predicting: 71it [00:43,  1.62it/s]Extractor Predicting: 72it [00:43,  1.62it/s]Extractor Predicting: 73it [00:44,  1.64it/s]Extractor Predicting: 74it [00:44,  1.66it/s]Extractor Predicting: 75it [00:45,  1.67it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:46,  1.62it/s]Extractor Predicting: 78it [00:47,  1.65it/s]Extractor Predicting: 79it [00:47,  1.69it/s]Extractor Predicting: 80it [00:48,  1.70it/s]Extractor Predicting: 81it [00:49,  1.70it/s]Extractor Predicting: 82it [00:49,  1.78it/s]Extractor Predicting: 83it [00:50,  1.75it/s]Extractor Predicting: 84it [00:50,  1.76it/s]Extractor Predicting: 85it [00:51,  1.79it/s]Extractor Predicting: 86it [00:51,  1.80it/s]Extractor Predicting: 87it [00:52,  1.76it/s]Extractor Predicting: 88it [00:53,  1.77it/s]Extractor Predicting: 89it [00:53,  1.76it/s]Extractor Predicting: 90it [00:54,  1.80it/s]Extractor Predicting: 91it [00:54,  1.81it/s]Extractor Predicting: 92it [00:55,  1.84it/s]Extractor Predicting: 93it [00:55,  1.77it/s]Extractor Predicting: 94it [00:56,  1.79it/s]Extractor Predicting: 95it [00:56,  1.84it/s]Extractor Predicting: 96it [00:57,  1.79it/s]Extractor Predicting: 97it [00:57,  1.85it/s]Extractor Predicting: 98it [00:58,  1.84it/s]Extractor Predicting: 99it [00:59,  1.81it/s]Extractor Predicting: 100it [00:59,  1.81it/s]Extractor Predicting: 101it [01:00,  1.78it/s]Extractor Predicting: 102it [01:00,  1.77it/s]Extractor Predicting: 103it [01:01,  1.79it/s]Extractor Predicting: 104it [01:01,  1.79it/s]Extractor Predicting: 105it [01:02,  1.76it/s]Extractor Predicting: 106it [01:03,  1.79it/s]Extractor Predicting: 107it [01:03,  1.78it/s]Extractor Predicting: 108it [01:04,  1.84it/s]Extractor Predicting: 109it [01:04,  1.83it/s]Extractor Predicting: 110it [01:05,  1.86it/s]Extractor Predicting: 111it [01:05,  1.84it/s]Extractor Predicting: 112it [01:06,  1.79it/s]Extractor Predicting: 113it [01:06,  1.79it/s]Extractor Predicting: 114it [01:07,  1.76it/s]Extractor Predicting: 115it [01:07,  1.82it/s]Extractor Predicting: 116it [01:08,  1.80it/s]Extractor Predicting: 117it [01:09,  1.75it/s]Extractor Predicting: 118it [01:09,  1.76it/s]Extractor Predicting: 119it [01:10,  1.80it/s]Extractor Predicting: 120it [01:10,  1.84it/s]Extractor Predicting: 121it [01:11,  1.84it/s]Extractor Predicting: 122it [01:12,  1.59it/s]Extractor Predicting: 123it [01:12,  1.66it/s]Extractor Predicting: 124it [01:13,  1.67it/s]Extractor Predicting: 125it [01:13,  1.75it/s]Extractor Predicting: 126it [01:14,  1.79it/s]Extractor Predicting: 127it [01:14,  1.83it/s]Extractor Predicting: 128it [01:15,  1.76it/s]Extractor Predicting: 129it [01:15,  1.79it/s]Extractor Predicting: 130it [01:16,  1.81it/s]Extractor Predicting: 131it [01:17,  1.83it/s]Extractor Predicting: 132it [01:17,  1.88it/s]Extractor Predicting: 133it [01:18,  1.86it/s]Extractor Predicting: 134it [01:18,  1.86it/s]Extractor Predicting: 135it [01:19,  1.81it/s]Extractor Predicting: 136it [01:19,  1.82it/s]Extractor Predicting: 137it [01:20,  1.81it/s]Extractor Predicting: 138it [01:20,  1.81it/s]Extractor Predicting: 139it [01:21,  1.77it/s]Extractor Predicting: 140it [01:22,  1.76it/s]Extractor Predicting: 141it [01:22,  1.80it/s]Extractor Predicting: 142it [01:23,  1.75it/s]Extractor Predicting: 143it [01:23,  1.81it/s]Extractor Predicting: 144it [01:24,  1.77it/s]Extractor Predicting: 145it [01:24,  1.80it/s]Extractor Predicting: 146it [01:25,  1.81it/s]Extractor Predicting: 147it [01:25,  1.81it/s]Extractor Predicting: 148it [01:26,  1.77it/s]Extractor Predicting: 149it [01:26,  1.85it/s]Extractor Predicting: 150it [01:27,  1.76it/s]Extractor Predicting: 151it [01:28,  1.70it/s]Extractor Predicting: 152it [01:28,  1.65it/s]Extractor Predicting: 153it [01:29,  1.62it/s]Extractor Predicting: 154it [01:30,  1.49it/s]Extractor Predicting: 155it [01:30,  1.53it/s]Extractor Predicting: 156it [01:31,  1.50it/s]Extractor Predicting: 157it [01:32,  1.54it/s]Extractor Predicting: 158it [01:32,  1.54it/s]Extractor Predicting: 159it [01:33,  1.50it/s]Extractor Predicting: 160it [01:34,  1.49it/s]Extractor Predicting: 161it [01:34,  1.54it/s]Extractor Predicting: 162it [01:35,  1.49it/s]Extractor Predicting: 163it [01:36,  1.49it/s]Extractor Predicting: 164it [01:37,  1.45it/s]Extractor Predicting: 165it [01:37,  1.43it/s]Extractor Predicting: 166it [01:38,  1.45it/s]Extractor Predicting: 167it [01:39,  1.46it/s]Extractor Predicting: 168it [01:39,  1.48it/s]Extractor Predicting: 169it [01:40,  1.42it/s]Extractor Predicting: 170it [01:41,  1.44it/s]Extractor Predicting: 171it [01:41,  1.46it/s]Extractor Predicting: 172it [01:42,  1.45it/s]Extractor Predicting: 173it [01:43,  1.47it/s]Extractor Predicting: 174it [01:43,  1.49it/s]Extractor Predicting: 175it [01:44,  1.49it/s]Extractor Predicting: 176it [01:45,  1.48it/s]Extractor Predicting: 177it [01:45,  1.50it/s]Extractor Predicting: 178it [01:46,  1.49it/s]Extractor Predicting: 179it [01:47,  1.48it/s]Extractor Predicting: 180it [01:47,  1.47it/s]Extractor Predicting: 181it [01:48,  1.46it/s]Extractor Predicting: 182it [01:49,  1.48it/s]Extractor Predicting: 183it [01:49,  1.51it/s]Extractor Predicting: 184it [01:50,  1.52it/s]Extractor Predicting: 185it [01:51,  1.54it/s]Extractor Predicting: 186it [01:51,  1.55it/s]Extractor Predicting: 187it [01:52,  1.53it/s]Extractor Predicting: 188it [01:53,  1.57it/s]Extractor Predicting: 189it [01:53,  1.48it/s]Extractor Predicting: 190it [01:54,  1.48it/s]Extractor Predicting: 191it [01:55,  1.45it/s]Extractor Predicting: 192it [01:56,  1.41it/s]Extractor Predicting: 193it [01:56,  1.46it/s]Extractor Predicting: 194it [01:57,  1.48it/s]Extractor Predicting: 195it [01:57,  1.49it/s]Extractor Predicting: 196it [01:58,  1.53it/s]Extractor Predicting: 197it [01:59,  1.55it/s]Extractor Predicting: 198it [01:59,  1.57it/s]Extractor Predicting: 199it [02:01,  1.11it/s]Extractor Predicting: 200it [02:01,  1.23it/s]Extractor Predicting: 201it [02:02,  1.28it/s]Extractor Predicting: 202it [02:03,  1.35it/s]Extractor Predicting: 203it [02:03,  1.41it/s]Extractor Predicting: 204it [02:04,  1.46it/s]Extractor Predicting: 205it [02:05,  1.54it/s]Extractor Predicting: 206it [02:05,  1.54it/s]Extractor Predicting: 207it [02:06,  1.57it/s]Extractor Predicting: 208it [02:07,  1.56it/s]Extractor Predicting: 209it [02:07,  1.62it/s]Extractor Predicting: 210it [02:08,  1.63it/s]Extractor Predicting: 211it [02:08,  1.56it/s]Extractor Predicting: 212it [02:09,  1.43it/s]Extractor Predicting: 213it [02:10,  1.48it/s]Extractor Predicting: 214it [02:10,  1.52it/s]Extractor Predicting: 215it [02:11,  1.58it/s]Extractor Predicting: 216it [02:12,  1.61it/s]Extractor Predicting: 217it [02:12,  1.60it/s]Extractor Predicting: 218it [02:13,  1.63it/s]Extractor Predicting: 219it [02:14,  1.59it/s]Extractor Predicting: 220it [02:14,  1.58it/s]Extractor Predicting: 221it [02:15,  1.61it/s]Extractor Predicting: 222it [02:15,  1.62it/s]Extractor Predicting: 223it [02:16,  1.64it/s]Extractor Predicting: 224it [02:17,  1.63it/s]Extractor Predicting: 225it [02:17,  1.66it/s]Extractor Predicting: 226it [02:18,  1.64it/s]Extractor Predicting: 227it [02:18,  1.63it/s]Extractor Predicting: 228it [02:19,  1.61it/s]Extractor Predicting: 229it [02:20,  1.62it/s]Extractor Predicting: 230it [02:20,  1.63it/s]Extractor Predicting: 231it [02:21,  1.67it/s]Extractor Predicting: 232it [02:21,  1.68it/s]Extractor Predicting: 233it [02:22,  1.65it/s]Extractor Predicting: 234it [02:23,  1.64it/s]Extractor Predicting: 235it [02:23,  1.62it/s]Extractor Predicting: 236it [02:24,  1.62it/s]Extractor Predicting: 237it [02:25,  1.57it/s]Extractor Predicting: 237it [02:25,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:12,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:12,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:12,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:12,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:12,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:47:13,323 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:47:13,324 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:47:13,953 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:47:15,166 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:47:15,166 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:18,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:18,404 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:18,404 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:18,404 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:18,404 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:47:19,324 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:47:19,325 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:47:19,973 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:47:20,232 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:47:20,232 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 7it [00:03,  1.85it/s]Extractor Predicting: 8it [00:04,  1.87it/s]Extractor Predicting: 9it [00:05,  1.85it/s]Extractor Predicting: 10it [00:05,  1.85it/s]Extractor Predicting: 11it [00:06,  1.84it/s]Extractor Predicting: 12it [00:06,  1.85it/s]Extractor Predicting: 13it [00:07,  1.82it/s]Extractor Predicting: 14it [00:07,  1.80it/s]Extractor Predicting: 15it [00:08,  1.79it/s]Extractor Predicting: 16it [00:08,  1.78it/s]Extractor Predicting: 17it [00:09,  1.78it/s]Extractor Predicting: 18it [00:10,  1.80it/s]Extractor Predicting: 19it [00:10,  1.78it/s]Extractor Predicting: 20it [00:11,  1.79it/s]Extractor Predicting: 21it [00:11,  1.84it/s]Extractor Predicting: 22it [00:12,  1.86it/s]Extractor Predicting: 23it [00:12,  1.89it/s]Extractor Predicting: 24it [00:13,  1.92it/s]Extractor Predicting: 25it [00:13,  1.88it/s]Extractor Predicting: 26it [00:14,  1.89it/s]Extractor Predicting: 27it [00:14,  1.88it/s]Extractor Predicting: 28it [00:15,  1.88it/s]Extractor Predicting: 29it [00:15,  1.85it/s]Extractor Predicting: 30it [00:16,  1.84it/s]Extractor Predicting: 31it [00:17,  1.80it/s]Extractor Predicting: 32it [00:17,  1.84it/s]Extractor Predicting: 33it [00:18,  1.80it/s]Extractor Predicting: 34it [00:18,  1.77it/s]Extractor Predicting: 35it [00:19,  1.78it/s]Extractor Predicting: 36it [00:19,  1.80it/s]Extractor Predicting: 37it [00:20,  1.82it/s]Extractor Predicting: 38it [00:20,  1.88it/s]Extractor Predicting: 39it [00:21,  1.83it/s]Extractor Predicting: 40it [00:22,  1.79it/s]Extractor Predicting: 41it [00:22,  1.72it/s]Extractor Predicting: 42it [00:23,  1.72it/s]Extractor Predicting: 43it [00:23,  1.69it/s]Extractor Predicting: 44it [00:24,  1.68it/s]Extractor Predicting: 45it [00:25,  1.69it/s]Extractor Predicting: 46it [00:25,  1.65it/s]Extractor Predicting: 47it [00:26,  1.65it/s]Extractor Predicting: 48it [00:26,  1.66it/s]Extractor Predicting: 49it [00:27,  1.67it/s]Extractor Predicting: 50it [00:28,  1.68it/s]Extractor Predicting: 51it [00:28,  1.70it/s]Extractor Predicting: 52it [00:29,  1.70it/s]Extractor Predicting: 53it [00:29,  1.71it/s]Extractor Predicting: 54it [00:30,  1.68it/s]Extractor Predicting: 55it [00:30,  1.70it/s]Extractor Predicting: 56it [00:31,  1.73it/s]Extractor Predicting: 57it [00:32,  1.70it/s]Extractor Predicting: 58it [00:32,  1.72it/s]Extractor Predicting: 59it [00:33,  1.71it/s]Extractor Predicting: 60it [00:33,  1.71it/s]Extractor Predicting: 61it [00:34,  1.68it/s]Extractor Predicting: 62it [00:35,  1.72it/s]Extractor Predicting: 63it [00:35,  1.73it/s]Extractor Predicting: 64it [00:36,  1.77it/s]Extractor Predicting: 65it [00:36,  1.76it/s]Extractor Predicting: 66it [00:37,  1.63it/s]Extractor Predicting: 67it [00:38,  1.67it/s]Extractor Predicting: 68it [00:38,  1.69it/s]Extractor Predicting: 69it [00:39,  1.71it/s]Extractor Predicting: 70it [00:39,  1.56it/s]Extractor Predicting: 71it [00:40,  1.62it/s]Extractor Predicting: 72it [00:41,  1.63it/s]Extractor Predicting: 73it [00:41,  1.63it/s]Extractor Predicting: 74it [00:42,  1.70it/s]Extractor Predicting: 75it [00:42,  1.71it/s]Extractor Predicting: 76it [00:43,  1.70it/s]Extractor Predicting: 77it [00:44,  1.71it/s]Extractor Predicting: 78it [00:44,  1.70it/s]Extractor Predicting: 79it [00:45,  1.72it/s]Extractor Predicting: 80it [00:45,  1.75it/s]Extractor Predicting: 81it [00:46,  1.73it/s]Extractor Predicting: 82it [00:46,  1.68it/s]Extractor Predicting: 83it [00:47,  1.72it/s]Extractor Predicting: 84it [00:48,  1.68it/s]Extractor Predicting: 85it [00:48,  1.66it/s]Extractor Predicting: 86it [00:49,  1.70it/s]Extractor Predicting: 87it [00:49,  1.75it/s]Extractor Predicting: 88it [00:50,  1.70it/s]Extractor Predicting: 89it [00:51,  1.69it/s]Extractor Predicting: 90it [00:51,  1.65it/s]Extractor Predicting: 91it [00:52,  1.66it/s]Extractor Predicting: 92it [00:52,  1.70it/s]Extractor Predicting: 93it [00:53,  1.75it/s]Extractor Predicting: 94it [00:54,  1.69it/s]Extractor Predicting: 95it [00:54,  1.68it/s]Extractor Predicting: 96it [00:55,  1.69it/s]Extractor Predicting: 97it [00:55,  1.69it/s]Extractor Predicting: 98it [00:56,  1.70it/s]Extractor Predicting: 99it [00:56,  1.72it/s]Extractor Predicting: 100it [00:57,  1.62it/s]Extractor Predicting: 101it [00:58,  1.66it/s]Extractor Predicting: 102it [00:58,  1.65it/s]Extractor Predicting: 103it [00:59,  1.69it/s]Extractor Predicting: 104it [00:59,  1.74it/s]Extractor Predicting: 105it [01:00,  1.73it/s]Extractor Predicting: 106it [01:01,  1.78it/s]Extractor Predicting: 107it [01:01,  1.78it/s]Extractor Predicting: 108it [01:02,  1.81it/s]Extractor Predicting: 109it [01:02,  1.76it/s]Extractor Predicting: 110it [01:03,  1.76it/s]Extractor Predicting: 111it [01:03,  1.74it/s]Extractor Predicting: 112it [01:04,  1.74it/s]Extractor Predicting: 113it [01:05,  1.76it/s]Extractor Predicting: 114it [01:05,  1.72it/s]Extractor Predicting: 115it [01:06,  1.70it/s]Extractor Predicting: 116it [01:06,  1.75it/s]Extractor Predicting: 117it [01:07,  1.74it/s]Extractor Predicting: 118it [01:07,  1.78it/s]Extractor Predicting: 119it [01:08,  1.73it/s]Extractor Predicting: 120it [01:09,  1.74it/s]Extractor Predicting: 121it [01:09,  1.68it/s]Extractor Predicting: 122it [01:10,  1.66it/s]Extractor Predicting: 123it [01:10,  1.69it/s]Extractor Predicting: 124it [01:11,  1.72it/s]Extractor Predicting: 125it [01:12,  1.72it/s]Extractor Predicting: 126it [01:12,  1.74it/s]Extractor Predicting: 127it [01:13,  1.72it/s]Extractor Predicting: 128it [01:13,  1.78it/s]Extractor Predicting: 129it [01:14,  1.78it/s]Extractor Predicting: 130it [01:14,  1.79it/s]Extractor Predicting: 131it [01:15,  1.75it/s]Extractor Predicting: 132it [01:16,  1.72it/s]Extractor Predicting: 133it [01:16,  1.71it/s]Extractor Predicting: 134it [01:17,  1.74it/s]Extractor Predicting: 135it [01:17,  1.75it/s]Extractor Predicting: 136it [01:18,  1.76it/s]Extractor Predicting: 137it [01:18,  1.70it/s]Extractor Predicting: 138it [01:19,  1.69it/s]Extractor Predicting: 139it [01:20,  1.68it/s]Extractor Predicting: 140it [01:20,  1.76it/s]Extractor Predicting: 141it [01:21,  1.75it/s]Extractor Predicting: 142it [01:21,  1.71it/s]Extractor Predicting: 143it [01:22,  1.66it/s]Extractor Predicting: 144it [01:23,  1.67it/s]Extractor Predicting: 145it [01:23,  1.70it/s]Extractor Predicting: 146it [01:24,  1.69it/s]Extractor Predicting: 147it [01:24,  1.70it/s]Extractor Predicting: 148it [01:25,  1.63it/s]Extractor Predicting: 149it [01:26,  1.60it/s]Extractor Predicting: 150it [01:26,  1.63it/s]Extractor Predicting: 151it [01:27,  1.67it/s]Extractor Predicting: 152it [01:27,  1.67it/s]Extractor Predicting: 153it [01:28,  1.69it/s]Extractor Predicting: 154it [01:29,  1.65it/s]Extractor Predicting: 155it [01:29,  1.65it/s]Extractor Predicting: 156it [01:30,  1.52it/s]Extractor Predicting: 157it [01:31,  1.53it/s]Extractor Predicting: 158it [01:31,  1.54it/s]Extractor Predicting: 159it [01:32,  1.56it/s]Extractor Predicting: 160it [01:32,  1.60it/s]Extractor Predicting: 161it [01:33,  1.63it/s]Extractor Predicting: 162it [01:34,  1.61it/s]Extractor Predicting: 163it [01:34,  1.58it/s]Extractor Predicting: 164it [01:35,  1.59it/s]Extractor Predicting: 165it [01:36,  1.60it/s]Extractor Predicting: 166it [01:36,  1.59it/s]Extractor Predicting: 167it [01:37,  1.62it/s]Extractor Predicting: 168it [01:37,  1.77it/s]Extractor Predicting: 168it [01:37,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,960 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:07,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:49:08,596 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:49:08,597 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:49:09,205 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:49:10,271 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:49:10,272 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,622 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:49:13,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:49:14,616 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:49:14,617 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:49:15,457 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:49:15,750 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:49:15,750 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.73it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:08,  1.65it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:10,  1.59it/s]Extractor Predicting: 19it [00:11,  1.73it/s]Extractor Predicting: 19it [00:11,  1.68it/s]
[INFO|configuration_utils.py:515] 2023-08-28 00:49:30,410 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:49:30,411 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:49:30,460 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:49:30,461 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 00:49:30,477 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:49:42,478 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 00:49:42,506 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 00:49:42,697 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:49:42,698 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:49:42,788 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:42,830 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:42,830 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:42,830 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:42,830 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:42,830 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:42,830 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 00:49:43,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:43,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:44,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:45,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:45,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:46,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:47,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:47,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:48,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:49,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:50,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:50,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:51,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:52,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:53,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:53,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:54,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:55,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:55,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:56,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:57,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:10, 14.53s/it][WARNING|generation_utils.py:914] 2023-08-28 00:49:57,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:58,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:59,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:49:59,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:00,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:01,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:01,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:02,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:02,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:03,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:04,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:04,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:05,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:06,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:07,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:07,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:08,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:09,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:09,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:10,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:11,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:11,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:12,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:13,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:14,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:14,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:15,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:16,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:17, 17.24s/it][WARNING|generation_utils.py:914] 2023-08-28 00:50:16,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:17,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:18,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:19,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:19,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:20,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:21,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:22,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:22,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:23,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:23,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:24,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:25,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:26,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:26,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:27,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:27,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:28,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:29,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:29,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:30,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:31,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:32,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:49<01:56, 16.62s/it][WARNING|generation_utils.py:914] 2023-08-28 00:50:32,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:33,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:33,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:34,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:35,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:35,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:36,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:37,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:38,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:38,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:39,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:39,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:40,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:41,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:41,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:42,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:42,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:43,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:44,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:44,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:45,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:46,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:46,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:04<01:34, 15.79s/it][WARNING|generation_utils.py:914] 2023-08-28 00:50:47,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:47,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:48,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:49,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:49,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:50,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:51,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:51,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:52,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:53,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:53,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:54,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:55,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:55,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:56,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:57,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:57,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:58,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:58,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:50:59,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:00,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:00,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:01,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:19<01:17, 15.49s/it][WARNING|generation_utils.py:914] 2023-08-28 00:51:02,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:02,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:03,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:04,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:04,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:05,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:05,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:06,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:07,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:07,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:08,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:09,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:09,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:10,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:10,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:11,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:12,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:12,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:13,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:14,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:14,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:15,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:16,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:33<01:00, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-28 00:51:16,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:17,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:17,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:18,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:19,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:19,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:20,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:21,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:21,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:22,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:23,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:23,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:24,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:25,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:26,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:26,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:27,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:28,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:28,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:29,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:29,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:30,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:31,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:31,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:32,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:49<00:46, 15.56s/it][WARNING|generation_utils.py:914] 2023-08-28 00:51:33,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:33,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:34,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:35,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:35,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:36,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:37,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:37,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:38,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:38,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:39,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:40,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:41,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:41,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:42,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:43,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:43,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:44,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:45,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:45,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:46,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:46,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:04<00:30, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 00:51:47,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:48,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:48,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:49,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:49,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:50,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:51,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:51,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:52,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:52,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:53,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:54,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:54,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:55,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:55,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:56,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:56,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:57,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:57,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:58,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:59,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:51:59,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:00,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:01,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:18<00:14, 14.95s/it][WARNING|generation_utils.py:914] 2023-08-28 00:52:01,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:02,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:03,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:03,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:04,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:05,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:06,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:06,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:07,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:08,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:08,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:09,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:10,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:11,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:11,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:12,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:13,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:14,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:14,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:15,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:16,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:16,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:17,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:35<00:00, 15.36s/it]Generating: 100%|██████████| 10/10 [02:35<00:00, 15.51s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:24,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:25,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:25,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:25,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:25,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:52:25,413 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:52:25,414 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:52:25,723 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:52:26,840 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:52:26,840 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:29,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:29,172 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:29,172 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:29,172 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:29,172 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:52:29,512 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:52:29,513 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:52:30,220 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:52:30,396 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:52:30,396 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9166666666666666, 'errors': {'', "('', 'composer', 'Jérémie Léger', 'The title track of the album was a track Iced Tea made by Jérémie Léger and written by Pierre Bousquet and Bernard Barrou , among others .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 397, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 443, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 489, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 555, 'raw': 800}
{'target': 600, 'success': 574, 'raw': 832}
{'target': 600, 'success': 593, 'raw': 864}
{'target': 600, 'success': 614, 'raw': 896}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.6852678571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : opposite of . Context : Later in the year ( 1143 ) , Ptolemaeus , the founder of the Macedonian Republic , married Ariadne , daughter of Ariadne , founder of Macedonian kings , Thebes and Ileani . Head Entity : Ileani , Tail Entity : Ileanius .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8491847826086957, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : creator .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.75375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Lothar', 'twinned administrative body', '', 'He died in the Battle of Mervyn at Rheinmetall in 645 along with his daughter Lothar , D.')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 12521
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12621, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.55it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:01,  1.52it/s]Extractor Estimating: 4it [00:02,  1.55it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.61it/s]Extractor Estimating: 8it [00:05,  1.60it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.55it/s]Extractor Estimating: 12it [00:07,  1.54it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.60it/s]Extractor Estimating: 16it [00:10,  1.54it/s]Extractor Estimating: 17it [00:10,  1.47it/s]Extractor Estimating: 18it [00:11,  1.50it/s]Extractor Estimating: 19it [00:12,  1.52it/s]Extractor Estimating: 20it [00:12,  1.53it/s]Extractor Estimating: 21it [00:13,  1.44it/s]Extractor Estimating: 22it [00:14,  1.50it/s]Extractor Estimating: 23it [00:14,  1.56it/s]Extractor Estimating: 24it [00:15,  1.51it/s]Extractor Estimating: 25it [00:16,  1.50it/s]Extractor Estimating: 26it [00:16,  1.53it/s]Extractor Estimating: 27it [00:17,  1.54it/s]Extractor Estimating: 28it [00:18,  1.53it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:19,  1.56it/s]Extractor Estimating: 31it [00:20,  1.58it/s]Extractor Estimating: 32it [00:20,  1.56it/s]Extractor Estimating: 33it [00:21,  1.52it/s]Extractor Estimating: 34it [00:22,  1.53it/s]Extractor Estimating: 35it [00:22,  1.51it/s]Extractor Estimating: 36it [00:23,  1.52it/s]Extractor Estimating: 37it [00:24,  1.48it/s]Extractor Estimating: 38it [00:24,  1.50it/s]Extractor Estimating: 39it [00:25,  1.49it/s]Extractor Estimating: 40it [00:26,  1.50it/s]Extractor Estimating: 41it [00:26,  1.52it/s]Extractor Estimating: 42it [00:27,  1.51it/s]Extractor Estimating: 43it [00:28,  1.54it/s]Extractor Estimating: 44it [00:28,  1.54it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:30,  1.52it/s]Extractor Estimating: 47it [00:30,  1.52it/s]Extractor Estimating: 48it [00:31,  1.51it/s]Extractor Estimating: 49it [00:32,  1.45it/s]Extractor Estimating: 50it [00:32,  1.48it/s]Extractor Estimating: 51it [00:33,  1.50it/s]Extractor Estimating: 52it [00:34,  1.52it/s]Extractor Estimating: 53it [00:34,  1.53it/s]Extractor Estimating: 54it [00:35,  1.52it/s]Extractor Estimating: 55it [00:36,  1.48it/s]Extractor Estimating: 56it [00:36,  1.51it/s]Extractor Estimating: 57it [00:37,  1.49it/s]Extractor Estimating: 58it [00:38,  1.49it/s]Extractor Estimating: 59it [00:38,  1.49it/s]Extractor Estimating: 60it [00:39,  1.52it/s]Extractor Estimating: 61it [00:39,  1.57it/s]Extractor Estimating: 62it [00:40,  1.55it/s]Extractor Estimating: 63it [00:41,  1.53it/s]Extractor Estimating: 64it [00:41,  1.53it/s]Extractor Estimating: 65it [00:42,  1.54it/s]Extractor Estimating: 66it [00:43,  1.61it/s]Extractor Estimating: 67it [00:43,  1.53it/s]Extractor Estimating: 68it [00:44,  1.61it/s]Extractor Estimating: 69it [00:44,  1.64it/s]Extractor Estimating: 70it [00:45,  1.70it/s]Extractor Estimating: 71it [00:46,  1.68it/s]Extractor Estimating: 72it [00:46,  1.63it/s]Extractor Estimating: 73it [00:47,  1.62it/s]Extractor Estimating: 74it [00:48,  1.61it/s]Extractor Estimating: 75it [00:48,  1.59it/s]Extractor Estimating: 76it [00:49,  1.60it/s]Extractor Estimating: 77it [00:49,  1.60it/s]Extractor Estimating: 78it [00:50,  1.54it/s]Extractor Estimating: 79it [00:51,  1.57it/s]Extractor Estimating: 80it [00:51,  1.53it/s]Extractor Estimating: 81it [00:52,  1.53it/s]Extractor Estimating: 82it [00:53,  1.54it/s]Extractor Estimating: 83it [00:53,  1.48it/s]Extractor Estimating: 84it [00:54,  1.43it/s]Extractor Estimating: 85it [00:55,  1.51it/s]Extractor Estimating: 86it [00:55,  1.55it/s]Extractor Estimating: 87it [00:56,  1.57it/s]Extractor Estimating: 88it [00:57,  1.56it/s]Extractor Estimating: 89it [00:57,  1.55it/s]Extractor Estimating: 90it [00:58,  1.57it/s]Extractor Estimating: 91it [00:59,  1.56it/s]Extractor Estimating: 92it [00:59,  1.56it/s]Extractor Estimating: 93it [01:00,  1.57it/s]Extractor Estimating: 94it [01:00,  1.58it/s]Extractor Estimating: 95it [01:01,  1.63it/s]Extractor Estimating: 96it [01:02,  1.63it/s]Extractor Estimating: 97it [01:02,  1.67it/s]Extractor Estimating: 98it [01:03,  1.68it/s]Extractor Estimating: 99it [01:03,  1.66it/s]Extractor Estimating: 100it [01:04,  1.66it/s]Extractor Estimating: 101it [01:05,  1.60it/s]Extractor Estimating: 102it [01:05,  1.61it/s]Extractor Estimating: 103it [01:06,  1.55it/s]Extractor Estimating: 104it [01:07,  1.52it/s]Extractor Estimating: 105it [01:07,  1.56it/s]Extractor Estimating: 106it [01:08,  1.56it/s]Extractor Estimating: 107it [01:09,  1.59it/s]Extractor Estimating: 108it [01:09,  1.59it/s]Extractor Estimating: 109it [01:10,  1.42it/s]Extractor Estimating: 110it [01:11,  1.50it/s]Extractor Estimating: 111it [01:11,  1.50it/s]Extractor Estimating: 112it [01:12,  1.57it/s]Extractor Estimating: 113it [01:12,  1.58it/s]Extractor Estimating: 114it [01:13,  1.57it/s]Extractor Estimating: 115it [01:14,  1.55it/s]Extractor Estimating: 116it [01:14,  1.59it/s]Extractor Estimating: 117it [01:15,  1.58it/s]Extractor Estimating: 118it [01:16,  1.54it/s]Extractor Estimating: 119it [01:16,  1.54it/s]Extractor Estimating: 120it [01:17,  1.54it/s]Extractor Estimating: 121it [01:18,  1.59it/s]Extractor Estimating: 122it [01:18,  1.60it/s]Extractor Estimating: 123it [01:19,  1.61it/s]Extractor Estimating: 124it [01:19,  1.60it/s]Extractor Estimating: 125it [01:20,  1.57it/s]Extractor Estimating: 126it [01:21,  1.54it/s]Extractor Estimating: 127it [01:21,  1.58it/s]Extractor Estimating: 128it [01:22,  1.60it/s]Extractor Estimating: 129it [01:23,  1.61it/s]Extractor Estimating: 130it [01:23,  1.65it/s]Extractor Estimating: 131it [01:24,  1.57it/s]Extractor Estimating: 132it [01:25,  1.55it/s]Extractor Estimating: 133it [01:25,  1.57it/s]Extractor Estimating: 134it [01:26,  1.62it/s]Extractor Estimating: 135it [01:26,  1.58it/s]Extractor Estimating: 136it [01:27,  1.56it/s]Extractor Estimating: 137it [01:28,  1.57it/s]Extractor Estimating: 138it [01:28,  1.58it/s]Extractor Estimating: 139it [01:29,  1.60it/s]Extractor Estimating: 140it [01:30,  1.59it/s]Extractor Estimating: 141it [01:30,  1.57it/s]Extractor Estimating: 142it [01:31,  1.56it/s]Extractor Estimating: 143it [01:32,  1.56it/s]Extractor Estimating: 144it [01:32,  1.54it/s]Extractor Estimating: 145it [01:33,  1.54it/s]Extractor Estimating: 146it [01:33,  1.55it/s]Extractor Estimating: 147it [01:34,  1.56it/s]Extractor Estimating: 148it [01:35,  1.56it/s]Extractor Estimating: 149it [01:35,  1.61it/s]Extractor Estimating: 150it [01:36,  1.60it/s]Extractor Estimating: 151it [01:37,  1.62it/s]Extractor Estimating: 152it [01:37,  1.58it/s]Extractor Estimating: 153it [01:38,  1.59it/s]Extractor Estimating: 154it [01:39,  1.44it/s]Extractor Estimating: 155it [01:39,  1.46it/s]Extractor Estimating: 156it [01:40,  1.49it/s]Extractor Estimating: 157it [01:41,  1.52it/s]Extractor Estimating: 158it [01:41,  1.55it/s]Extractor Estimating: 159it [01:42,  1.54it/s]Extractor Estimating: 160it [01:42,  1.57it/s]Extractor Estimating: 161it [01:43,  1.56it/s]Extractor Estimating: 162it [01:44,  1.59it/s]Extractor Estimating: 163it [01:44,  1.55it/s]Extractor Estimating: 164it [01:45,  1.50it/s]Extractor Estimating: 165it [01:46,  1.53it/s]Extractor Estimating: 166it [01:46,  1.56it/s]Extractor Estimating: 167it [01:47,  1.58it/s]Extractor Estimating: 168it [01:48,  1.63it/s]Extractor Estimating: 169it [01:48,  1.60it/s]Extractor Estimating: 170it [01:49,  1.55it/s]Extractor Estimating: 171it [01:50,  1.53it/s]Extractor Estimating: 172it [01:50,  1.57it/s]Extractor Estimating: 173it [01:51,  1.57it/s]Extractor Estimating: 174it [01:51,  1.55it/s]Extractor Estimating: 175it [01:52,  1.56it/s]Extractor Estimating: 176it [01:53,  1.58it/s]Extractor Estimating: 177it [01:53,  1.50it/s]Extractor Estimating: 178it [01:54,  1.52it/s]Extractor Estimating: 179it [01:55,  1.49it/s]Extractor Estimating: 180it [01:55,  1.52it/s]Extractor Estimating: 181it [01:56,  1.57it/s]Extractor Estimating: 182it [01:57,  1.58it/s]Extractor Estimating: 183it [01:57,  1.60it/s]Extractor Estimating: 184it [01:58,  1.59it/s]Extractor Estimating: 185it [01:59,  1.56it/s]Extractor Estimating: 186it [01:59,  1.55it/s]Extractor Estimating: 187it [02:00,  1.59it/s]Extractor Estimating: 188it [02:00,  1.56it/s]Extractor Estimating: 189it [02:01,  1.52it/s]Extractor Estimating: 190it [02:02,  1.56it/s]Extractor Estimating: 191it [02:02,  1.59it/s]Extractor Estimating: 192it [02:03,  1.57it/s]Extractor Estimating: 193it [02:04,  1.56it/s]Extractor Estimating: 194it [02:04,  1.51it/s]Extractor Estimating: 195it [02:05,  1.53it/s]Extractor Estimating: 196it [02:06,  1.52it/s]Extractor Estimating: 197it [02:06,  1.55it/s]Extractor Estimating: 198it [02:07,  1.55it/s]Extractor Estimating: 199it [02:08,  1.55it/s]Extractor Estimating: 200it [02:08,  1.55it/s]Extractor Estimating: 201it [02:09,  1.58it/s]Extractor Estimating: 202it [02:09,  1.61it/s]Extractor Estimating: 203it [02:10,  1.61it/s]Extractor Estimating: 204it [02:11,  1.58it/s]Extractor Estimating: 205it [02:11,  1.59it/s]Extractor Estimating: 206it [02:12,  1.61it/s]Extractor Estimating: 207it [02:13,  1.64it/s]Extractor Estimating: 208it [02:13,  1.64it/s]Extractor Estimating: 209it [02:14,  1.64it/s]Extractor Estimating: 210it [02:14,  1.64it/s]Extractor Estimating: 211it [02:15,  1.65it/s]Extractor Estimating: 212it [02:16,  1.63it/s]Extractor Estimating: 213it [02:16,  1.67it/s]Extractor Estimating: 214it [02:17,  1.70it/s]Extractor Estimating: 215it [02:17,  1.71it/s]Extractor Estimating: 216it [02:18,  1.71it/s]Extractor Estimating: 217it [02:19,  1.67it/s]Extractor Estimating: 218it [02:19,  1.68it/s]Extractor Estimating: 219it [02:20,  1.69it/s]Extractor Estimating: 220it [02:20,  1.70it/s]Extractor Estimating: 221it [02:21,  1.67it/s]Extractor Estimating: 222it [02:22,  1.61it/s]Extractor Estimating: 223it [02:22,  1.60it/s]Extractor Estimating: 224it [02:23,  1.60it/s]Extractor Estimating: 225it [02:23,  1.61it/s]Extractor Estimating: 226it [02:24,  1.61it/s]Extractor Estimating: 227it [02:25,  1.59it/s]Extractor Estimating: 228it [02:25,  1.57it/s]Extractor Estimating: 229it [02:26,  1.55it/s]Extractor Estimating: 230it [02:27,  1.62it/s]Extractor Estimating: 231it [02:27,  1.57it/s]Extractor Estimating: 232it [02:28,  1.60it/s]Extractor Estimating: 233it [02:28,  1.60it/s]Extractor Estimating: 234it [02:29,  1.64it/s]Extractor Estimating: 235it [02:30,  1.61it/s]Extractor Estimating: 236it [02:30,  1.60it/s]Extractor Estimating: 237it [02:31,  1.57it/s]Extractor Estimating: 238it [02:32,  1.45it/s]Extractor Estimating: 239it [02:32,  1.47it/s]Extractor Estimating: 240it [02:33,  1.48it/s]Extractor Estimating: 241it [02:34,  1.55it/s]Extractor Estimating: 242it [02:34,  1.57it/s]Extractor Estimating: 243it [02:35,  1.55it/s]Extractor Estimating: 244it [02:36,  1.37it/s]Extractor Estimating: 245it [02:36,  1.46it/s]Extractor Estimating: 246it [02:37,  1.46it/s]Extractor Estimating: 247it [02:38,  1.52it/s]Extractor Estimating: 248it [02:38,  1.56it/s]Extractor Estimating: 249it [02:39,  1.53it/s]Extractor Estimating: 250it [02:40,  1.66it/s]Extractor Estimating: 250it [02:40,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:35,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:35,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:35,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:35,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:35,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:55:36,319 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:55:36,320 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:55:37,089 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:55:38,276 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:55:38,302 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,422 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,464 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,465 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,465 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,465 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:55:42,415 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:55:42,416 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:55:43,238 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:55:43,482 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:55:43,482 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 02:34:54,894 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 02:34:55,043 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5203 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 26129
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26229, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26229, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.042, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 83, avg_time 1.034, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 183, avg_time 1.040, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 66, avg_time 1.046, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 166, avg_time 2.840, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 49, avg_time 1.043, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 149, avg_time 1.037, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 32, avg_time 1.041, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 132, avg_time 1.042, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 15, avg_time 2.828, loss:nan
g_step 1200, step 115, avg_time 1.049, loss:nan
g_step 1300, step 215, avg_time 1.037, loss:nan
g_step 1400, step 98, avg_time 1.046, loss:nan
g_step 1500, step 198, avg_time 1.032, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 81, avg_time 2.847, loss:nan
g_step 1700, step 181, avg_time 1.037, loss:nan
g_step 1800, step 64, avg_time 1.026, loss:nan
g_step 1900, step 164, avg_time 1.043, loss:nan
g_step 2000, step 47, avg_time 1.046, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 147, avg_time 2.829, loss:nan
g_step 2200, step 30, avg_time 1.048, loss:nan
g_step 2300, step 130, avg_time 1.033, loss:nan
g_step 2400, step 13, avg_time 1.047, loss:nan
g_step 2500, step 113, avg_time 1.045, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 213, avg_time 2.827, loss:nan
g_step 2700, step 96, avg_time 1.035, loss:nan
g_step 2800, step 196, avg_time 1.033, loss:nan
g_step 2900, step 79, avg_time 1.039, loss:nan
g_step 3000, step 179, avg_time 1.032, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 62, avg_time 2.831, loss:nan
g_step 3200, step 162, avg_time 1.044, loss:nan
g_step 3300, step 45, avg_time 1.040, loss:nan
g_step 3400, step 145, avg_time 1.033, loss:nan
g_step 3500, step 28, avg_time 1.041, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 128, avg_time 2.833, loss:nan
g_step 3700, step 11, avg_time 1.035, loss:nan
g_step 3800, step 111, avg_time 1.041, loss:nan
g_step 3900, step 211, avg_time 1.036, loss:nan
g_step 4000, step 94, avg_time 1.030, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 194, avg_time 2.837, loss:nan
g_step 4200, step 77, avg_time 1.023, loss:nan
g_step 4300, step 177, avg_time 1.037, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 02:34:55 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 02:34:55 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_02-34-54_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 02:34:56 - WARNING - datasets.builder -   Using custom data configuration default-be33c03de27b2bd8
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-be33c03de27b2bd8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 02:34:57,751 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:34:57,752 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:34:57,753 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:34:57,754 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:34:57,810 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:34:57,853 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:34:57,853 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:34:57,853 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:34:57,853 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:34:57,853 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:34:57,853 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 02:34:58,165 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:35:01,358 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 02:35:01,380 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-be33c03de27b2bd8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.96ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.79ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.10ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.23ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.32ba/s]100%|██████████| 6/6 [00:01<00:00,  4.70ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.22ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.83ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.09ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.22ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.12ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.22ba/s]100%|██████████| 7/7 [00:01<00:00,  3.72ba/s]100%|██████████| 7/7 [00:01<00:00,  3.89ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  4.28ba/s] 50%|█████     | 3/6 [00:00<00:00,  7.84ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.21ba/s]100%|██████████| 6/6 [00:00<00:00,  9.70ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.21ba/s] 43%|████▎     | 3/7 [00:00<00:00,  7.83ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.18ba/s]100%|██████████| 7/7 [00:00<00:00, 10.06ba/s]100%|██████████| 7/7 [00:00<00:00,  9.07ba/s]
[INFO|trainer.py:414] 2023-08-28 02:35:07,800 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 02:35:07,935 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 02:35:07,935 >>   Num examples = 5240
[INFO|trainer.py:1149] 2023-08-28 02:35:07,935 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 02:35:07,935 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 02:35:07,935 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 02:35:07,935 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 02:35:07,935 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:57,  3.47it/s]  0%|          | 2/410 [00:00<01:54,  3.56it/s]  1%|          | 3/410 [00:00<01:53,  3.59it/s]  1%|          | 4/410 [00:01<01:52,  3.60it/s]  1%|          | 5/410 [00:01<01:52,  3.59it/s]  1%|▏         | 6/410 [00:01<01:52,  3.58it/s]  2%|▏         | 7/410 [00:01<01:52,  3.58it/s]  2%|▏         | 8/410 [00:02<01:52,  3.58it/s]  2%|▏         | 9/410 [00:02<01:52,  3.58it/s]  2%|▏         | 10/410 [00:02<01:51,  3.58it/s]  3%|▎         | 11/410 [00:03<01:51,  3.58it/s]  3%|▎         | 12/410 [00:03<01:54,  3.48it/s]  3%|▎         | 13/410 [00:03<01:53,  3.51it/s]  3%|▎         | 14/410 [00:03<01:52,  3.53it/s]  4%|▎         | 15/410 [00:04<01:51,  3.54it/s]  4%|▍         | 16/410 [00:04<01:50,  3.55it/s]  4%|▍         | 17/410 [00:04<01:50,  3.56it/s]  4%|▍         | 18/410 [00:05<01:50,  3.56it/s]  5%|▍         | 19/410 [00:05<01:49,  3.57it/s]  5%|▍         | 20/410 [00:05<01:49,  3.57it/s]  5%|▌         | 21/410 [00:05<01:48,  3.57it/s]  5%|▌         | 22/410 [00:06<01:48,  3.57it/s]  6%|▌         | 23/410 [00:06<01:51,  3.46it/s]  6%|▌         | 24/410 [00:06<01:50,  3.49it/s]  6%|▌         | 25/410 [00:07<01:49,  3.51it/s]  6%|▋         | 26/410 [00:07<01:48,  3.52it/s]  7%|▋         | 27/410 [00:07<01:48,  3.53it/s]  7%|▋         | 28/410 [00:07<01:48,  3.54it/s]  7%|▋         | 29/410 [00:08<01:47,  3.55it/s]  7%|▋         | 30/410 [00:08<01:46,  3.55it/s]  8%|▊         | 31/410 [00:08<01:46,  3.56it/s]  8%|▊         | 32/410 [00:09<01:46,  3.56it/s]  8%|▊         | 33/410 [00:09<01:45,  3.56it/s]  8%|▊         | 34/410 [00:09<01:48,  3.46it/s]  9%|▊         | 35/410 [00:09<01:47,  3.49it/s]  9%|▉         | 36/410 [00:10<01:46,  3.51it/s]  9%|▉         | 37/410 [00:10<01:45,  3.52it/s]  9%|▉         | 38/410 [00:10<01:45,  3.53it/s] 10%|▉         | 39/410 [00:11<01:44,  3.54it/s] 10%|▉         | 40/410 [00:11<01:44,  3.54it/s] 10%|█         | 41/410 [00:11<01:44,  3.55it/s] 10%|█         | 42/410 [00:11<01:43,  3.55it/s] 10%|█         | 43/410 [00:12<01:43,  3.56it/s] 11%|█         | 44/410 [00:12<01:42,  3.56it/s] 11%|█         | 45/410 [00:12<01:45,  3.46it/s] 11%|█         | 46/410 [00:13<01:44,  3.49it/s] 11%|█▏        | 47/410 [00:13<01:43,  3.51it/s] 12%|█▏        | 48/410 [00:13<01:42,  3.52it/s] 12%|█▏        | 49/410 [00:13<01:42,  3.53it/s] 12%|█▏        | 50/410 [00:14<01:41,  3.54it/s] 12%|█▏        | 51/410 [00:14<01:41,  3.55it/s] 13%|█▎        | 52/410 [00:14<01:40,  3.55it/s] 13%|█▎        | 53/410 [00:14<01:40,  3.55it/s] 13%|█▎        | 54/410 [00:15<01:40,  3.56it/s] 13%|█▎        | 55/410 [00:15<01:39,  3.56it/s] 14%|█▎        | 56/410 [00:15<01:41,  3.49it/s] 14%|█▍        | 57/410 [00:16<01:40,  3.51it/s] 14%|█▍        | 58/410 [00:16<01:39,  3.53it/s] 14%|█▍        | 59/410 [00:16<01:39,  3.54it/s] 15%|█▍        | 60/410 [00:16<01:38,  3.55it/s] 15%|█▍        | 61/410 [00:17<01:38,  3.55it/s] 15%|█▌        | 62/410 [00:17<01:37,  3.56it/s] 15%|█▌        | 63/410 [00:17<01:44,  3.31it/s] 16%|█▌        | 64/410 [00:18<01:42,  3.38it/s] 16%|█▌        | 65/410 [00:18<01:40,  3.43it/s] 16%|█▌        | 66/410 [00:18<01:39,  3.47it/s] 16%|█▋        | 67/410 [00:18<01:38,  3.50it/s] 17%|█▋        | 68/410 [00:19<01:37,  3.52it/s] 17%|█▋        | 69/410 [00:19<01:36,  3.53it/s] 17%|█▋        | 70/410 [00:19<01:36,  3.54it/s] 17%|█▋        | 71/410 [00:20<01:35,  3.55it/s] 18%|█▊        | 72/410 [00:20<01:35,  3.55it/s] 18%|█▊        | 73/410 [00:20<01:34,  3.56it/s] 18%|█▊        | 74/410 [00:20<01:37,  3.43it/s] 18%|█▊        | 75/410 [00:21<01:36,  3.47it/s] 19%|█▊        | 76/410 [00:21<01:35,  3.50it/s] 19%|█▉        | 77/410 [00:21<01:34,  3.52it/s] 19%|█▉        | 78/410 [00:22<01:34,  3.53it/s] 19%|█▉        | 79/410 [00:22<01:33,  3.54it/s] 20%|█▉        | 80/410 [00:22<01:32,  3.55it/s] 20%|█▉        | 81/410 [00:22<01:32,  3.55it/s] 20%|██        | 82/410 [00:23<01:28,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 02:35:31,138 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:35:31,138 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 02:35:31,138 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.82it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.73it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.04it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.08it/s][A
  3%|▎         | 27/861 [00:00<00:18, 43.98it/s][A
  4%|▎         | 32/861 [00:00<00:18, 43.92it/s][A
  4%|▍         | 37/861 [00:00<00:18, 43.85it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.94it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.27it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.50it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.66it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.79it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.44it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.33it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.16it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.11it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.25it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.40it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.56it/s][A
 12%|█▏        | 102/861 [00:02<00:16, 44.69it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.62it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.55it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.34it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.08it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.11it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.17it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.28it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.42it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.51it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.64it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.58it/s][A
 19%|█▉        | 162/861 [00:03<00:16, 43.46it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 43.61it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.79it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.92it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.10it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.27it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.43it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.42it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.44it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.33it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.25it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.25it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.25it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.38it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.48it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.51it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.45it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.46it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.35it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.21it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.23it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.36it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.40it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.49it/s][A
 33%|███▎      | 282/861 [00:06<00:12, 44.54it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.45it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.38it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 43.71it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 43.88it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.04it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.19it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.31it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.37it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 43.72it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 44.02it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.04it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.00it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.16it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.18it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.23it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.25it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.23it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.36it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.45it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.31it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.33it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.38it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.42it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.40it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.37it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.43it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.50it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.44it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.27it/s][A
 50%|█████     | 432/861 [00:09<00:10, 42.25it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.03it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 43.44it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.77it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.01it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.18it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.30it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.18it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.94it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.00it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.26it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.35it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.45it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.51it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.31it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.51it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.22it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.17it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.20it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.33it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.44it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.46it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.48it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.50it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.39it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.08it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.09it/s][A
 66%|██████▌   | 567/861 [00:12<00:07, 36.93it/s][A
 66%|██████▋   | 572/861 [00:12<00:07, 39.04it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 40.63it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 41.81it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 42.69it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 43.31it/s][A
 69%|██████▉   | 597/861 [00:13<00:06, 43.80it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 43.95it/s][A
 70%|███████   | 607/861 [00:13<00:05, 43.65it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.47it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 43.63it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.84it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.20it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.37it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.50it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.62it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.46it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.21it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 43.99it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.96it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.04it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.32it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.45it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.56it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.59it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.56it/s][A
 81%|████████  | 697/861 [00:15<00:04, 39.58it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 40.99it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 42.01it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 42.85it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.36it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.81it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.05it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.20it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 43.89it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.73it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.75it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.07it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.24it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.39it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.51it/s][A
 90%|████████▉ | 772/861 [00:17<00:01, 44.56it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.37it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.15it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 43.91it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 43.88it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.19it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.46it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.54it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.61it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.52it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.32it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.16it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 40.81it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 42.01it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 42.84it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.44it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.76it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.11it/s][A
                                                 [A                                                
100%|██████████| 861/861 [00:19<00:00, 44.11it/s][A 20%|██        | 82/410 [00:42<01:28,  3.69it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:35:51,028 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 02:35:51,395 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:35:55,515 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:35:56,117 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:35:56,241 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:51<46:39,  8.56s/it] 20%|██        | 84/410 [00:51<33:00,  6.08s/it] 21%|██        | 85/410 [00:51<23:35,  4.35s/it] 21%|██        | 86/410 [00:51<16:54,  3.13s/it] 21%|██        | 87/410 [00:52<12:15,  2.28s/it] 21%|██▏       | 88/410 [00:52<09:00,  1.68s/it] 22%|██▏       | 89/410 [00:52<06:44,  1.26s/it] 22%|██▏       | 90/410 [00:53<05:08,  1.04it/s] 22%|██▏       | 91/410 [00:53<04:02,  1.32it/s] 22%|██▏       | 92/410 [00:53<03:15,  1.62it/s] 23%|██▎       | 93/410 [00:53<02:43,  1.94it/s] 23%|██▎       | 94/410 [00:54<02:20,  2.25it/s] 23%|██▎       | 95/410 [00:54<02:04,  2.52it/s] 23%|██▎       | 96/410 [00:54<01:55,  2.72it/s] 24%|██▎       | 97/410 [00:55<01:46,  2.93it/s] 24%|██▍       | 98/410 [00:55<01:40,  3.09it/s] 24%|██▍       | 99/410 [00:55<01:36,  3.22it/s] 24%|██▍       | 100/410 [00:55<01:33,  3.32it/s] 25%|██▍       | 101/410 [00:56<01:31,  3.38it/s] 25%|██▍       | 102/410 [00:56<01:29,  3.43it/s] 25%|██▌       | 103/410 [00:56<01:28,  3.47it/s] 25%|██▌       | 104/410 [00:57<01:27,  3.49it/s] 26%|██▌       | 105/410 [00:57<01:26,  3.51it/s] 26%|██▌       | 106/410 [00:57<01:26,  3.53it/s] 26%|██▌       | 107/410 [00:57<01:28,  3.41it/s] 26%|██▋       | 108/410 [00:58<01:27,  3.45it/s] 27%|██▋       | 109/410 [00:58<01:26,  3.48it/s] 27%|██▋       | 110/410 [00:58<01:25,  3.51it/s] 27%|██▋       | 111/410 [00:59<01:24,  3.52it/s] 27%|██▋       | 112/410 [00:59<01:24,  3.53it/s] 28%|██▊       | 113/410 [00:59<01:23,  3.54it/s] 28%|██▊       | 114/410 [00:59<01:23,  3.55it/s] 28%|██▊       | 115/410 [01:00<01:22,  3.56it/s] 28%|██▊       | 116/410 [01:00<01:22,  3.56it/s] 29%|██▊       | 117/410 [01:00<01:22,  3.56it/s] 29%|██▉       | 118/410 [01:01<01:25,  3.43it/s] 29%|██▉       | 119/410 [01:01<01:23,  3.47it/s] 29%|██▉       | 120/410 [01:01<01:22,  3.50it/s] 30%|██▉       | 121/410 [01:01<01:22,  3.51it/s] 30%|██▉       | 122/410 [01:02<01:21,  3.53it/s] 30%|███       | 123/410 [01:02<01:21,  3.54it/s] 30%|███       | 124/410 [01:02<01:20,  3.55it/s] 30%|███       | 125/410 [01:03<01:20,  3.55it/s] 31%|███       | 126/410 [01:03<01:19,  3.55it/s] 31%|███       | 127/410 [01:03<01:19,  3.56it/s] 31%|███       | 128/410 [01:03<01:19,  3.56it/s] 31%|███▏      | 129/410 [01:04<01:22,  3.40it/s] 32%|███▏      | 130/410 [01:04<01:21,  3.44it/s] 32%|███▏      | 131/410 [01:04<01:20,  3.47it/s] 32%|███▏      | 132/410 [01:05<01:19,  3.50it/s] 32%|███▏      | 133/410 [01:05<01:18,  3.51it/s] 33%|███▎      | 134/410 [01:05<01:18,  3.53it/s] 33%|███▎      | 135/410 [01:05<01:17,  3.54it/s] 33%|███▎      | 136/410 [01:06<01:17,  3.55it/s] 33%|███▎      | 137/410 [01:06<01:16,  3.55it/s] 34%|███▎      | 138/410 [01:06<01:16,  3.55it/s] 34%|███▍      | 139/410 [01:07<01:16,  3.56it/s] 34%|███▍      | 140/410 [01:07<01:17,  3.47it/s] 34%|███▍      | 141/410 [01:07<01:17,  3.49it/s] 35%|███▍      | 142/410 [01:07<01:16,  3.51it/s] 35%|███▍      | 143/410 [01:08<01:15,  3.52it/s] 35%|███▌      | 144/410 [01:08<01:15,  3.53it/s] 35%|███▌      | 145/410 [01:08<01:14,  3.54it/s] 36%|███▌      | 146/410 [01:09<01:14,  3.54it/s] 36%|███▌      | 147/410 [01:09<01:14,  3.55it/s] 36%|███▌      | 148/410 [01:09<01:13,  3.55it/s] 36%|███▋      | 149/410 [01:09<01:13,  3.56it/s] 37%|███▋      | 150/410 [01:10<01:13,  3.56it/s] 37%|███▋      | 151/410 [01:10<01:14,  3.47it/s] 37%|███▋      | 152/410 [01:10<01:13,  3.50it/s] 37%|███▋      | 153/410 [01:11<01:13,  3.52it/s] 38%|███▊      | 154/410 [01:11<01:12,  3.53it/s] 38%|███▊      | 155/410 [01:11<01:12,  3.54it/s] 38%|███▊      | 156/410 [01:11<01:11,  3.54it/s] 38%|███▊      | 157/410 [01:12<01:11,  3.55it/s] 39%|███▊      | 158/410 [01:12<01:11,  3.55it/s] 39%|███▉      | 159/410 [01:12<01:10,  3.55it/s] 39%|███▉      | 160/410 [01:12<01:10,  3.55it/s] 39%|███▉      | 161/410 [01:13<01:10,  3.55it/s] 40%|███▉      | 162/410 [01:13<01:14,  3.34it/s] 40%|███▉      | 163/410 [01:13<01:12,  3.40it/s] 40%|████      | 164/410 [01:14<01:08,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 02:36:22,062 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:36:22,062 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 02:36:22,062 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5844, 'eval_samples_per_second': 351.504, 'eval_steps_per_second': 43.964, 'epoch': 1.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.78it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.65it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.04it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.93it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.38it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.85it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.47it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.16it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.25it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.48it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.53it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.68it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.65it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.44it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.27it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.12it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.04it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.03it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.20it/s][A
 12%|█▏        | 102/861 [00:02<00:18, 41.32it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 42.41it/s][A
 13%|█▎        | 112/861 [00:02<00:17, 43.11it/s][A
 14%|█▎        | 117/861 [00:02<00:17, 43.57it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.70it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.77it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.86it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.96it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 43.89it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.01it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.26it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.42it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.54it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.47it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.42it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.19it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.16it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.15it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.14it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.41it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.48it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.54it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.54it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.39it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.25it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.07it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.18it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.24it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.29it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.45it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.49it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.51it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.26it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.19it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.06it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.16it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.34it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.36it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.49it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.47it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.38it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.29it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.11it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.08it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.21it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.23it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.31it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.41it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.43it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.34it/s][A
 41%|████      | 352/861 [00:07<00:12, 42.35it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 42.90it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.30it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.51it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 43.83it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.06it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.24it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.15it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 43.98it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 43.89it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.23it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.23it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.21it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.31it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.39it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.38it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.19it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.13it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.15it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.29it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.34it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.24it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.30it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.44it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.37it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.22it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.11it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 42.18it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 42.97it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 43.51it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 43.84it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.04it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.15it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.21it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.07it/s][A
 61%|██████    | 527/861 [00:11<00:07, 43.85it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.90it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.10it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.32it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.48it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.49it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.41it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.33it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.04it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.00it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.03it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.16it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.31it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.51it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.53it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.46it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.23it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.01it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.06it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.64it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.86it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.10it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.26it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.39it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.26it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.13it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.04it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 43.91it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 43.99it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.06it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.32it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.48it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.54it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.41it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.23it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.02it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.99it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.99it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.02it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.33it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.97it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.26it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.31it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.14it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.00it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.96it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 42.84it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.45it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.80it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.12it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.36it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.32it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.23it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.06it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.80it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 43.87it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.02it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.24it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.42it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.48it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.48it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.33it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.18it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.96it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.01it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.15it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.37it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 44.37it/s][A 40%|████      | 164/410 [01:33<01:08,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:36:41,680 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 02:36:41,869 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:36:45,786 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:36:46,163 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:36:46,382 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:40<33:13,  8.14s/it] 40%|████      | 166/410 [01:40<23:30,  5.78s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:41<16:46,  4.14s/it] 41%|████      | 168/410 [01:41<12:02,  2.98s/it] 41%|████      | 169/410 [01:41<08:43,  2.17s/it] 41%|████▏     | 170/410 [01:42<06:25,  1.61s/it] 42%|████▏     | 171/410 [01:42<04:48,  1.21s/it] 42%|████▏     | 172/410 [01:42<03:41,  1.08it/s] 42%|████▏     | 173/410 [01:42<02:54,  1.36it/s] 42%|████▏     | 174/410 [01:43<02:21,  1.67it/s] 43%|████▎     | 175/410 [01:43<01:58,  1.99it/s] 43%|████▎     | 176/410 [01:43<01:42,  2.29it/s] 43%|████▎     | 177/410 [01:44<01:30,  2.57it/s] 43%|████▎     | 178/410 [01:44<01:24,  2.74it/s] 44%|████▎     | 179/410 [01:44<01:18,  2.94it/s] 44%|████▍     | 180/410 [01:44<01:14,  3.10it/s] 44%|████▍     | 181/410 [01:45<01:11,  3.22it/s] 44%|████▍     | 182/410 [01:45<01:08,  3.32it/s] 45%|████▍     | 183/410 [01:45<01:07,  3.39it/s] 45%|████▍     | 184/410 [01:46<01:05,  3.44it/s] 45%|████▌     | 185/410 [01:46<01:04,  3.47it/s] 45%|████▌     | 186/410 [01:46<01:04,  3.49it/s] 46%|████▌     | 187/410 [01:46<01:03,  3.51it/s] 46%|████▌     | 188/410 [01:47<01:03,  3.52it/s] 46%|████▌     | 189/410 [01:47<01:04,  3.43it/s] 46%|████▋     | 190/410 [01:47<01:04,  3.42it/s] 47%|████▋     | 191/410 [01:48<01:03,  3.45it/s] 47%|████▋     | 192/410 [01:48<01:02,  3.48it/s] 47%|████▋     | 193/410 [01:48<01:01,  3.50it/s] 47%|████▋     | 194/410 [01:48<01:01,  3.52it/s] 48%|████▊     | 195/410 [01:49<01:05,  3.29it/s] 48%|████▊     | 196/410 [01:49<01:15,  2.83it/s] 48%|████▊     | 197/410 [01:49<01:10,  3.01it/s] 48%|████▊     | 198/410 [01:50<01:07,  3.15it/s] 49%|████▊     | 199/410 [01:50<01:06,  3.17it/s] 49%|████▉     | 200/410 [01:50<01:04,  3.28it/s] 49%|████▉     | 201/410 [01:51<01:02,  3.36it/s] 49%|████▉     | 202/410 [01:51<01:00,  3.41it/s] 50%|████▉     | 203/410 [01:51<00:59,  3.46it/s] 50%|████▉     | 204/410 [01:51<00:59,  3.49it/s] 50%|█████     | 205/410 [01:52<00:58,  3.51it/s] 50%|█████     | 206/410 [01:52<00:57,  3.52it/s] 50%|█████     | 207/410 [01:52<00:57,  3.54it/s] 51%|█████     | 208/410 [01:53<00:56,  3.54it/s] 51%|█████     | 209/410 [01:53<00:56,  3.55it/s] 51%|█████     | 210/410 [01:53<00:58,  3.43it/s] 51%|█████▏    | 211/410 [01:53<00:57,  3.49it/s] 52%|█████▏    | 212/410 [01:54<00:56,  3.52it/s] 52%|█████▏    | 213/410 [01:54<00:55,  3.55it/s] 52%|█████▏    | 214/410 [01:54<00:54,  3.57it/s] 52%|█████▏    | 215/410 [01:55<00:54,  3.58it/s] 53%|█████▎    | 216/410 [01:55<00:54,  3.59it/s] 53%|█████▎    | 217/410 [01:55<00:53,  3.60it/s] 53%|█████▎    | 218/410 [01:55<00:53,  3.60it/s] 53%|█████▎    | 219/410 [01:56<00:53,  3.60it/s] 54%|█████▎    | 220/410 [01:56<00:52,  3.60it/s] 54%|█████▍    | 221/410 [01:56<00:53,  3.51it/s] 54%|█████▍    | 222/410 [01:57<00:53,  3.54it/s] 54%|█████▍    | 223/410 [01:57<00:52,  3.56it/s] 55%|█████▍    | 224/410 [01:57<00:52,  3.57it/s] 55%|█████▍    | 225/410 [01:57<00:51,  3.59it/s] 55%|█████▌    | 226/410 [01:58<00:51,  3.59it/s] 55%|█████▌    | 227/410 [01:58<00:50,  3.60it/s] 56%|█████▌    | 228/410 [01:58<00:50,  3.60it/s] 56%|█████▌    | 229/410 [01:58<00:50,  3.60it/s] 56%|█████▌    | 230/410 [01:59<00:49,  3.60it/s] 56%|█████▋    | 231/410 [01:59<00:49,  3.61it/s] 57%|█████▋    | 232/410 [01:59<00:50,  3.51it/s] 57%|█████▋    | 233/410 [02:00<00:49,  3.54it/s] 57%|█████▋    | 234/410 [02:00<00:49,  3.56it/s] 57%|█████▋    | 235/410 [02:00<00:48,  3.58it/s] 58%|█████▊    | 236/410 [02:00<00:48,  3.59it/s] 58%|█████▊    | 237/410 [02:01<00:48,  3.59it/s] 58%|█████▊    | 238/410 [02:01<00:47,  3.59it/s] 58%|█████▊    | 239/410 [02:01<00:47,  3.59it/s] 59%|█████▊    | 240/410 [02:02<00:47,  3.59it/s] 59%|█████▉    | 241/410 [02:02<00:46,  3.60it/s] 59%|█████▉    | 242/410 [02:02<00:46,  3.60it/s] 59%|█████▉    | 243/410 [02:02<00:47,  3.54it/s] 60%|█████▉    | 244/410 [02:03<00:46,  3.56it/s] 60%|█████▉    | 245/410 [02:03<00:46,  3.58it/s] 60%|██████    | 246/410 [02:03<00:44,  3.72it/s][INFO|trainer.py:2140] 2023-08-28 02:37:11,628 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:37:11,628 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 02:37:11,628 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5098, 'eval_samples_per_second': 352.848, 'eval_steps_per_second': 44.132, 'epoch': 2.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.85it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.52it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.70it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.72it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.98it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.52it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.32it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.22it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.37it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.40it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.60it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.49it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.34it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.29it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.07it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.00it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.04it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.20it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.35it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.51it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.50it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.37it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.17it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.04it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.10it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.07it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.16it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.36it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.49it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.43it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.32it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.13it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.03it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.08it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.13it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.14it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.28it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.46it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.37it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 42.20it/s][A
 24%|██▍       | 207/861 [00:04<00:15, 42.80it/s][A
 25%|██▍       | 212/861 [00:04<00:15, 43.16it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.37it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.63it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.04it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.27it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.32it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.04it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.03it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.09it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.02it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.02it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.06it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.18it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.39it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.31it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.28it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.26it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.22it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.01it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.07it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.11it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.28it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.31it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.29it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.32it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 43.88it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.00it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.94it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.00it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.04it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.19it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.17it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.22it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.19it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.26it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.23it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.17it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.17it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.15it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.24it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.27it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.34it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.20it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.19it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.26it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.22it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.20it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.25it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.26it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.28it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.30it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.22it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.23it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.22it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.23it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.30it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.26it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.28it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.20it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.23it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.19it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.22it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.20it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.20it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.26it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.27it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.27it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.18it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.21it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.07it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.15it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.15it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.15it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.26it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.28it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.25it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.22it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.23it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.16it/s][A
 70%|███████   | 607/861 [00:13<00:05, 43.78it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.90it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.01it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.11it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.13it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.19it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.14it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.29it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.18it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.22it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.13it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.18it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.21it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.26it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.27it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.19it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.20it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.18it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.18it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.14it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.19it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.26it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.25it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.23it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.22it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.17it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.26it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.66it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.77it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.02it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.11it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.06it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.99it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.17it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.30it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.11it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.14it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.19it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.26it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.30it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.23it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.20it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.10it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.25it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.19it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.23it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.24it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.32it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.34it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.20it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.14it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 44.14it/s][A 60%|██████    | 246/410 [02:23<00:44,  3.72it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:37:31,358 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 02:37:31,512 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:37:34,614 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:37:34,762 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:37:34,852 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [02:28<20:35,  7.58s/it] 60%|██████    | 248/410 [02:28<14:33,  5.39s/it] 61%|██████    | 249/410 [02:28<10:20,  3.86s/it] 61%|██████    | 250/410 [02:29<07:25,  2.78s/it] 61%|██████    | 251/410 [02:29<05:23,  2.03s/it] 61%|██████▏   | 252/410 [02:29<03:58,  1.51s/it] 62%|██████▏   | 253/410 [02:30<02:59,  1.15s/it] 62%|██████▏   | 254/410 [02:30<02:18,  1.13it/s] 62%|██████▏   | 255/410 [02:30<01:49,  1.42it/s] 62%|██████▏   | 256/410 [02:30<01:28,  1.73it/s] 63%|██████▎   | 257/410 [02:31<01:14,  2.05it/s] 63%|██████▎   | 258/410 [02:31<01:04,  2.34it/s] 63%|██████▎   | 259/410 [02:31<00:57,  2.61it/s] 63%|██████▎   | 260/410 [02:32<00:52,  2.84it/s] 64%|██████▎   | 261/410 [02:32<00:49,  3.02it/s] 64%|██████▍   | 262/410 [02:32<00:46,  3.16it/s] 64%|██████▍   | 263/410 [02:32<00:44,  3.27it/s] 64%|██████▍   | 264/410 [02:33<00:44,  3.27it/s] 65%|██████▍   | 265/410 [02:33<00:43,  3.35it/s] 65%|██████▍   | 266/410 [02:33<00:42,  3.41it/s] 65%|██████▌   | 267/410 [02:33<00:41,  3.45it/s] 65%|██████▌   | 268/410 [02:34<00:40,  3.48it/s] 66%|██████▌   | 269/410 [02:34<00:40,  3.50it/s] 66%|██████▌   | 270/410 [02:34<00:39,  3.52it/s] 66%|██████▌   | 271/410 [02:35<00:39,  3.53it/s] 66%|██████▋   | 272/410 [02:35<00:39,  3.53it/s] 67%|██████▋   | 273/410 [02:35<00:38,  3.54it/s] 67%|██████▋   | 274/410 [02:35<00:38,  3.55it/s] 67%|██████▋   | 275/410 [02:36<00:38,  3.55it/s] 67%|██████▋   | 276/410 [02:36<00:37,  3.55it/s] 68%|██████▊   | 277/410 [02:36<00:37,  3.55it/s] 68%|██████▊   | 278/410 [02:37<00:37,  3.56it/s] 68%|██████▊   | 279/410 [02:37<00:36,  3.56it/s] 68%|██████▊   | 280/410 [02:37<00:36,  3.56it/s] 69%|██████▊   | 281/410 [02:37<00:36,  3.56it/s] 69%|██████▉   | 282/410 [02:38<00:35,  3.56it/s] 69%|██████▉   | 283/410 [02:38<00:35,  3.56it/s] 69%|██████▉   | 284/410 [02:38<00:35,  3.56it/s] 70%|██████▉   | 285/410 [02:39<00:37,  3.34it/s] 70%|██████▉   | 286/410 [02:39<00:36,  3.40it/s] 70%|███████   | 287/410 [02:39<00:35,  3.44it/s] 70%|███████   | 288/410 [02:39<00:35,  3.47it/s] 70%|███████   | 289/410 [02:40<00:34,  3.50it/s] 71%|███████   | 290/410 [02:40<00:34,  3.52it/s] 71%|███████   | 291/410 [02:40<00:33,  3.52it/s] 71%|███████   | 292/410 [02:41<00:33,  3.53it/s] 71%|███████▏  | 293/410 [02:41<00:33,  3.54it/s] 72%|███████▏  | 294/410 [02:41<00:32,  3.55it/s] 72%|███████▏  | 295/410 [02:41<00:32,  3.55it/s] 72%|███████▏  | 296/410 [02:42<00:34,  3.30it/s] 72%|███████▏  | 297/410 [02:42<00:33,  3.37it/s] 73%|███████▎  | 298/410 [02:42<00:32,  3.43it/s] 73%|███████▎  | 299/410 [02:43<00:32,  3.47it/s] 73%|███████▎  | 300/410 [02:43<00:31,  3.49it/s] 73%|███████▎  | 301/410 [02:43<00:31,  3.51it/s] 74%|███████▎  | 302/410 [02:43<00:30,  3.53it/s] 74%|███████▍  | 303/410 [02:44<00:30,  3.54it/s] 74%|███████▍  | 304/410 [02:44<00:29,  3.54it/s] 74%|███████▍  | 305/410 [02:44<00:29,  3.55it/s] 75%|███████▍  | 306/410 [02:45<00:29,  3.55it/s] 75%|███████▍  | 307/410 [02:45<00:31,  3.28it/s] 75%|███████▌  | 308/410 [02:45<00:30,  3.36it/s] 75%|███████▌  | 309/410 [02:46<00:29,  3.41it/s] 76%|███████▌  | 310/410 [02:46<00:28,  3.45it/s] 76%|███████▌  | 311/410 [02:46<00:28,  3.48it/s] 76%|███████▌  | 312/410 [02:46<00:27,  3.50it/s] 76%|███████▋  | 313/410 [02:47<00:27,  3.52it/s] 77%|███████▋  | 314/410 [02:47<00:27,  3.53it/s] 77%|███████▋  | 315/410 [02:47<00:26,  3.54it/s] 77%|███████▋  | 316/410 [02:48<00:27,  3.42it/s] 77%|███████▋  | 317/410 [02:48<00:27,  3.44it/s] 78%|███████▊  | 318/410 [02:48<00:27,  3.36it/s] 78%|███████▊  | 319/410 [02:48<00:26,  3.42it/s] 78%|███████▊  | 320/410 [02:49<00:26,  3.34it/s] 78%|███████▊  | 321/410 [02:49<00:26,  3.33it/s] 79%|███████▊  | 322/410 [02:49<00:29,  3.02it/s] 79%|███████▉  | 323/410 [02:50<00:27,  3.16it/s] 79%|███████▉  | 324/410 [02:50<00:26,  3.27it/s] 79%|███████▉  | 325/410 [02:50<00:25,  3.35it/s] 80%|███████▉  | 326/410 [02:51<00:24,  3.41it/s] 80%|███████▉  | 327/410 [02:51<00:24,  3.45it/s] 80%|████████  | 328/410 [02:51<00:23,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 02:37:59,541 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:37:59,541 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 02:37:59,542 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.4898, 'eval_samples_per_second': 353.211, 'eval_steps_per_second': 44.177, 'epoch': 3.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.67it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.41it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.10it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.15it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.56it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.93it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.28it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.90it/s][A
  5%|▌         | 47/861 [00:01<00:18, 43.98it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.26it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.39it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.63it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.69it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.48it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.25it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.96it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.81it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.87it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.05it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.34it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.49it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.62it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.54it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.25it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.51it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.54it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.70it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 43.98it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.21it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.41it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.60it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.45it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.24it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.95it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.99it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.97it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.16it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.26it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.36it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.51it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.50it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.16it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.07it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.05it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.02it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.21it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.25it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.44it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.45it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.45it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.22it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.26it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.50it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.71it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.97it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.10it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.34it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.39it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.19it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 43.97it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.04it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.09it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.21it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.98it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.21it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.38it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.32it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.22it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.07it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.05it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.07it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.21it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.31it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.47it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.46it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.28it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.17it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.08it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 43.02it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.36it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.74it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.00it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.08it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.20it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.01it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.97it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.88it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 43.97it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.71it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.52it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.46it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.47it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.39it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.28it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.02it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.03it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.04it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.17it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.30it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.40it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.45it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.35it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.13it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.02it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.05it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.04it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.08it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.37it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.38it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.45it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.28it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.06it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.02it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 43.97it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.14it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.15it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.37it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.46it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.42it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.26it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.08it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.06it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.03it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.17it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.28it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.38it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.37it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.42it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.28it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.09it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.10it/s][A
 77%|███████▋  | 662/861 [00:15<00:05, 38.42it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 40.10it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 41.32it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 42.31it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 42.97it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 43.56it/s][A
 80%|████████  | 692/861 [00:15<00:03, 43.90it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.02it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 43.61it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.45it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.53it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.85it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.09it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.34it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.36it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.52it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.45it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.06it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.81it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.78it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.06it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.23it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.35it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.41it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.45it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.30it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 43.43it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.39it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 43.60it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 43.87it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.21it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.37it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.41it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.41it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.09it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 43.94it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.78it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.99it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.07it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.20it/s][A                                                 
                                                 [A 80%|████████  | 328/410 [03:11<00:23,  3.51it/s]
100%|██████████| 861/861 [00:19<00:00, 44.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:38:19,214 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 02:38:19,392 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:38:22,378 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:38:22,521 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:38:22,590 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [03:15<10:07,  7.50s/it] 80%|████████  | 330/410 [03:16<07:06,  5.34s/it] 81%|████████  | 331/410 [03:16<05:01,  3.82s/it] 81%|████████  | 332/410 [03:16<03:35,  2.76s/it] 81%|████████  | 333/410 [03:17<02:35,  2.02s/it] 81%|████████▏ | 334/410 [03:17<01:53,  1.49s/it] 82%|████████▏ | 335/410 [03:17<01:24,  1.13s/it] 82%|████████▏ | 336/410 [03:17<01:05,  1.13it/s] 82%|████████▏ | 337/410 [03:18<00:51,  1.42it/s] 82%|████████▏ | 338/410 [03:18<00:41,  1.74it/s] 83%|████████▎ | 339/410 [03:18<00:34,  2.05it/s] 83%|████████▎ | 340/410 [03:19<00:29,  2.35it/s] 83%|████████▎ | 341/410 [03:19<00:26,  2.62it/s] 83%|████████▎ | 342/410 [03:19<00:23,  2.84it/s] 84%|████████▎ | 343/410 [03:19<00:22,  3.02it/s] 84%|████████▍ | 344/410 [03:20<00:20,  3.16it/s] 84%|████████▍ | 345/410 [03:20<00:19,  3.27it/s] 84%|████████▍ | 346/410 [03:20<00:19,  3.35it/s] 85%|████████▍ | 347/410 [03:21<00:18,  3.36it/s] 85%|████████▍ | 348/410 [03:21<00:18,  3.42it/s] 85%|████████▌ | 349/410 [03:21<00:17,  3.46it/s] 85%|████████▌ | 350/410 [03:21<00:17,  3.49it/s] 86%|████████▌ | 351/410 [03:22<00:16,  3.51it/s] 86%|████████▌ | 352/410 [03:22<00:16,  3.52it/s] 86%|████████▌ | 353/410 [03:22<00:16,  3.53it/s] 86%|████████▋ | 354/410 [03:23<00:15,  3.54it/s] 87%|████████▋ | 355/410 [03:23<00:15,  3.55it/s] 87%|████████▋ | 356/410 [03:23<00:15,  3.55it/s] 87%|████████▋ | 357/410 [03:23<00:14,  3.55it/s] 87%|████████▋ | 358/410 [03:24<00:14,  3.50it/s] 88%|████████▊ | 359/410 [03:24<00:14,  3.52it/s] 88%|████████▊ | 360/410 [03:24<00:14,  3.53it/s] 88%|████████▊ | 361/410 [03:24<00:13,  3.54it/s] 88%|████████▊ | 362/410 [03:25<00:13,  3.55it/s] 89%|████████▊ | 363/410 [03:25<00:13,  3.55it/s] 89%|████████▉ | 364/410 [03:25<00:12,  3.55it/s] 89%|████████▉ | 365/410 [03:26<00:12,  3.56it/s] 89%|████████▉ | 366/410 [03:26<00:12,  3.56it/s] 90%|████████▉ | 367/410 [03:26<00:12,  3.56it/s] 90%|████████▉ | 368/410 [03:26<00:11,  3.56it/s] 90%|█████████ | 369/410 [03:27<00:11,  3.48it/s] 90%|█████████ | 370/410 [03:27<00:11,  3.50it/s] 90%|█████████ | 371/410 [03:27<00:11,  3.51it/s] 91%|█████████ | 372/410 [03:28<00:10,  3.53it/s] 91%|█████████ | 373/410 [03:28<00:10,  3.54it/s] 91%|█████████ | 374/410 [03:28<00:10,  3.54it/s] 91%|█████████▏| 375/410 [03:28<00:09,  3.55it/s] 92%|█████████▏| 376/410 [03:29<00:09,  3.55it/s] 92%|█████████▏| 377/410 [03:29<00:09,  3.56it/s] 92%|█████████▏| 378/410 [03:29<00:08,  3.56it/s] 92%|█████████▏| 379/410 [03:30<00:08,  3.56it/s] 93%|█████████▎| 380/410 [03:30<00:08,  3.43it/s] 93%|█████████▎| 381/410 [03:30<00:08,  3.47it/s] 93%|█████████▎| 382/410 [03:30<00:08,  3.49it/s] 93%|█████████▎| 383/410 [03:31<00:07,  3.51it/s] 94%|█████████▎| 384/410 [03:31<00:07,  3.53it/s] 94%|█████████▍| 385/410 [03:31<00:07,  3.54it/s] 94%|█████████▍| 386/410 [03:32<00:06,  3.54it/s] 94%|█████████▍| 387/410 [03:32<00:06,  3.55it/s] 95%|█████████▍| 388/410 [03:32<00:06,  3.55it/s] 95%|█████████▍| 389/410 [03:32<00:05,  3.55it/s] 95%|█████████▌| 390/410 [03:33<00:05,  3.55it/s] 95%|█████████▌| 391/410 [03:33<00:05,  3.43it/s] 96%|█████████▌| 392/410 [03:33<00:05,  3.47it/s] 96%|█████████▌| 393/410 [03:34<00:04,  3.49it/s] 96%|█████████▌| 394/410 [03:34<00:04,  3.51it/s] 96%|█████████▋| 395/410 [03:34<00:04,  3.52it/s] 97%|█████████▋| 396/410 [03:34<00:03,  3.53it/s] 97%|█████████▋| 397/410 [03:35<00:03,  3.54it/s] 97%|█████████▋| 398/410 [03:35<00:03,  3.54it/s] 97%|█████████▋| 399/410 [03:35<00:03,  3.55it/s] 98%|█████████▊| 400/410 [03:36<00:02,  3.55it/s] 98%|█████████▊| 401/410 [03:36<00:02,  3.55it/s] 98%|█████████▊| 402/410 [03:36<00:02,  3.44it/s] 98%|█████████▊| 403/410 [03:36<00:02,  3.47it/s] 99%|█████████▊| 404/410 [03:37<00:01,  3.49it/s] 99%|█████████▉| 405/410 [03:37<00:01,  3.41it/s] 99%|█████████▉| 406/410 [03:37<00:01,  3.45it/s] 99%|█████████▉| 407/410 [03:38<00:00,  3.48it/s]100%|█████████▉| 408/410 [03:38<00:00,  3.50it/s]100%|█████████▉| 409/410 [03:38<00:00,  3.52it/s]100%|██████████| 410/410 [03:38<00:00,  3.66it/s][INFO|trainer.py:2140] 2023-08-28 02:38:46,829 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:38:46,829 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 02:38:46,829 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5415, 'eval_samples_per_second': 352.276, 'eval_steps_per_second': 44.06, 'epoch': 4.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.70it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.68it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.04it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.97it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.28it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.71it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.27it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.04it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.14it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.42it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.54it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.60it/s][A
  8%|▊         | 67/861 [00:01<00:19, 41.44it/s][A
  8%|▊         | 72/861 [00:01<00:18, 42.38it/s][A
  9%|▉         | 77/861 [00:01<00:18, 42.86it/s][A
 10%|▉         | 82/861 [00:01<00:18, 43.00it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.27it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.62it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.98it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.11it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 43.98it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.14it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.29it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.32it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.09it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.00it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.10it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.30it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.29it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.25it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.41it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.35it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.33it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.21it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.12it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.23it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.34it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.26it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.27it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 41.34it/s][A
 24%|██▍       | 207/861 [00:04<00:15, 42.33it/s][A
 25%|██▍       | 212/861 [00:04<00:15, 42.93it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.38it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.65it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.96it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.05it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.04it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 43.80it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 43.97it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.15it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.18it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.34it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.33it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.41it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.33it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.15it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 44.05it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.13it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.23it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.25it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.41it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.44it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.43it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.31it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.14it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 44.07it/s][A
 39%|███▉      | 337/861 [00:07<00:13, 39.81it/s][A
 40%|███▉      | 342/861 [00:07<00:12, 41.18it/s][A
 40%|████      | 347/861 [00:07<00:12, 42.29it/s][A
 41%|████      | 352/861 [00:08<00:11, 42.99it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.61it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.80it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.93it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 43.83it/s][A
 44%|████▍     | 377/861 [00:08<00:11, 43.55it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 43.63it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 43.92it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.11it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 44.34it/s][A
 47%|████▋     | 402/861 [00:09<00:11, 38.80it/s][A
 47%|████▋     | 407/861 [00:09<00:11, 40.56it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 41.73it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 42.60it/s][A
 49%|████▉     | 422/861 [00:09<00:10, 43.16it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 43.41it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.74it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.01it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.86it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.67it/s][A
 52%|█████▏    | 452/861 [00:10<00:10, 37.76it/s][A
 53%|█████▎    | 457/861 [00:10<00:10, 39.91it/s][A
 54%|█████▎    | 462/861 [00:10<00:10, 38.99it/s][A
 54%|█████▍    | 466/861 [00:10<00:10, 38.32it/s][A
 55%|█████▍    | 471/861 [00:10<00:09, 40.17it/s][A
 55%|█████▌    | 476/861 [00:11<00:15, 25.50it/s][A
 56%|█████▌    | 481/861 [00:11<00:12, 29.40it/s][A
 56%|█████▋    | 486/861 [00:11<00:11, 32.84it/s][A
 57%|█████▋    | 491/861 [00:11<00:10, 35.75it/s][A
 58%|█████▊    | 496/861 [00:11<00:09, 38.06it/s][A
 58%|█████▊    | 501/861 [00:11<00:09, 39.87it/s][A
 59%|█████▉    | 506/861 [00:11<00:08, 41.21it/s][A
 59%|█████▉    | 511/861 [00:11<00:08, 42.31it/s][A
 60%|█████▉    | 516/861 [00:12<00:08, 42.53it/s][A
 61%|██████    | 521/861 [00:12<00:07, 42.62it/s][A
 61%|██████    | 526/861 [00:12<00:07, 42.82it/s][A
 62%|██████▏   | 531/861 [00:12<00:07, 43.29it/s][A
 62%|██████▏   | 536/861 [00:12<00:07, 43.65it/s][A
 63%|██████▎   | 541/861 [00:12<00:07, 43.80it/s][A
 63%|██████▎   | 546/861 [00:12<00:07, 44.15it/s][A
 64%|██████▍   | 551/861 [00:12<00:06, 44.32it/s][A
 65%|██████▍   | 556/861 [00:13<00:06, 44.48it/s][A
 65%|██████▌   | 561/861 [00:13<00:06, 44.29it/s][A
 66%|██████▌   | 566/861 [00:13<00:06, 43.97it/s][A
 66%|██████▋   | 571/861 [00:13<00:06, 43.80it/s][A
 67%|██████▋   | 576/861 [00:13<00:06, 43.93it/s][A
 67%|██████▋   | 581/861 [00:13<00:06, 44.12it/s][A
 68%|██████▊   | 586/861 [00:13<00:06, 44.31it/s][A
 69%|██████▊   | 591/861 [00:13<00:06, 42.88it/s][A
 69%|██████▉   | 596/861 [00:13<00:06, 43.41it/s][A
 70%|██████▉   | 601/861 [00:14<00:05, 43.70it/s][A
 70%|███████   | 606/861 [00:14<00:05, 43.97it/s][A
 71%|███████   | 611/861 [00:14<00:05, 43.92it/s][A
 72%|███████▏  | 616/861 [00:14<00:05, 43.96it/s][A
 72%|███████▏  | 621/861 [00:14<00:05, 43.92it/s][A
 73%|███████▎  | 626/861 [00:14<00:05, 44.12it/s][A
 73%|███████▎  | 631/861 [00:14<00:05, 44.13it/s][A
 74%|███████▍  | 636/861 [00:14<00:05, 44.28it/s][A
 74%|███████▍  | 641/861 [00:14<00:04, 44.36it/s][A
 75%|███████▌  | 646/861 [00:15<00:04, 44.38it/s][A
 76%|███████▌  | 651/861 [00:15<00:04, 44.38it/s][A
 76%|███████▌  | 656/861 [00:15<00:04, 44.20it/s][A
 77%|███████▋  | 661/861 [00:15<00:04, 44.14it/s][A
 77%|███████▋  | 666/861 [00:15<00:04, 44.14it/s][A
 78%|███████▊  | 671/861 [00:15<00:04, 44.10it/s][A
 79%|███████▊  | 676/861 [00:15<00:04, 44.15it/s][A
 79%|███████▉  | 681/861 [00:15<00:04, 44.29it/s][A
 80%|███████▉  | 686/861 [00:15<00:03, 44.45it/s][A
 80%|████████  | 691/861 [00:16<00:03, 44.41it/s][A
 81%|████████  | 696/861 [00:16<00:03, 44.31it/s][A
 81%|████████▏ | 701/861 [00:16<00:03, 44.19it/s][A
 82%|████████▏ | 706/861 [00:16<00:03, 44.16it/s][A
 83%|████████▎ | 711/861 [00:16<00:03, 44.13it/s][A
 83%|████████▎ | 716/861 [00:16<00:03, 44.12it/s][A
 84%|████████▎ | 721/861 [00:16<00:03, 44.19it/s][A
 84%|████████▍ | 726/861 [00:16<00:03, 42.98it/s][A
 85%|████████▍ | 731/861 [00:16<00:02, 43.53it/s][A
 85%|████████▌ | 736/861 [00:17<00:02, 43.80it/s][A
 86%|████████▌ | 741/861 [00:17<00:02, 43.95it/s][A
 87%|████████▋ | 746/861 [00:17<00:02, 43.93it/s][A
 87%|████████▋ | 751/861 [00:17<00:02, 44.00it/s][A
 88%|████████▊ | 756/861 [00:17<00:02, 44.03it/s][A
 88%|████████▊ | 761/861 [00:17<00:02, 44.02it/s][A
 89%|████████▉ | 766/861 [00:17<00:02, 44.01it/s][A
 90%|████████▉ | 771/861 [00:17<00:02, 44.13it/s][A
 90%|█████████ | 776/861 [00:18<00:01, 44.30it/s][A
 91%|█████████ | 781/861 [00:18<00:01, 44.28it/s][A
 91%|█████████▏| 786/861 [00:18<00:01, 44.40it/s][A
 92%|█████████▏| 791/861 [00:18<00:01, 44.36it/s][A
 92%|█████████▏| 796/861 [00:18<00:01, 44.22it/s][A
 93%|█████████▎| 801/861 [00:18<00:01, 44.20it/s][A
 94%|█████████▎| 806/861 [00:18<00:01, 44.10it/s][A
 94%|█████████▍| 811/861 [00:18<00:01, 44.15it/s][A
 95%|█████████▍| 816/861 [00:18<00:01, 44.14it/s][A
 95%|█████████▌| 821/861 [00:19<00:00, 44.32it/s][A
 96%|█████████▌| 826/861 [00:19<00:00, 44.34it/s][A
 97%|█████████▋| 831/861 [00:19<00:00, 44.34it/s][A
 97%|█████████▋| 836/861 [00:19<00:00, 44.32it/s][A
 98%|█████████▊| 841/861 [00:19<00:00, 44.27it/s][A
 98%|█████████▊| 846/861 [00:19<00:00, 44.12it/s][A
 99%|█████████▉| 851/861 [00:19<00:00, 44.06it/s][A
 99%|█████████▉| 856/861 [00:19<00:00, 44.08it/s][A
100%|██████████| 861/861 [00:19<00:00, 42.14it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 42.14it/s][A100%|██████████| 410/410 [03:58<00:00,  3.66it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:39:06,945 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 02:39:07,091 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:39:10,089 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:39:10,241 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:39:10,292 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 02:39:11,287 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 02:39:11,287 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82 (score: 1.0493918657302856).
                                                 100%|██████████| 410/410 [04:12<00:00,  3.66it/s]100%|██████████| 410/410 [04:12<00:00,  1.62it/s]
[INFO|trainer.py:1894] 2023-08-28 02:39:20,346 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 02:39:20,463 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:39:24,314 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:39:24,409 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:39:24,467 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:39:24,882 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:24,883 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:24,883 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:24,883 >>   train_runtime            = 0:04:12.32
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:24,883 >>   train_samples            =       5240
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:24,883 >>   train_samples_per_second =    103.835
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:24,883 >>   train_steps_per_second   =      1.625
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.9654, 'eval_samples_per_second': 344.796, 'eval_steps_per_second': 43.125, 'epoch': 5.0}
{'train_runtime': 252.323, 'train_samples_per_second': 103.835, 'train_steps_per_second': 1.625, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 02:39:25 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 02:39:25,105 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:39:25,105 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 02:39:25,105 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 55.00it/s]  1%|▏         | 12/861 [00:00<00:17, 48.39it/s]  2%|▏         | 17/861 [00:00<00:17, 46.94it/s]  3%|▎         | 22/861 [00:00<00:18, 46.11it/s]  3%|▎         | 27/861 [00:00<00:18, 45.77it/s]  4%|▎         | 32/861 [00:00<00:18, 45.48it/s]  4%|▍         | 37/861 [00:00<00:18, 45.30it/s]  5%|▍         | 42/861 [00:00<00:18, 44.83it/s]  5%|▌         | 47/861 [00:01<00:18, 44.25it/s]  6%|▌         | 52/861 [00:01<00:18, 43.97it/s]  7%|▋         | 57/861 [00:01<00:18, 44.05it/s]  7%|▋         | 62/861 [00:01<00:18, 44.30it/s]  8%|▊         | 67/861 [00:01<00:17, 44.45it/s]  8%|▊         | 72/861 [00:01<00:17, 44.60it/s]  9%|▉         | 77/861 [00:01<00:17, 44.61it/s] 10%|▉         | 82/861 [00:01<00:17, 44.61it/s] 10%|█         | 87/861 [00:01<00:17, 44.34it/s] 11%|█         | 92/861 [00:02<00:17, 44.00it/s] 11%|█▏        | 97/861 [00:02<00:17, 43.85it/s] 12%|█▏        | 102/861 [00:02<00:17, 43.98it/s] 12%|█▏        | 107/861 [00:02<00:19, 39.64it/s] 13%|█▎        | 112/861 [00:02<00:18, 41.12it/s] 14%|█▎        | 117/861 [00:02<00:17, 42.20it/s] 14%|█▍        | 122/861 [00:02<00:17, 42.91it/s] 15%|█▍        | 127/861 [00:02<00:16, 43.44it/s] 15%|█▌        | 132/861 [00:02<00:16, 43.85it/s] 16%|█▌        | 137/861 [00:03<00:16, 43.74it/s] 16%|█▋        | 142/861 [00:03<00:16, 43.81it/s] 17%|█▋        | 147/861 [00:03<00:16, 43.56it/s] 18%|█▊        | 152/861 [00:03<00:16, 43.63it/s] 18%|█▊        | 157/861 [00:03<00:16, 43.93it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.15it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.36it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.48it/s] 21%|██        | 177/861 [00:04<00:15, 44.45it/s] 21%|██        | 182/861 [00:04<00:15, 44.32it/s] 22%|██▏       | 187/861 [00:04<00:15, 44.12it/s] 22%|██▏       | 192/861 [00:04<00:15, 43.87it/s] 23%|██▎       | 197/861 [00:04<00:15, 43.92it/s] 23%|██▎       | 202/861 [00:04<00:14, 44.11it/s] 24%|██▍       | 207/861 [00:04<00:14, 44.27it/s] 25%|██▍       | 212/861 [00:04<00:14, 44.46it/s] 25%|██▌       | 217/861 [00:04<00:14, 44.41it/s] 26%|██▌       | 222/861 [00:05<00:14, 44.57it/s] 26%|██▋       | 227/861 [00:05<00:14, 44.56it/s] 27%|██▋       | 232/861 [00:05<00:14, 44.37it/s] 28%|██▊       | 237/861 [00:05<00:14, 44.17it/s] 28%|██▊       | 242/861 [00:05<00:14, 44.09it/s] 29%|██▊       | 247/861 [00:05<00:13, 44.12it/s] 29%|██▉       | 252/861 [00:05<00:13, 44.33it/s] 30%|██▉       | 257/861 [00:05<00:13, 44.52it/s] 30%|███       | 262/861 [00:05<00:13, 44.16it/s] 31%|███       | 267/861 [00:06<00:13, 44.67it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.63it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.48it/s] 33%|███▎      | 282/861 [00:06<00:13, 44.35it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.30it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.37it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.58it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.61it/s] 36%|███▌      | 307/861 [00:06<00:12, 44.57it/s] 36%|███▌      | 312/861 [00:07<00:12, 44.54it/s] 37%|███▋      | 317/861 [00:07<00:12, 44.48it/s] 37%|███▋      | 322/861 [00:07<00:12, 44.39it/s] 38%|███▊      | 327/861 [00:07<00:12, 44.26it/s] 39%|███▊      | 332/861 [00:07<00:11, 44.31it/s] 39%|███▉      | 337/861 [00:07<00:11, 44.43it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.56it/s] 40%|████      | 347/861 [00:07<00:11, 44.60it/s] 41%|████      | 352/861 [00:07<00:11, 44.51it/s] 41%|████▏     | 357/861 [00:08<00:11, 44.52it/s] 42%|████▏     | 362/861 [00:08<00:11, 44.43it/s] 43%|████▎     | 367/861 [00:08<00:11, 44.36it/s] 43%|████▎     | 372/861 [00:08<00:11, 44.31it/s] 44%|████▍     | 377/861 [00:08<00:11, 42.99it/s] 44%|████▍     | 382/861 [00:08<00:10, 43.65it/s] 45%|████▍     | 387/861 [00:08<00:10, 44.01it/s] 46%|████▌     | 392/861 [00:08<00:10, 44.21it/s] 46%|████▌     | 397/861 [00:08<00:10, 44.32it/s] 47%|████▋     | 402/861 [00:09<00:10, 44.33it/s] 47%|████▋     | 407/861 [00:09<00:10, 44.32it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.29it/s] 48%|████▊     | 417/861 [00:09<00:10, 44.03it/s] 49%|████▉     | 422/861 [00:09<00:09, 44.05it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.33it/s] 50%|█████     | 432/861 [00:09<00:09, 44.58it/s] 51%|█████     | 437/861 [00:09<00:09, 44.57it/s] 51%|█████▏    | 442/861 [00:09<00:09, 44.52it/s] 52%|█████▏    | 447/861 [00:10<00:09, 44.53it/s] 52%|█████▏    | 452/861 [00:10<00:09, 44.41it/s] 53%|█████▎    | 457/861 [00:10<00:09, 44.33it/s] 54%|█████▎    | 462/861 [00:10<00:09, 44.23it/s] 54%|█████▍    | 467/861 [00:10<00:08, 44.18it/s] 55%|█████▍    | 472/861 [00:10<00:08, 44.36it/s] 55%|█████▌    | 477/861 [00:10<00:08, 44.58it/s] 56%|█████▌    | 482/861 [00:10<00:08, 44.62it/s] 57%|█████▋    | 487/861 [00:10<00:08, 44.57it/s] 57%|█████▋    | 492/861 [00:11<00:08, 44.51it/s] 58%|█████▊    | 497/861 [00:11<00:08, 44.46it/s] 58%|█████▊    | 502/861 [00:11<00:08, 44.33it/s] 59%|█████▉    | 507/861 [00:11<00:08, 44.23it/s] 59%|█████▉    | 512/861 [00:11<00:08, 43.28it/s] 60%|██████    | 517/861 [00:11<00:07, 43.72it/s] 61%|██████    | 522/861 [00:11<00:07, 43.93it/s] 61%|██████    | 527/861 [00:11<00:07, 44.15it/s] 62%|██████▏   | 532/861 [00:12<00:07, 44.34it/s] 62%|██████▏   | 537/861 [00:12<00:07, 44.53it/s] 63%|██████▎   | 542/861 [00:12<00:07, 44.46it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.38it/s] 64%|██████▍   | 552/861 [00:12<00:06, 44.16it/s] 65%|██████▍   | 557/861 [00:12<00:06, 44.18it/s] 65%|██████▌   | 562/861 [00:12<00:06, 44.18it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.45it/s] 66%|██████▋   | 572/861 [00:12<00:06, 44.46it/s] 67%|██████▋   | 577/861 [00:13<00:06, 44.47it/s] 68%|██████▊   | 582/861 [00:13<00:06, 44.54it/s] 68%|██████▊   | 587/861 [00:13<00:06, 44.51it/s] 69%|██████▉   | 592/861 [00:13<00:06, 44.30it/s] 69%|██████▉   | 597/861 [00:13<00:06, 43.21it/s] 70%|██████▉   | 602/861 [00:13<00:05, 43.65it/s] 70%|███████   | 607/861 [00:13<00:05, 43.96it/s] 71%|███████   | 612/861 [00:13<00:05, 44.11it/s] 72%|███████▏  | 617/861 [00:13<00:05, 44.35it/s] 72%|███████▏  | 622/861 [00:14<00:05, 44.37it/s] 73%|███████▎  | 627/861 [00:14<00:05, 44.39it/s] 73%|███████▎  | 632/861 [00:14<00:05, 44.24it/s] 74%|███████▍  | 637/861 [00:14<00:05, 44.11it/s] 75%|███████▍  | 642/861 [00:14<00:04, 44.25it/s] 75%|███████▌  | 647/861 [00:14<00:05, 42.09it/s] 76%|███████▌  | 652/861 [00:14<00:04, 42.86it/s] 76%|███████▋  | 657/861 [00:14<00:04, 43.44it/s] 77%|███████▋  | 662/861 [00:14<00:04, 43.85it/s] 77%|███████▋  | 667/861 [00:15<00:04, 44.18it/s] 78%|███████▊  | 672/861 [00:15<00:04, 44.39it/s] 79%|███████▊  | 677/861 [00:15<00:04, 44.26it/s] 79%|███████▉  | 682/861 [00:15<00:04, 44.11it/s] 80%|███████▉  | 687/861 [00:15<00:03, 43.95it/s] 80%|████████  | 692/861 [00:15<00:03, 44.09it/s] 81%|████████  | 697/861 [00:15<00:03, 44.21it/s] 82%|████████▏ | 702/861 [00:15<00:03, 44.33it/s] 82%|████████▏ | 707/861 [00:15<00:03, 44.47it/s] 83%|████████▎ | 712/861 [00:16<00:03, 44.61it/s] 83%|████████▎ | 717/861 [00:16<00:03, 44.63it/s] 84%|████████▍ | 722/861 [00:16<00:03, 44.46it/s] 84%|████████▍ | 727/861 [00:16<00:03, 44.30it/s] 85%|████████▌ | 732/861 [00:16<00:02, 44.24it/s] 86%|████████▌ | 737/861 [00:16<00:02, 44.22it/s] 86%|████████▌ | 742/861 [00:16<00:02, 44.30it/s] 87%|████████▋ | 747/861 [00:16<00:02, 44.43it/s] 87%|████████▋ | 752/861 [00:17<00:02, 44.53it/s] 88%|████████▊ | 757/861 [00:17<00:02, 44.64it/s] 89%|████████▊ | 762/861 [00:17<00:02, 44.55it/s] 89%|████████▉ | 767/861 [00:17<00:02, 44.37it/s] 90%|████████▉ | 772/861 [00:17<00:02, 44.32it/s] 90%|█████████ | 777/861 [00:17<00:01, 44.17it/s] 91%|█████████ | 782/861 [00:17<00:01, 43.81it/s] 91%|█████████▏| 787/861 [00:17<00:01, 43.96it/s] 92%|█████████▏| 792/861 [00:17<00:01, 44.22it/s] 93%|█████████▎| 797/861 [00:18<00:01, 44.41it/s] 93%|█████████▎| 802/861 [00:18<00:01, 44.48it/s] 94%|█████████▎| 807/861 [00:18<00:01, 44.46it/s] 94%|█████████▍| 812/861 [00:18<00:01, 44.33it/s] 95%|█████████▍| 817/861 [00:18<00:00, 44.20it/s] 95%|█████████▌| 822/861 [00:18<00:00, 44.07it/s] 96%|█████████▌| 827/861 [00:18<00:00, 44.12it/s] 97%|█████████▋| 832/861 [00:18<00:00, 44.25it/s] 97%|█████████▋| 837/861 [00:18<00:00, 44.33it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.50it/s] 98%|█████████▊| 847/861 [00:19<00:00, 44.58it/s] 99%|█████████▉| 852/861 [00:19<00:00, 44.50it/s]100%|█████████▉| 857/861 [00:19<00:00, 44.40it/s]100%|██████████| 861/861 [00:19<00:00, 44.23it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:39:44,590 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:44,590 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:44,590 >>   eval_loss               =     1.0494
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:44,590 >>   eval_runtime            = 0:00:19.48
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:44,590 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:44,590 >>   eval_samples_per_second =      353.3
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:44,590 >>   eval_steps_per_second   =     44.188
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:39:44,590 >>   perplexity              =     2.8559
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:39:55,688 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:39:55,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:39:55,734 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:39:55,734 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:39:55,734 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:39:56,408 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:39:56,409 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:39:56,795 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:39:57,957 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:39:57,957 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:01,141 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:01,161 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:01,162 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:01,162 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:01,162 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:40:02,007 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:40:02,008 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:40:02,756 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:40:02,973 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:40:02,973 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.62it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.67it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:09,  1.76it/s]Extractor Predicting: 16it [00:09,  1.75it/s]Extractor Predicting: 17it [00:10,  1.71it/s]Extractor Predicting: 18it [00:11,  1.72it/s]Extractor Predicting: 19it [00:11,  1.73it/s]Extractor Predicting: 20it [00:12,  1.69it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.70it/s]Extractor Predicting: 23it [00:14,  1.68it/s]Extractor Predicting: 24it [00:14,  1.71it/s]Extractor Predicting: 25it [00:15,  1.69it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:18,  1.63it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:19,  1.70it/s]Extractor Predicting: 34it [00:20,  1.69it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.66it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.68it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:25,  1.68it/s]Extractor Predicting: 44it [00:26,  1.70it/s]Extractor Predicting: 45it [00:27,  1.73it/s]Extractor Predicting: 46it [00:27,  1.74it/s]Extractor Predicting: 47it [00:28,  1.71it/s]Extractor Predicting: 48it [00:28,  1.71it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.69it/s]Extractor Predicting: 51it [00:30,  1.67it/s]Extractor Predicting: 52it [00:31,  1.66it/s]Extractor Predicting: 53it [00:31,  1.63it/s]Extractor Predicting: 54it [00:32,  1.63it/s]Extractor Predicting: 55it [00:33,  1.62it/s]Extractor Predicting: 56it [00:33,  1.64it/s]Extractor Predicting: 57it [00:34,  1.68it/s]Extractor Predicting: 58it [00:34,  1.69it/s]Extractor Predicting: 59it [00:35,  1.64it/s]Extractor Predicting: 60it [00:36,  1.61it/s]Extractor Predicting: 61it [00:36,  1.65it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:38,  1.49it/s]Extractor Predicting: 64it [00:38,  1.53it/s]Extractor Predicting: 65it [00:39,  1.58it/s]Extractor Predicting: 66it [00:40,  1.60it/s]Extractor Predicting: 67it [00:40,  1.62it/s]Extractor Predicting: 68it [00:41,  1.63it/s]Extractor Predicting: 69it [00:41,  1.63it/s]Extractor Predicting: 70it [00:42,  1.61it/s]Extractor Predicting: 71it [00:43,  1.61it/s]Extractor Predicting: 72it [00:43,  1.57it/s]Extractor Predicting: 73it [00:44,  1.60it/s]Extractor Predicting: 74it [00:44,  1.63it/s]Extractor Predicting: 75it [00:45,  1.66it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:46,  1.58it/s]Extractor Predicting: 78it [00:47,  1.62it/s]Extractor Predicting: 79it [00:47,  1.67it/s]Extractor Predicting: 80it [00:48,  1.70it/s]Extractor Predicting: 81it [00:49,  1.73it/s]Extractor Predicting: 82it [00:49,  1.81it/s]Extractor Predicting: 83it [00:50,  1.72it/s]Extractor Predicting: 84it [00:50,  1.74it/s]Extractor Predicting: 85it [00:51,  1.77it/s]Extractor Predicting: 86it [00:51,  1.79it/s]Extractor Predicting: 87it [00:52,  1.77it/s]Extractor Predicting: 88it [00:53,  1.77it/s]Extractor Predicting: 89it [00:53,  1.73it/s]Extractor Predicting: 90it [00:54,  1.78it/s]Extractor Predicting: 91it [00:54,  1.79it/s]Extractor Predicting: 92it [00:55,  1.82it/s]Extractor Predicting: 93it [00:55,  1.79it/s]Extractor Predicting: 94it [00:56,  1.81it/s]Extractor Predicting: 95it [00:56,  1.84it/s]Extractor Predicting: 96it [00:57,  1.79it/s]Extractor Predicting: 97it [00:57,  1.84it/s]Extractor Predicting: 98it [00:58,  1.84it/s]Extractor Predicting: 99it [00:59,  1.84it/s]Extractor Predicting: 100it [00:59,  1.83it/s]Extractor Predicting: 101it [01:00,  1.77it/s]Extractor Predicting: 102it [01:00,  1.76it/s]Extractor Predicting: 103it [01:01,  1.79it/s]Extractor Predicting: 104it [01:01,  1.79it/s]Extractor Predicting: 105it [01:02,  1.80it/s]Extractor Predicting: 106it [01:02,  1.82it/s]Extractor Predicting: 107it [01:03,  1.76it/s]Extractor Predicting: 108it [01:04,  1.83it/s]Extractor Predicting: 109it [01:04,  1.82it/s]Extractor Predicting: 110it [01:05,  1.85it/s]Extractor Predicting: 111it [01:05,  1.86it/s]Extractor Predicting: 112it [01:06,  1.81it/s]Extractor Predicting: 113it [01:06,  1.75it/s]Extractor Predicting: 114it [01:07,  1.74it/s]Extractor Predicting: 115it [01:07,  1.80it/s]Extractor Predicting: 116it [01:08,  1.78it/s]Extractor Predicting: 117it [01:09,  1.77it/s]Extractor Predicting: 118it [01:09,  1.77it/s]Extractor Predicting: 119it [01:10,  1.81it/s]Extractor Predicting: 120it [01:10,  1.84it/s]Extractor Predicting: 121it [01:11,  1.81it/s]Extractor Predicting: 122it [01:11,  1.75it/s]Extractor Predicting: 123it [01:12,  1.82it/s]Extractor Predicting: 124it [01:13,  1.78it/s]Extractor Predicting: 125it [01:13,  1.83it/s]Extractor Predicting: 126it [01:14,  1.84it/s]Extractor Predicting: 127it [01:14,  1.84it/s]Extractor Predicting: 128it [01:15,  1.76it/s]Extractor Predicting: 129it [01:15,  1.81it/s]Extractor Predicting: 130it [01:16,  1.83it/s]Extractor Predicting: 131it [01:16,  1.84it/s]Extractor Predicting: 132it [01:17,  1.88it/s]Extractor Predicting: 133it [01:17,  1.83it/s]Extractor Predicting: 134it [01:18,  1.83it/s]Extractor Predicting: 135it [01:18,  1.84it/s]Extractor Predicting: 136it [01:19,  1.85it/s]Extractor Predicting: 137it [01:20,  1.83it/s]Extractor Predicting: 138it [01:20,  1.82it/s]Extractor Predicting: 139it [01:21,  1.76it/s]Extractor Predicting: 140it [01:21,  1.76it/s]Extractor Predicting: 141it [01:22,  1.79it/s]Extractor Predicting: 142it [01:22,  1.78it/s]Extractor Predicting: 143it [01:23,  1.83it/s]Extractor Predicting: 144it [01:24,  1.78it/s]Extractor Predicting: 145it [01:24,  1.79it/s]Extractor Predicting: 146it [01:25,  1.81it/s]Extractor Predicting: 147it [01:25,  1.81it/s]Extractor Predicting: 148it [01:26,  1.82it/s]Extractor Predicting: 149it [01:26,  1.87it/s]Extractor Predicting: 150it [01:27,  1.78it/s]Extractor Predicting: 151it [01:28,  1.68it/s]Extractor Predicting: 152it [01:28,  1.63it/s]Extractor Predicting: 153it [01:29,  1.60it/s]Extractor Predicting: 154it [01:30,  1.51it/s]Extractor Predicting: 155it [01:30,  1.56it/s]Extractor Predicting: 156it [01:31,  1.48it/s]Extractor Predicting: 157it [01:32,  1.53it/s]Extractor Predicting: 158it [01:32,  1.53it/s]Extractor Predicting: 159it [01:33,  1.52it/s]Extractor Predicting: 160it [01:34,  1.50it/s]Extractor Predicting: 161it [01:34,  1.53it/s]Extractor Predicting: 162it [01:35,  1.49it/s]Extractor Predicting: 163it [01:36,  1.49it/s]Extractor Predicting: 164it [01:36,  1.49it/s]Extractor Predicting: 165it [01:37,  1.46it/s]Extractor Predicting: 166it [01:38,  1.45it/s]Extractor Predicting: 167it [01:38,  1.46it/s]Extractor Predicting: 168it [01:39,  1.34it/s]Extractor Predicting: 169it [01:40,  1.36it/s]Extractor Predicting: 170it [01:41,  1.40it/s]Extractor Predicting: 171it [01:41,  1.43it/s]Extractor Predicting: 172it [01:42,  1.42it/s]Extractor Predicting: 173it [01:43,  1.45it/s]Extractor Predicting: 174it [01:43,  1.50it/s]Extractor Predicting: 175it [01:44,  1.49it/s]Extractor Predicting: 176it [01:45,  1.48it/s]Extractor Predicting: 177it [01:45,  1.46it/s]Extractor Predicting: 178it [01:46,  1.47it/s]Extractor Predicting: 179it [01:47,  1.47it/s]Extractor Predicting: 180it [01:47,  1.47it/s]Extractor Predicting: 181it [01:48,  1.47it/s]Extractor Predicting: 182it [01:49,  1.44it/s]Extractor Predicting: 183it [01:49,  1.48it/s]Extractor Predicting: 184it [01:50,  1.51it/s]Extractor Predicting: 185it [01:51,  1.53it/s]Extractor Predicting: 186it [01:51,  1.54it/s]Extractor Predicting: 187it [01:52,  1.49it/s]Extractor Predicting: 188it [01:53,  1.54it/s]Extractor Predicting: 189it [01:53,  1.49it/s]Extractor Predicting: 190it [01:54,  1.48it/s]Extractor Predicting: 191it [01:55,  1.46it/s]Extractor Predicting: 192it [01:56,  1.38it/s]Extractor Predicting: 193it [01:56,  1.44it/s]Extractor Predicting: 194it [01:57,  1.49it/s]Extractor Predicting: 195it [01:57,  1.50it/s]Extractor Predicting: 196it [01:58,  1.53it/s]Extractor Predicting: 197it [01:59,  1.53it/s]Extractor Predicting: 198it [01:59,  1.55it/s]Extractor Predicting: 199it [02:00,  1.58it/s]Extractor Predicting: 200it [02:01,  1.59it/s]Extractor Predicting: 201it [02:01,  1.52it/s]Extractor Predicting: 202it [02:02,  1.52it/s]Extractor Predicting: 203it [02:03,  1.56it/s]Extractor Predicting: 204it [02:03,  1.56it/s]Extractor Predicting: 205it [02:04,  1.62it/s]Extractor Predicting: 206it [02:04,  1.59it/s]Extractor Predicting: 207it [02:05,  1.58it/s]Extractor Predicting: 208it [02:06,  1.59it/s]Extractor Predicting: 209it [02:06,  1.64it/s]Extractor Predicting: 210it [02:07,  1.63it/s]Extractor Predicting: 211it [02:08,  1.56it/s]Extractor Predicting: 212it [02:08,  1.57it/s]Extractor Predicting: 213it [02:09,  1.59it/s]Extractor Predicting: 214it [02:09,  1.61it/s]Extractor Predicting: 215it [02:10,  1.65it/s]Extractor Predicting: 216it [02:11,  1.65it/s]Extractor Predicting: 217it [02:11,  1.63it/s]Extractor Predicting: 218it [02:12,  1.68it/s]Extractor Predicting: 219it [02:12,  1.59it/s]Extractor Predicting: 220it [02:13,  1.59it/s]Extractor Predicting: 221it [02:14,  1.61it/s]Extractor Predicting: 222it [02:14,  1.62it/s]Extractor Predicting: 223it [02:15,  1.65it/s]Extractor Predicting: 224it [02:16,  1.48it/s]Extractor Predicting: 225it [02:16,  1.54it/s]Extractor Predicting: 226it [02:17,  1.56it/s]Extractor Predicting: 227it [02:18,  1.57it/s]Extractor Predicting: 228it [02:18,  1.59it/s]Extractor Predicting: 229it [02:19,  1.59it/s]Extractor Predicting: 230it [02:19,  1.60it/s]Extractor Predicting: 231it [02:20,  1.65it/s]Extractor Predicting: 232it [02:21,  1.67it/s]Extractor Predicting: 233it [02:21,  1.64it/s]Extractor Predicting: 234it [02:22,  1.58it/s]Extractor Predicting: 235it [02:22,  1.58it/s]Extractor Predicting: 236it [02:23,  1.61it/s]Extractor Predicting: 237it [02:24,  1.55it/s]Extractor Predicting: 237it [02:24,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:38,884 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:38,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:38,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:38,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:38,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:42:39,672 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:42:39,673 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:42:40,256 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:42:41,361 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:42:41,361 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:44,303 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:44,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:44,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:44,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:42:44,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:42:45,141 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:42:45,142 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:42:45,745 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:42:45,952 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:42:45,952 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.76it/s]Extractor Predicting: 6it [00:03,  1.82it/s]Extractor Predicting: 7it [00:03,  1.90it/s]Extractor Predicting: 8it [00:04,  1.83it/s]Extractor Predicting: 9it [00:05,  1.82it/s]Extractor Predicting: 10it [00:05,  1.82it/s]Extractor Predicting: 11it [00:06,  1.80it/s]Extractor Predicting: 12it [00:06,  1.82it/s]Extractor Predicting: 13it [00:07,  1.82it/s]Extractor Predicting: 14it [00:07,  1.73it/s]Extractor Predicting: 15it [00:08,  1.74it/s]Extractor Predicting: 16it [00:09,  1.75it/s]Extractor Predicting: 17it [00:09,  1.75it/s]Extractor Predicting: 18it [00:10,  1.78it/s]Extractor Predicting: 19it [00:10,  1.80it/s]Extractor Predicting: 20it [00:11,  1.75it/s]Extractor Predicting: 21it [00:11,  1.80it/s]Extractor Predicting: 22it [00:12,  1.84it/s]Extractor Predicting: 23it [00:12,  1.88it/s]Extractor Predicting: 24it [00:13,  1.91it/s]Extractor Predicting: 25it [00:13,  1.90it/s]Extractor Predicting: 26it [00:14,  1.83it/s]Extractor Predicting: 27it [00:15,  1.83it/s]Extractor Predicting: 28it [00:15,  1.84it/s]Extractor Predicting: 29it [00:16,  1.82it/s]Extractor Predicting: 30it [00:16,  1.81it/s]Extractor Predicting: 31it [00:17,  1.82it/s]Extractor Predicting: 32it [00:17,  1.82it/s]Extractor Predicting: 33it [00:18,  1.78it/s]Extractor Predicting: 34it [00:18,  1.76it/s]Extractor Predicting: 35it [00:19,  1.77it/s]Extractor Predicting: 36it [00:20,  1.68it/s]Extractor Predicting: 37it [00:20,  1.77it/s]Extractor Predicting: 38it [00:21,  1.80it/s]Extractor Predicting: 39it [00:21,  1.76it/s]Extractor Predicting: 40it [00:22,  1.74it/s]Extractor Predicting: 41it [00:23,  1.67it/s]Extractor Predicting: 42it [00:23,  1.69it/s]Extractor Predicting: 43it [00:24,  1.72it/s]Extractor Predicting: 44it [00:24,  1.69it/s]Extractor Predicting: 45it [00:25,  1.70it/s]Extractor Predicting: 46it [00:26,  1.65it/s]Extractor Predicting: 47it [00:26,  1.66it/s]Extractor Predicting: 48it [00:27,  1.69it/s]Extractor Predicting: 49it [00:27,  1.67it/s]Extractor Predicting: 50it [00:28,  1.68it/s]Extractor Predicting: 51it [00:28,  1.70it/s]Extractor Predicting: 52it [00:29,  1.69it/s]Extractor Predicting: 53it [00:30,  1.71it/s]Extractor Predicting: 54it [00:30,  1.70it/s]Extractor Predicting: 55it [00:31,  1.71it/s]Extractor Predicting: 56it [00:31,  1.73it/s]Extractor Predicting: 57it [00:32,  1.70it/s]Extractor Predicting: 58it [00:33,  1.72it/s]Extractor Predicting: 59it [00:33,  1.70it/s]Extractor Predicting: 60it [00:34,  1.72it/s]Extractor Predicting: 61it [00:34,  1.69it/s]Extractor Predicting: 62it [00:35,  1.69it/s]Extractor Predicting: 63it [00:35,  1.71it/s]Extractor Predicting: 64it [00:36,  1.74it/s]Extractor Predicting: 65it [00:37,  1.74it/s]Extractor Predicting: 66it [00:37,  1.75it/s]Extractor Predicting: 67it [00:38,  1.76it/s]Extractor Predicting: 68it [00:38,  1.73it/s]Extractor Predicting: 69it [00:39,  1.74it/s]Extractor Predicting: 70it [00:39,  1.72it/s]Extractor Predicting: 71it [00:40,  1.74it/s]Extractor Predicting: 72it [00:41,  1.74it/s]Extractor Predicting: 73it [00:41,  1.70it/s]Extractor Predicting: 74it [00:42,  1.74it/s]Extractor Predicting: 75it [00:42,  1.74it/s]Extractor Predicting: 76it [00:43,  1.72it/s]Extractor Predicting: 77it [00:44,  1.72it/s]Extractor Predicting: 78it [00:44,  1.73it/s]Extractor Predicting: 79it [00:45,  1.75it/s]Extractor Predicting: 80it [00:45,  1.76it/s]Extractor Predicting: 81it [00:46,  1.73it/s]Extractor Predicting: 82it [00:46,  1.68it/s]Extractor Predicting: 83it [00:47,  1.74it/s]Extractor Predicting: 84it [00:48,  1.72it/s]Extractor Predicting: 85it [00:48,  1.70it/s]Extractor Predicting: 86it [00:49,  1.71it/s]Extractor Predicting: 87it [00:49,  1.75it/s]Extractor Predicting: 88it [00:50,  1.70it/s]Extractor Predicting: 89it [00:51,  1.70it/s]Extractor Predicting: 90it [00:51,  1.70it/s]Extractor Predicting: 91it [00:52,  1.69it/s]Extractor Predicting: 92it [00:52,  1.72it/s]Extractor Predicting: 93it [00:53,  1.76it/s]Extractor Predicting: 94it [00:53,  1.70it/s]Extractor Predicting: 95it [00:54,  1.69it/s]Extractor Predicting: 96it [00:55,  1.73it/s]Extractor Predicting: 97it [00:55,  1.72it/s]Extractor Predicting: 98it [00:56,  1.71it/s]Extractor Predicting: 99it [00:56,  1.73it/s]Extractor Predicting: 100it [00:57,  1.62it/s]Extractor Predicting: 101it [00:58,  1.54it/s]Extractor Predicting: 102it [00:58,  1.57it/s]Extractor Predicting: 103it [00:59,  1.60it/s]Extractor Predicting: 104it [00:59,  1.68it/s]Extractor Predicting: 105it [01:00,  1.68it/s]Extractor Predicting: 106it [01:01,  1.73it/s]Extractor Predicting: 107it [01:01,  1.77it/s]Extractor Predicting: 108it [01:02,  1.80it/s]Extractor Predicting: 109it [01:02,  1.73it/s]Extractor Predicting: 110it [01:03,  1.73it/s]Extractor Predicting: 111it [01:03,  1.71it/s]Extractor Predicting: 112it [01:04,  1.71it/s]Extractor Predicting: 113it [01:05,  1.77it/s]Extractor Predicting: 114it [01:05,  1.73it/s]Extractor Predicting: 115it [01:06,  1.66it/s]Extractor Predicting: 116it [01:06,  1.72it/s]Extractor Predicting: 117it [01:07,  1.72it/s]Extractor Predicting: 118it [01:08,  1.76it/s]Extractor Predicting: 119it [01:08,  1.75it/s]Extractor Predicting: 120it [01:09,  1.74it/s]Extractor Predicting: 121it [01:09,  1.63it/s]Extractor Predicting: 122it [01:10,  1.63it/s]Extractor Predicting: 123it [01:11,  1.67it/s]Extractor Predicting: 124it [01:11,  1.70it/s]Extractor Predicting: 125it [01:12,  1.73it/s]Extractor Predicting: 126it [01:12,  1.71it/s]Extractor Predicting: 127it [01:13,  1.70it/s]Extractor Predicting: 128it [01:13,  1.76it/s]Extractor Predicting: 129it [01:14,  1.76it/s]Extractor Predicting: 130it [01:15,  1.78it/s]Extractor Predicting: 131it [01:15,  1.78it/s]Extractor Predicting: 132it [01:16,  1.72it/s]Extractor Predicting: 133it [01:16,  1.72it/s]Extractor Predicting: 134it [01:17,  1.75it/s]Extractor Predicting: 135it [01:17,  1.75it/s]Extractor Predicting: 136it [01:18,  1.76it/s]Extractor Predicting: 137it [01:19,  1.74it/s]Extractor Predicting: 138it [01:19,  1.69it/s]Extractor Predicting: 139it [01:20,  1.67it/s]Extractor Predicting: 140it [01:20,  1.75it/s]Extractor Predicting: 141it [01:21,  1.74it/s]Extractor Predicting: 142it [01:22,  1.70it/s]Extractor Predicting: 143it [01:22,  1.68it/s]Extractor Predicting: 144it [01:23,  1.66it/s]Extractor Predicting: 145it [01:23,  1.69it/s]Extractor Predicting: 146it [01:24,  1.68it/s]Extractor Predicting: 147it [01:24,  1.70it/s]Extractor Predicting: 148it [01:25,  1.64it/s]Extractor Predicting: 149it [01:26,  1.59it/s]Extractor Predicting: 150it [01:26,  1.62it/s]Extractor Predicting: 151it [01:27,  1.66it/s]Extractor Predicting: 152it [01:28,  1.66it/s]Extractor Predicting: 153it [01:28,  1.68it/s]Extractor Predicting: 154it [01:29,  1.63it/s]Extractor Predicting: 155it [01:29,  1.65it/s]Extractor Predicting: 156it [01:30,  1.68it/s]Extractor Predicting: 157it [01:31,  1.64it/s]Extractor Predicting: 158it [01:31,  1.63it/s]Extractor Predicting: 159it [01:32,  1.62it/s]Extractor Predicting: 160it [01:32,  1.66it/s]Extractor Predicting: 161it [01:33,  1.66it/s]Extractor Predicting: 162it [01:34,  1.64it/s]Extractor Predicting: 163it [01:34,  1.61it/s]Extractor Predicting: 164it [01:35,  1.61it/s]Extractor Predicting: 165it [01:35,  1.64it/s]Extractor Predicting: 166it [01:36,  1.61it/s]Extractor Predicting: 167it [01:37,  1.63it/s]Extractor Predicting: 168it [01:37,  1.77it/s]Extractor Predicting: 168it [01:37,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:33,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:33,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:33,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:33,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:33,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:44:34,535 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:44:34,536 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:44:35,139 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:44:36,217 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:44:36,217 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:39,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:39,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:39,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:39,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:44:39,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:44:39,964 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:44:39,965 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:44:40,564 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:44:40,758 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:44:40,758 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.71it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:04,  1.75it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.71it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:08,  1.67it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:10,  1.59it/s]Extractor Predicting: 19it [00:11,  1.72it/s]Extractor Predicting: 19it [00:11,  1.69it/s]
[INFO|configuration_utils.py:515] 2023-08-28 02:44:53,557 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:44:53,558 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:44:53,618 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:44:53,619 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 02:44:53,642 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:45:08,602 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 02:45:08,623 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 02:45:08,745 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:45:08,746 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:45:08,795 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:45:08,839 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:45:08,839 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:45:08,839 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:45:08,839 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:45:08,839 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:45:08,839 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 02:45:09,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:10,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:10,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:11,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:12,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:12,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:13,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:13,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:14,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:15,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:16,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:16,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:17,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:18,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:19,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:19,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:20,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:21,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:21,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:22,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:22,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:10, 14.45s/it][WARNING|generation_utils.py:914] 2023-08-28 02:45:23,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:24,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:25,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:25,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:26,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:27,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:27,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:28,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:28,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:29,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:30,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:30,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:31,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:32,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:32,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:33,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:34,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:34,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:35,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:36,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:36,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:37,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:38,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:39,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:39,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:40,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:41,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:41,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:16, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 02:45:42,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:43,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:44,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:44,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:45,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:46,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:46,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:47,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:48,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:48,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:49,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:50,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:50,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:51,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:52,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:52,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:53,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:53,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:54,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:55,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:55,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:56,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:57,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:48<01:55, 16.43s/it][WARNING|generation_utils.py:914] 2023-08-28 02:45:58,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:58,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:59,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:00,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:00,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:01,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:02,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:02,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:03,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:04,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:04,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:05,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:05,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:06,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:07,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:07,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:08,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:09,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:09,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:10,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:10,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:11,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:12,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:03<01:34, 15.67s/it][WARNING|generation_utils.py:914] 2023-08-28 02:46:12,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:13,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:14,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:14,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:15,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:16,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:16,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:17,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:18,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:18,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:19,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:19,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:20,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:21,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:21,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:22,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:23,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:23,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:24,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:25,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:25,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:26,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:27,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:18<01:17, 15.48s/it][WARNING|generation_utils.py:914] 2023-08-28 02:46:27,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:28,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:29,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:29,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:30,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:30,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:31,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:32,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:32,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:33,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:34,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:34,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:35,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:35,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:36,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:37,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:38,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:38,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:39,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:40,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:40,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:41,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:42,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:33<01:00, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 02:46:42,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:43,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:43,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:44,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:45,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:45,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:46,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:46,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:47,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:48,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:49,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:49,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:50,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:51,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:52,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:52,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:53,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:54,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:54,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:55,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:55,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:56,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:57,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:57,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:58,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:49<00:46, 15.66s/it][WARNING|generation_utils.py:914] 2023-08-28 02:46:59,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:46:59,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:00,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:01,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:01,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:02,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:03,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:03,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:04,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:05,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:05,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:06,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:07,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:07,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:08,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:09,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:09,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:10,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:11,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:12,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:12,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:13,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:04<00:30, 15.39s/it][WARNING|generation_utils.py:914] 2023-08-28 02:47:13,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:14,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:15,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:15,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:16,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:16,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:17,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:17,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:18,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:19,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:19,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:20,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:20,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:21,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:22,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:22,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:23,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:23,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:24,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:25,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:25,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:26,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:27,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:27,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:19<00:15, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-28 02:47:28,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:29,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:29,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:30,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:31,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:31,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:32,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:33,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:33,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:34,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:35,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:36,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:36,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:37,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:38,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:38,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:39,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:40,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:41,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:42,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:42,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:43,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:47:44,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:35<00:00, 15.52s/it]Generating: 100%|██████████| 10/10 [02:35<00:00, 15.57s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:51,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:51,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:51,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:51,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:51,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:47:52,764 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:47:52,765 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:47:53,352 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:47:54,498 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:47:54,498 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:57,498 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:57,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:57,521 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:57,521 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:47:57,521 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:47:58,249 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:47:58,251 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:47:58,885 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:47:59,290 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:47:59,290 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9166666666666666, 'errors': {'', "('', 'composer', 'Jérémie Léger', 'The title track of the album was a track Iced Tea made by Jérémie Léger and written by Pierre Bousquet and Bernard Barrou , among others .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 397, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 443, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 489, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 555, 'raw': 800}
{'target': 600, 'success': 574, 'raw': 832}
{'target': 600, 'success': 593, 'raw': 864}
{'target': 600, 'success': 614, 'raw': 896}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.6852678571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : opposite of . Context : Later in the year ( 1143 ) , Ptolemaeus , the founder of the Macedonian Republic , married Ariadne , daughter of Ariadne , founder of Macedonian kings , Thebes and Ileani . Head Entity : Ileani , Tail Entity : Ileanius .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8491847826086957, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : creator .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.75375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Lothar', 'twinned administrative body', '', 'He died in the Battle of Mervyn at Rheinmetall in 645 along with his daughter Lothar , D.')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 12521
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12621, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.44it/s]Extractor Estimating: 3it [00:02,  1.51it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.54it/s]Extractor Estimating: 7it [00:04,  1.60it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.56it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.54it/s]Extractor Estimating: 12it [00:07,  1.53it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:08,  1.58it/s]Extractor Estimating: 15it [00:09,  1.50it/s]Extractor Estimating: 16it [00:10,  1.48it/s]Extractor Estimating: 17it [00:11,  1.42it/s]Extractor Estimating: 18it [00:11,  1.46it/s]Extractor Estimating: 19it [00:12,  1.49it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:13,  1.48it/s]Extractor Estimating: 22it [00:14,  1.53it/s]Extractor Estimating: 23it [00:15,  1.58it/s]Extractor Estimating: 24it [00:15,  1.52it/s]Extractor Estimating: 25it [00:16,  1.53it/s]Extractor Estimating: 26it [00:17,  1.52it/s]Extractor Estimating: 27it [00:17,  1.54it/s]Extractor Estimating: 28it [00:18,  1.53it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:19,  1.56it/s]Extractor Estimating: 31it [00:20,  1.55it/s]Extractor Estimating: 32it [00:20,  1.53it/s]Extractor Estimating: 33it [00:21,  1.50it/s]Extractor Estimating: 34it [00:22,  1.52it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:23,  1.51it/s]Extractor Estimating: 37it [00:24,  1.46it/s]Extractor Estimating: 38it [00:24,  1.49it/s]Extractor Estimating: 39it [00:25,  1.48it/s]Extractor Estimating: 40it [00:26,  1.50it/s]Extractor Estimating: 41it [00:26,  1.50it/s]Extractor Estimating: 42it [00:27,  1.49it/s]Extractor Estimating: 43it [00:28,  1.52it/s]Extractor Estimating: 44it [00:28,  1.53it/s]Extractor Estimating: 45it [00:29,  1.53it/s]Extractor Estimating: 46it [00:30,  1.50it/s]Extractor Estimating: 47it [00:30,  1.50it/s]Extractor Estimating: 48it [00:31,  1.49it/s]Extractor Estimating: 49it [00:32,  1.44it/s]Extractor Estimating: 50it [00:32,  1.49it/s]Extractor Estimating: 51it [00:33,  1.47it/s]Extractor Estimating: 52it [00:34,  1.50it/s]Extractor Estimating: 53it [00:34,  1.51it/s]Extractor Estimating: 54it [00:35,  1.50it/s]Extractor Estimating: 55it [00:36,  1.49it/s]Extractor Estimating: 56it [00:36,  1.51it/s]Extractor Estimating: 57it [00:37,  1.49it/s]Extractor Estimating: 58it [00:38,  1.49it/s]Extractor Estimating: 59it [00:38,  1.48it/s]Extractor Estimating: 60it [00:39,  1.54it/s]Extractor Estimating: 61it [00:40,  1.57it/s]Extractor Estimating: 62it [00:40,  1.55it/s]Extractor Estimating: 63it [00:41,  1.54it/s]Extractor Estimating: 64it [00:42,  1.53it/s]Extractor Estimating: 65it [00:42,  1.54it/s]Extractor Estimating: 66it [00:43,  1.59it/s]Extractor Estimating: 67it [00:44,  1.55it/s]Extractor Estimating: 68it [00:44,  1.62it/s]Extractor Estimating: 69it [00:45,  1.65it/s]Extractor Estimating: 70it [00:45,  1.70it/s]Extractor Estimating: 71it [00:46,  1.68it/s]Extractor Estimating: 72it [00:47,  1.60it/s]Extractor Estimating: 73it [00:47,  1.63it/s]Extractor Estimating: 74it [00:48,  1.62it/s]Extractor Estimating: 75it [00:48,  1.59it/s]Extractor Estimating: 76it [00:49,  1.60it/s]Extractor Estimating: 77it [00:50,  1.59it/s]Extractor Estimating: 78it [00:50,  1.57it/s]Extractor Estimating: 79it [00:51,  1.46it/s]Extractor Estimating: 80it [00:52,  1.46it/s]Extractor Estimating: 81it [00:52,  1.47it/s]Extractor Estimating: 82it [00:53,  1.50it/s]Extractor Estimating: 83it [00:54,  1.47it/s]Extractor Estimating: 84it [00:54,  1.54it/s]Extractor Estimating: 85it [00:55,  1.59it/s]Extractor Estimating: 86it [00:56,  1.61it/s]Extractor Estimating: 87it [00:56,  1.61it/s]Extractor Estimating: 88it [00:57,  1.60it/s]Extractor Estimating: 89it [00:57,  1.58it/s]Extractor Estimating: 90it [00:58,  1.58it/s]Extractor Estimating: 91it [00:59,  1.58it/s]Extractor Estimating: 92it [00:59,  1.58it/s]Extractor Estimating: 93it [01:00,  1.56it/s]Extractor Estimating: 94it [01:01,  1.58it/s]Extractor Estimating: 95it [01:01,  1.62it/s]Extractor Estimating: 96it [01:02,  1.63it/s]Extractor Estimating: 97it [01:02,  1.67it/s]Extractor Estimating: 98it [01:03,  1.64it/s]Extractor Estimating: 99it [01:04,  1.66it/s]Extractor Estimating: 100it [01:04,  1.66it/s]Extractor Estimating: 101it [01:05,  1.60it/s]Extractor Estimating: 102it [01:06,  1.60it/s]Extractor Estimating: 103it [01:06,  1.51it/s]Extractor Estimating: 104it [01:07,  1.52it/s]Extractor Estimating: 105it [01:08,  1.57it/s]Extractor Estimating: 106it [01:08,  1.56it/s]Extractor Estimating: 107it [01:09,  1.59it/s]Extractor Estimating: 108it [01:09,  1.58it/s]Extractor Estimating: 109it [01:10,  1.60it/s]Extractor Estimating: 110it [01:11,  1.64it/s]Extractor Estimating: 111it [01:11,  1.59it/s]Extractor Estimating: 112it [01:12,  1.63it/s]Extractor Estimating: 113it [01:12,  1.61it/s]Extractor Estimating: 114it [01:13,  1.59it/s]Extractor Estimating: 115it [01:14,  1.59it/s]Extractor Estimating: 116it [01:14,  1.61it/s]Extractor Estimating: 117it [01:15,  1.60it/s]Extractor Estimating: 118it [01:16,  1.53it/s]Extractor Estimating: 119it [01:16,  1.54it/s]Extractor Estimating: 120it [01:17,  1.56it/s]Extractor Estimating: 121it [01:18,  1.60it/s]Extractor Estimating: 122it [01:18,  1.60it/s]Extractor Estimating: 123it [01:19,  1.60it/s]Extractor Estimating: 124it [01:19,  1.59it/s]Extractor Estimating: 125it [01:20,  1.58it/s]Extractor Estimating: 126it [01:21,  1.55it/s]Extractor Estimating: 127it [01:21,  1.58it/s]Extractor Estimating: 128it [01:22,  1.61it/s]Extractor Estimating: 129it [01:23,  1.61it/s]Extractor Estimating: 130it [01:23,  1.66it/s]Extractor Estimating: 131it [01:24,  1.57it/s]Extractor Estimating: 132it [01:25,  1.54it/s]Extractor Estimating: 133it [01:25,  1.55it/s]Extractor Estimating: 134it [01:26,  1.60it/s]Extractor Estimating: 135it [01:26,  1.60it/s]Extractor Estimating: 136it [01:27,  1.58it/s]Extractor Estimating: 137it [01:28,  1.56it/s]Extractor Estimating: 138it [01:28,  1.58it/s]Extractor Estimating: 139it [01:29,  1.59it/s]Extractor Estimating: 140it [01:30,  1.60it/s]Extractor Estimating: 141it [01:30,  1.57it/s]Extractor Estimating: 142it [01:31,  1.54it/s]Extractor Estimating: 143it [01:32,  1.55it/s]Extractor Estimating: 144it [01:32,  1.53it/s]Extractor Estimating: 145it [01:33,  1.56it/s]Extractor Estimating: 146it [01:34,  1.44it/s]Extractor Estimating: 147it [01:34,  1.47it/s]Extractor Estimating: 148it [01:35,  1.50it/s]Extractor Estimating: 149it [01:35,  1.56it/s]Extractor Estimating: 150it [01:36,  1.58it/s]Extractor Estimating: 151it [01:37,  1.61it/s]Extractor Estimating: 152it [01:37,  1.54it/s]Extractor Estimating: 153it [01:38,  1.57it/s]Extractor Estimating: 154it [01:39,  1.54it/s]Extractor Estimating: 155it [01:39,  1.54it/s]Extractor Estimating: 156it [01:40,  1.55it/s]Extractor Estimating: 157it [01:41,  1.56it/s]Extractor Estimating: 158it [01:41,  1.58it/s]Extractor Estimating: 159it [01:42,  1.55it/s]Extractor Estimating: 160it [01:42,  1.58it/s]Extractor Estimating: 161it [01:43,  1.57it/s]Extractor Estimating: 162it [01:44,  1.59it/s]Extractor Estimating: 163it [01:44,  1.55it/s]Extractor Estimating: 164it [01:45,  1.52it/s]Extractor Estimating: 165it [01:46,  1.53it/s]Extractor Estimating: 166it [01:46,  1.57it/s]Extractor Estimating: 167it [01:47,  1.57it/s]Extractor Estimating: 168it [01:48,  1.63it/s]Extractor Estimating: 169it [01:48,  1.64it/s]Extractor Estimating: 170it [01:49,  1.58it/s]Extractor Estimating: 171it [01:50,  1.55it/s]Extractor Estimating: 172it [01:50,  1.57it/s]Extractor Estimating: 173it [01:51,  1.58it/s]Extractor Estimating: 174it [01:51,  1.60it/s]Extractor Estimating: 175it [01:52,  1.59it/s]Extractor Estimating: 176it [01:53,  1.61it/s]Extractor Estimating: 177it [01:53,  1.52it/s]Extractor Estimating: 178it [01:54,  1.53it/s]Extractor Estimating: 179it [01:55,  1.54it/s]Extractor Estimating: 180it [01:55,  1.54it/s]Extractor Estimating: 181it [01:56,  1.59it/s]Extractor Estimating: 182it [01:57,  1.58it/s]Extractor Estimating: 183it [01:57,  1.61it/s]Extractor Estimating: 184it [01:58,  1.61it/s]Extractor Estimating: 185it [01:59,  1.46it/s]Extractor Estimating: 186it [01:59,  1.47it/s]Extractor Estimating: 187it [02:00,  1.53it/s]Extractor Estimating: 188it [02:00,  1.52it/s]Extractor Estimating: 189it [02:01,  1.51it/s]Extractor Estimating: 190it [02:02,  1.52it/s]Extractor Estimating: 191it [02:02,  1.56it/s]Extractor Estimating: 192it [02:03,  1.55it/s]Extractor Estimating: 193it [02:04,  1.53it/s]Extractor Estimating: 194it [02:04,  1.52it/s]Extractor Estimating: 195it [02:05,  1.50it/s]Extractor Estimating: 196it [02:06,  1.51it/s]Extractor Estimating: 197it [02:06,  1.53it/s]Extractor Estimating: 198it [02:07,  1.54it/s]Extractor Estimating: 199it [02:08,  1.56it/s]Extractor Estimating: 200it [02:08,  1.53it/s]Extractor Estimating: 201it [02:09,  1.57it/s]Extractor Estimating: 202it [02:10,  1.59it/s]Extractor Estimating: 203it [02:10,  1.60it/s]Extractor Estimating: 204it [02:11,  1.60it/s]Extractor Estimating: 205it [02:11,  1.58it/s]Extractor Estimating: 206it [02:12,  1.60it/s]Extractor Estimating: 207it [02:13,  1.63it/s]Extractor Estimating: 208it [02:13,  1.63it/s]Extractor Estimating: 209it [02:14,  1.63it/s]Extractor Estimating: 210it [02:14,  1.61it/s]Extractor Estimating: 211it [02:15,  1.63it/s]Extractor Estimating: 212it [02:16,  1.63it/s]Extractor Estimating: 213it [02:16,  1.52it/s]Extractor Estimating: 214it [02:17,  1.58it/s]Extractor Estimating: 215it [02:18,  1.61it/s]Extractor Estimating: 216it [02:18,  1.63it/s]Extractor Estimating: 217it [02:19,  1.62it/s]Extractor Estimating: 218it [02:19,  1.66it/s]Extractor Estimating: 219it [02:20,  1.68it/s]Extractor Estimating: 220it [02:21,  1.69it/s]Extractor Estimating: 221it [02:21,  1.64it/s]Extractor Estimating: 222it [02:22,  1.59it/s]Extractor Estimating: 223it [02:23,  1.59it/s]Extractor Estimating: 224it [02:23,  1.59it/s]Extractor Estimating: 225it [02:24,  1.60it/s]Extractor Estimating: 226it [02:24,  1.60it/s]Extractor Estimating: 227it [02:25,  1.59it/s]Extractor Estimating: 228it [02:26,  1.58it/s]Extractor Estimating: 229it [02:26,  1.53it/s]Extractor Estimating: 230it [02:27,  1.61it/s]Extractor Estimating: 231it [02:28,  1.56it/s]Extractor Estimating: 232it [02:28,  1.59it/s]Extractor Estimating: 233it [02:29,  1.62it/s]Extractor Estimating: 234it [02:30,  1.53it/s]Extractor Estimating: 235it [02:30,  1.54it/s]Extractor Estimating: 236it [02:31,  1.55it/s]Extractor Estimating: 237it [02:31,  1.54it/s]Extractor Estimating: 238it [02:32,  1.60it/s]Extractor Estimating: 239it [02:33,  1.56it/s]Extractor Estimating: 240it [02:33,  1.54it/s]Extractor Estimating: 241it [02:34,  1.60it/s]Extractor Estimating: 242it [02:35,  1.62it/s]Extractor Estimating: 243it [02:35,  1.61it/s]Extractor Estimating: 244it [02:36,  1.39it/s]Extractor Estimating: 245it [02:37,  1.48it/s]Extractor Estimating: 246it [02:37,  1.48it/s]Extractor Estimating: 247it [02:38,  1.54it/s]Extractor Estimating: 248it [02:39,  1.59it/s]Extractor Estimating: 249it [02:39,  1.52it/s]Extractor Estimating: 250it [02:40,  1.65it/s]Extractor Estimating: 250it [02:40,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:01,626 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:01,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:01,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:01,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:01,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:51:02,274 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:51:02,275 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:51:02,896 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:51:04,057 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:51:04,057 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:07,234 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:07,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:07,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:07,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:07,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:51:08,231 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:51:08,232 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:51:08,889 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:51:09,121 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:51:09,121 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 04:32:17,950 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 04:32:18,156 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 5278 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 25209
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25309, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25309, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.053, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.047, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 80, avg_time 1.055, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 180, avg_time 1.050, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 60, avg_time 1.054, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 160, avg_time 2.843, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 40, avg_time 1.061, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 140, avg_time 1.048, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 20, avg_time 1.058, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 120, avg_time 1.052, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 220, avg_time 2.833, loss:nan
g_step 1200, step 100, avg_time 1.057, loss:nan
g_step 1300, step 200, avg_time 1.047, loss:nan
g_step 1400, step 80, avg_time 1.060, loss:nan
g_step 1500, step 180, avg_time 1.046, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 60, avg_time 2.834, loss:nan
g_step 1700, step 160, avg_time 1.050, loss:nan
g_step 1800, step 40, avg_time 1.060, loss:nan
g_step 1900, step 140, avg_time 1.050, loss:nan
g_step 2000, step 20, avg_time 1.047, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 120, avg_time 2.830, loss:nan
g_step 2200, step 220, avg_time 1.050, loss:nan
g_step 2300, step 100, avg_time 1.052, loss:nan
g_step 2400, step 200, avg_time 1.056, loss:nan
g_step 2500, step 80, avg_time 1.051, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 180, avg_time 2.847, loss:nan
g_step 2700, step 60, avg_time 1.062, loss:nan
g_step 2800, step 160, avg_time 1.046, loss:nan
g_step 2900, step 40, avg_time 1.052, loss:nan
g_step 3000, step 140, avg_time 1.045, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 20, avg_time 2.849, loss:nan
g_step 3200, step 120, avg_time 1.048, loss:nan
g_step 3300, step 220, avg_time 1.057, loss:nan
g_step 3400, step 100, avg_time 1.056, loss:nan
g_step 3500, step 200, avg_time 1.046, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 80, avg_time 2.844, loss:nan
g_step 3700, step 180, avg_time 1.043, loss:nan
g_step 3800, step 60, avg_time 1.049, loss:nan
g_step 3900, step 160, avg_time 1.052, loss:nan
g_step 4000, step 40, avg_time 1.072, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 140, avg_time 2.831, loss:nan
g_step 4200, step 20, avg_time 1.055, loss:nan
g_step 4300, step 120, avg_time 1.050, loss:nan
g_step 4400, step 220, avg_time 1.052, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 04:32:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 04:32:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_04-32-17_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 04:32:19 - WARNING - datasets.builder -   Using custom data configuration default-ef31648e0bc2bb81
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ef31648e0bc2bb81/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 04:32:20,454 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:32:20,455 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:32:20,456 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:32:20,457 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:32:20,505 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:32:20,532 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:32:20,532 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:32:20,532 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:32:20,532 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:32:20,532 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:32:20,532 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 04:32:20,820 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:32:23,987 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 04:32:24,020 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ef31648e0bc2bb81/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.74ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.62ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.02ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.23ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.33ba/s]100%|██████████| 6/6 [00:01<00:00,  4.60ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.94ba/s] 29%|██▊       | 2/7 [00:00<00:01,  2.82ba/s] 43%|████▎     | 3/7 [00:00<00:01,  3.39ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  3.76ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  3.98ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.14ba/s]100%|██████████| 7/7 [00:01<00:00,  4.05ba/s]100%|██████████| 7/7 [00:01<00:00,  3.77ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.98ba/s] 50%|█████     | 3/6 [00:00<00:00,  7.56ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.03ba/s]100%|██████████| 6/6 [00:00<00:00,  9.35ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.03ba/s] 43%|████▎     | 3/7 [00:00<00:00,  7.70ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.14ba/s]100%|██████████| 7/7 [00:00<00:00, 10.14ba/s]100%|██████████| 7/7 [00:00<00:00,  9.05ba/s]
[INFO|trainer.py:414] 2023-08-28 04:32:30,118 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 04:32:30,206 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 04:32:30,241 >>   Num examples = 5300
[INFO|trainer.py:1149] 2023-08-28 04:32:30,241 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 04:32:30,242 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 04:32:30,242 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 04:32:30,242 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 04:32:30,242 >>   Total optimization steps = 415
  0%|          | 0/415 [00:00<?, ?it/s]  0%|          | 1/415 [00:00<01:59,  3.45it/s]  0%|          | 2/415 [00:00<01:56,  3.55it/s]  1%|          | 3/415 [00:00<01:55,  3.58it/s]  1%|          | 4/415 [00:01<01:54,  3.59it/s]  1%|          | 5/415 [00:01<01:53,  3.60it/s]  1%|▏         | 6/415 [00:01<01:53,  3.61it/s]  2%|▏         | 7/415 [00:01<01:52,  3.62it/s]  2%|▏         | 8/415 [00:02<01:52,  3.62it/s]  2%|▏         | 9/415 [00:02<01:52,  3.62it/s]  2%|▏         | 10/415 [00:02<01:51,  3.62it/s]  3%|▎         | 11/415 [00:03<01:51,  3.62it/s]  3%|▎         | 12/415 [00:03<01:57,  3.43it/s]  3%|▎         | 13/415 [00:03<01:55,  3.48it/s]  3%|▎         | 14/415 [00:03<01:53,  3.52it/s]  4%|▎         | 15/415 [00:04<01:52,  3.55it/s]  4%|▍         | 16/415 [00:04<01:51,  3.57it/s]  4%|▍         | 17/415 [00:04<01:51,  3.58it/s]  4%|▍         | 18/415 [00:05<01:50,  3.59it/s]  5%|▍         | 19/415 [00:05<01:49,  3.61it/s]  5%|▍         | 20/415 [00:05<01:49,  3.61it/s]  5%|▌         | 21/415 [00:05<01:49,  3.61it/s]  5%|▌         | 22/415 [00:06<01:48,  3.61it/s]  6%|▌         | 23/415 [00:06<01:51,  3.52it/s]  6%|▌         | 24/415 [00:06<01:50,  3.55it/s]  6%|▌         | 25/415 [00:06<01:49,  3.56it/s]  6%|▋         | 26/415 [00:07<01:48,  3.58it/s]  7%|▋         | 27/415 [00:07<01:48,  3.59it/s]  7%|▋         | 28/415 [00:07<01:47,  3.60it/s]  7%|▋         | 29/415 [00:08<01:47,  3.60it/s]  7%|▋         | 30/415 [00:08<01:46,  3.60it/s]  7%|▋         | 31/415 [00:08<01:46,  3.60it/s]  8%|▊         | 32/415 [00:08<01:46,  3.61it/s]  8%|▊         | 33/415 [00:09<01:45,  3.61it/s]  8%|▊         | 34/415 [00:09<01:47,  3.54it/s]  8%|▊         | 35/415 [00:09<01:46,  3.56it/s]  9%|▊         | 36/415 [00:10<01:46,  3.57it/s]  9%|▉         | 37/415 [00:10<01:45,  3.58it/s]  9%|▉         | 38/415 [00:10<01:45,  3.59it/s]  9%|▉         | 39/415 [00:10<01:44,  3.60it/s] 10%|▉         | 40/415 [00:11<01:44,  3.60it/s] 10%|▉         | 41/415 [00:11<01:43,  3.61it/s] 10%|█         | 42/415 [00:11<01:43,  3.61it/s] 10%|█         | 43/415 [00:12<01:43,  3.61it/s] 11%|█         | 44/415 [00:12<01:42,  3.61it/s] 11%|█         | 45/415 [00:12<01:45,  3.50it/s] 11%|█         | 46/415 [00:12<01:44,  3.53it/s] 11%|█▏        | 47/415 [00:13<01:43,  3.55it/s] 12%|█▏        | 48/415 [00:13<01:42,  3.56it/s] 12%|█▏        | 49/415 [00:13<01:42,  3.58it/s] 12%|█▏        | 50/415 [00:13<01:41,  3.59it/s] 12%|█▏        | 51/415 [00:14<01:41,  3.59it/s] 13%|█▎        | 52/415 [00:14<01:40,  3.60it/s] 13%|█▎        | 53/415 [00:14<01:40,  3.60it/s] 13%|█▎        | 54/415 [00:15<01:40,  3.60it/s] 13%|█▎        | 55/415 [00:15<01:39,  3.60it/s] 13%|█▎        | 56/415 [00:15<01:41,  3.55it/s] 14%|█▎        | 57/415 [00:15<01:40,  3.57it/s] 14%|█▍        | 58/415 [00:16<01:39,  3.58it/s] 14%|█▍        | 59/415 [00:16<01:39,  3.58it/s] 14%|█▍        | 60/415 [00:16<01:38,  3.59it/s] 15%|█▍        | 61/415 [00:17<01:38,  3.59it/s] 15%|█▍        | 62/415 [00:17<01:38,  3.60it/s] 15%|█▌        | 63/415 [00:17<01:37,  3.60it/s] 15%|█▌        | 64/415 [00:17<01:37,  3.60it/s] 16%|█▌        | 65/415 [00:18<01:37,  3.61it/s] 16%|█▌        | 66/415 [00:18<01:36,  3.61it/s] 16%|█▌        | 67/415 [00:18<01:36,  3.61it/s] 16%|█▋        | 68/415 [00:18<01:36,  3.61it/s] 17%|█▋        | 69/415 [00:19<01:35,  3.60it/s] 17%|█▋        | 70/415 [00:19<01:35,  3.61it/s] 17%|█▋        | 71/415 [00:19<01:35,  3.61it/s] 17%|█▋        | 72/415 [00:20<01:34,  3.61it/s] 18%|█▊        | 73/415 [00:20<01:34,  3.61it/s] 18%|█▊        | 74/415 [00:20<01:34,  3.61it/s] 18%|█▊        | 75/415 [00:20<01:34,  3.61it/s] 18%|█▊        | 76/415 [00:21<01:34,  3.60it/s] 19%|█▊        | 77/415 [00:21<01:33,  3.60it/s] 19%|█▉        | 78/415 [00:21<01:34,  3.55it/s] 19%|█▉        | 79/415 [00:22<01:34,  3.57it/s] 19%|█▉        | 80/415 [00:22<01:33,  3.58it/s] 20%|█▉        | 81/415 [00:22<01:33,  3.59it/s] 20%|█▉        | 82/415 [00:22<01:32,  3.59it/s] 20%|██        | 83/415 [00:23<01:27,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 04:32:53,350 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:32:53,350 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 04:32:53,350 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.55it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.69it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.08it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.86it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.19it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.65it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.10it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.96it/s][A
  5%|▌         | 47/861 [00:01<00:18, 43.93it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.28it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.47it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.56it/s][A
  8%|▊         | 67/861 [00:01<00:18, 42.83it/s][A
  8%|▊         | 72/861 [00:01<00:18, 43.42it/s][A
  9%|▉         | 77/861 [00:01<00:18, 43.53it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.40it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.52it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.59it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.96it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.21it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.32it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.49it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.44it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.19it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.98it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.86it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.85it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.03it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.28it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.36it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.48it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.52it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.23it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.18it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.98it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.88it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.99it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.21it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.24it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 43.07it/s][A
 24%|██▍       | 207/861 [00:04<00:15, 43.55it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.67it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.67it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.75it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.83it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 43.92it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.10it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.11it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.21it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.36it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.17it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.06it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.03it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.10it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.18it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.13it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.25it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.24it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.39it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.33it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.10it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.09it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.21it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.15it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.09it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.16it/s][A
 39%|███▉      | 337/861 [00:07<00:12, 42.79it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.34it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.57it/s][A
 41%|████      | 352/861 [00:07<00:11, 43.68it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.70it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.94it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.96it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.00it/s][A
 44%|████▍     | 377/861 [00:08<00:11, 43.91it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.01it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.16it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.18it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.25it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.17it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.22it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.25it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.00it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.01it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.10it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.26it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.29it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.26it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.15it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.25it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.15it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.04it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.03it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.51it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.85it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.06it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.19it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.22it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.21it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.18it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.06it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 43.98it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.06it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.09it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.20it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.28it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.24it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.28it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.24it/s][A
 64%|██████▍   | 552/861 [00:12<00:07, 44.08it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.07it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.12it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.18it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.15it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.24it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.25it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.16it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.22it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.04it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.10it/s][A
 70%|███████   | 607/861 [00:13<00:05, 43.22it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.55it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 43.86it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.93it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.10it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.07it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.05it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 43.94it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 43.95it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.02it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.22it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.25it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.25it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.36it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.25it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.01it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.01it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.07it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.08it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.27it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 44.21it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.14it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.27it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.17it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.96it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 43.96it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.10it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.30it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.69it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.87it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.05it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.08it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.02it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.90it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 43.91it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 43.96it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.13it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.25it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.31it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.20it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.25it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.16it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.09it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 43.95it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.05it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.08it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.19it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.27it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.19it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.15it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.16it/s][A
                                                 [A                                                
100%|██████████| 861/861 [00:19<00:00, 44.16it/s][A 20%|██        | 83/415 [00:42<01:27,  3.79it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:33:13,076 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83
[INFO|configuration_utils.py:351] 2023-08-28 04:33:13,315 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:33:16,136 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:33:16,267 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:33:16,355 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83/special_tokens_map.json
 20%|██        | 84/415 [00:47<40:44,  7.39s/it] 20%|██        | 85/415 [00:47<28:54,  5.26s/it] 21%|██        | 86/415 [00:47<20:37,  3.76s/it] 21%|██        | 87/415 [00:48<15:03,  2.75s/it] 21%|██        | 88/415 [00:48<10:58,  2.01s/it] 21%|██▏       | 89/415 [00:48<08:06,  1.49s/it] 22%|██▏       | 90/415 [00:48<06:06,  1.13s/it] 22%|██▏       | 91/415 [00:49<04:44,  1.14it/s] 22%|██▏       | 92/415 [00:49<03:45,  1.43it/s] 22%|██▏       | 93/415 [00:49<03:04,  1.75it/s] 23%|██▎       | 94/415 [00:50<02:35,  2.07it/s] 23%|██▎       | 95/415 [00:50<02:14,  2.37it/s] 23%|██▎       | 96/415 [00:50<02:00,  2.64it/s] 23%|██▎       | 97/415 [00:50<01:50,  2.87it/s] 24%|██▎       | 98/415 [00:51<01:43,  3.06it/s] 24%|██▍       | 99/415 [00:51<01:38,  3.21it/s] 24%|██▍       | 100/415 [00:51<01:34,  3.32it/s] 24%|██▍       | 101/415 [00:51<01:32,  3.40it/s] 25%|██▍       | 102/415 [00:52<01:30,  3.46it/s] 25%|██▍       | 103/415 [00:52<01:31,  3.42it/s] 25%|██▌       | 104/415 [00:52<01:29,  3.48it/s] 25%|██▌       | 105/415 [00:53<01:28,  3.52it/s] 26%|██▌       | 106/415 [00:53<01:27,  3.54it/s] 26%|██▌       | 107/415 [00:53<01:26,  3.56it/s] 26%|██▌       | 108/415 [00:53<01:25,  3.58it/s] 26%|██▋       | 109/415 [00:54<01:25,  3.58it/s] 27%|██▋       | 110/415 [00:54<01:24,  3.59it/s] 27%|██▋       | 111/415 [00:54<01:24,  3.60it/s] 27%|██▋       | 112/415 [00:55<01:24,  3.60it/s] 27%|██▋       | 113/415 [00:55<01:23,  3.60it/s] 27%|██▋       | 114/415 [00:55<01:24,  3.55it/s] 28%|██▊       | 115/415 [00:55<01:24,  3.57it/s] 28%|██▊       | 116/415 [00:56<01:23,  3.58it/s] 28%|██▊       | 117/415 [00:56<01:23,  3.59it/s] 28%|██▊       | 118/415 [00:56<01:22,  3.60it/s] 29%|██▊       | 119/415 [00:57<01:22,  3.60it/s] 29%|██▉       | 120/415 [00:57<01:21,  3.60it/s] 29%|██▉       | 121/415 [00:57<01:21,  3.60it/s] 29%|██▉       | 122/415 [00:57<01:21,  3.60it/s] 30%|██▉       | 123/415 [00:58<01:20,  3.61it/s] 30%|██▉       | 124/415 [00:58<01:20,  3.61it/s] 30%|███       | 125/415 [00:58<01:25,  3.40it/s] 30%|███       | 126/415 [00:59<01:23,  3.46it/s] 31%|███       | 127/415 [00:59<01:22,  3.50it/s] 31%|███       | 128/415 [00:59<01:21,  3.53it/s] 31%|███       | 129/415 [00:59<01:20,  3.56it/s] 31%|███▏      | 130/415 [01:00<01:19,  3.57it/s] 32%|███▏      | 131/415 [01:00<01:19,  3.58it/s] 32%|███▏      | 132/415 [01:00<01:18,  3.59it/s] 32%|███▏      | 133/415 [01:00<01:18,  3.59it/s] 32%|███▏      | 134/415 [01:01<01:18,  3.60it/s] 33%|███▎      | 135/415 [01:01<01:17,  3.60it/s] 33%|███▎      | 136/415 [01:01<01:20,  3.47it/s] 33%|███▎      | 137/415 [01:02<01:19,  3.51it/s] 33%|███▎      | 138/415 [01:02<01:18,  3.54it/s] 33%|███▎      | 139/415 [01:02<01:17,  3.56it/s] 34%|███▎      | 140/415 [01:02<01:17,  3.57it/s] 34%|███▍      | 141/415 [01:03<01:16,  3.58it/s] 34%|███▍      | 142/415 [01:03<01:16,  3.59it/s] 34%|███▍      | 143/415 [01:03<01:15,  3.59it/s] 35%|███▍      | 144/415 [01:04<01:15,  3.60it/s] 35%|███▍      | 145/415 [01:04<01:14,  3.60it/s] 35%|███▌      | 146/415 [01:04<01:14,  3.60it/s] 35%|███▌      | 147/415 [01:04<01:17,  3.44it/s] 36%|███▌      | 148/415 [01:05<01:16,  3.49it/s] 36%|███▌      | 149/415 [01:05<01:15,  3.52it/s] 36%|███▌      | 150/415 [01:05<01:14,  3.55it/s] 36%|███▋      | 151/415 [01:06<01:14,  3.56it/s] 37%|███▋      | 152/415 [01:06<01:13,  3.58it/s] 37%|███▋      | 153/415 [01:06<01:13,  3.59it/s] 37%|███▋      | 154/415 [01:06<01:12,  3.59it/s] 37%|███▋      | 155/415 [01:07<01:12,  3.60it/s] 38%|███▊      | 156/415 [01:07<01:11,  3.60it/s] 38%|███▊      | 157/415 [01:07<01:11,  3.60it/s] 38%|███▊      | 158/415 [01:08<01:15,  3.43it/s] 38%|███▊      | 159/415 [01:08<01:13,  3.48it/s] 39%|███▊      | 160/415 [01:08<01:12,  3.51it/s] 39%|███▉      | 161/415 [01:08<01:11,  3.54it/s] 39%|███▉      | 162/415 [01:09<01:11,  3.56it/s] 39%|███▉      | 163/415 [01:09<01:10,  3.57it/s] 40%|███▉      | 164/415 [01:09<01:09,  3.59it/s] 40%|███▉      | 165/415 [01:09<01:09,  3.59it/s] 40%|████      | 166/415 [01:10<01:05,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 04:33:40,422 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:33:40,422 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 04:33:40,422 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5371, 'eval_samples_per_second': 352.355, 'eval_steps_per_second': 44.07, 'epoch': 1.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.27it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.69it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.60it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.58it/s][A
  3%|▎         | 27/861 [00:00<00:19, 43.69it/s][A
  4%|▎         | 32/861 [00:00<00:19, 43.23it/s][A
  4%|▍         | 37/861 [00:00<00:19, 43.27it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.44it/s][A
  5%|▌         | 47/861 [00:01<00:18, 43.73it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.06it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.27it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.29it/s][A
  8%|▊         | 67/861 [00:01<00:18, 44.06it/s][A
  8%|▊         | 72/861 [00:01<00:17, 43.94it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.87it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.84it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.83it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.95it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.18it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.31it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.25it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.19it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.14it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.90it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.91it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.95it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.03it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.22it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.28it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.27it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.20it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.07it/s][A
 19%|█▉        | 167/861 [00:03<00:16, 42.75it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.11it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.44it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.73it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.92it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.09it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.13it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 43.95it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 43.81it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.91it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.97it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.97it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.14it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.29it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.25it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.22it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.12it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 43.96it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.95it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.02it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.06it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.06it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.22it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.28it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.19it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.05it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 43.92it/s][A
 35%|███▌      | 302/861 [00:06<00:13, 42.95it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 43.41it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.64it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.76it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.93it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.03it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 43.93it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 43.95it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.81it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.93it/s][A
 41%|████      | 352/861 [00:07<00:11, 43.98it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.19it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.13it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.21it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.22it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.13it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.00it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 43.91it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.06it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 44.13it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.13it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.25it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.22it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.14it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.06it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 43.91it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.90it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.01it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.14it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.27it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.16it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.19it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.19it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.10it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.94it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.05it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.08it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.97it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.16it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.17it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.11it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.06it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.04it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.98it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.00it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.10it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.14it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.18it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.19it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.16it/s][A
 64%|██████▍   | 552/861 [00:12<00:07, 44.05it/s][A
 65%|██████▍   | 557/861 [00:12<00:07, 42.98it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 43.38it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 43.58it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 43.79it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 43.91it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.01it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.06it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.11it/s][A
 69%|██████▉   | 597/861 [00:13<00:06, 43.91it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.00it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.10it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.90it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 44.02it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.14it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.22it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.11it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 43.91it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 43.90it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.01it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.14it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.15it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.18it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.25it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.21it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.14it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.00it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 43.88it/s][A
 80%|████████  | 692/861 [00:15<00:04, 42.23it/s][A
 81%|████████  | 697/861 [00:15<00:03, 42.95it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 43.36it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.78it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.91it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.98it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.93it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.90it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 43.61it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 43.70it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.93it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.16it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.36it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.31it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.19it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.10it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.97it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 43.82it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 43.78it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 43.98it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.20it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.30it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.25it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.21it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.05it/s][A
 95%|█████████▍| 817/861 [00:18<00:01, 43.99it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 43.88it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 42.84it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 43.36it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 43.79it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.99it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.08it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.96it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.86it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.86it/s][A 40%|████      | 166/415 [01:29<01:05,  3.79it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:34:00,206 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 04:34:00,417 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:34:03,456 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:34:03,587 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:34:03,663 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 40%|████      | 167/415 [01:34<31:07,  7.53s/it] 40%|████      | 168/415 [01:34<22:03,  5.36s/it] 41%|████      | 169/415 [01:35<15:43,  3.83s/it] 41%|████      | 170/415 [01:35<11:18,  2.77s/it] 41%|████      | 171/415 [01:35<08:13,  2.02s/it] 41%|████▏     | 172/415 [01:36<06:04,  1.50s/it] 42%|████▏     | 173/415 [01:36<04:34,  1.13s/it] 42%|████▏     | 174/415 [01:36<03:33,  1.13it/s] 42%|████▏     | 175/415 [01:36<02:49,  1.42it/s] 42%|████▏     | 176/415 [01:37<02:18,  1.73it/s] 43%|████▎     | 177/415 [01:37<01:56,  2.05it/s] 43%|████▎     | 178/415 [01:37<01:41,  2.34it/s] 43%|████▎     | 179/415 [01:38<01:30,  2.61it/s] 43%|████▎     | 180/415 [01:38<01:22,  2.84it/s] 44%|████▎     | 181/415 [01:38<01:17,  3.02it/s] 44%|████▍     | 182/415 [01:38<01:13,  3.16it/s] 44%|████▍     | 183/415 [01:39<01:10,  3.27it/s] 44%|████▍     | 184/415 [01:39<01:09,  3.35it/s] 45%|████▍     | 185/415 [01:39<01:09,  3.30it/s] 45%|████▍     | 186/415 [01:40<01:07,  3.38it/s] 45%|████▌     | 187/415 [01:40<01:06,  3.43it/s] 45%|████▌     | 188/415 [01:40<01:05,  3.46it/s] 46%|████▌     | 189/415 [01:40<01:05,  3.47it/s] 46%|████▌     | 190/415 [01:41<01:04,  3.49it/s] 46%|████▌     | 191/415 [01:41<01:03,  3.51it/s] 46%|████▋     | 192/415 [01:41<01:03,  3.53it/s] 47%|████▋     | 193/415 [01:42<01:02,  3.54it/s] 47%|████▋     | 194/415 [01:42<01:02,  3.54it/s] 47%|████▋     | 195/415 [01:42<01:01,  3.55it/s] 47%|████▋     | 196/415 [01:42<01:02,  3.51it/s] 47%|████▋     | 197/415 [01:43<01:01,  3.53it/s] 48%|████▊     | 198/415 [01:43<01:01,  3.53it/s] 48%|████▊     | 199/415 [01:43<01:01,  3.54it/s] 48%|████▊     | 200/415 [01:44<01:00,  3.56it/s] 48%|████▊     | 201/415 [01:44<00:59,  3.57it/s] 49%|████▊     | 202/415 [01:44<00:59,  3.58it/s] 49%|████▉     | 203/415 [01:44<00:59,  3.59it/s] 49%|████▉     | 204/415 [01:45<00:58,  3.59it/s] 49%|████▉     | 205/415 [01:45<00:58,  3.60it/s] 50%|████▉     | 206/415 [01:45<00:58,  3.60it/s] 50%|████▉     | 207/415 [01:46<01:00,  3.43it/s] 50%|█████     | 208/415 [01:46<00:59,  3.46it/s] 50%|█████     | 209/415 [01:46<00:58,  3.51it/s] 51%|█████     | 210/415 [01:46<00:57,  3.54it/s] 51%|█████     | 211/415 [01:47<00:57,  3.56it/s] 51%|█████     | 212/415 [01:47<00:56,  3.57it/s] 51%|█████▏    | 213/415 [01:47<00:56,  3.58it/s] 52%|█████▏    | 214/415 [01:48<01:13,  2.75it/s] 52%|█████▏    | 215/415 [01:48<01:07,  2.95it/s] 52%|█████▏    | 216/415 [01:48<01:03,  3.12it/s] 52%|█████▏    | 217/415 [01:49<01:01,  3.20it/s] 53%|█████▎    | 218/415 [01:49<00:59,  3.31it/s] 53%|█████▎    | 219/415 [01:49<00:57,  3.39it/s] 53%|█████▎    | 220/415 [01:49<00:56,  3.45it/s] 53%|█████▎    | 221/415 [01:50<00:55,  3.50it/s] 53%|█████▎    | 222/415 [01:50<00:54,  3.53it/s] 54%|█████▎    | 223/415 [01:50<00:55,  3.44it/s] 54%|█████▍    | 224/415 [01:51<00:54,  3.49it/s] 54%|█████▍    | 225/415 [01:51<00:53,  3.52it/s] 54%|█████▍    | 226/415 [01:51<00:53,  3.55it/s] 55%|█████▍    | 227/415 [01:51<00:52,  3.56it/s] 55%|█████▍    | 228/415 [01:52<00:52,  3.58it/s] 55%|█████▌    | 229/415 [01:52<00:51,  3.58it/s] 55%|█████▌    | 230/415 [01:52<00:51,  3.59it/s] 56%|█████▌    | 231/415 [01:53<00:51,  3.59it/s] 56%|█████▌    | 232/415 [01:53<00:50,  3.60it/s] 56%|█████▌    | 233/415 [01:53<00:50,  3.60it/s] 56%|█████▋    | 234/415 [01:53<00:51,  3.50it/s] 57%|█████▋    | 235/415 [01:54<00:50,  3.53it/s] 57%|█████▋    | 236/415 [01:54<00:50,  3.55it/s] 57%|█████▋    | 237/415 [01:54<00:49,  3.57it/s] 57%|█████▋    | 238/415 [01:54<00:49,  3.58it/s] 58%|█████▊    | 239/415 [01:55<00:49,  3.59it/s] 58%|█████▊    | 240/415 [01:55<00:48,  3.59it/s] 58%|█████▊    | 241/415 [01:55<00:48,  3.59it/s] 58%|█████▊    | 242/415 [01:56<00:48,  3.59it/s] 59%|█████▊    | 243/415 [01:56<00:47,  3.60it/s] 59%|█████▉    | 244/415 [01:56<00:47,  3.60it/s] 59%|█████▉    | 245/415 [01:56<00:48,  3.54it/s] 59%|█████▉    | 246/415 [01:57<00:47,  3.56it/s] 60%|█████▉    | 247/415 [01:57<00:47,  3.57it/s] 60%|█████▉    | 248/415 [01:57<00:46,  3.58it/s] 60%|██████    | 249/415 [01:57<00:43,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 04:34:28,244 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:34:28,244 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 04:34:28,244 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5818, 'eval_samples_per_second': 351.55, 'eval_steps_per_second': 43.969, 'epoch': 2.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.84it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.57it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.89it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.84it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.05it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.48it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.17it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.00it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.02it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.19it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.28it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.41it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.40it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.23it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.03it/s][A
 10%|▉         | 82/861 [00:01<00:19, 40.76it/s][A
 10%|█         | 87/861 [00:01<00:18, 41.89it/s][A
 11%|█         | 92/861 [00:02<00:18, 42.59it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.16it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.65it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 43.84it/s][A
 13%|█▎        | 112/861 [00:02<00:17, 43.91it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 43.81it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.57it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.55it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.68it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.02it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.16it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.37it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.31it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.10it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 43.97it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 43.73it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.76it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.88it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.10it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.24it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.31it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.38it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.12it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 43.96it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.78it/s][A
 25%|██▌       | 217/861 [00:04<00:15, 40.58it/s][A
 26%|██▌       | 222/861 [00:05<00:15, 41.78it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 42.57it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 43.20it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 43.59it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 43.80it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 43.95it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 43.71it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.44it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.50it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.73it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.08it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.14it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.33it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.28it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.12it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 43.83it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 43.64it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 43.82it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.97it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.18it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.26it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.36it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.24it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.08it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.88it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.79it/s][A
 41%|████      | 352/861 [00:08<00:12, 41.56it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 42.39it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.10it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.46it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 43.87it/s][A
 44%|████▍     | 377/861 [00:08<00:11, 43.90it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 43.83it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 43.73it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 43.53it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 43.69it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.89it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.19it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.32it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.38it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.03it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 43.91it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.68it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.70it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.81it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.97it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.24it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.40it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.35it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.16it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.93it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.77it/s][A
 56%|█████▌    | 482/861 [00:11<00:08, 43.64it/s][A
 57%|█████▋    | 487/861 [00:11<00:09, 39.06it/s][A
 57%|█████▋    | 492/861 [00:11<00:09, 40.68it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 41.82it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 42.67it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 43.25it/s][A
 59%|█████▉    | 512/861 [00:11<00:08, 43.57it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.73it/s][A
 61%|██████    | 522/861 [00:11<00:07, 43.66it/s][A
 61%|██████    | 527/861 [00:12<00:07, 43.45it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.45it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 43.69it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 43.95it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.17it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.39it/s][A
 65%|██████▍   | 557/861 [00:12<00:07, 43.43it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.00it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 43.87it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 43.69it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 43.68it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.89it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.95it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.13it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.22it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.37it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.29it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.93it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 43.78it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.42it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.71it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 43.91it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.00it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.09it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.26it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.11it/s][A
 76%|███████▋  | 657/861 [00:15<00:04, 43.89it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.67it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 43.83it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.07it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.10it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.23it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.29it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.31it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.10it/s][A
 82%|████████▏ | 702/861 [00:16<00:03, 43.94it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.90it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.96it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.13it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.12it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.25it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.28it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.25it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.06it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 43.92it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.98it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 42.89it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.42it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.67it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.89it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.00it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.00it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 43.89it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 43.85it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.72it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 43.89it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.05it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.15it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.27it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.31it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.15it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 43.98it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 43.89it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.80it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.86it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.11it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.13it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 44.13it/s][A 60%|██████    | 249/415 [02:17<00:43,  3.78it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:34:48,108 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249
[INFO|configuration_utils.py:351] 2023-08-28 04:34:48,237 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:34:50,985 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:34:51,157 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:34:51,228 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249/special_tokens_map.json
 60%|██████    | 250/415 [02:22<20:44,  7.55s/it] 60%|██████    | 251/415 [02:22<14:40,  5.37s/it] 61%|██████    | 252/415 [02:23<10:26,  3.84s/it] 61%|██████    | 253/415 [02:23<07:29,  2.77s/it] 61%|██████    | 254/415 [02:23<05:26,  2.03s/it] 61%|██████▏   | 255/415 [02:23<04:00,  1.50s/it] 62%|██████▏   | 256/415 [02:24<03:00,  1.14s/it] 62%|██████▏   | 257/415 [02:24<02:19,  1.13it/s] 62%|██████▏   | 258/415 [02:24<01:50,  1.42it/s] 62%|██████▏   | 259/415 [02:25<01:29,  1.74it/s] 63%|██████▎   | 260/415 [02:25<01:15,  2.05it/s] 63%|██████▎   | 261/415 [02:25<01:05,  2.35it/s] 63%|██████▎   | 262/415 [02:25<00:58,  2.61it/s] 63%|██████▎   | 263/415 [02:26<00:53,  2.84it/s] 64%|██████▎   | 264/415 [02:26<00:50,  3.02it/s] 64%|██████▍   | 265/415 [02:26<00:47,  3.16it/s] 64%|██████▍   | 266/415 [02:27<00:45,  3.27it/s] 64%|██████▍   | 267/415 [02:27<00:44,  3.35it/s] 65%|██████▍   | 268/415 [02:27<00:44,  3.30it/s] 65%|██████▍   | 269/415 [02:27<00:43,  3.37it/s] 65%|██████▌   | 270/415 [02:28<00:42,  3.43it/s] 65%|██████▌   | 271/415 [02:28<00:41,  3.46it/s] 66%|██████▌   | 272/415 [02:28<00:40,  3.49it/s] 66%|██████▌   | 273/415 [02:29<00:40,  3.51it/s] 66%|██████▌   | 274/415 [02:29<00:40,  3.52it/s] 66%|██████▋   | 275/415 [02:29<00:39,  3.53it/s] 67%|██████▋   | 276/415 [02:29<00:39,  3.54it/s] 67%|██████▋   | 277/415 [02:30<00:38,  3.54it/s] 67%|██████▋   | 278/415 [02:30<00:38,  3.54it/s] 67%|██████▋   | 279/415 [02:30<00:39,  3.45it/s] 67%|██████▋   | 280/415 [02:31<00:38,  3.48it/s] 68%|██████▊   | 281/415 [02:31<00:38,  3.50it/s] 68%|██████▊   | 282/415 [02:31<00:37,  3.52it/s] 68%|██████▊   | 283/415 [02:31<00:37,  3.53it/s] 68%|██████▊   | 284/415 [02:32<00:37,  3.54it/s] 69%|██████▊   | 285/415 [02:32<00:36,  3.54it/s] 69%|██████▉   | 286/415 [02:32<00:36,  3.55it/s] 69%|██████▉   | 287/415 [02:33<00:36,  3.55it/s] 69%|██████▉   | 288/415 [02:33<00:35,  3.55it/s] 70%|██████▉   | 289/415 [02:33<00:35,  3.55it/s] 70%|██████▉   | 290/415 [02:33<00:36,  3.46it/s] 70%|███████   | 291/415 [02:34<00:35,  3.48it/s] 70%|███████   | 292/415 [02:34<00:35,  3.51it/s] 71%|███████   | 293/415 [02:34<00:34,  3.52it/s] 71%|███████   | 294/415 [02:35<00:34,  3.53it/s] 71%|███████   | 295/415 [02:35<00:33,  3.53it/s] 71%|███████▏  | 296/415 [02:35<00:33,  3.54it/s] 72%|███████▏  | 297/415 [02:35<00:33,  3.54it/s] 72%|███████▏  | 298/415 [02:36<00:33,  3.54it/s] 72%|███████▏  | 299/415 [02:36<00:32,  3.55it/s] 72%|███████▏  | 300/415 [02:36<00:32,  3.55it/s] 73%|███████▎  | 301/415 [02:36<00:32,  3.48it/s] 73%|███████▎  | 302/415 [02:37<00:32,  3.50it/s] 73%|███████▎  | 303/415 [02:37<00:31,  3.52it/s] 73%|███████▎  | 304/415 [02:37<00:31,  3.53it/s] 73%|███████▎  | 305/415 [02:38<00:31,  3.54it/s] 74%|███████▎  | 306/415 [02:38<00:30,  3.55it/s] 74%|███████▍  | 307/415 [02:38<00:30,  3.55it/s] 74%|███████▍  | 308/415 [02:38<00:30,  3.55it/s] 74%|███████▍  | 309/415 [02:39<00:29,  3.55it/s] 75%|███████▍  | 310/415 [02:39<00:29,  3.55it/s] 75%|███████▍  | 311/415 [02:39<00:29,  3.55it/s] 75%|███████▌  | 312/415 [02:40<00:29,  3.47it/s] 75%|███████▌  | 313/415 [02:40<00:29,  3.50it/s] 76%|███████▌  | 314/415 [02:40<00:28,  3.51it/s] 76%|███████▌  | 315/415 [02:40<00:28,  3.52it/s] 76%|███████▌  | 316/415 [02:41<00:28,  3.53it/s] 76%|███████▋  | 317/415 [02:41<00:27,  3.54it/s] 77%|███████▋  | 318/415 [02:41<00:27,  3.54it/s] 77%|███████▋  | 319/415 [02:42<00:27,  3.54it/s] 77%|███████▋  | 320/415 [02:42<00:26,  3.55it/s] 77%|███████▋  | 321/415 [02:42<00:26,  3.55it/s] 78%|███████▊  | 322/415 [02:42<00:26,  3.55it/s] 78%|███████▊  | 323/415 [02:43<00:26,  3.47it/s] 78%|███████▊  | 324/415 [02:43<00:26,  3.49it/s] 78%|███████▊  | 325/415 [02:43<00:25,  3.51it/s] 79%|███████▊  | 326/415 [02:44<00:25,  3.52it/s] 79%|███████▉  | 327/415 [02:44<00:24,  3.53it/s] 79%|███████▉  | 328/415 [02:44<00:24,  3.54it/s] 79%|███████▉  | 329/415 [02:44<00:24,  3.54it/s] 80%|███████▉  | 330/415 [02:45<00:23,  3.55it/s] 80%|███████▉  | 331/415 [02:45<00:23,  3.55it/s] 80%|████████  | 332/415 [02:45<00:22,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 04:35:15,965 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:35:15,966 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 04:35:15,966 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.662, 'eval_samples_per_second': 350.117, 'eval_steps_per_second': 43.79, 'epoch': 3.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.53it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.42it/s][A
  2%|▏         | 17/861 [00:00<00:18, 45.32it/s][A
  3%|▎         | 22/861 [00:00<00:18, 44.59it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.29it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.03it/s][A
  4%|▍         | 37/861 [00:00<00:18, 43.90it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.91it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.04it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.16it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.34it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.35it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.30it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.07it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.00it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.89it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.92it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.04it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.18it/s][A
 12%|█▏        | 102/861 [00:02<00:27, 27.16it/s][A
 12%|█▏        | 107/861 [00:02<00:24, 30.89it/s][A
 13%|█▎        | 112/861 [00:02<00:22, 34.00it/s][A
 14%|█▎        | 117/861 [00:02<00:20, 36.63it/s][A
 14%|█▍        | 122/861 [00:02<00:19, 38.75it/s][A
 15%|█▍        | 127/861 [00:03<00:18, 40.33it/s][A
 15%|█▌        | 132/861 [00:03<00:17, 41.55it/s][A
 16%|█▌        | 137/861 [00:03<00:17, 42.40it/s][A
 16%|█▋        | 142/861 [00:03<00:17, 41.46it/s][A
 17%|█▋        | 147/861 [00:03<00:17, 41.90it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 42.49it/s][A
 18%|█▊        | 157/861 [00:03<00:16, 43.09it/s][A
 19%|█▉        | 162/861 [00:03<00:16, 43.59it/s][A
 19%|█▉        | 167/861 [00:04<00:15, 43.96it/s][A
 20%|█▉        | 172/861 [00:04<00:15, 44.08it/s][A
 21%|██        | 177/861 [00:04<00:15, 44.26it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.96it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.68it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 43.56it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 43.83it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 43.91it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.02it/s][A
 25%|██▍       | 212/861 [00:05<00:14, 44.23it/s][A
 25%|██▌       | 217/861 [00:05<00:14, 44.38it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.30it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.10it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 43.85it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 43.67it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 43.87it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.04it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.11it/s][A
 30%|██▉       | 257/861 [00:06<00:13, 44.29it/s][A
 30%|███       | 262/861 [00:06<00:13, 44.39it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.06it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.81it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.74it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 43.67it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 43.95it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 43.98it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.17it/s][A
 35%|███▌      | 302/861 [00:07<00:12, 44.33it/s][A
 36%|███▌      | 307/861 [00:07<00:12, 44.39it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.31it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.01it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.89it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 43.82it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 43.81it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.00it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.14it/s][A
 40%|████      | 347/861 [00:08<00:11, 44.33it/s][A
 41%|████      | 352/861 [00:08<00:11, 44.31it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.18it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.99it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.91it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 43.76it/s][A
 44%|████▍     | 377/861 [00:08<00:11, 43.90it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.05it/s][A
 45%|████▍     | 387/861 [00:09<00:10, 44.12it/s][A
 46%|████▌     | 392/861 [00:09<00:10, 44.29it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 44.36it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.18it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.44it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.50it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 43.63it/s][A
 49%|████▉     | 422/861 [00:09<00:10, 43.81it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 43.95it/s][A
 50%|█████     | 432/861 [00:10<00:09, 44.05it/s][A
 51%|█████     | 437/861 [00:10<00:09, 44.23it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.00it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.03it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.89it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.86it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 43.95it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.01it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.03it/s][A
 55%|█████▌    | 477/861 [00:11<00:08, 44.22it/s][A
 56%|█████▌    | 482/861 [00:11<00:08, 44.21it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.18it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.14it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 43.89it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 43.96it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 43.99it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.11it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.12it/s][A
 61%|██████    | 522/861 [00:12<00:07, 44.24it/s][A
 61%|██████    | 527/861 [00:12<00:07, 44.17it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.23it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 42.47it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 42.89it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 43.21it/s][A
 64%|██████▍   | 552/861 [00:12<00:07, 43.49it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 43.77it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 43.89it/s][A
 66%|██████▌   | 567/861 [00:13<00:06, 44.08it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 43.97it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 43.79it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.89it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.98it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 43.95it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.10it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.15it/s][A
 70%|███████   | 607/861 [00:14<00:05, 44.08it/s][A
 71%|███████   | 612/861 [00:14<00:05, 44.14it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 43.99it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.04it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.09it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 43.98it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.09it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.06it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.14it/s][A
 76%|███████▌  | 652/861 [00:15<00:04, 43.89it/s][A
 76%|███████▋  | 657/861 [00:15<00:04, 44.24it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.22it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.11it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 39.32it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 40.81it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 41.91it/s][A
 80%|███████▉  | 687/861 [00:15<00:04, 42.64it/s][A
 80%|████████  | 692/861 [00:15<00:03, 43.25it/s][A
 81%|████████  | 697/861 [00:16<00:03, 43.66it/s][A
 82%|████████▏ | 702/861 [00:16<00:03, 43.78it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.75it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.43it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.47it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.68it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.91it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.10it/s][A
 86%|████████▌ | 737/861 [00:17<00:02, 44.28it/s][A
 86%|████████▌ | 742/861 [00:17<00:02, 44.42it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 44.32it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.03it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.74it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.71it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.74it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.94it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.09it/s][A
 91%|█████████ | 782/861 [00:18<00:01, 44.25it/s][A
 91%|█████████▏| 787/861 [00:18<00:01, 44.42it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.21it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.04it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 43.78it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 39.65it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 41.03it/s][A
 95%|█████████▍| 817/861 [00:18<00:01, 42.02it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 42.72it/s][A
 96%|█████████▌| 827/861 [00:19<00:00, 43.32it/s][A
 97%|█████████▋| 832/861 [00:19<00:00, 43.66it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 43.97it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.95it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.59it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.44it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.51it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.51it/s][A 80%|████████  | 332/415 [03:05<00:22,  3.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:35:36,069 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 04:35:36,491 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:35:40,707 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:35:40,919 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:35:41,021 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332/special_tokens_map.json
 80%|████████  | 333/415 [03:12<11:23,  8.34s/it] 80%|████████  | 334/415 [03:13<07:59,  5.92s/it] 81%|████████  | 335/415 [03:13<05:38,  4.23s/it] 81%|████████  | 336/415 [03:13<04:00,  3.05s/it] 81%|████████  | 337/415 [03:14<02:53,  2.23s/it] 81%|████████▏ | 338/415 [03:14<02:06,  1.64s/it] 82%|████████▏ | 339/415 [03:14<01:33,  1.24s/it] 82%|████████▏ | 340/415 [03:14<01:11,  1.05it/s] 82%|████████▏ | 341/415 [03:15<00:55,  1.33it/s] 82%|████████▏ | 342/415 [03:15<00:44,  1.64it/s] 83%|████████▎ | 343/415 [03:15<00:36,  1.96it/s] 83%|████████▎ | 344/415 [03:16<00:31,  2.26it/s] 83%|████████▎ | 345/415 [03:16<00:27,  2.54it/s] 83%|████████▎ | 346/415 [03:16<00:24,  2.78it/s] 84%|████████▎ | 347/415 [03:16<00:22,  2.98it/s] 84%|████████▍ | 348/415 [03:17<00:22,  3.04it/s] 84%|████████▍ | 349/415 [03:17<00:20,  3.18it/s] 84%|████████▍ | 350/415 [03:17<00:19,  3.28it/s] 85%|████████▍ | 351/415 [03:18<00:19,  3.36it/s] 85%|████████▍ | 352/415 [03:18<00:18,  3.42it/s] 85%|████████▌ | 353/415 [03:18<00:17,  3.46it/s] 85%|████████▌ | 354/415 [03:18<00:17,  3.49it/s] 86%|████████▌ | 355/415 [03:19<00:17,  3.51it/s] 86%|████████▌ | 356/415 [03:19<00:16,  3.52it/s] 86%|████████▌ | 357/415 [03:19<00:16,  3.53it/s] 86%|████████▋ | 358/415 [03:19<00:16,  3.54it/s] 87%|████████▋ | 359/415 [03:20<00:16,  3.47it/s] 87%|████████▋ | 360/415 [03:20<00:15,  3.49it/s] 87%|████████▋ | 361/415 [03:20<00:15,  3.52it/s] 87%|████████▋ | 362/415 [03:21<00:14,  3.55it/s] 87%|████████▋ | 363/415 [03:21<00:14,  3.57it/s] 88%|████████▊ | 364/415 [03:21<00:14,  3.58it/s] 88%|████████▊ | 365/415 [03:21<00:13,  3.59it/s] 88%|████████▊ | 366/415 [03:22<00:13,  3.60it/s] 88%|████████▊ | 367/415 [03:22<00:13,  3.60it/s] 89%|████████▊ | 368/415 [03:22<00:13,  3.60it/s] 89%|████████▉ | 369/415 [03:23<00:12,  3.60it/s] 89%|████████▉ | 370/415 [03:23<00:12,  3.60it/s] 89%|████████▉ | 371/415 [03:23<00:12,  3.60it/s] 90%|████████▉ | 372/415 [03:23<00:11,  3.61it/s] 90%|████████▉ | 373/415 [03:24<00:11,  3.61it/s] 90%|█████████ | 374/415 [03:24<00:11,  3.61it/s] 90%|█████████ | 375/415 [03:24<00:11,  3.61it/s] 91%|█████████ | 376/415 [03:25<00:10,  3.61it/s] 91%|█████████ | 377/415 [03:25<00:10,  3.61it/s] 91%|█████████ | 378/415 [03:25<00:10,  3.61it/s] 91%|█████████▏| 379/415 [03:25<00:09,  3.61it/s] 92%|█████████▏| 380/415 [03:26<00:10,  3.47it/s] 92%|█████████▏| 381/415 [03:26<00:09,  3.51it/s] 92%|█████████▏| 382/415 [03:26<00:09,  3.54it/s] 92%|█████████▏| 383/415 [03:26<00:08,  3.56it/s] 93%|█████████▎| 384/415 [03:27<00:08,  3.57it/s] 93%|█████████▎| 385/415 [03:27<00:08,  3.59it/s] 93%|█████████▎| 386/415 [03:27<00:08,  3.59it/s] 93%|█████████▎| 387/415 [03:28<00:07,  3.60it/s] 93%|█████████▎| 388/415 [03:28<00:07,  3.60it/s] 94%|█████████▎| 389/415 [03:28<00:07,  3.60it/s] 94%|█████████▍| 390/415 [03:28<00:06,  3.60it/s] 94%|█████████▍| 391/415 [03:29<00:06,  3.45it/s] 94%|█████████▍| 392/415 [03:29<00:06,  3.50it/s] 95%|█████████▍| 393/415 [03:29<00:06,  3.53it/s] 95%|█████████▍| 394/415 [03:30<00:05,  3.55it/s] 95%|█████████▌| 395/415 [03:30<00:05,  3.57it/s] 95%|█████████▌| 396/415 [03:30<00:05,  3.58it/s] 96%|█████████▌| 397/415 [03:30<00:05,  3.59it/s] 96%|█████████▌| 398/415 [03:31<00:04,  3.59it/s] 96%|█████████▌| 399/415 [03:31<00:04,  3.60it/s] 96%|█████████▋| 400/415 [03:31<00:04,  3.60it/s] 97%|█████████▋| 401/415 [03:32<00:03,  3.60it/s] 97%|█████████▋| 402/415 [03:32<00:03,  3.46it/s] 97%|█████████▋| 403/415 [03:32<00:03,  3.51it/s] 97%|█████████▋| 404/415 [03:32<00:03,  3.53it/s] 98%|█████████▊| 405/415 [03:33<00:02,  3.55it/s] 98%|█████████▊| 406/415 [03:33<00:02,  3.57it/s] 98%|█████████▊| 407/415 [03:33<00:02,  3.58it/s] 98%|█████████▊| 408/415 [03:34<00:01,  3.59it/s] 99%|█████████▊| 409/415 [03:34<00:01,  3.60it/s] 99%|█████████▉| 410/415 [03:34<00:01,  3.60it/s] 99%|█████████▉| 411/415 [03:34<00:01,  3.60it/s] 99%|█████████▉| 412/415 [03:35<00:00,  3.60it/s]100%|█████████▉| 413/415 [03:35<00:00,  3.51it/s]100%|█████████▉| 414/415 [03:35<00:00,  3.54it/s]100%|██████████| 415/415 [03:35<00:00,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 04:36:06,167 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:36:06,167 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 04:36:06,167 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.8778, 'eval_samples_per_second': 346.316, 'eval_steps_per_second': 43.315, 'epoch': 4.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.58it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.49it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.62it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.59it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.86it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.44it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.25it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.09it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.12it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.17it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.32it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.44it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.29it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.20it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.02it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.95it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.00it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.93it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.05it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.77it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.00it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.07it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.04it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.83it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.98it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.98it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.91it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.01it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.14it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.26it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.18it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.11it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.08it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.01it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.92it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.94it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.05it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.09it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.21it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.25it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.13it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.06it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.97it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.00it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.02it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.00it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 43.09it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 43.54it/s][A
 29%|██▊       | 247/861 [00:05<00:14, 43.77it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 43.71it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.84it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.88it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.95it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.92it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.94it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.08it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.28it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.17it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.12it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.11it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.05it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.08it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.97it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.10it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.11it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.29it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.30it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.16it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.12it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.10it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.06it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.94it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.98it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 43.60it/s][A
 44%|████▍     | 377/861 [00:08<00:11, 43.92it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.02it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.01it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 43.96it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.04it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.92it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.89it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.91it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.08it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.21it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.29it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.20it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.05it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.05it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.06it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.58it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.61it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 43.88it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.05it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.18it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.13it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.03it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.78it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.90it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 43.90it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.04it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.17it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.24it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.22it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.26it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.13it/s][A
 62%|██████▏   | 532/861 [00:12<00:08, 36.99it/s][A
 62%|██████▏   | 536/861 [00:12<00:08, 36.11it/s][A
 63%|██████▎   | 541/861 [00:12<00:08, 38.89it/s][A
 63%|██████▎   | 546/861 [00:12<00:07, 40.57it/s][A
 64%|██████▍   | 551/861 [00:12<00:07, 41.74it/s][A
 65%|██████▍   | 556/861 [00:12<00:07, 42.56it/s][A
 65%|██████▌   | 561/861 [00:12<00:06, 43.25it/s][A
 66%|██████▌   | 566/861 [00:12<00:06, 43.57it/s][A
 66%|██████▋   | 571/861 [00:13<00:06, 43.67it/s][A
 67%|██████▋   | 576/861 [00:13<00:06, 43.49it/s][A
 67%|██████▋   | 581/861 [00:13<00:06, 43.28it/s][A
 68%|██████▊   | 586/861 [00:13<00:06, 43.40it/s][A
 69%|██████▊   | 591/861 [00:13<00:06, 43.71it/s][A
 69%|██████▉   | 596/861 [00:13<00:06, 43.99it/s][A
 70%|██████▉   | 601/861 [00:13<00:05, 44.21it/s][A
 70%|███████   | 606/861 [00:13<00:05, 44.34it/s][A
 71%|███████   | 611/861 [00:13<00:05, 44.35it/s][A
 72%|███████▏  | 616/861 [00:14<00:05, 44.21it/s][A
 72%|███████▏  | 621/861 [00:14<00:05, 43.86it/s][A
 73%|███████▎  | 626/861 [00:14<00:05, 43.72it/s][A
 73%|███████▎  | 631/861 [00:14<00:05, 43.68it/s][A
 74%|███████▍  | 636/861 [00:14<00:05, 43.25it/s][A
 74%|███████▍  | 641/861 [00:14<00:05, 43.60it/s][A
 75%|███████▌  | 646/861 [00:14<00:04, 43.86it/s][A
 76%|███████▌  | 651/861 [00:14<00:04, 44.08it/s][A
 76%|███████▌  | 656/861 [00:14<00:04, 44.17it/s][A
 77%|███████▋  | 661/861 [00:15<00:04, 44.12it/s][A
 77%|███████▋  | 666/861 [00:15<00:04, 44.03it/s][A
 78%|███████▊  | 671/861 [00:15<00:04, 43.90it/s][A
 79%|███████▊  | 676/861 [00:15<00:04, 43.83it/s][A
 79%|███████▉  | 681/861 [00:15<00:04, 43.81it/s][A
 80%|███████▉  | 686/861 [00:15<00:03, 44.06it/s][A
 80%|████████  | 691/861 [00:15<00:03, 44.23it/s][A
 81%|████████  | 696/861 [00:15<00:03, 44.16it/s][A
 81%|████████▏ | 701/861 [00:15<00:03, 44.23it/s][A
 82%|████████▏ | 706/861 [00:16<00:03, 44.11it/s][A
 83%|████████▎ | 711/861 [00:16<00:03, 43.97it/s][A
 83%|████████▎ | 716/861 [00:16<00:03, 43.89it/s][A
 84%|████████▎ | 721/861 [00:16<00:03, 43.87it/s][A
 84%|████████▍ | 726/861 [00:16<00:03, 43.99it/s][A
 85%|████████▍ | 731/861 [00:16<00:02, 44.16it/s][A
 85%|████████▌ | 736/861 [00:16<00:02, 44.18it/s][A
 86%|████████▌ | 741/861 [00:16<00:02, 44.25it/s][A
 87%|████████▋ | 746/861 [00:17<00:02, 44.18it/s][A
 87%|████████▋ | 751/861 [00:17<00:02, 44.15it/s][A
 88%|████████▊ | 756/861 [00:17<00:02, 44.08it/s][A
 88%|████████▊ | 761/861 [00:17<00:02, 43.98it/s][A
 89%|████████▉ | 766/861 [00:17<00:02, 43.93it/s][A
 90%|████████▉ | 771/861 [00:17<00:02, 43.93it/s][A
 90%|█████████ | 776/861 [00:17<00:01, 44.16it/s][A
 91%|█████████ | 781/861 [00:17<00:01, 44.24it/s][A
 91%|█████████▏| 786/861 [00:17<00:01, 44.22it/s][A
 92%|█████████▏| 791/861 [00:18<00:01, 44.19it/s][A
 92%|█████████▏| 796/861 [00:18<00:01, 44.10it/s][A
 93%|█████████▎| 801/861 [00:18<00:01, 44.00it/s][A
 94%|█████████▎| 806/861 [00:18<00:01, 44.04it/s][A
 94%|█████████▍| 811/861 [00:18<00:01, 43.98it/s][A
 95%|█████████▍| 816/861 [00:18<00:01, 43.99it/s][A
 95%|█████████▌| 821/861 [00:18<00:00, 44.08it/s][A
 96%|█████████▌| 826/861 [00:18<00:00, 44.20it/s][A
 97%|█████████▋| 831/861 [00:18<00:00, 44.12it/s][A
 97%|█████████▋| 836/861 [00:19<00:00, 44.22it/s][A
 98%|█████████▊| 841/861 [00:19<00:00, 44.07it/s][A
 98%|█████████▊| 846/861 [00:19<00:00, 44.08it/s][A
 99%|█████████▉| 851/861 [00:19<00:00, 44.06it/s][A
 99%|█████████▉| 856/861 [00:19<00:00, 44.03it/s][A
100%|██████████| 861/861 [00:19<00:00, 44.02it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 44.02it/s][A100%|██████████| 415/415 [03:55<00:00,  3.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:36:25,991 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415
[INFO|configuration_utils.py:351] 2023-08-28 04:36:26,150 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:36:28,892 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:36:28,986 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:36:29,029 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 04:36:29,869 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 04:36:29,869 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83 (score: 1.0493918657302856).
                                                 100%|██████████| 415/415 [04:12<00:00,  3.75it/s]100%|██████████| 415/415 [04:12<00:00,  1.64it/s]
[INFO|trainer.py:1894] 2023-08-28 04:36:43,014 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 04:36:43,319 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:36:46,194 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:36:46,323 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:36:46,395 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:36:46,881 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:36:46,881 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:36:46,881 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:36:46,881 >>   train_runtime            = 0:04:12.62
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:36:46,881 >>   train_samples            =       5300
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:36:46,881 >>   train_samples_per_second =    104.899
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:36:46,881 >>   train_steps_per_second   =      1.643
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.6366, 'eval_samples_per_second': 350.569, 'eval_steps_per_second': 43.847, 'epoch': 5.0}
{'train_runtime': 252.6243, 'train_samples_per_second': 104.899, 'train_steps_per_second': 1.643, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 04:36:47 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 04:36:47,067 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:36:47,067 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 04:36:47,067 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 54.73it/s]  1%|▏         | 12/861 [00:00<00:17, 48.51it/s]  2%|▏         | 17/861 [00:00<00:17, 47.00it/s]  3%|▎         | 22/861 [00:00<00:18, 46.15it/s]  3%|▎         | 27/861 [00:00<00:18, 45.78it/s]  4%|▎         | 32/861 [00:00<00:18, 45.48it/s]  4%|▍         | 37/861 [00:00<00:18, 45.37it/s]  5%|▍         | 42/861 [00:00<00:18, 44.96it/s]  5%|▌         | 47/861 [00:01<00:18, 44.34it/s]  6%|▌         | 52/861 [00:01<00:18, 44.10it/s]  7%|▋         | 57/861 [00:01<00:18, 44.21it/s]  7%|▋         | 62/861 [00:01<00:18, 44.32it/s]  8%|▊         | 67/861 [00:01<00:17, 44.49it/s]  8%|▊         | 72/861 [00:01<00:17, 44.60it/s]  9%|▉         | 77/861 [00:01<00:17, 44.74it/s] 10%|▉         | 82/861 [00:01<00:17, 44.76it/s] 10%|█         | 87/861 [00:01<00:17, 44.43it/s] 11%|█         | 92/861 [00:02<00:17, 44.06it/s] 11%|█▏        | 97/861 [00:02<00:17, 44.00it/s] 12%|█▏        | 102/861 [00:02<00:17, 43.78it/s] 12%|█▏        | 107/861 [00:02<00:17, 44.08it/s] 13%|█▎        | 112/861 [00:02<00:16, 44.33it/s] 14%|█▎        | 117/861 [00:02<00:16, 44.47it/s] 14%|█▍        | 122/861 [00:02<00:16, 44.59it/s] 15%|█▍        | 127/861 [00:02<00:16, 44.59it/s] 15%|█▌        | 132/861 [00:02<00:16, 44.33it/s] 16%|█▌        | 137/861 [00:03<00:16, 44.19it/s] 16%|█▋        | 142/861 [00:03<00:16, 43.96it/s] 17%|█▋        | 147/861 [00:03<00:16, 44.07it/s] 18%|█▊        | 152/861 [00:03<00:16, 44.20it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.44it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.51it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.62it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.60it/s] 21%|██        | 177/861 [00:03<00:15, 44.37it/s] 21%|██        | 182/861 [00:04<00:15, 44.12it/s] 22%|██▏       | 187/861 [00:04<00:15, 44.05it/s] 22%|██▏       | 192/861 [00:04<00:15, 44.08it/s] 23%|██▎       | 197/861 [00:04<00:14, 44.28it/s] 23%|██▎       | 202/861 [00:04<00:14, 44.37it/s] 24%|██▍       | 207/861 [00:04<00:14, 44.50it/s] 25%|██▍       | 212/861 [00:04<00:14, 44.62it/s] 25%|██▌       | 217/861 [00:04<00:14, 44.53it/s] 26%|██▌       | 222/861 [00:04<00:14, 44.26it/s] 26%|██▋       | 227/861 [00:05<00:14, 44.09it/s] 27%|██▋       | 232/861 [00:05<00:14, 44.10it/s] 28%|██▊       | 237/861 [00:05<00:14, 42.64it/s] 28%|██▊       | 242/861 [00:05<00:14, 43.35it/s] 29%|██▊       | 247/861 [00:05<00:14, 43.79it/s] 29%|██▉       | 252/861 [00:05<00:13, 44.12it/s] 30%|██▉       | 257/861 [00:05<00:13, 44.25it/s] 30%|███       | 262/861 [00:05<00:13, 44.26it/s] 31%|███       | 267/861 [00:06<00:13, 44.05it/s] 32%|███▏      | 272/861 [00:06<00:13, 43.98it/s] 32%|███▏      | 277/861 [00:06<00:13, 43.82it/s] 33%|███▎      | 282/861 [00:06<00:13, 43.99it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.20it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.40it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.67it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.65it/s] 36%|███▌      | 307/861 [00:06<00:12, 44.50it/s] 36%|███▌      | 312/861 [00:07<00:12, 44.27it/s] 37%|███▋      | 317/861 [00:07<00:12, 44.04it/s] 37%|███▋      | 322/861 [00:07<00:12, 43.93it/s] 38%|███▊      | 327/861 [00:07<00:12, 44.00it/s] 39%|███▊      | 332/861 [00:07<00:11, 44.21it/s] 39%|███▉      | 337/861 [00:07<00:11, 44.49it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.59it/s] 40%|████      | 347/861 [00:07<00:11, 44.52it/s] 41%|████      | 352/861 [00:07<00:11, 44.44it/s] 41%|████▏     | 357/861 [00:08<00:11, 44.23it/s] 42%|████▏     | 362/861 [00:08<00:11, 44.11it/s] 43%|████▎     | 367/861 [00:08<00:11, 44.01it/s] 43%|████▎     | 372/861 [00:08<00:11, 44.01it/s] 44%|████▍     | 377/861 [00:08<00:10, 44.20it/s] 44%|████▍     | 382/861 [00:08<00:10, 44.42it/s] 45%|████▍     | 387/861 [00:08<00:10, 44.59it/s] 46%|████▌     | 392/861 [00:08<00:10, 44.54it/s] 46%|████▌     | 397/861 [00:08<00:10, 44.43it/s] 47%|████▋     | 402/861 [00:09<00:10, 44.37it/s] 47%|████▋     | 407/861 [00:09<00:10, 44.13it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.09it/s] 48%|████▊     | 417/861 [00:09<00:10, 44.09it/s] 49%|████▉     | 422/861 [00:09<00:09, 44.29it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.49it/s] 50%|█████     | 432/861 [00:09<00:09, 44.55it/s] 51%|█████     | 437/861 [00:09<00:09, 44.49it/s] 51%|█████▏    | 442/861 [00:09<00:09, 44.41it/s] 52%|█████▏    | 447/861 [00:10<00:09, 44.33it/s] 52%|█████▏    | 452/861 [00:10<00:09, 44.18it/s] 53%|█████▎    | 457/861 [00:10<00:09, 44.05it/s] 54%|█████▎    | 462/861 [00:10<00:09, 44.08it/s] 54%|█████▍    | 467/861 [00:10<00:08, 44.31it/s] 55%|█████▍    | 472/861 [00:10<00:08, 44.47it/s] 55%|█████▌    | 477/861 [00:10<00:08, 44.53it/s] 56%|█████▌    | 482/861 [00:10<00:08, 44.36it/s] 57%|█████▋    | 487/861 [00:10<00:08, 44.33it/s] 57%|█████▋    | 492/861 [00:11<00:08, 44.21it/s] 58%|█████▊    | 497/861 [00:11<00:08, 44.15it/s] 58%|█████▊    | 502/861 [00:11<00:08, 43.97it/s] 59%|█████▉    | 507/861 [00:11<00:08, 44.16it/s] 59%|█████▉    | 512/861 [00:11<00:07, 44.43it/s] 60%|██████    | 517/861 [00:11<00:07, 44.54it/s] 61%|██████    | 522/861 [00:11<00:07, 44.55it/s] 61%|██████    | 527/861 [00:11<00:07, 44.43it/s] 62%|██████▏   | 532/861 [00:11<00:07, 44.28it/s] 62%|██████▏   | 537/861 [00:12<00:07, 44.11it/s] 63%|██████▎   | 542/861 [00:12<00:07, 44.13it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.10it/s] 64%|██████▍   | 552/861 [00:12<00:06, 44.16it/s] 65%|██████▍   | 557/861 [00:12<00:06, 44.41it/s] 65%|██████▌   | 562/861 [00:12<00:06, 44.51it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.60it/s] 66%|██████▋   | 572/861 [00:12<00:06, 44.43it/s] 67%|██████▋   | 577/861 [00:13<00:06, 44.29it/s] 68%|██████▊   | 582/861 [00:13<00:06, 44.20it/s] 68%|██████▊   | 587/861 [00:13<00:06, 44.11it/s] 69%|██████▉   | 592/861 [00:13<00:06, 44.06it/s] 69%|██████▉   | 597/861 [00:13<00:06, 41.32it/s] 70%|██████▉   | 602/861 [00:13<00:06, 42.38it/s] 70%|███████   | 607/861 [00:13<00:05, 43.15it/s] 71%|███████   | 612/861 [00:13<00:05, 43.65it/s] 72%|███████▏  | 617/861 [00:13<00:05, 43.90it/s] 72%|███████▏  | 622/861 [00:14<00:05, 44.03it/s] 73%|███████▎  | 627/861 [00:14<00:05, 44.03it/s] 73%|███████▎  | 632/861 [00:14<00:05, 43.95it/s] 74%|███████▍  | 637/861 [00:14<00:05, 43.52it/s] 75%|███████▍  | 642/861 [00:14<00:05, 43.76it/s] 75%|███████▌  | 647/861 [00:14<00:04, 44.06it/s] 76%|███████▌  | 652/861 [00:14<00:04, 44.32it/s] 76%|███████▋  | 657/861 [00:14<00:04, 44.46it/s] 77%|███████▋  | 662/861 [00:14<00:04, 44.55it/s] 77%|███████▋  | 667/861 [00:15<00:04, 44.52it/s] 78%|███████▊  | 672/861 [00:15<00:04, 44.40it/s] 79%|███████▊  | 677/861 [00:15<00:04, 44.14it/s] 79%|███████▉  | 682/861 [00:15<00:04, 43.84it/s] 80%|███████▉  | 687/861 [00:15<00:03, 43.95it/s] 80%|████████  | 692/861 [00:15<00:03, 44.10it/s] 81%|████████  | 697/861 [00:15<00:03, 44.27it/s] 82%|████████▏ | 702/861 [00:15<00:03, 44.48it/s] 82%|████████▏ | 707/861 [00:15<00:03, 44.60it/s] 83%|████████▎ | 712/861 [00:16<00:03, 44.49it/s] 83%|████████▎ | 717/861 [00:16<00:03, 44.35it/s] 84%|████████▍ | 722/861 [00:16<00:03, 44.12it/s] 84%|████████▍ | 727/861 [00:16<00:03, 43.97it/s] 85%|████████▌ | 732/861 [00:16<00:03, 42.66it/s] 86%|████████▌ | 737/861 [00:16<00:02, 43.26it/s] 86%|████████▌ | 742/861 [00:16<00:02, 43.70it/s] 87%|████████▋ | 747/861 [00:16<00:02, 44.13it/s] 87%|████████▋ | 752/861 [00:16<00:02, 44.30it/s] 88%|████████▊ | 757/861 [00:17<00:02, 44.30it/s] 89%|████████▊ | 762/861 [00:17<00:02, 44.23it/s] 89%|████████▉ | 767/861 [00:17<00:02, 44.04it/s] 90%|████████▉ | 772/861 [00:17<00:02, 43.79it/s] 90%|█████████ | 777/861 [00:17<00:01, 43.82it/s] 91%|█████████ | 782/861 [00:17<00:01, 44.18it/s] 91%|█████████▏| 787/861 [00:17<00:01, 44.31it/s] 92%|█████████▏| 792/861 [00:17<00:01, 44.50it/s] 93%|█████████▎| 797/861 [00:18<00:01, 44.54it/s] 93%|█████████▎| 802/861 [00:18<00:01, 44.48it/s] 94%|█████████▎| 807/861 [00:18<00:01, 44.29it/s] 94%|█████████▍| 812/861 [00:18<00:01, 44.12it/s] 95%|█████████▍| 817/861 [00:18<00:01, 43.90it/s] 95%|█████████▌| 822/861 [00:18<00:00, 43.90it/s] 96%|█████████▌| 827/861 [00:18<00:00, 44.13it/s] 97%|█████████▋| 832/861 [00:18<00:00, 44.33it/s] 97%|█████████▋| 837/861 [00:18<00:00, 44.48it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.55it/s] 98%|█████████▊| 847/861 [00:19<00:00, 44.49it/s] 99%|█████████▉| 852/861 [00:19<00:00, 44.30it/s]100%|█████████▉| 857/861 [00:19<00:00, 44.10it/s]100%|██████████| 861/861 [00:19<00:00, 44.24it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:37:06,568 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:37:06,569 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:37:06,569 >>   eval_loss               =     1.0494
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:37:06,569 >>   eval_runtime            = 0:00:19.47
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:37:06,569 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:37:06,569 >>   eval_samples_per_second =    353.438
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:37:06,569 >>   eval_steps_per_second   =     44.205
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:37:06,569 >>   perplexity              =     2.8559
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:17,248 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:17,272 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:17,273 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:17,273 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:17,273 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:37:17,989 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:37:17,990 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:37:18,606 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:37:19,695 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:37:19,695 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:22,857 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:22,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:22,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:22,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:37:22,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:37:23,623 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:37:23,624 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:37:24,235 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:37:24,454 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:37:24,455 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.48it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.70it/s]Extractor Predicting: 15it [00:09,  1.72it/s]Extractor Predicting: 16it [00:10,  1.72it/s]Extractor Predicting: 17it [00:10,  1.71it/s]Extractor Predicting: 18it [00:11,  1.68it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:13,  1.63it/s]Extractor Predicting: 22it [00:13,  1.68it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:14,  1.69it/s]Extractor Predicting: 25it [00:15,  1.70it/s]Extractor Predicting: 26it [00:16,  1.70it/s]Extractor Predicting: 27it [00:16,  1.64it/s]Extractor Predicting: 28it [00:17,  1.67it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:20,  1.69it/s]Extractor Predicting: 34it [00:20,  1.68it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:22,  1.65it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.67it/s]Extractor Predicting: 39it [00:23,  1.69it/s]Extractor Predicting: 40it [00:24,  1.68it/s]Extractor Predicting: 41it [00:25,  1.69it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.70it/s]Extractor Predicting: 44it [00:26,  1.69it/s]Extractor Predicting: 45it [00:27,  1.71it/s]Extractor Predicting: 46it [00:27,  1.73it/s]Extractor Predicting: 47it [00:28,  1.64it/s]Extractor Predicting: 48it [00:29,  1.66it/s]Extractor Predicting: 49it [00:29,  1.66it/s]Extractor Predicting: 50it [00:30,  1.66it/s]Extractor Predicting: 51it [00:31,  1.64it/s]Extractor Predicting: 52it [00:31,  1.62it/s]Extractor Predicting: 53it [00:32,  1.60it/s]Extractor Predicting: 54it [00:32,  1.62it/s]Extractor Predicting: 55it [00:33,  1.61it/s]Extractor Predicting: 56it [00:34,  1.64it/s]Extractor Predicting: 57it [00:34,  1.67it/s]Extractor Predicting: 58it [00:35,  1.68it/s]Extractor Predicting: 59it [00:35,  1.63it/s]Extractor Predicting: 60it [00:36,  1.62it/s]Extractor Predicting: 61it [00:37,  1.64it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:38,  1.62it/s]Extractor Predicting: 64it [00:39,  1.62it/s]Extractor Predicting: 65it [00:39,  1.64it/s]Extractor Predicting: 66it [00:40,  1.65it/s]Extractor Predicting: 67it [00:40,  1.69it/s]Extractor Predicting: 68it [00:41,  1.69it/s]Extractor Predicting: 69it [00:42,  1.64it/s]Extractor Predicting: 70it [00:42,  1.62it/s]Extractor Predicting: 71it [00:43,  1.62it/s]Extractor Predicting: 72it [00:43,  1.62it/s]Extractor Predicting: 73it [00:44,  1.63it/s]Extractor Predicting: 74it [00:45,  1.66it/s]Extractor Predicting: 75it [00:45,  1.69it/s]Extractor Predicting: 76it [00:46,  1.66it/s]Extractor Predicting: 77it [00:46,  1.64it/s]Extractor Predicting: 78it [00:47,  1.64it/s]Extractor Predicting: 79it [00:48,  1.69it/s]Extractor Predicting: 80it [00:48,  1.72it/s]Extractor Predicting: 81it [00:49,  1.75it/s]Extractor Predicting: 82it [00:49,  1.82it/s]Extractor Predicting: 83it [00:50,  1.77it/s]Extractor Predicting: 84it [00:50,  1.78it/s]Extractor Predicting: 85it [00:51,  1.79it/s]Extractor Predicting: 86it [00:51,  1.81it/s]Extractor Predicting: 87it [00:52,  1.74it/s]Extractor Predicting: 88it [00:53,  1.76it/s]Extractor Predicting: 89it [00:53,  1.75it/s]Extractor Predicting: 90it [00:54,  1.80it/s]Extractor Predicting: 91it [00:54,  1.80it/s]Extractor Predicting: 92it [00:55,  1.83it/s]Extractor Predicting: 93it [00:55,  1.80it/s]Extractor Predicting: 94it [00:56,  1.81it/s]Extractor Predicting: 95it [00:56,  1.86it/s]Extractor Predicting: 96it [00:57,  1.79it/s]Extractor Predicting: 97it [00:58,  1.85it/s]Extractor Predicting: 98it [00:58,  1.84it/s]Extractor Predicting: 99it [00:59,  1.85it/s]Extractor Predicting: 100it [00:59,  1.84it/s]Extractor Predicting: 101it [01:00,  1.80it/s]Extractor Predicting: 102it [01:00,  1.79it/s]Extractor Predicting: 103it [01:01,  1.60it/s]Extractor Predicting: 104it [01:02,  1.64it/s]Extractor Predicting: 105it [01:02,  1.66it/s]Extractor Predicting: 106it [01:03,  1.71it/s]Extractor Predicting: 107it [01:03,  1.71it/s]Extractor Predicting: 108it [01:04,  1.78it/s]Extractor Predicting: 109it [01:04,  1.78it/s]Extractor Predicting: 110it [01:05,  1.80it/s]Extractor Predicting: 111it [01:06,  1.82it/s]Extractor Predicting: 112it [01:06,  1.77it/s]Extractor Predicting: 113it [01:07,  1.77it/s]Extractor Predicting: 114it [01:07,  1.71it/s]Extractor Predicting: 115it [01:08,  1.77it/s]Extractor Predicting: 116it [01:08,  1.75it/s]Extractor Predicting: 117it [01:09,  1.74it/s]Extractor Predicting: 118it [01:10,  1.75it/s]Extractor Predicting: 119it [01:10,  1.78it/s]Extractor Predicting: 120it [01:11,  1.82it/s]Extractor Predicting: 121it [01:11,  1.80it/s]Extractor Predicting: 122it [01:12,  1.74it/s]Extractor Predicting: 123it [01:12,  1.76it/s]Extractor Predicting: 124it [01:13,  1.74it/s]Extractor Predicting: 125it [01:13,  1.79it/s]Extractor Predicting: 126it [01:14,  1.81it/s]Extractor Predicting: 127it [01:15,  1.84it/s]Extractor Predicting: 128it [01:15,  1.75it/s]Extractor Predicting: 129it [01:16,  1.80it/s]Extractor Predicting: 130it [01:16,  1.82it/s]Extractor Predicting: 131it [01:17,  1.83it/s]Extractor Predicting: 132it [01:17,  1.87it/s]Extractor Predicting: 133it [01:18,  1.85it/s]Extractor Predicting: 134it [01:18,  1.85it/s]Extractor Predicting: 135it [01:19,  1.85it/s]Extractor Predicting: 136it [01:19,  1.84it/s]Extractor Predicting: 137it [01:20,  1.81it/s]Extractor Predicting: 138it [01:21,  1.81it/s]Extractor Predicting: 139it [01:21,  1.77it/s]Extractor Predicting: 140it [01:22,  1.76it/s]Extractor Predicting: 141it [01:22,  1.76it/s]Extractor Predicting: 142it [01:23,  1.76it/s]Extractor Predicting: 143it [01:23,  1.82it/s]Extractor Predicting: 144it [01:24,  1.77it/s]Extractor Predicting: 145it [01:25,  1.79it/s]Extractor Predicting: 146it [01:25,  1.80it/s]Extractor Predicting: 147it [01:26,  1.81it/s]Extractor Predicting: 148it [01:26,  1.80it/s]Extractor Predicting: 149it [01:27,  1.86it/s]Extractor Predicting: 150it [01:27,  1.76it/s]Extractor Predicting: 151it [01:28,  1.69it/s]Extractor Predicting: 152it [01:29,  1.64it/s]Extractor Predicting: 153it [01:29,  1.61it/s]Extractor Predicting: 154it [01:30,  1.51it/s]Extractor Predicting: 155it [01:31,  1.56it/s]Extractor Predicting: 156it [01:31,  1.52it/s]Extractor Predicting: 157it [01:32,  1.55it/s]Extractor Predicting: 158it [01:33,  1.53it/s]Extractor Predicting: 159it [01:33,  1.52it/s]Extractor Predicting: 160it [01:34,  1.50it/s]Extractor Predicting: 161it [01:35,  1.54it/s]Extractor Predicting: 162it [01:35,  1.50it/s]Extractor Predicting: 163it [01:36,  1.50it/s]Extractor Predicting: 164it [01:37,  1.49it/s]Extractor Predicting: 165it [01:37,  1.46it/s]Extractor Predicting: 166it [01:38,  1.45it/s]Extractor Predicting: 167it [01:39,  1.46it/s]Extractor Predicting: 168it [01:39,  1.47it/s]Extractor Predicting: 169it [01:40,  1.46it/s]Extractor Predicting: 170it [01:41,  1.47it/s]Extractor Predicting: 171it [01:41,  1.48it/s]Extractor Predicting: 172it [01:42,  1.46it/s]Extractor Predicting: 173it [01:43,  1.45it/s]Extractor Predicting: 174it [01:43,  1.50it/s]Extractor Predicting: 175it [01:44,  1.50it/s]Extractor Predicting: 176it [01:45,  1.48it/s]Extractor Predicting: 177it [01:45,  1.51it/s]Extractor Predicting: 178it [01:46,  1.50it/s]Extractor Predicting: 179it [01:47,  1.50it/s]Extractor Predicting: 180it [01:47,  1.48it/s]Extractor Predicting: 181it [01:48,  1.47it/s]Extractor Predicting: 182it [01:49,  1.48it/s]Extractor Predicting: 183it [01:49,  1.51it/s]Extractor Predicting: 184it [01:50,  1.53it/s]Extractor Predicting: 185it [01:51,  1.55it/s]Extractor Predicting: 186it [01:51,  1.55it/s]Extractor Predicting: 187it [01:52,  1.52it/s]Extractor Predicting: 188it [01:53,  1.56it/s]Extractor Predicting: 189it [01:53,  1.49it/s]Extractor Predicting: 190it [01:54,  1.47it/s]Extractor Predicting: 191it [01:55,  1.45it/s]Extractor Predicting: 192it [01:56,  1.42it/s]Extractor Predicting: 193it [01:56,  1.46it/s]Extractor Predicting: 194it [01:57,  1.49it/s]Extractor Predicting: 195it [01:58,  1.36it/s]Extractor Predicting: 196it [01:58,  1.40it/s]Extractor Predicting: 197it [01:59,  1.45it/s]Extractor Predicting: 198it [02:00,  1.49it/s]Extractor Predicting: 199it [02:00,  1.49it/s]Extractor Predicting: 200it [02:01,  1.52it/s]Extractor Predicting: 201it [02:02,  1.48it/s]Extractor Predicting: 202it [02:02,  1.50it/s]Extractor Predicting: 203it [02:03,  1.53it/s]Extractor Predicting: 204it [02:04,  1.52it/s]Extractor Predicting: 205it [02:04,  1.59it/s]Extractor Predicting: 206it [02:05,  1.56it/s]Extractor Predicting: 207it [02:05,  1.58it/s]Extractor Predicting: 208it [02:06,  1.58it/s]Extractor Predicting: 209it [02:07,  1.64it/s]Extractor Predicting: 210it [02:07,  1.62it/s]Extractor Predicting: 211it [02:08,  1.55it/s]Extractor Predicting: 212it [02:09,  1.53it/s]Extractor Predicting: 213it [02:09,  1.57it/s]Extractor Predicting: 214it [02:10,  1.59it/s]Extractor Predicting: 215it [02:10,  1.63it/s]Extractor Predicting: 216it [02:11,  1.63it/s]Extractor Predicting: 217it [02:12,  1.62it/s]Extractor Predicting: 218it [02:12,  1.66it/s]Extractor Predicting: 219it [02:13,  1.61it/s]Extractor Predicting: 220it [02:14,  1.55it/s]Extractor Predicting: 221it [02:14,  1.59it/s]Extractor Predicting: 222it [02:15,  1.60it/s]Extractor Predicting: 223it [02:15,  1.64it/s]Extractor Predicting: 224it [02:16,  1.63it/s]Extractor Predicting: 225it [02:17,  1.65it/s]Extractor Predicting: 226it [02:17,  1.63it/s]Extractor Predicting: 227it [02:18,  1.63it/s]Extractor Predicting: 228it [02:18,  1.62it/s]Extractor Predicting: 229it [02:19,  1.59it/s]Extractor Predicting: 230it [02:20,  1.60it/s]Extractor Predicting: 231it [02:20,  1.65it/s]Extractor Predicting: 232it [02:21,  1.66it/s]Extractor Predicting: 233it [02:22,  1.64it/s]Extractor Predicting: 234it [02:22,  1.63it/s]Extractor Predicting: 235it [02:23,  1.61it/s]Extractor Predicting: 236it [02:23,  1.62it/s]Extractor Predicting: 237it [02:24,  1.55it/s]Extractor Predicting: 237it [02:24,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:39:59,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:39:59,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:39:59,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:39:59,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:39:59,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:40:00,892 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:40:00,893 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:40:01,501 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:40:02,796 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:40:02,796 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:05,862 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:05,917 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:05,917 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:05,917 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:05,917 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:40:06,740 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:40:06,741 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:40:07,350 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:40:07,583 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:40:07,583 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.72it/s]Extractor Predicting: 6it [00:03,  1.78it/s]Extractor Predicting: 7it [00:04,  1.85it/s]Extractor Predicting: 8it [00:04,  1.86it/s]Extractor Predicting: 9it [00:05,  1.83it/s]Extractor Predicting: 10it [00:05,  1.83it/s]Extractor Predicting: 11it [00:06,  1.82it/s]Extractor Predicting: 12it [00:06,  1.79it/s]Extractor Predicting: 13it [00:07,  1.79it/s]Extractor Predicting: 14it [00:07,  1.77it/s]Extractor Predicting: 15it [00:08,  1.76it/s]Extractor Predicting: 16it [00:09,  1.75it/s]Extractor Predicting: 17it [00:09,  1.76it/s]Extractor Predicting: 18it [00:10,  1.78it/s]Extractor Predicting: 19it [00:10,  1.79it/s]Extractor Predicting: 20it [00:11,  1.80it/s]Extractor Predicting: 21it [00:11,  1.81it/s]Extractor Predicting: 22it [00:12,  1.83it/s]Extractor Predicting: 23it [00:12,  1.87it/s]Extractor Predicting: 24it [00:13,  1.90it/s]Extractor Predicting: 25it [00:13,  1.88it/s]Extractor Predicting: 26it [00:14,  1.90it/s]Extractor Predicting: 27it [00:15,  1.87it/s]Extractor Predicting: 28it [00:15,  1.86it/s]Extractor Predicting: 29it [00:16,  1.84it/s]Extractor Predicting: 30it [00:16,  1.83it/s]Extractor Predicting: 31it [00:17,  1.80it/s]Extractor Predicting: 32it [00:17,  1.83it/s]Extractor Predicting: 33it [00:18,  1.79it/s]Extractor Predicting: 34it [00:18,  1.77it/s]Extractor Predicting: 35it [00:19,  1.78it/s]Extractor Predicting: 36it [00:20,  1.80it/s]Extractor Predicting: 37it [00:20,  1.86it/s]Extractor Predicting: 38it [00:21,  1.91it/s]Extractor Predicting: 39it [00:21,  1.84it/s]Extractor Predicting: 40it [00:22,  1.76it/s]Extractor Predicting: 41it [00:22,  1.70it/s]Extractor Predicting: 42it [00:23,  1.70it/s]Extractor Predicting: 43it [00:24,  1.72it/s]Extractor Predicting: 44it [00:24,  1.70it/s]Extractor Predicting: 45it [00:25,  1.71it/s]Extractor Predicting: 46it [00:25,  1.66it/s]Extractor Predicting: 47it [00:26,  1.66it/s]Extractor Predicting: 48it [00:27,  1.57it/s]Extractor Predicting: 49it [00:27,  1.56it/s]Extractor Predicting: 50it [00:28,  1.59it/s]Extractor Predicting: 51it [00:29,  1.63it/s]Extractor Predicting: 52it [00:29,  1.64it/s]Extractor Predicting: 53it [00:30,  1.67it/s]Extractor Predicting: 54it [00:30,  1.67it/s]Extractor Predicting: 55it [00:31,  1.68it/s]Extractor Predicting: 56it [00:31,  1.72it/s]Extractor Predicting: 57it [00:32,  1.64it/s]Extractor Predicting: 58it [00:33,  1.67it/s]Extractor Predicting: 59it [00:33,  1.66it/s]Extractor Predicting: 60it [00:34,  1.68it/s]Extractor Predicting: 61it [00:34,  1.66it/s]Extractor Predicting: 62it [00:35,  1.65it/s]Extractor Predicting: 63it [00:36,  1.68it/s]Extractor Predicting: 64it [00:36,  1.72it/s]Extractor Predicting: 65it [00:37,  1.73it/s]Extractor Predicting: 66it [00:37,  1.73it/s]Extractor Predicting: 67it [00:38,  1.74it/s]Extractor Predicting: 68it [00:39,  1.69it/s]Extractor Predicting: 69it [00:39,  1.71it/s]Extractor Predicting: 70it [00:40,  1.70it/s]Extractor Predicting: 71it [00:40,  1.71it/s]Extractor Predicting: 72it [00:41,  1.71it/s]Extractor Predicting: 73it [00:42,  1.68it/s]Extractor Predicting: 74it [00:42,  1.72it/s]Extractor Predicting: 75it [00:43,  1.72it/s]Extractor Predicting: 76it [00:43,  1.69it/s]Extractor Predicting: 77it [00:44,  1.70it/s]Extractor Predicting: 78it [00:44,  1.71it/s]Extractor Predicting: 79it [00:45,  1.73it/s]Extractor Predicting: 80it [00:46,  1.73it/s]Extractor Predicting: 81it [00:46,  1.71it/s]Extractor Predicting: 82it [00:47,  1.66it/s]Extractor Predicting: 83it [00:47,  1.72it/s]Extractor Predicting: 84it [00:48,  1.70it/s]Extractor Predicting: 85it [00:49,  1.68it/s]Extractor Predicting: 86it [00:49,  1.69it/s]Extractor Predicting: 87it [00:50,  1.73it/s]Extractor Predicting: 88it [00:50,  1.69it/s]Extractor Predicting: 89it [00:51,  1.69it/s]Extractor Predicting: 90it [00:51,  1.70it/s]Extractor Predicting: 91it [00:52,  1.69it/s]Extractor Predicting: 92it [00:53,  1.70it/s]Extractor Predicting: 93it [00:53,  1.76it/s]Extractor Predicting: 94it [00:54,  1.68it/s]Extractor Predicting: 95it [00:54,  1.68it/s]Extractor Predicting: 96it [00:55,  1.73it/s]Extractor Predicting: 97it [00:56,  1.72it/s]Extractor Predicting: 98it [00:56,  1.70it/s]Extractor Predicting: 99it [00:57,  1.73it/s]Extractor Predicting: 100it [00:57,  1.63it/s]Extractor Predicting: 101it [00:58,  1.70it/s]Extractor Predicting: 102it [00:59,  1.68it/s]Extractor Predicting: 103it [00:59,  1.71it/s]Extractor Predicting: 104it [01:00,  1.75it/s]Extractor Predicting: 105it [01:00,  1.73it/s]Extractor Predicting: 106it [01:01,  1.78it/s]Extractor Predicting: 107it [01:01,  1.80it/s]Extractor Predicting: 108it [01:02,  1.83it/s]Extractor Predicting: 109it [01:02,  1.77it/s]Extractor Predicting: 110it [01:03,  1.74it/s]Extractor Predicting: 111it [01:04,  1.72it/s]Extractor Predicting: 112it [01:04,  1.72it/s]Extractor Predicting: 113it [01:05,  1.78it/s]Extractor Predicting: 114it [01:05,  1.73it/s]Extractor Predicting: 115it [01:06,  1.71it/s]Extractor Predicting: 116it [01:06,  1.75it/s]Extractor Predicting: 117it [01:07,  1.75it/s]Extractor Predicting: 118it [01:08,  1.78it/s]Extractor Predicting: 119it [01:08,  1.74it/s]Extractor Predicting: 120it [01:09,  1.74it/s]Extractor Predicting: 121it [01:09,  1.67it/s]Extractor Predicting: 122it [01:10,  1.65it/s]Extractor Predicting: 123it [01:11,  1.68it/s]Extractor Predicting: 124it [01:11,  1.71it/s]Extractor Predicting: 125it [01:12,  1.75it/s]Extractor Predicting: 126it [01:12,  1.76it/s]Extractor Predicting: 127it [01:13,  1.71it/s]Extractor Predicting: 128it [01:13,  1.77it/s]Extractor Predicting: 129it [01:14,  1.76it/s]Extractor Predicting: 130it [01:15,  1.78it/s]Extractor Predicting: 131it [01:15,  1.77it/s]Extractor Predicting: 132it [01:16,  1.73it/s]Extractor Predicting: 133it [01:16,  1.72it/s]Extractor Predicting: 134it [01:17,  1.75it/s]Extractor Predicting: 135it [01:17,  1.76it/s]Extractor Predicting: 136it [01:18,  1.74it/s]Extractor Predicting: 137it [01:19,  1.72it/s]Extractor Predicting: 138it [01:19,  1.71it/s]Extractor Predicting: 139it [01:20,  1.69it/s]Extractor Predicting: 140it [01:20,  1.76it/s]Extractor Predicting: 141it [01:21,  1.75it/s]Extractor Predicting: 142it [01:22,  1.71it/s]Extractor Predicting: 143it [01:22,  1.69it/s]Extractor Predicting: 144it [01:23,  1.68it/s]Extractor Predicting: 145it [01:23,  1.69it/s]Extractor Predicting: 146it [01:24,  1.68it/s]Extractor Predicting: 147it [01:24,  1.70it/s]Extractor Predicting: 148it [01:25,  1.64it/s]Extractor Predicting: 149it [01:26,  1.63it/s]Extractor Predicting: 150it [01:26,  1.66it/s]Extractor Predicting: 151it [01:27,  1.68it/s]Extractor Predicting: 152it [01:28,  1.68it/s]Extractor Predicting: 153it [01:28,  1.68it/s]Extractor Predicting: 154it [01:29,  1.60it/s]Extractor Predicting: 155it [01:29,  1.63it/s]Extractor Predicting: 156it [01:30,  1.66it/s]Extractor Predicting: 157it [01:31,  1.48it/s]Extractor Predicting: 158it [01:31,  1.51it/s]Extractor Predicting: 159it [01:32,  1.49it/s]Extractor Predicting: 160it [01:33,  1.56it/s]Extractor Predicting: 161it [01:33,  1.60it/s]Extractor Predicting: 162it [01:34,  1.59it/s]Extractor Predicting: 163it [01:35,  1.57it/s]Extractor Predicting: 164it [01:35,  1.55it/s]Extractor Predicting: 165it [01:36,  1.60it/s]Extractor Predicting: 166it [01:36,  1.59it/s]Extractor Predicting: 167it [01:37,  1.62it/s]Extractor Predicting: 168it [01:38,  1.75it/s]Extractor Predicting: 168it [01:38,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:41:54,738 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:41:54,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:41:54,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:41:54,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:41:54,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:41:55,747 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:41:55,748 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:41:56,060 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:41:57,159 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:41:57,159 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:00,347 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:00,366 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:00,366 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:00,366 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:00,366 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:42:01,177 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:42:01,178 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:42:01,825 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:42:02,037 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:42:02,037 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.72it/s]Extractor Predicting: 8it [00:04,  1.74it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.68it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:08,  1.65it/s]Extractor Predicting: 16it [00:09,  1.61it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:10,  1.60it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 19it [00:11,  1.67it/s]
[INFO|configuration_utils.py:515] 2023-08-28 04:42:15,263 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:42:15,264 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:42:15,293 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:42:15,294 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 04:42:15,313 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:42:24,931 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 04:42:24,952 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 04:42:25,052 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:42:25,053 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:42:25,112 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:42:25,142 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:42:25,142 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:42:25,142 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:42:25,142 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:42:25,142 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:42:25,142 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 04:42:25,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:26,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:26,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:27,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:28,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:28,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:29,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:30,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:30,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:31,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:32,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:33,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:33,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:34,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:35,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:35,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:36,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:37,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:37,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:38,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:39,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:11, 14.60s/it][WARNING|generation_utils.py:914] 2023-08-28 04:42:40,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:40,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:41,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:42,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:42,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:43,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:44,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:44,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:45,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:46,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:46,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:47,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:48,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:48,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:49,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:50,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:50,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:51,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:52,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:52,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:53,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:54,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:55,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:55,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:56,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:57,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:57,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:58,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:18, 17.28s/it][WARNING|generation_utils.py:914] 2023-08-28 04:42:59,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:42:59,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:00,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:01,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:02,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:03,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:03,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:04,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:05,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:05,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:06,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:06,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:07,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:08,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:08,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:09,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:10,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:10,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:11,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:12,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:12,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:13,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:14,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:49<01:56, 16.63s/it][WARNING|generation_utils.py:914] 2023-08-28 04:43:15,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:15,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:16,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:17,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:17,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:18,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:19,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:19,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:20,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:20,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:21,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:22,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:22,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:23,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:24,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:24,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:25,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:25,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:26,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:27,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:27,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:28,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:29,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:04<01:34, 15.82s/it][WARNING|generation_utils.py:914] 2023-08-28 04:43:29,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:30,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:31,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:31,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:32,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:33,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:33,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:34,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:35,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:35,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:36,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:36,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:37,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:38,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:38,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:39,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:40,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:40,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:41,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:42,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:42,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:43,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:44,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:19<01:17, 15.56s/it][WARNING|generation_utils.py:914] 2023-08-28 04:43:44,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:45,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:45,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:46,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:47,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:47,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:48,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:49,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:49,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:50,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:51,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:51,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:52,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:52,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:53,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:54,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:54,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:55,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:56,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:56,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:57,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:58,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:58,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:33<01:00, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 04:43:59,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:43:59,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:00,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:01,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:01,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:02,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:03,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:03,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:04,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:05,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:05,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:06,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:07,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:07,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:08,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:09,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:10,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:10,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:11,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:11,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:12,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:13,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:13,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:14,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:15,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:50<00:46, 15.64s/it][WARNING|generation_utils.py:914] 2023-08-28 04:44:15,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:16,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:17,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:17,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:18,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:19,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:19,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:20,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:21,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:21,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:22,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:23,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:23,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:24,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:25,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:25,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:26,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:27,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:27,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:28,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:29,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:29,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:05<00:30, 15.35s/it][WARNING|generation_utils.py:914] 2023-08-28 04:44:30,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:31,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:31,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:32,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:32,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:33,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:34,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:34,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:35,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:35,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:36,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:36,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:37,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:38,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:38,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:39,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:39,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:40,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:40,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:41,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:42,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:43,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:43,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:44,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:19<00:15, 15.06s/it][WARNING|generation_utils.py:914] 2023-08-28 04:44:44,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:45,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:46,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:47,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:47,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:48,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:49,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:49,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:50,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:51,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:51,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:52,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:53,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:54,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:54,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:55,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:56,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:57,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:58,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:58,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:44:59,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:45:00,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:45:00,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:36<00:00, 15.55s/it]Generating: 100%|██████████| 10/10 [02:36<00:00, 15.62s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:08,170 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:08,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:08,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:08,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:08,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:45:08,813 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:45:08,814 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:45:09,402 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:45:10,483 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:45:10,483 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:13,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:13,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:13,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:13,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:13,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:45:14,186 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:45:14,187 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:45:14,774 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:45:14,943 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:45:14,943 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9166666666666666, 'errors': {'', "('', 'composer', 'Jérémie Léger', 'The title track of the album was a track Iced Tea made by Jérémie Léger and written by Pierre Bousquet and Bernard Barrou , among others .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 397, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 443, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 489, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 555, 'raw': 800}
{'target': 600, 'success': 574, 'raw': 832}
{'target': 600, 'success': 593, 'raw': 864}
{'target': 600, 'success': 614, 'raw': 896}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.6852678571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : opposite of . Context : Later in the year ( 1143 ) , Ptolemaeus , the founder of the Macedonian Republic , married Ariadne , daughter of Ariadne , founder of Macedonian kings , Thebes and Ileani . Head Entity : Ileani , Tail Entity : Ileanius .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8491847826086957, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : creator .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.75375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Lothar', 'twinned administrative body', '', 'He died in the Battle of Mervyn at Rheinmetall in 645 along with his daughter Lothar , D.')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 12521
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12621, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.49it/s]Extractor Estimating: 4it [00:02,  1.52it/s]Extractor Estimating: 5it [00:03,  1.55it/s]Extractor Estimating: 6it [00:03,  1.54it/s]Extractor Estimating: 7it [00:04,  1.60it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.56it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:07,  1.55it/s]Extractor Estimating: 12it [00:07,  1.54it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:09,  1.58it/s]Extractor Estimating: 15it [00:09,  1.59it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:11,  1.47it/s]Extractor Estimating: 18it [00:11,  1.49it/s]Extractor Estimating: 19it [00:12,  1.50it/s]Extractor Estimating: 20it [00:12,  1.54it/s]Extractor Estimating: 21it [00:13,  1.52it/s]Extractor Estimating: 22it [00:14,  1.56it/s]Extractor Estimating: 23it [00:14,  1.60it/s]Extractor Estimating: 24it [00:15,  1.52it/s]Extractor Estimating: 25it [00:16,  1.53it/s]Extractor Estimating: 26it [00:16,  1.55it/s]Extractor Estimating: 27it [00:17,  1.56it/s]Extractor Estimating: 28it [00:18,  1.54it/s]Extractor Estimating: 29it [00:18,  1.55it/s]Extractor Estimating: 30it [00:19,  1.55it/s]Extractor Estimating: 31it [00:20,  1.57it/s]Extractor Estimating: 32it [00:20,  1.55it/s]Extractor Estimating: 33it [00:21,  1.52it/s]Extractor Estimating: 34it [00:22,  1.52it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:23,  1.52it/s]Extractor Estimating: 37it [00:24,  1.48it/s]Extractor Estimating: 38it [00:24,  1.49it/s]Extractor Estimating: 39it [00:25,  1.47it/s]Extractor Estimating: 40it [00:26,  1.50it/s]Extractor Estimating: 41it [00:26,  1.52it/s]Extractor Estimating: 42it [00:27,  1.50it/s]Extractor Estimating: 43it [00:28,  1.53it/s]Extractor Estimating: 44it [00:28,  1.50it/s]Extractor Estimating: 45it [00:29,  1.42it/s]Extractor Estimating: 46it [00:30,  1.44it/s]Extractor Estimating: 47it [00:30,  1.45it/s]Extractor Estimating: 48it [00:31,  1.45it/s]Extractor Estimating: 49it [00:32,  1.39it/s]Extractor Estimating: 50it [00:32,  1.44it/s]Extractor Estimating: 51it [00:33,  1.46it/s]Extractor Estimating: 52it [00:34,  1.49it/s]Extractor Estimating: 53it [00:34,  1.50it/s]Extractor Estimating: 54it [00:35,  1.47it/s]Extractor Estimating: 55it [00:36,  1.47it/s]Extractor Estimating: 56it [00:36,  1.50it/s]Extractor Estimating: 57it [00:37,  1.49it/s]Extractor Estimating: 58it [00:38,  1.48it/s]Extractor Estimating: 59it [00:39,  1.46it/s]Extractor Estimating: 60it [00:39,  1.52it/s]Extractor Estimating: 61it [00:40,  1.57it/s]Extractor Estimating: 62it [00:40,  1.55it/s]Extractor Estimating: 63it [00:41,  1.53it/s]Extractor Estimating: 64it [00:42,  1.53it/s]Extractor Estimating: 65it [00:42,  1.54it/s]Extractor Estimating: 66it [00:43,  1.58it/s]Extractor Estimating: 67it [00:44,  1.54it/s]Extractor Estimating: 68it [00:44,  1.62it/s]Extractor Estimating: 69it [00:45,  1.65it/s]Extractor Estimating: 70it [00:45,  1.70it/s]Extractor Estimating: 71it [00:46,  1.68it/s]Extractor Estimating: 72it [00:47,  1.60it/s]Extractor Estimating: 73it [00:47,  1.63it/s]Extractor Estimating: 74it [00:48,  1.61it/s]Extractor Estimating: 75it [00:48,  1.59it/s]Extractor Estimating: 76it [00:49,  1.60it/s]Extractor Estimating: 77it [00:50,  1.58it/s]Extractor Estimating: 78it [00:50,  1.57it/s]Extractor Estimating: 79it [00:51,  1.59it/s]Extractor Estimating: 80it [00:52,  1.55it/s]Extractor Estimating: 81it [00:52,  1.53it/s]Extractor Estimating: 82it [00:53,  1.53it/s]Extractor Estimating: 83it [00:54,  1.51it/s]Extractor Estimating: 84it [00:54,  1.57it/s]Extractor Estimating: 85it [00:55,  1.62it/s]Extractor Estimating: 86it [00:55,  1.63it/s]Extractor Estimating: 87it [00:56,  1.60it/s]Extractor Estimating: 88it [00:57,  1.64it/s]Extractor Estimating: 89it [00:57,  1.60it/s]Extractor Estimating: 90it [00:58,  1.60it/s]Extractor Estimating: 91it [00:59,  1.59it/s]Extractor Estimating: 92it [00:59,  1.56it/s]Extractor Estimating: 93it [01:00,  1.59it/s]Extractor Estimating: 94it [01:01,  1.59it/s]Extractor Estimating: 95it [01:01,  1.63it/s]Extractor Estimating: 96it [01:02,  1.64it/s]Extractor Estimating: 97it [01:02,  1.65it/s]Extractor Estimating: 98it [01:03,  1.66it/s]Extractor Estimating: 99it [01:03,  1.68it/s]Extractor Estimating: 100it [01:04,  1.67it/s]Extractor Estimating: 101it [01:05,  1.61it/s]Extractor Estimating: 102it [01:05,  1.59it/s]Extractor Estimating: 103it [01:06,  1.55it/s]Extractor Estimating: 104it [01:07,  1.55it/s]Extractor Estimating: 105it [01:07,  1.59it/s]Extractor Estimating: 106it [01:08,  1.57it/s]Extractor Estimating: 107it [01:09,  1.58it/s]Extractor Estimating: 108it [01:09,  1.58it/s]Extractor Estimating: 109it [01:10,  1.60it/s]Extractor Estimating: 110it [01:10,  1.64it/s]Extractor Estimating: 111it [01:11,  1.59it/s]Extractor Estimating: 112it [01:12,  1.63it/s]Extractor Estimating: 113it [01:12,  1.48it/s]Extractor Estimating: 114it [01:13,  1.49it/s]Extractor Estimating: 115it [01:14,  1.49it/s]Extractor Estimating: 116it [01:14,  1.54it/s]Extractor Estimating: 117it [01:15,  1.54it/s]Extractor Estimating: 118it [01:16,  1.50it/s]Extractor Estimating: 119it [01:16,  1.51it/s]Extractor Estimating: 120it [01:17,  1.52it/s]Extractor Estimating: 121it [01:18,  1.56it/s]Extractor Estimating: 122it [01:18,  1.58it/s]Extractor Estimating: 123it [01:19,  1.59it/s]Extractor Estimating: 124it [01:20,  1.58it/s]Extractor Estimating: 125it [01:20,  1.49it/s]Extractor Estimating: 126it [01:21,  1.49it/s]Extractor Estimating: 127it [01:22,  1.52it/s]Extractor Estimating: 128it [01:22,  1.56it/s]Extractor Estimating: 129it [01:23,  1.57it/s]Extractor Estimating: 130it [01:27,  1.80s/it]Extractor Estimating: 131it [01:28,  1.48s/it]Extractor Estimating: 132it [01:29,  1.24s/it]Extractor Estimating: 133it [01:29,  1.05s/it]Extractor Estimating: 134it [01:30,  1.10it/s]Extractor Estimating: 135it [01:31,  1.21it/s]Extractor Estimating: 136it [01:31,  1.26it/s]Extractor Estimating: 137it [01:32,  1.34it/s]Extractor Estimating: 138it [01:33,  1.41it/s]Extractor Estimating: 139it [01:33,  1.46it/s]Extractor Estimating: 140it [01:34,  1.50it/s]Extractor Estimating: 141it [01:34,  1.48it/s]Extractor Estimating: 142it [01:35,  1.49it/s]Extractor Estimating: 143it [01:36,  1.51it/s]Extractor Estimating: 144it [01:36,  1.50it/s]Extractor Estimating: 145it [01:37,  1.53it/s]Extractor Estimating: 146it [01:38,  1.50it/s]Extractor Estimating: 147it [01:38,  1.52it/s]Extractor Estimating: 148it [01:39,  1.53it/s]Extractor Estimating: 149it [01:40,  1.59it/s]Extractor Estimating: 150it [01:40,  1.60it/s]Extractor Estimating: 151it [01:41,  1.59it/s]Extractor Estimating: 152it [01:42,  1.56it/s]Extractor Estimating: 153it [01:42,  1.57it/s]Extractor Estimating: 154it [01:43,  1.54it/s]Extractor Estimating: 155it [01:43,  1.55it/s]Extractor Estimating: 156it [01:44,  1.53it/s]Extractor Estimating: 157it [01:45,  1.54it/s]Extractor Estimating: 158it [01:45,  1.57it/s]Extractor Estimating: 159it [01:46,  1.55it/s]Extractor Estimating: 160it [01:47,  1.58it/s]Extractor Estimating: 161it [01:47,  1.55it/s]Extractor Estimating: 162it [01:48,  1.59it/s]Extractor Estimating: 163it [01:49,  1.54it/s]Extractor Estimating: 164it [01:49,  1.51it/s]Extractor Estimating: 165it [01:50,  1.53it/s]Extractor Estimating: 166it [01:51,  1.53it/s]Extractor Estimating: 167it [01:51,  1.55it/s]Extractor Estimating: 168it [01:52,  1.61it/s]Extractor Estimating: 169it [01:52,  1.63it/s]Extractor Estimating: 170it [01:53,  1.57it/s]Extractor Estimating: 171it [01:54,  1.52it/s]Extractor Estimating: 172it [01:54,  1.56it/s]Extractor Estimating: 173it [01:55,  1.57it/s]Extractor Estimating: 174it [01:56,  1.60it/s]Extractor Estimating: 175it [01:56,  1.59it/s]Extractor Estimating: 176it [01:57,  1.58it/s]Extractor Estimating: 177it [01:58,  1.50it/s]Extractor Estimating: 178it [01:58,  1.52it/s]Extractor Estimating: 179it [01:59,  1.53it/s]Extractor Estimating: 180it [02:00,  1.55it/s]Extractor Estimating: 181it [02:00,  1.57it/s]Extractor Estimating: 182it [02:01,  1.58it/s]Extractor Estimating: 183it [02:01,  1.60it/s]Extractor Estimating: 184it [02:02,  1.61it/s]Extractor Estimating: 185it [02:03,  1.58it/s]Extractor Estimating: 186it [02:03,  1.53it/s]Extractor Estimating: 187it [02:04,  1.44it/s]Extractor Estimating: 188it [02:05,  1.45it/s]Extractor Estimating: 189it [02:06,  1.46it/s]Extractor Estimating: 190it [02:06,  1.51it/s]Extractor Estimating: 191it [02:07,  1.53it/s]Extractor Estimating: 192it [02:07,  1.52it/s]Extractor Estimating: 193it [02:08,  1.51it/s]Extractor Estimating: 194it [02:09,  1.50it/s]Extractor Estimating: 195it [02:09,  1.52it/s]Extractor Estimating: 196it [02:10,  1.50it/s]Extractor Estimating: 197it [02:11,  1.53it/s]Extractor Estimating: 198it [02:11,  1.53it/s]Extractor Estimating: 199it [02:12,  1.55it/s]Extractor Estimating: 200it [02:13,  1.55it/s]Extractor Estimating: 201it [02:13,  1.58it/s]Extractor Estimating: 202it [02:14,  1.60it/s]Extractor Estimating: 203it [02:15,  1.60it/s]Extractor Estimating: 204it [02:15,  1.57it/s]Extractor Estimating: 205it [02:16,  1.59it/s]Extractor Estimating: 206it [02:16,  1.61it/s]Extractor Estimating: 207it [02:17,  1.63it/s]Extractor Estimating: 208it [02:18,  1.63it/s]Extractor Estimating: 209it [02:18,  1.61it/s]Extractor Estimating: 210it [02:19,  1.62it/s]Extractor Estimating: 211it [02:19,  1.63it/s]Extractor Estimating: 212it [02:20,  1.63it/s]Extractor Estimating: 213it [02:21,  1.67it/s]Extractor Estimating: 214it [02:21,  1.67it/s]Extractor Estimating: 215it [02:22,  1.69it/s]Extractor Estimating: 216it [02:22,  1.69it/s]Extractor Estimating: 217it [02:23,  1.65it/s]Extractor Estimating: 218it [02:24,  1.69it/s]Extractor Estimating: 219it [02:24,  1.70it/s]Extractor Estimating: 220it [02:25,  1.68it/s]Extractor Estimating: 221it [02:25,  1.67it/s]Extractor Estimating: 222it [02:26,  1.60it/s]Extractor Estimating: 223it [02:27,  1.60it/s]Extractor Estimating: 224it [02:27,  1.60it/s]Extractor Estimating: 225it [02:28,  1.58it/s]Extractor Estimating: 226it [02:29,  1.59it/s]Extractor Estimating: 227it [02:29,  1.57it/s]Extractor Estimating: 228it [02:30,  1.57it/s]Extractor Estimating: 229it [02:31,  1.55it/s]Extractor Estimating: 230it [02:31,  1.58it/s]Extractor Estimating: 231it [02:32,  1.54it/s]Extractor Estimating: 232it [02:32,  1.57it/s]Extractor Estimating: 233it [02:33,  1.61it/s]Extractor Estimating: 234it [02:34,  1.65it/s]Extractor Estimating: 235it [02:34,  1.59it/s]Extractor Estimating: 236it [02:35,  1.59it/s]Extractor Estimating: 237it [02:36,  1.56it/s]Extractor Estimating: 238it [02:36,  1.61it/s]Extractor Estimating: 239it [02:37,  1.58it/s]Extractor Estimating: 240it [02:38,  1.52it/s]Extractor Estimating: 241it [02:38,  1.58it/s]Extractor Estimating: 242it [02:39,  1.60it/s]Extractor Estimating: 243it [02:39,  1.59it/s]Extractor Estimating: 244it [02:40,  1.40it/s]Extractor Estimating: 245it [02:41,  1.46it/s]Extractor Estimating: 246it [02:42,  1.47it/s]Extractor Estimating: 247it [02:42,  1.52it/s]Extractor Estimating: 248it [02:43,  1.58it/s]Extractor Estimating: 249it [02:43,  1.54it/s]Extractor Estimating: 250it [02:44,  1.66it/s]Extractor Estimating: 250it [02:44,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:18,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:18,348 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:18,348 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:18,348 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:18,348 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:48:19,063 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:48:19,064 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:48:19,382 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:48:20,436 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:48:20,436 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:23,067 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:23,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:23,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:23,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:23,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:48:23,424 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:48:23,425 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:48:24,127 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:48:24,288 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:48:24,288 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:29:36,624 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:29:36,996 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 5226 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 24197
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24297, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24297, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.057, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.067, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 82, avg_time 1.063, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 182, avg_time 1.065, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.052, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 2.864, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 46, avg_time 1.066, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 146, avg_time 1.072, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.059, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.064, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 10, avg_time 2.856, loss:nan
g_step 1200, step 110, avg_time 1.060, loss:nan
g_step 1300, step 210, avg_time 1.071, loss:nan
g_step 1400, step 92, avg_time 1.062, loss:nan
g_step 1500, step 192, avg_time 1.065, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 74, avg_time 2.854, loss:nan
g_step 1700, step 174, avg_time 1.071, loss:nan
g_step 1800, step 56, avg_time 1.051, loss:nan
g_step 1900, step 156, avg_time 1.058, loss:nan
g_step 2000, step 38, avg_time 1.065, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 138, avg_time 2.859, loss:nan
g_step 2200, step 20, avg_time 1.062, loss:nan
g_step 2300, step 120, avg_time 1.060, loss:nan
g_step 2400, step 2, avg_time 1.067, loss:nan
g_step 2500, step 102, avg_time 1.067, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 202, avg_time 2.852, loss:nan
g_step 2700, step 84, avg_time 1.059, loss:nan
g_step 2800, step 184, avg_time 1.059, loss:nan
g_step 2900, step 66, avg_time 1.074, loss:nan
g_step 3000, step 166, avg_time 1.059, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.843, loss:nan
g_step 3200, step 148, avg_time 1.061, loss:nan
g_step 3300, step 30, avg_time 1.061, loss:nan
g_step 3400, step 130, avg_time 1.067, loss:nan
g_step 3500, step 12, avg_time 1.065, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 2.862, loss:nan
g_step 3700, step 212, avg_time 1.054, loss:nan
g_step 3800, step 94, avg_time 1.068, loss:nan
g_step 3900, step 194, avg_time 1.059, loss:nan
g_step 4000, step 76, avg_time 1.060, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 2.851, loss:nan
g_step 4200, step 58, avg_time 1.066, loss:nan
g_step 4300, step 158, avg_time 1.068, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:29:36 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:29:36 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-29-36_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:29:38 - WARNING - datasets.builder -   Using custom data configuration default-6cba594810c2b753
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6cba594810c2b753/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:29:39,459 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:29:39,460 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:29:39,461 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:29:39,462 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:29:39,528 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:29:39,558 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:29:39,558 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:29:39,558 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:29:39,558 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:29:39,558 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:29:39,558 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:29:39,879 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:29:43,034 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:29:43,055 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6cba594810c2b753/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.82ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.68ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.06ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.23ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.31ba/s]100%|██████████| 6/6 [00:01<00:00,  4.66ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.18ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.82ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.07ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.22ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.29ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.35ba/s]100%|██████████| 7/7 [00:01<00:00,  4.56ba/s]100%|██████████| 7/7 [00:01<00:00,  4.22ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  6.13ba/s] 33%|███▎      | 2/6 [00:00<00:00,  7.34ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.36ba/s]100%|██████████| 6/6 [00:00<00:00, 11.99ba/s]100%|██████████| 6/6 [00:00<00:00, 10.46ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  5.92ba/s] 43%|████▎     | 3/7 [00:00<00:00,  9.02ba/s] 71%|███████▏  | 5/7 [00:00<00:00, 10.07ba/s]100%|██████████| 7/7 [00:00<00:00,  8.14ba/s]100%|██████████| 7/7 [00:00<00:00,  8.37ba/s]
[INFO|trainer.py:414] 2023-08-28 06:29:48,307 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:29:48,359 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:29:48,359 >>   Num examples = 5239
[INFO|trainer.py:1149] 2023-08-28 06:29:48,359 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:29:48,359 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:29:48,359 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:29:48,359 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:29:48,359 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:59,  3.43it/s]  0%|          | 2/410 [00:00<01:55,  3.53it/s]  1%|          | 3/410 [00:00<01:53,  3.57it/s]  1%|          | 4/410 [00:01<01:52,  3.59it/s]  1%|          | 5/410 [00:01<01:52,  3.59it/s]  1%|▏         | 6/410 [00:01<01:53,  3.55it/s]  2%|▏         | 7/410 [00:01<01:53,  3.56it/s]  2%|▏         | 8/410 [00:02<01:52,  3.56it/s]  2%|▏         | 9/410 [00:02<01:52,  3.56it/s]  2%|▏         | 10/410 [00:02<01:52,  3.56it/s]  3%|▎         | 11/410 [00:03<01:52,  3.56it/s]  3%|▎         | 12/410 [00:03<01:51,  3.56it/s]  3%|▎         | 13/410 [00:03<01:51,  3.56it/s]  3%|▎         | 14/410 [00:03<01:51,  3.56it/s]  4%|▎         | 15/410 [00:04<01:50,  3.56it/s]  4%|▍         | 16/410 [00:04<01:50,  3.56it/s]  4%|▍         | 17/410 [00:04<01:50,  3.57it/s]  4%|▍         | 18/410 [00:05<01:49,  3.57it/s]  5%|▍         | 19/410 [00:05<01:49,  3.57it/s]  5%|▍         | 20/410 [00:05<01:49,  3.57it/s]  5%|▌         | 21/410 [00:05<01:49,  3.57it/s]  5%|▌         | 22/410 [00:06<01:48,  3.57it/s]  6%|▌         | 23/410 [00:06<01:48,  3.57it/s]  6%|▌         | 24/410 [00:06<01:49,  3.51it/s]  6%|▌         | 25/410 [00:07<01:49,  3.52it/s]  6%|▋         | 26/410 [00:07<01:48,  3.53it/s]  7%|▋         | 27/410 [00:07<01:48,  3.54it/s]  7%|▋         | 28/410 [00:07<01:47,  3.55it/s]  7%|▋         | 29/410 [00:08<01:47,  3.55it/s]  7%|▋         | 30/410 [00:08<01:46,  3.56it/s]  8%|▊         | 31/410 [00:08<01:46,  3.56it/s]  8%|▊         | 32/410 [00:08<01:46,  3.56it/s]  8%|▊         | 33/410 [00:09<01:45,  3.56it/s]  8%|▊         | 34/410 [00:09<01:45,  3.56it/s]  9%|▊         | 35/410 [00:09<01:45,  3.56it/s]  9%|▉         | 36/410 [00:10<01:45,  3.56it/s]  9%|▉         | 37/410 [00:10<01:44,  3.56it/s]  9%|▉         | 38/410 [00:10<01:44,  3.56it/s] 10%|▉         | 39/410 [00:10<01:44,  3.56it/s] 10%|▉         | 40/410 [00:11<01:44,  3.56it/s] 10%|█         | 41/410 [00:11<01:43,  3.55it/s] 10%|█         | 42/410 [00:11<01:45,  3.49it/s] 10%|█         | 43/410 [00:12<01:44,  3.51it/s] 11%|█         | 44/410 [00:12<01:44,  3.52it/s] 11%|█         | 45/410 [00:12<01:43,  3.53it/s] 11%|█         | 46/410 [00:12<01:42,  3.54it/s] 11%|█▏        | 47/410 [00:13<01:42,  3.54it/s] 12%|█▏        | 48/410 [00:13<01:41,  3.55it/s] 12%|█▏        | 49/410 [00:13<01:43,  3.50it/s] 12%|█▏        | 50/410 [00:14<01:42,  3.52it/s] 12%|█▏        | 51/410 [00:14<01:41,  3.53it/s] 13%|█▎        | 52/410 [00:14<01:41,  3.54it/s] 13%|█▎        | 53/410 [00:14<01:40,  3.54it/s] 13%|█▎        | 54/410 [00:15<01:40,  3.55it/s] 13%|█▎        | 55/410 [00:15<01:39,  3.55it/s] 14%|█▎        | 56/410 [00:15<01:39,  3.56it/s] 14%|█▍        | 57/410 [00:16<01:39,  3.56it/s] 14%|█▍        | 58/410 [00:16<01:38,  3.56it/s] 14%|█▍        | 59/410 [00:16<01:38,  3.56it/s] 15%|█▍        | 60/410 [00:16<01:40,  3.48it/s] 15%|█▍        | 61/410 [00:17<01:39,  3.50it/s] 15%|█▌        | 62/410 [00:17<01:38,  3.52it/s] 15%|█▌        | 63/410 [00:17<01:38,  3.53it/s] 16%|█▌        | 64/410 [00:18<01:37,  3.53it/s] 16%|█▌        | 65/410 [00:18<01:37,  3.54it/s] 16%|█▌        | 66/410 [00:18<01:37,  3.55it/s] 16%|█▋        | 67/410 [00:18<01:36,  3.55it/s] 17%|█▋        | 68/410 [00:19<01:36,  3.55it/s] 17%|█▋        | 69/410 [00:19<01:36,  3.55it/s] 17%|█▋        | 70/410 [00:19<01:35,  3.55it/s] 17%|█▋        | 71/410 [00:20<01:35,  3.56it/s] 18%|█▊        | 72/410 [00:20<01:35,  3.56it/s] 18%|█▊        | 73/410 [00:20<01:34,  3.56it/s] 18%|█▊        | 74/410 [00:20<01:34,  3.56it/s] 18%|█▊        | 75/410 [00:21<01:34,  3.56it/s] 19%|█▊        | 76/410 [00:21<01:34,  3.55it/s] 19%|█▉        | 77/410 [00:21<01:33,  3.55it/s] 19%|█▉        | 78/410 [00:22<01:36,  3.46it/s] 19%|█▉        | 79/410 [00:22<01:34,  3.49it/s] 20%|█▉        | 80/410 [00:22<01:34,  3.51it/s] 20%|█▉        | 81/410 [00:22<01:33,  3.53it/s] 20%|██        | 82/410 [00:23<01:29,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 06:30:11,460 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:30:11,460 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 06:30:11,460 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.76it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.22it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.61it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.51it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.05it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.56it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.41it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.41it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.37it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.58it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.56it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.43it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.33it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.21it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.12it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.09it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.25it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.26it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.50it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.44it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.35it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.17it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.18it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.09it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.11it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.25it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.24it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.42it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.38it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.30it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.22it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.22it/s][A
 19%|█▉        | 167/861 [00:03<00:16, 41.44it/s][A
 20%|█▉        | 172/861 [00:03<00:16, 42.47it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.10it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.67it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.96it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 43.94it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 43.93it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 43.97it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 43.68it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.82it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.11it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.30it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.46it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.52it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.36it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.18it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.13it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 43.95it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.00it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.59it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.99it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.20it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.37it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.24it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.20it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.05it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.05it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 43.90it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.08it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.19it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.44it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.47it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.38it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.30it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.14it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.05it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.95it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.13it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.95it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.27it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.51it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.45it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.32it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.17it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.07it/s][A
 46%|████▌     | 392/861 [00:08<00:11, 41.76it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 42.58it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.26it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.60it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.95it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.19it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.02it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 43.91it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.65it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.90it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.00it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.23it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.23it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.35it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.37it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.24it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.98it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.87it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 43.96it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.12it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.17it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.29it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.43it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.43it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.29it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.12it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.08it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.06it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.11it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.17it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.39it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.49it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.35it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.20it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 43.87it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 43.93it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.05it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.06it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.37it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.38it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.41it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.31it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.16it/s][A
 70%|███████   | 607/861 [00:13<00:05, 43.98it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.06it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 42.67it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.28it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.68it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 43.92it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.26it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.24it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.19it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.05it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 43.92it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.90it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.10it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.23it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.37it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.39it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.37it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.26it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.09it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 43.86it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 44.03it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.11it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.33it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.39it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.44it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.23it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.24it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.10it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.90it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.96it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.08it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.31it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.38it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.44it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.20it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.10it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.00it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 43.92it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.00it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.03it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.25it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.39it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.39it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.28it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.07it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 43.98it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 43.99it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.04it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.01it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.42it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.72it/s][A
                                                 [A                                                
100%|██████████| 861/861 [00:19<00:00, 43.72it/s][A 20%|██        | 82/410 [00:42<01:29,  3.68it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:30:31,086 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 06:30:31,310 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:30:35,367 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:30:35,655 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:30:35,774 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:48<43:18,  7.95s/it] 20%|██        | 84/410 [00:49<30:40,  5.65s/it] 21%|██        | 85/410 [00:49<21:52,  4.04s/it] 21%|██        | 86/410 [00:49<15:43,  2.91s/it] 21%|██        | 87/410 [00:50<11:25,  2.12s/it] 21%|██▏       | 88/410 [00:50<08:27,  1.58s/it] 22%|██▏       | 89/410 [00:50<06:21,  1.19s/it] 22%|██▏       | 90/410 [00:50<04:52,  1.09it/s] 22%|██▏       | 91/410 [00:51<03:51,  1.38it/s] 22%|██▏       | 92/410 [00:51<03:08,  1.69it/s] 23%|██▎       | 93/410 [00:51<02:38,  2.01it/s] 23%|██▎       | 94/410 [00:52<02:16,  2.31it/s] 23%|██▎       | 95/410 [00:52<02:02,  2.58it/s] 23%|██▎       | 96/410 [00:52<01:51,  2.81it/s] 24%|██▎       | 97/410 [00:52<01:44,  3.00it/s] 24%|██▍       | 98/410 [00:53<01:39,  3.15it/s] 24%|██▍       | 99/410 [00:53<01:37,  3.19it/s] 24%|██▍       | 100/410 [00:53<01:34,  3.29it/s] 25%|██▍       | 101/410 [00:54<01:31,  3.36it/s] 25%|██▍       | 102/410 [00:54<01:30,  3.42it/s] 25%|██▌       | 103/410 [00:54<01:28,  3.46it/s] 25%|██▌       | 104/410 [00:54<01:27,  3.49it/s] 26%|██▌       | 105/410 [00:55<01:26,  3.51it/s] 26%|██▌       | 106/410 [00:55<01:26,  3.52it/s] 26%|██▌       | 107/410 [00:55<01:25,  3.54it/s] 26%|██▋       | 108/410 [00:56<01:25,  3.54it/s] 27%|██▋       | 109/410 [00:56<01:24,  3.55it/s] 27%|██▋       | 110/410 [00:56<01:26,  3.48it/s] 27%|██▋       | 111/410 [00:56<01:25,  3.50it/s] 27%|██▋       | 112/410 [00:57<01:24,  3.52it/s] 28%|██▊       | 113/410 [00:57<01:24,  3.53it/s] 28%|██▊       | 114/410 [00:57<01:23,  3.54it/s] 28%|██▊       | 115/410 [00:58<01:23,  3.55it/s] 28%|██▊       | 116/410 [00:58<01:22,  3.55it/s] 29%|██▊       | 117/410 [00:58<01:22,  3.55it/s] 29%|██▉       | 118/410 [00:58<01:22,  3.55it/s] 29%|██▉       | 119/410 [00:59<01:22,  3.55it/s] 29%|██▉       | 120/410 [00:59<01:21,  3.54it/s] 30%|██▉       | 121/410 [00:59<01:24,  3.44it/s] 30%|██▉       | 122/410 [01:00<01:22,  3.47it/s] 30%|███       | 123/410 [01:00<01:22,  3.49it/s] 30%|███       | 124/410 [01:00<01:21,  3.51it/s] 30%|███       | 125/410 [01:00<01:20,  3.52it/s] 31%|███       | 126/410 [01:01<01:20,  3.53it/s] 31%|███       | 127/410 [01:01<01:19,  3.54it/s] 31%|███       | 128/410 [01:01<01:19,  3.55it/s] 31%|███▏      | 129/410 [01:01<01:19,  3.55it/s] 32%|███▏      | 130/410 [01:02<01:18,  3.55it/s] 32%|███▏      | 131/410 [01:02<01:18,  3.55it/s] 32%|███▏      | 132/410 [01:02<01:20,  3.47it/s] 32%|███▏      | 133/410 [01:03<01:19,  3.49it/s] 33%|███▎      | 134/410 [01:03<01:18,  3.50it/s] 33%|███▎      | 135/410 [01:03<01:18,  3.52it/s] 33%|███▎      | 136/410 [01:03<01:17,  3.53it/s] 33%|███▎      | 137/410 [01:04<01:17,  3.54it/s] 34%|███▎      | 138/410 [01:04<01:16,  3.54it/s] 34%|███▍      | 139/410 [01:04<01:16,  3.55it/s] 34%|███▍      | 140/410 [01:05<01:16,  3.55it/s] 34%|███▍      | 141/410 [01:05<01:15,  3.55it/s] 35%|███▍      | 142/410 [01:05<01:15,  3.55it/s] 35%|███▍      | 143/410 [01:05<01:16,  3.48it/s] 35%|███▌      | 144/410 [01:06<01:16,  3.50it/s] 35%|███▌      | 145/410 [01:06<01:15,  3.51it/s] 36%|███▌      | 146/410 [01:06<01:14,  3.52it/s] 36%|███▌      | 147/410 [01:07<01:14,  3.53it/s] 36%|███▌      | 148/410 [01:07<01:14,  3.54it/s] 36%|███▋      | 149/410 [01:07<01:13,  3.54it/s] 37%|███▋      | 150/410 [01:07<01:13,  3.55it/s] 37%|███▋      | 151/410 [01:08<01:12,  3.55it/s] 37%|███▋      | 152/410 [01:08<01:12,  3.55it/s] 37%|███▋      | 153/410 [01:08<01:12,  3.55it/s] 38%|███▊      | 154/410 [01:09<01:14,  3.43it/s] 38%|███▊      | 155/410 [01:09<01:13,  3.47it/s] 38%|███▊      | 156/410 [01:09<01:12,  3.49it/s] 38%|███▊      | 157/410 [01:09<01:12,  3.51it/s] 39%|███▊      | 158/410 [01:10<01:11,  3.52it/s] 39%|███▉      | 159/410 [01:10<01:11,  3.53it/s] 39%|███▉      | 160/410 [01:10<01:10,  3.54it/s] 39%|███▉      | 161/410 [01:11<01:10,  3.54it/s] 40%|███▉      | 162/410 [01:11<01:09,  3.54it/s] 40%|███▉      | 163/410 [01:11<01:09,  3.54it/s] 40%|████      | 164/410 [01:11<01:06,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 06:31:00,235 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:31:00,236 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 06:31:00,236 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5309, 'eval_samples_per_second': 352.468, 'eval_steps_per_second': 44.084, 'epoch': 1.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.41it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.53it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.95it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.81it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.89it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.42it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.06it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.97it/s][A
  5%|▌         | 47/861 [00:01<00:18, 42.96it/s][A
  6%|▌         | 52/861 [00:01<00:18, 43.48it/s][A
  7%|▋         | 57/861 [00:01<00:18, 43.80it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.00it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.34it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.23it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.03it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.78it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.77it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.91it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.99it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.20it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.42it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.41it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.26it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.12it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.89it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.85it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.88it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.11it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.24it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.42it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.28it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.18it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.01it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.87it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.90it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.01it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.16it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.37it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.36it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.30it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.14it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.93it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.91it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.98it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.04it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.16it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.31it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.35it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.13it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.02it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.93it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.96it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.97it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.13it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 41.95it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 42.71it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 43.24it/s][A
 34%|███▍      | 292/861 [00:06<00:13, 43.46it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 43.69it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 43.71it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 43.74it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.89it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.75it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.87it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 43.90it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.21it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.28it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.30it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.19it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.11it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.08it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.00it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.02it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.03it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.21it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.22it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.24it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.23it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.13it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.04it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.02it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.90it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 43.99it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.16it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.15it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.32it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.20it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.16it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.08it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.14it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.09it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.06it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.19it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.15it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.22it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.23it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.09it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.07it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.06it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 43.25it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 43.62it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 43.80it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.96it/s][A
 61%|██████    | 522/861 [00:11<00:07, 43.99it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.08it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.99it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.00it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 43.98it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 43.96it/s][A
 64%|██████▍   | 552/861 [00:12<00:07, 44.09it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.18it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.26it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.13it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.14it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.23it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.13it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.94it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 39.58it/s][A
 69%|██████▉   | 597/861 [00:13<00:06, 40.99it/s][A
 70%|██████▉   | 602/861 [00:13<00:06, 42.06it/s][A
 70%|███████   | 607/861 [00:13<00:05, 42.72it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.19it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 43.65it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.92it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.86it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 43.52it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 43.49it/s][A
 75%|███████▍  | 642/861 [00:14<00:05, 43.76it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 43.96it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.15it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.32it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.43it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.43it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.04it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 43.80it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 43.69it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 43.87it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.11it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.15it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.32it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 44.43it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.39it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.04it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 41.00it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 41.94it/s][A
 85%|████████▌ | 732/861 [00:16<00:03, 42.72it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 43.17it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.51it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 43.86it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.03it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.91it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.60it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.64it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.86it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.07it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.13it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.21it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.32it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.32it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.10it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 43.87it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 43.75it/s][A
 95%|█████████▍| 817/861 [00:18<00:01, 43.80it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.03it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.23it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.30it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 44.39it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.30it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.10it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.84it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.73it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.73it/s][A 40%|████      | 164/410 [01:31<01:06,  3.69it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:31:20,068 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 06:31:20,298 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:31:23,502 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:31:23,723 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:31:23,809 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:37<31:36,  7.74s/it] 40%|████      | 166/410 [01:37<22:22,  5.50s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:37<15:56,  3.94s/it] 41%|████      | 168/410 [01:37<11:27,  2.84s/it] 41%|████      | 169/410 [01:38<08:19,  2.07s/it] 41%|████▏     | 170/410 [01:38<06:08,  1.54s/it] 42%|████▏     | 171/410 [01:38<04:37,  1.16s/it] 42%|████▏     | 172/410 [01:39<03:33,  1.12it/s] 42%|████▏     | 173/410 [01:39<02:48,  1.40it/s] 42%|████▏     | 174/410 [01:39<02:17,  1.71it/s] 43%|████▎     | 175/410 [01:39<01:55,  2.03it/s] 43%|████▎     | 176/410 [01:40<01:40,  2.33it/s] 43%|████▎     | 177/410 [01:40<01:29,  2.60it/s] 43%|████▎     | 178/410 [01:40<01:22,  2.83it/s] 44%|████▎     | 179/410 [01:40<01:16,  3.01it/s] 44%|████▍     | 180/410 [01:41<01:12,  3.16it/s] 44%|████▍     | 181/410 [01:41<01:11,  3.21it/s] 44%|████▍     | 182/410 [01:41<01:09,  3.30it/s] 45%|████▍     | 183/410 [01:42<01:07,  3.37it/s] 45%|████▍     | 184/410 [01:42<01:05,  3.42it/s] 45%|████▌     | 185/410 [01:42<01:05,  3.46it/s] 45%|████▌     | 186/410 [01:42<01:04,  3.48it/s] 46%|████▌     | 187/410 [01:43<01:03,  3.50it/s] 46%|████▌     | 188/410 [01:43<01:03,  3.52it/s] 46%|████▌     | 189/410 [01:43<01:02,  3.53it/s] 46%|████▋     | 190/410 [01:44<01:02,  3.54it/s] 47%|████▋     | 191/410 [01:44<01:01,  3.54it/s] 47%|████▋     | 192/410 [01:44<01:03,  3.46it/s] 47%|████▋     | 193/410 [01:44<01:02,  3.49it/s] 47%|████▋     | 194/410 [01:45<01:01,  3.50it/s] 48%|████▊     | 195/410 [01:45<01:01,  3.52it/s] 48%|████▊     | 196/410 [01:45<01:00,  3.53it/s] 48%|████▊     | 197/410 [01:46<01:00,  3.53it/s] 48%|████▊     | 198/410 [01:46<00:59,  3.54it/s] 49%|████▊     | 199/410 [01:46<00:59,  3.54it/s] 49%|████▉     | 200/410 [01:46<00:59,  3.55it/s] 49%|████▉     | 201/410 [01:47<01:00,  3.45it/s] 49%|████▉     | 202/410 [01:47<00:59,  3.49it/s] 50%|████▉     | 203/410 [01:47<01:00,  3.43it/s] 50%|████▉     | 204/410 [01:48<00:59,  3.48it/s] 50%|█████     | 205/410 [01:48<01:26,  2.38it/s] 50%|█████     | 206/410 [01:49<01:17,  2.64it/s] 50%|█████     | 207/410 [01:49<01:10,  2.87it/s] 51%|█████     | 208/410 [01:49<01:06,  3.06it/s] 51%|█████     | 209/410 [01:49<01:02,  3.20it/s] 51%|█████     | 210/410 [01:50<01:00,  3.31it/s] 51%|█████▏    | 211/410 [01:50<00:58,  3.39it/s] 52%|█████▏    | 212/410 [01:50<00:57,  3.45it/s] 52%|█████▏    | 213/410 [01:51<00:58,  3.35it/s] 52%|█████▏    | 214/410 [01:51<00:57,  3.42it/s] 52%|█████▏    | 215/410 [01:51<00:56,  3.48it/s] 53%|█████▎    | 216/410 [01:51<00:55,  3.51it/s] 53%|█████▎    | 217/410 [01:52<00:54,  3.54it/s] 53%|█████▎    | 218/410 [01:52<00:53,  3.56it/s] 53%|█████▎    | 219/410 [01:52<00:53,  3.57it/s] 54%|█████▎    | 220/410 [01:53<00:53,  3.58it/s] 54%|█████▍    | 221/410 [01:53<00:52,  3.59it/s] 54%|█████▍    | 222/410 [01:53<00:52,  3.59it/s] 54%|█████▍    | 223/410 [01:53<00:51,  3.60it/s] 55%|█████▍    | 224/410 [01:54<00:53,  3.50it/s] 55%|█████▍    | 225/410 [01:54<00:52,  3.53it/s] 55%|█████▌    | 226/410 [01:54<00:51,  3.55it/s] 55%|█████▌    | 227/410 [01:55<00:51,  3.57it/s] 56%|█████▌    | 228/410 [01:55<00:50,  3.58it/s] 56%|█████▌    | 229/410 [01:55<00:50,  3.58it/s] 56%|█████▌    | 230/410 [01:55<00:50,  3.59it/s] 56%|█████▋    | 231/410 [01:56<00:49,  3.59it/s] 57%|█████▋    | 232/410 [01:56<00:49,  3.59it/s] 57%|█████▋    | 233/410 [01:56<00:49,  3.59it/s] 57%|█████▋    | 234/410 [01:56<00:48,  3.60it/s] 57%|█████▋    | 235/410 [01:57<00:49,  3.50it/s] 58%|█████▊    | 236/410 [01:57<00:49,  3.53it/s] 58%|█████▊    | 237/410 [01:57<00:48,  3.56it/s] 58%|█████▊    | 238/410 [01:58<00:48,  3.57it/s] 58%|█████▊    | 239/410 [01:58<00:47,  3.58it/s] 59%|█████▊    | 240/410 [01:58<00:47,  3.59it/s] 59%|█████▉    | 241/410 [01:58<00:47,  3.59it/s] 59%|█████▉    | 242/410 [01:59<00:46,  3.59it/s] 59%|█████▉    | 243/410 [01:59<00:46,  3.59it/s] 60%|█████▉    | 244/410 [01:59<00:46,  3.60it/s] 60%|█████▉    | 245/410 [02:00<00:45,  3.60it/s] 60%|██████    | 246/410 [02:00<00:45,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 06:31:48,688 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:31:48,688 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 06:31:48,688 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.6111, 'eval_samples_per_second': 351.026, 'eval_steps_per_second': 43.904, 'epoch': 2.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.74it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.71it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.85it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.06it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.49it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.71it/s][A
  4%|▍         | 37/861 [00:00<00:19, 42.93it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.18it/s][A
  5%|▌         | 47/861 [00:01<00:18, 43.59it/s][A
  6%|▌         | 52/861 [00:01<00:18, 43.88it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.13it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.34it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.35it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.30it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.90it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.74it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.80it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.97it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.21it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.35it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.43it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.46it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.30it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.92it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.86it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.89it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.11it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.20it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.31it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.40it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.32it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.18it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 43.90it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.32it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.58it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.80it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.13it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.26it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.32it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.25it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.08it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.83it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.82it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.90it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.01it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.18it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.40it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.26it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.34it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.00it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.79it/s][A
 30%|███       | 262/861 [00:05<00:13, 42.94it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.37it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.54it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.89it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.13it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.22it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.23it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 43.97it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 43.71it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 43.82it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.04it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.11it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.28it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.34it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.38it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.32it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.98it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.87it/s][A
 41%|████      | 352/861 [00:07<00:11, 43.85it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.06it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.13it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.21it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.26it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.33it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.33it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.15it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 43.92it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 42.94it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.41it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.77it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.91it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.05it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.16it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.14it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.01it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.76it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.80it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.95it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.07it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.20it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.34it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.37it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.24it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.03it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 43.93it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.68it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.84it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 43.96it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.21it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.27it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.30it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.22it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.03it/s][A
 61%|██████    | 527/861 [00:11<00:07, 43.92it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.01it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.00it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.17it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.31it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.34it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.27it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.11it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.04it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 43.92it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.00it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.02it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.26it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.36it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.32it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.28it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.09it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.00it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 44.02it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.33it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.69it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 43.88it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.15it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.14it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.07it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 43.95it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 43.95it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.82it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 43.95it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.18it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.25it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.44it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.34it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.24it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.08it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 43.86it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.88it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.94it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.16it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.34it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.43it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.32it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.24it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.02it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.93it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.85it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.03it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.13it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.24it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.38it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.25it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.14it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.02it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 43.94it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.87it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.09it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.20it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.31it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.32it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.22it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.06it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.02it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 43.99it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.96it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.35it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.66it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.95it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.95it/s][A 60%|██████    | 246/410 [02:19<00:45,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:32:08,387 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 06:32:08,523 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:32:12,159 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:32:12,724 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:32:12,837 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [02:26<21:31,  7.92s/it] 60%|██████    | 248/410 [02:26<15:12,  5.63s/it] 61%|██████    | 249/410 [02:26<10:48,  4.03s/it] 61%|██████    | 250/410 [02:26<07:44,  2.90s/it] 61%|██████    | 251/410 [02:27<05:38,  2.13s/it] 61%|██████▏   | 252/410 [02:27<04:08,  1.57s/it] 62%|██████▏   | 253/410 [02:27<03:06,  1.19s/it] 62%|██████▏   | 254/410 [02:28<02:22,  1.09it/s] 62%|██████▏   | 255/410 [02:28<01:52,  1.38it/s] 62%|██████▏   | 256/410 [02:28<01:31,  1.69it/s] 63%|██████▎   | 257/410 [02:28<01:16,  2.01it/s] 63%|██████▎   | 258/410 [02:29<01:05,  2.31it/s] 63%|██████▎   | 259/410 [02:29<00:58,  2.58it/s] 63%|██████▎   | 260/410 [02:29<00:53,  2.81it/s] 64%|██████▎   | 261/410 [02:30<00:49,  3.00it/s] 64%|██████▍   | 262/410 [02:30<00:49,  2.99it/s] 64%|██████▍   | 263/410 [02:30<00:46,  3.14it/s] 64%|██████▍   | 264/410 [02:30<00:44,  3.25it/s] 65%|██████▍   | 265/410 [02:31<00:43,  3.33it/s] 65%|██████▍   | 266/410 [02:31<00:42,  3.40it/s] 65%|██████▌   | 267/410 [02:31<00:41,  3.45it/s] 65%|██████▌   | 268/410 [02:32<00:40,  3.48it/s] 66%|██████▌   | 269/410 [02:32<00:40,  3.50it/s] 66%|██████▌   | 270/410 [02:32<00:39,  3.52it/s] 66%|██████▌   | 271/410 [02:32<00:39,  3.53it/s] 66%|██████▋   | 272/410 [02:33<00:39,  3.54it/s] 67%|██████▋   | 273/410 [02:33<00:43,  3.18it/s] 67%|██████▋   | 274/410 [02:33<00:41,  3.28it/s] 67%|██████▋   | 275/410 [02:34<00:40,  3.36it/s] 67%|██████▋   | 276/410 [02:34<00:39,  3.42it/s] 68%|██████▊   | 277/410 [02:34<00:38,  3.46it/s] 68%|██████▊   | 278/410 [02:35<00:37,  3.49it/s] 68%|██████▊   | 279/410 [02:35<00:37,  3.51it/s] 68%|██████▊   | 280/410 [02:35<00:36,  3.52it/s] 69%|██████▊   | 281/410 [02:35<00:36,  3.53it/s] 69%|██████▉   | 282/410 [02:36<00:36,  3.54it/s] 69%|██████▉   | 283/410 [02:36<00:35,  3.54it/s] 69%|██████▉   | 284/410 [02:36<00:36,  3.41it/s] 70%|██████▉   | 285/410 [02:37<00:36,  3.45it/s] 70%|██████▉   | 286/410 [02:37<00:35,  3.48it/s] 70%|███████   | 287/410 [02:37<00:35,  3.50it/s] 70%|███████   | 288/410 [02:37<00:34,  3.51it/s] 70%|███████   | 289/410 [02:38<00:34,  3.53it/s] 71%|███████   | 290/410 [02:38<00:33,  3.54it/s] 71%|███████   | 291/410 [02:38<00:33,  3.54it/s] 71%|███████   | 292/410 [02:38<00:33,  3.55it/s] 71%|███████▏  | 293/410 [02:39<00:32,  3.55it/s] 72%|███████▏  | 294/410 [02:39<00:32,  3.55it/s] 72%|███████▏  | 295/410 [02:39<00:33,  3.44it/s] 72%|███████▏  | 296/410 [02:40<00:32,  3.47it/s] 72%|███████▏  | 297/410 [02:40<00:32,  3.50it/s] 73%|███████▎  | 298/410 [02:40<00:31,  3.52it/s] 73%|███████▎  | 299/410 [02:40<00:31,  3.53it/s] 73%|███████▎  | 300/410 [02:41<00:31,  3.54it/s] 73%|███████▎  | 301/410 [02:41<00:30,  3.54it/s] 74%|███████▎  | 302/410 [02:41<00:30,  3.54it/s] 74%|███████▍  | 303/410 [02:42<00:30,  3.54it/s] 74%|███████▍  | 304/410 [02:42<00:29,  3.54it/s] 74%|███████▍  | 305/410 [02:42<00:29,  3.54it/s] 75%|███████▍  | 306/410 [02:42<00:29,  3.49it/s] 75%|███████▍  | 307/410 [02:43<00:29,  3.51it/s] 75%|███████▌  | 308/410 [02:43<00:28,  3.54it/s] 75%|███████▌  | 309/410 [02:43<00:28,  3.56it/s] 76%|███████▌  | 310/410 [02:44<00:27,  3.57it/s] 76%|███████▌  | 311/410 [02:44<00:27,  3.58it/s] 76%|███████▌  | 312/410 [02:44<00:27,  3.59it/s] 76%|███████▋  | 313/410 [02:44<00:26,  3.59it/s] 77%|███████▋  | 314/410 [02:45<00:26,  3.60it/s] 77%|███████▋  | 315/410 [02:45<00:26,  3.60it/s] 77%|███████▋  | 316/410 [02:45<00:26,  3.60it/s] 77%|███████▋  | 317/410 [02:46<00:26,  3.54it/s] 78%|███████▊  | 318/410 [02:46<00:25,  3.56it/s] 78%|███████▊  | 319/410 [02:46<00:25,  3.58it/s] 78%|███████▊  | 320/410 [02:46<00:25,  3.58it/s] 78%|███████▊  | 321/410 [02:47<00:24,  3.59it/s] 79%|███████▊  | 322/410 [02:47<00:25,  3.51it/s] 79%|███████▉  | 323/410 [02:47<00:24,  3.53it/s] 79%|███████▉  | 324/410 [02:48<00:24,  3.55it/s] 79%|███████▉  | 325/410 [02:48<00:23,  3.56it/s] 80%|███████▉  | 326/410 [02:48<00:29,  2.87it/s] 80%|███████▉  | 327/410 [02:49<00:28,  2.94it/s] 80%|████████  | 328/410 [02:49<00:25,  3.21it/s][INFO|trainer.py:2140] 2023-08-28 06:32:37,729 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:32:37,729 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 06:32:37,729 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5388, 'eval_samples_per_second': 352.324, 'eval_steps_per_second': 44.066, 'epoch': 3.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.26it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.59it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.00it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.08it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.65it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.89it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.16it/s][A
  5%|▍         | 42/861 [00:00<00:19, 42.72it/s][A
  5%|▌         | 47/861 [00:01<00:18, 43.29it/s][A
  6%|▌         | 52/861 [00:01<00:18, 43.73it/s][A
  7%|▋         | 57/861 [00:01<00:18, 43.96it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.22it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.34it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.19it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.89it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.59it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.67it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.96it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.17it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.32it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.41it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.38it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.29it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.96it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.79it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.73it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.95it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.12it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.28it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.47it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.36it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.18it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 43.97it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.85it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.09it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.55it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.66it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 43.98it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.18it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.26it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 43.96it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.92it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.68it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.84it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.00it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.26it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.39it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.43it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.25it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.05it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.86it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.73it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.79it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.90it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.16it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.34it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.45it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.28it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.11it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 43.96it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 43.86it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.29it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.66it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.98it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.22it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.27it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.10it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.10it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.96it/s][A
 41%|████      | 352/861 [00:07<00:11, 43.81it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.81it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.00it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.21it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.42it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.45it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.27it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.08it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 43.97it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 43.90it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.85it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 41.97it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 42.94it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 43.44it/s][A
 49%|████▉     | 422/861 [00:09<00:10, 43.77it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.14it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.07it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.90it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.80it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 42.84it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.28it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.60it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 43.83it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.04it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.29it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.30it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.13it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.82it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.82it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 43.98it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.09it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.17it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.27it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.39it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.36it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.05it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.95it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 43.86it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 43.97it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.15it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.26it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.27it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.34it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.21it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.01it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 43.95it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 42.20it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 42.92it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 43.41it/s][A
 69%|██████▉   | 597/861 [00:13<00:06, 43.80it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 43.98it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.02it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.90it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 43.86it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.66it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.86it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.02it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.19it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.24it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.26it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.08it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 43.90it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.85it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 43.74it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 43.97it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.03it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.23it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.42it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.34it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.25it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 43.99it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.95it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.94it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 42.58it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.21it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.53it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 43.88it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.11it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.91it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.77it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.77it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.73it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.78it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.00it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.19it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.33it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.32it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.17it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.02it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.92it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 43.83it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 43.98it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.07it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.29it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.42it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.33it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.13it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 43.97it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.96it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.91it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 42.63it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.20it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.20it/s][A 80%|████████  | 328/410 [03:08<00:25,  3.21it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:32:57,396 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 06:32:57,499 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:33:01,507 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:33:01,618 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:33:01,652 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [03:14<10:24,  7.71s/it] 80%|████████  | 330/410 [03:14<07:18,  5.48s/it] 81%|████████  | 331/410 [03:14<05:09,  3.92s/it] 81%|████████  | 332/410 [03:15<03:40,  2.83s/it] 81%|████████  | 333/410 [03:15<02:38,  2.06s/it] 81%|████████▏ | 334/410 [03:15<01:56,  1.53s/it] 82%|████████▏ | 335/410 [03:16<01:26,  1.15s/it] 82%|████████▏ | 336/410 [03:16<01:06,  1.12it/s] 82%|████████▏ | 337/410 [03:16<00:51,  1.41it/s] 82%|████████▏ | 338/410 [03:16<00:41,  1.72it/s] 83%|████████▎ | 339/410 [03:17<00:34,  2.03it/s] 83%|████████▎ | 340/410 [03:17<00:30,  2.33it/s] 83%|████████▎ | 341/410 [03:17<00:26,  2.60it/s] 83%|████████▎ | 342/410 [03:17<00:24,  2.83it/s] 84%|████████▎ | 343/410 [03:18<00:22,  3.01it/s] 84%|████████▍ | 344/410 [03:18<00:20,  3.16it/s] 84%|████████▍ | 345/410 [03:18<00:19,  3.27it/s] 84%|████████▍ | 346/410 [03:19<00:19,  3.35it/s] 85%|████████▍ | 347/410 [03:19<00:18,  3.41it/s] 85%|████████▍ | 348/410 [03:19<00:17,  3.45it/s] 85%|████████▌ | 349/410 [03:19<00:17,  3.48it/s] 85%|████████▌ | 350/410 [03:20<00:17,  3.50it/s] 86%|████████▌ | 351/410 [03:20<00:16,  3.51it/s] 86%|████████▌ | 352/410 [03:20<00:16,  3.52it/s] 86%|████████▌ | 353/410 [03:21<00:16,  3.53it/s] 86%|████████▋ | 354/410 [03:21<00:16,  3.42it/s] 87%|████████▋ | 355/410 [03:21<00:15,  3.46it/s] 87%|████████▋ | 356/410 [03:21<00:15,  3.49it/s] 87%|████████▋ | 357/410 [03:22<00:15,  3.50it/s] 87%|████████▋ | 358/410 [03:22<00:14,  3.52it/s] 88%|████████▊ | 359/410 [03:22<00:14,  3.53it/s] 88%|████████▊ | 360/410 [03:23<00:14,  3.54it/s] 88%|████████▊ | 361/410 [03:23<00:13,  3.54it/s] 88%|████████▊ | 362/410 [03:23<00:13,  3.55it/s] 89%|████████▊ | 363/410 [03:23<00:13,  3.55it/s] 89%|████████▉ | 364/410 [03:24<00:12,  3.55it/s] 89%|████████▉ | 365/410 [03:24<00:13,  3.44it/s] 89%|████████▉ | 366/410 [03:24<00:12,  3.47it/s] 90%|████████▉ | 367/410 [03:25<00:12,  3.50it/s] 90%|████████▉ | 368/410 [03:25<00:11,  3.51it/s] 90%|█████████ | 369/410 [03:25<00:11,  3.53it/s] 90%|█████████ | 370/410 [03:25<00:11,  3.52it/s] 90%|█████████ | 371/410 [03:26<00:11,  3.53it/s] 91%|█████████ | 372/410 [03:26<00:10,  3.54it/s] 91%|█████████ | 373/410 [03:26<00:10,  3.54it/s] 91%|█████████ | 374/410 [03:27<00:10,  3.54it/s] 91%|█████████▏| 375/410 [03:27<00:09,  3.54it/s] 92%|█████████▏| 376/410 [03:27<00:10,  3.35it/s] 92%|█████████▏| 377/410 [03:27<00:09,  3.41it/s] 92%|█████████▏| 378/410 [03:28<00:09,  3.46it/s] 92%|█████████▏| 379/410 [03:28<00:08,  3.50it/s] 93%|█████████▎| 380/410 [03:28<00:08,  3.53it/s] 93%|█████████▎| 381/410 [03:29<00:08,  3.55it/s] 93%|█████████▎| 382/410 [03:29<00:07,  3.57it/s] 93%|█████████▎| 383/410 [03:29<00:07,  3.58it/s] 94%|█████████▎| 384/410 [03:29<00:07,  3.59it/s] 94%|█████████▍| 385/410 [03:30<00:06,  3.60it/s] 94%|█████████▍| 386/410 [03:30<00:06,  3.60it/s] 94%|█████████▍| 387/410 [03:30<00:06,  3.44it/s] 95%|█████████▍| 388/410 [03:31<00:06,  3.49it/s] 95%|█████████▍| 389/410 [03:31<00:05,  3.52it/s] 95%|█████████▌| 390/410 [03:31<00:05,  3.55it/s] 95%|█████████▌| 391/410 [03:31<00:05,  3.56it/s] 96%|█████████▌| 392/410 [03:32<00:05,  3.58it/s] 96%|█████████▌| 393/410 [03:32<00:04,  3.59it/s] 96%|█████████▌| 394/410 [03:32<00:04,  3.59it/s] 96%|█████████▋| 395/410 [03:33<00:04,  3.60it/s] 97%|█████████▋| 396/410 [03:33<00:03,  3.60it/s] 97%|█████████▋| 397/410 [03:33<00:03,  3.60it/s] 97%|█████████▋| 398/410 [03:33<00:03,  3.37it/s] 97%|█████████▋| 399/410 [03:34<00:03,  3.44it/s] 98%|█████████▊| 400/410 [03:34<00:02,  3.49it/s] 98%|█████████▊| 401/410 [03:34<00:02,  3.52it/s] 98%|█████████▊| 402/410 [03:35<00:02,  3.55it/s] 98%|█████████▊| 403/410 [03:35<00:01,  3.56it/s] 99%|█████████▊| 404/410 [03:35<00:01,  3.58it/s] 99%|█████████▉| 405/410 [03:35<00:01,  3.58it/s] 99%|█████████▉| 406/410 [03:36<00:01,  3.59it/s] 99%|█████████▉| 407/410 [03:36<00:00,  3.59it/s]100%|█████████▉| 408/410 [03:36<00:00,  3.60it/s]100%|█████████▉| 409/410 [03:37<00:00,  3.37it/s]100%|██████████| 410/410 [03:37<00:00,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 06:33:25,618 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:33:25,618 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 06:33:25,618 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5873, 'eval_samples_per_second': 351.452, 'eval_steps_per_second': 43.957, 'epoch': 4.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.98it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.85it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.84it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.30it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.67it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.25it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.01it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.99it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.09it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.25it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.43it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.47it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.35it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.19it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.85it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.80it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.87it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.05it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.09it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.40it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.49it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.32it/s][A
 14%|█▎        | 117/861 [00:02<00:17, 43.31it/s][A
 14%|█▍        | 122/861 [00:02<00:17, 43.39it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.62it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.72it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.95it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.10it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.23it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.27it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.16it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.03it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 43.89it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.93it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.98it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.20it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.27it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.22it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.25it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.17it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.00it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.95it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.06it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.99it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.20it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.21it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.28it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.16it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.13it/s][A
 29%|██▉       | 252/861 [00:05<00:14, 43.42it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.61it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.75it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.96it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.08it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.15it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.09it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 43.97it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 43.85it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 43.91it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 43.92it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.04it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.17it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.14it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.25it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.07it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 43.98it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 43.94it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.96it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.99it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.14it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.27it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.26it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.26it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.04it/s][A
 44%|████▍     | 377/861 [00:08<00:11, 43.91it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 43.87it/s][A
 45%|████▍     | 387/861 [00:08<00:11, 40.70it/s][A
 46%|████▌     | 392/861 [00:08<00:11, 41.73it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 42.59it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.22it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.72it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.80it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 43.87it/s][A
 49%|████▉     | 422/861 [00:09<00:10, 43.67it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 43.52it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.40it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.65it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.92it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.16it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.56it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.04it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.03it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 43.87it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.59it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.61it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 43.73it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.95it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.16it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.31it/s][A
 58%|█████▊    | 502/861 [00:11<00:13, 26.01it/s][A
 59%|█████▉    | 507/861 [00:11<00:11, 29.81it/s][A
 59%|█████▉    | 511/861 [00:11<00:11, 30.59it/s][A
 60%|█████▉    | 516/861 [00:12<00:10, 34.13it/s][A
 61%|██████    | 521/861 [00:12<00:09, 36.83it/s][A
 61%|██████    | 526/861 [00:12<00:08, 38.91it/s][A
 62%|██████▏   | 531/861 [00:12<00:08, 40.45it/s][A
 62%|██████▏   | 536/861 [00:12<00:07, 41.54it/s][A
 63%|██████▎   | 541/861 [00:12<00:07, 41.96it/s][A
 63%|██████▎   | 546/861 [00:12<00:07, 42.43it/s][A
 64%|██████▍   | 551/861 [00:12<00:07, 42.83it/s][A
 65%|██████▍   | 556/861 [00:12<00:07, 43.06it/s][A
 65%|██████▌   | 561/861 [00:13<00:06, 43.55it/s][A
 66%|██████▌   | 566/861 [00:13<00:06, 43.86it/s][A
 66%|██████▋   | 571/861 [00:13<00:06, 44.16it/s][A
 67%|██████▋   | 576/861 [00:13<00:06, 44.19it/s][A
 67%|██████▋   | 581/861 [00:13<00:06, 44.05it/s][A
 68%|██████▊   | 586/861 [00:13<00:06, 43.74it/s][A
 69%|██████▊   | 591/861 [00:13<00:06, 43.59it/s][A
 69%|██████▉   | 596/861 [00:13<00:06, 43.63it/s][A
 70%|██████▉   | 601/861 [00:13<00:05, 43.82it/s][A
 70%|███████   | 606/861 [00:14<00:05, 44.09it/s][A
 71%|███████   | 611/861 [00:14<00:05, 44.22it/s][A
 72%|███████▏  | 616/861 [00:14<00:05, 44.34it/s][A
 72%|███████▏  | 621/861 [00:14<00:05, 43.42it/s][A
 73%|███████▎  | 626/861 [00:14<00:05, 43.57it/s][A
 73%|███████▎  | 631/861 [00:14<00:05, 43.58it/s][A
 74%|███████▍  | 636/861 [00:14<00:05, 43.57it/s][A
 74%|███████▍  | 641/861 [00:14<00:05, 43.56it/s][A
 75%|███████▌  | 646/861 [00:14<00:04, 43.72it/s][A
 76%|███████▌  | 651/861 [00:15<00:04, 44.02it/s][A
 76%|███████▌  | 656/861 [00:15<00:04, 44.19it/s][A
 77%|███████▋  | 661/861 [00:15<00:04, 44.27it/s][A
 77%|███████▋  | 666/861 [00:15<00:04, 44.31it/s][A
 78%|███████▊  | 671/861 [00:15<00:04, 44.19it/s][A
 79%|███████▊  | 676/861 [00:15<00:04, 44.01it/s][A
 79%|███████▉  | 681/861 [00:15<00:04, 43.82it/s][A
 80%|███████▉  | 686/861 [00:15<00:03, 43.86it/s][A
 80%|████████  | 691/861 [00:15<00:03, 43.96it/s][A
 81%|████████  | 696/861 [00:16<00:03, 44.18it/s][A
 81%|████████▏ | 701/861 [00:16<00:03, 44.19it/s][A
 82%|████████▏ | 706/861 [00:16<00:03, 44.30it/s][A
 83%|████████▎ | 711/861 [00:16<00:03, 44.27it/s][A
 83%|████████▎ | 716/861 [00:16<00:03, 44.11it/s][A
 84%|████████▎ | 721/861 [00:16<00:03, 43.93it/s][A
 84%|████████▍ | 726/861 [00:16<00:03, 43.92it/s][A
 85%|████████▍ | 731/861 [00:16<00:02, 44.03it/s][A
 85%|████████▌ | 736/861 [00:17<00:02, 44.04it/s][A
 86%|████████▌ | 741/861 [00:17<00:02, 44.14it/s][A
 87%|████████▋ | 746/861 [00:17<00:02, 44.22it/s][A
 87%|████████▋ | 751/861 [00:17<00:02, 44.28it/s][A
 88%|████████▊ | 756/861 [00:17<00:02, 43.99it/s][A
 88%|████████▊ | 761/861 [00:17<00:02, 43.95it/s][A
 89%|████████▉ | 766/861 [00:17<00:02, 43.89it/s][A
 90%|████████▉ | 771/861 [00:17<00:02, 43.90it/s][A
 90%|█████████ | 776/861 [00:17<00:01, 43.93it/s][A
 91%|█████████ | 781/861 [00:18<00:01, 44.03it/s][A
 91%|█████████▏| 786/861 [00:18<00:01, 44.11it/s][A
 92%|█████████▏| 791/861 [00:18<00:01, 44.28it/s][A
 92%|█████████▏| 796/861 [00:18<00:01, 44.22it/s][A
 93%|█████████▎| 801/861 [00:18<00:01, 44.15it/s][A
 94%|█████████▎| 806/861 [00:18<00:01, 44.08it/s][A
 94%|█████████▍| 811/861 [00:18<00:01, 44.00it/s][A
 95%|█████████▍| 816/861 [00:18<00:01, 43.94it/s][A
 95%|█████████▌| 821/861 [00:18<00:00, 43.98it/s][A
 96%|█████████▌| 826/861 [00:19<00:00, 44.13it/s][A
 97%|█████████▋| 831/861 [00:19<00:00, 44.20it/s][A
 97%|█████████▋| 836/861 [00:19<00:00, 44.17it/s][A
 98%|█████████▊| 841/861 [00:19<00:00, 44.17it/s][A
 98%|█████████▊| 846/861 [00:19<00:00, 44.08it/s][A
 99%|█████████▉| 851/861 [00:19<00:00, 44.10it/s][A
 99%|█████████▉| 856/861 [00:19<00:00, 43.90it/s][A
100%|██████████| 861/861 [00:19<00:00, 43.92it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.92it/s][A100%|██████████| 410/410 [03:57<00:00,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:33:45,641 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 06:33:45,812 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:33:49,166 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:33:49,324 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:33:49,397 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:33:50,351 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:33:50,351 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82 (score: 1.0493918657302856).
                                                 100%|██████████| 410/410 [04:10<00:00,  3.57it/s]100%|██████████| 410/410 [04:10<00:00,  1.64it/s]
[INFO|trainer.py:1894] 2023-08-28 06:33:58,671 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 06:33:58,842 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:34:01,457 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:34:01,546 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:34:01,592 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:34:02,008 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:02,009 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:02,009 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:02,009 >>   train_runtime            = 0:04:10.20
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:02,009 >>   train_samples            =       5239
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:02,009 >>   train_samples_per_second =    104.694
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:02,009 >>   train_steps_per_second   =      1.639
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.8607, 'eval_samples_per_second': 346.614, 'eval_steps_per_second': 43.352, 'epoch': 5.0}
{'train_runtime': 250.2055, 'train_samples_per_second': 104.694, 'train_steps_per_second': 1.639, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:34:02 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:34:02,176 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:34:02,176 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 06:34:02,176 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 55.39it/s]  1%|▏         | 12/861 [00:00<00:17, 48.48it/s]  2%|▏         | 17/861 [00:00<00:17, 47.16it/s]  3%|▎         | 22/861 [00:00<00:18, 46.38it/s]  3%|▎         | 27/861 [00:00<00:18, 45.99it/s]  4%|▎         | 32/861 [00:00<00:18, 45.51it/s]  4%|▍         | 37/861 [00:00<00:18, 45.35it/s]  5%|▍         | 42/861 [00:00<00:18, 44.85it/s]  5%|▌         | 47/861 [00:01<00:18, 44.35it/s]  6%|▌         | 52/861 [00:01<00:18, 44.04it/s]  7%|▋         | 57/861 [00:01<00:18, 44.06it/s]  7%|▋         | 62/861 [00:01<00:18, 44.28it/s]  8%|▊         | 67/861 [00:01<00:17, 44.52it/s]  8%|▊         | 72/861 [00:01<00:17, 44.66it/s]  9%|▉         | 77/861 [00:01<00:17, 44.69it/s] 10%|▉         | 82/861 [00:01<00:17, 44.69it/s] 10%|█         | 87/861 [00:01<00:17, 44.44it/s] 11%|█         | 92/861 [00:02<00:17, 44.02it/s] 11%|█▏        | 97/861 [00:02<00:17, 43.94it/s] 12%|█▏        | 102/861 [00:02<00:17, 44.02it/s] 12%|█▏        | 107/861 [00:02<00:17, 44.21it/s] 13%|█▎        | 112/861 [00:02<00:16, 44.43it/s] 14%|█▎        | 117/861 [00:02<00:17, 42.25it/s] 14%|█▍        | 122/861 [00:02<00:17, 42.98it/s] 15%|█▍        | 127/861 [00:02<00:16, 43.52it/s] 15%|█▌        | 132/861 [00:02<00:16, 43.70it/s] 16%|█▌        | 137/861 [00:03<00:16, 43.65it/s] 16%|█▋        | 142/861 [00:03<00:16, 43.67it/s] 17%|█▋        | 147/861 [00:03<00:16, 43.85it/s] 18%|█▊        | 152/861 [00:03<00:16, 44.11it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.09it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.23it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.43it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.42it/s] 21%|██        | 177/861 [00:03<00:15, 44.32it/s] 21%|██        | 182/861 [00:04<00:15, 44.15it/s] 22%|██▏       | 187/861 [00:04<00:15, 44.16it/s] 22%|██▏       | 192/861 [00:04<00:15, 44.13it/s] 23%|██▎       | 197/861 [00:04<00:15, 44.24it/s] 23%|██▎       | 202/861 [00:04<00:14, 44.31it/s] 24%|██▍       | 207/861 [00:04<00:14, 44.28it/s] 25%|██▍       | 212/861 [00:04<00:14, 44.51it/s] 25%|██▌       | 217/861 [00:04<00:14, 44.50it/s] 26%|██▌       | 222/861 [00:04<00:14, 44.38it/s] 26%|██▋       | 227/861 [00:05<00:14, 44.24it/s] 27%|██▋       | 232/861 [00:05<00:14, 44.15it/s] 28%|██▊       | 237/861 [00:05<00:14, 44.17it/s] 28%|██▊       | 242/861 [00:05<00:13, 44.24it/s] 29%|██▊       | 247/861 [00:05<00:13, 44.30it/s] 29%|██▉       | 252/861 [00:05<00:13, 44.27it/s] 30%|██▉       | 257/861 [00:05<00:13, 44.42it/s] 30%|███       | 262/861 [00:05<00:13, 44.46it/s] 31%|███       | 267/861 [00:06<00:13, 44.34it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.24it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.23it/s] 33%|███▎      | 282/861 [00:06<00:13, 44.19it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.24it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.18it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.32it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.47it/s] 36%|███▌      | 307/861 [00:06<00:12, 44.54it/s] 36%|███▌      | 312/861 [00:07<00:12, 44.31it/s] 37%|███▋      | 317/861 [00:07<00:12, 44.14it/s] 37%|███▋      | 322/861 [00:07<00:12, 44.21it/s] 38%|███▊      | 327/861 [00:07<00:12, 44.26it/s] 39%|███▊      | 332/861 [00:07<00:11, 44.21it/s] 39%|███▉      | 337/861 [00:07<00:11, 44.17it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.33it/s] 40%|████      | 347/861 [00:07<00:11, 44.39it/s] 41%|████      | 352/861 [00:07<00:11, 44.41it/s] 41%|████▏     | 357/861 [00:08<00:11, 44.29it/s] 42%|████▏     | 362/861 [00:08<00:11, 44.21it/s] 43%|████▎     | 367/861 [00:08<00:11, 44.16it/s] 43%|████▎     | 372/861 [00:08<00:11, 44.22it/s] 44%|████▍     | 377/861 [00:08<00:10, 44.20it/s] 44%|████▍     | 382/861 [00:08<00:11, 43.34it/s] 45%|████▍     | 387/861 [00:08<00:10, 43.73it/s] 46%|████▌     | 392/861 [00:08<00:10, 44.02it/s] 46%|████▌     | 397/861 [00:08<00:10, 44.05it/s] 47%|████▋     | 402/861 [00:09<00:10, 44.06it/s] 47%|████▋     | 407/861 [00:09<00:10, 44.08it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.17it/s] 48%|████▊     | 417/861 [00:09<00:10, 44.13it/s] 49%|████▉     | 422/861 [00:09<00:09, 44.01it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.19it/s] 50%|█████     | 432/861 [00:09<00:09, 44.39it/s] 51%|█████     | 437/861 [00:09<00:09, 44.39it/s] 51%|█████▏    | 442/861 [00:09<00:09, 44.41it/s] 52%|█████▏    | 447/861 [00:10<00:09, 44.28it/s] 52%|█████▏    | 452/861 [00:10<00:09, 44.23it/s] 53%|█████▎    | 457/861 [00:10<00:09, 44.20it/s] 54%|█████▎    | 462/861 [00:10<00:09, 44.10it/s] 54%|█████▍    | 467/861 [00:10<00:08, 44.01it/s] 55%|█████▍    | 472/861 [00:10<00:08, 44.21it/s] 55%|█████▌    | 477/861 [00:10<00:08, 44.38it/s] 56%|█████▌    | 482/861 [00:10<00:08, 44.44it/s] 57%|█████▋    | 487/861 [00:10<00:08, 44.47it/s] 57%|█████▋    | 492/861 [00:11<00:08, 44.24it/s] 58%|█████▊    | 497/861 [00:11<00:08, 44.30it/s] 58%|█████▊    | 502/861 [00:11<00:08, 44.18it/s] 59%|█████▉    | 507/861 [00:11<00:08, 44.13it/s] 59%|█████▉    | 512/861 [00:11<00:07, 44.19it/s] 60%|██████    | 517/861 [00:11<00:08, 41.62it/s] 61%|██████    | 522/861 [00:11<00:07, 42.60it/s] 61%|██████    | 527/861 [00:11<00:07, 43.29it/s] 62%|██████▏   | 532/861 [00:12<00:07, 43.60it/s] 62%|██████▏   | 537/861 [00:12<00:07, 43.81it/s] 63%|██████▎   | 542/861 [00:12<00:07, 43.88it/s] 64%|██████▎   | 547/861 [00:12<00:07, 43.87it/s] 64%|██████▍   | 552/861 [00:12<00:07, 43.86it/s] 65%|██████▍   | 557/861 [00:12<00:06, 43.68it/s] 65%|██████▌   | 562/861 [00:12<00:06, 43.93it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.16it/s] 66%|██████▋   | 572/861 [00:12<00:06, 44.27it/s] 67%|██████▋   | 577/861 [00:13<00:06, 44.51it/s] 68%|██████▊   | 582/861 [00:13<00:06, 44.37it/s] 68%|██████▊   | 587/861 [00:13<00:06, 44.24it/s] 69%|██████▉   | 592/861 [00:13<00:06, 44.14it/s] 69%|██████▉   | 597/861 [00:13<00:06, 43.85it/s] 70%|██████▉   | 602/861 [00:13<00:05, 43.94it/s] 70%|███████   | 607/861 [00:13<00:05, 44.06it/s] 71%|███████   | 612/861 [00:13<00:05, 44.19it/s] 72%|███████▏  | 617/861 [00:13<00:05, 44.32it/s] 72%|███████▏  | 622/861 [00:14<00:05, 44.49it/s] 73%|███████▎  | 627/861 [00:14<00:05, 44.42it/s] 73%|███████▎  | 632/861 [00:14<00:05, 44.28it/s] 74%|███████▍  | 637/861 [00:14<00:05, 44.14it/s] 75%|███████▍  | 642/861 [00:14<00:04, 44.05it/s] 75%|███████▌  | 647/861 [00:14<00:04, 44.15it/s] 76%|███████▌  | 652/861 [00:14<00:05, 39.23it/s] 76%|███████▋  | 657/861 [00:14<00:05, 40.78it/s] 77%|███████▋  | 662/861 [00:15<00:04, 41.95it/s] 77%|███████▋  | 667/861 [00:15<00:04, 42.77it/s] 78%|███████▊  | 672/861 [00:15<00:04, 43.36it/s] 79%|███████▊  | 677/861 [00:15<00:04, 43.76it/s] 79%|███████▉  | 682/861 [00:15<00:04, 44.00it/s] 80%|███████▉  | 687/861 [00:15<00:03, 43.94it/s] 80%|████████  | 692/861 [00:15<00:03, 43.65it/s] 81%|████████  | 697/861 [00:15<00:03, 43.52it/s] 82%|████████▏ | 702/861 [00:15<00:03, 43.75it/s] 82%|████████▏ | 707/861 [00:16<00:03, 44.09it/s] 83%|████████▎ | 712/861 [00:16<00:03, 44.30it/s] 83%|████████▎ | 717/861 [00:16<00:03, 44.48it/s] 84%|████████▍ | 722/861 [00:16<00:03, 44.53it/s] 84%|████████▍ | 727/861 [00:16<00:03, 44.47it/s] 85%|████████▌ | 732/861 [00:16<00:02, 44.28it/s] 86%|████████▌ | 737/861 [00:16<00:02, 43.88it/s] 86%|████████▌ | 742/861 [00:16<00:02, 43.77it/s] 87%|████████▋ | 747/861 [00:16<00:02, 43.92it/s] 87%|████████▋ | 752/861 [00:17<00:02, 44.18it/s] 88%|████████▊ | 757/861 [00:17<00:02, 44.42it/s] 89%|████████▊ | 762/861 [00:17<00:02, 44.48it/s] 89%|████████▉ | 767/861 [00:17<00:02, 44.65it/s] 90%|████████▉ | 772/861 [00:17<00:01, 44.51it/s] 90%|█████████ | 777/861 [00:17<00:01, 44.26it/s] 91%|█████████ | 782/861 [00:17<00:01, 43.94it/s] 91%|█████████▏| 787/861 [00:17<00:01, 40.96it/s] 92%|█████████▏| 792/861 [00:17<00:01, 42.05it/s] 93%|█████████▎| 797/861 [00:18<00:01, 42.86it/s] 93%|█████████▎| 802/861 [00:18<00:01, 43.39it/s] 94%|█████████▎| 807/861 [00:18<00:01, 43.80it/s] 94%|█████████▍| 812/861 [00:18<00:01, 44.10it/s] 95%|█████████▍| 817/861 [00:18<00:00, 44.12it/s] 95%|█████████▌| 822/861 [00:18<00:00, 43.85it/s] 96%|█████████▌| 827/861 [00:18<00:00, 43.65it/s] 97%|█████████▋| 832/861 [00:18<00:00, 43.74it/s] 97%|█████████▋| 837/861 [00:18<00:00, 43.92it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.13it/s] 98%|█████████▊| 847/861 [00:19<00:00, 44.34it/s] 99%|█████████▉| 852/861 [00:19<00:00, 44.51it/s]100%|█████████▉| 857/861 [00:19<00:00, 44.53it/s]100%|██████████| 861/861 [00:19<00:00, 44.07it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:34:21,730 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:21,730 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:21,730 >>   eval_loss               =     1.0494
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:21,730 >>   eval_runtime            = 0:00:19.55
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:21,730 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:21,730 >>   eval_samples_per_second =     352.05
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:21,730 >>   eval_steps_per_second   =     44.032
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:34:21,730 >>   perplexity              =     2.8559
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:34,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:34,803 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:34,803 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:34,803 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:34,803 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:34:35,676 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:34:35,677 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:34:36,307 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:34:37,435 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:34:37,435 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:40,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:40,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:40,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:40,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:34:40,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:34:41,628 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:34:41,629 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:34:42,247 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:34:42,464 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:34:42,465 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.59it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.65it/s]Extractor Predicting: 14it [00:08,  1.72it/s]Extractor Predicting: 15it [00:09,  1.74it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:11,  1.70it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:12,  1.67it/s]Extractor Predicting: 21it [00:13,  1.64it/s]Extractor Predicting: 22it [00:13,  1.70it/s]Extractor Predicting: 23it [00:14,  1.68it/s]Extractor Predicting: 24it [00:14,  1.71it/s]Extractor Predicting: 25it [00:15,  1.71it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.68it/s]Extractor Predicting: 29it [00:17,  1.68it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:18,  1.63it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:20,  1.68it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:21,  1.66it/s]Extractor Predicting: 36it [00:21,  1.65it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:24,  1.67it/s]Extractor Predicting: 41it [00:24,  1.68it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.70it/s]Extractor Predicting: 44it [00:26,  1.69it/s]Extractor Predicting: 45it [00:27,  1.71it/s]Extractor Predicting: 46it [00:27,  1.73it/s]Extractor Predicting: 47it [00:28,  1.55it/s]Extractor Predicting: 48it [00:29,  1.60it/s]Extractor Predicting: 49it [00:29,  1.59it/s]Extractor Predicting: 50it [00:30,  1.62it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:31,  1.61it/s]Extractor Predicting: 53it [00:32,  1.59it/s]Extractor Predicting: 54it [00:33,  1.58it/s]Extractor Predicting: 55it [00:33,  1.59it/s]Extractor Predicting: 56it [00:34,  1.61it/s]Extractor Predicting: 57it [00:34,  1.65it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:36,  1.58it/s]Extractor Predicting: 60it [00:36,  1.58it/s]Extractor Predicting: 61it [00:37,  1.63it/s]Extractor Predicting: 62it [00:37,  1.61it/s]Extractor Predicting: 63it [00:38,  1.60it/s]Extractor Predicting: 64it [00:39,  1.57it/s]Extractor Predicting: 65it [00:39,  1.61it/s]Extractor Predicting: 66it [00:40,  1.62it/s]Extractor Predicting: 67it [00:41,  1.67it/s]Extractor Predicting: 68it [00:41,  1.67it/s]Extractor Predicting: 69it [00:42,  1.64it/s]Extractor Predicting: 70it [00:42,  1.62it/s]Extractor Predicting: 71it [00:43,  1.62it/s]Extractor Predicting: 72it [00:44,  1.62it/s]Extractor Predicting: 73it [00:44,  1.64it/s]Extractor Predicting: 74it [00:45,  1.66it/s]Extractor Predicting: 75it [00:45,  1.69it/s]Extractor Predicting: 76it [00:46,  1.65it/s]Extractor Predicting: 77it [00:47,  1.60it/s]Extractor Predicting: 78it [00:47,  1.63it/s]Extractor Predicting: 79it [00:48,  1.68it/s]Extractor Predicting: 80it [00:48,  1.71it/s]Extractor Predicting: 81it [00:49,  1.74it/s]Extractor Predicting: 82it [00:49,  1.81it/s]Extractor Predicting: 83it [00:50,  1.75it/s]Extractor Predicting: 84it [00:51,  1.76it/s]Extractor Predicting: 85it [00:51,  1.78it/s]Extractor Predicting: 86it [00:52,  1.80it/s]Extractor Predicting: 87it [00:52,  1.77it/s]Extractor Predicting: 88it [00:53,  1.78it/s]Extractor Predicting: 89it [00:53,  1.74it/s]Extractor Predicting: 90it [00:54,  1.79it/s]Extractor Predicting: 91it [00:54,  1.80it/s]Extractor Predicting: 92it [00:55,  1.83it/s]Extractor Predicting: 93it [00:56,  1.80it/s]Extractor Predicting: 94it [00:56,  1.82it/s]Extractor Predicting: 95it [00:57,  1.84it/s]Extractor Predicting: 96it [00:57,  1.80it/s]Extractor Predicting: 97it [00:58,  1.85it/s]Extractor Predicting: 98it [00:58,  1.84it/s]Extractor Predicting: 99it [00:59,  1.85it/s]Extractor Predicting: 100it [00:59,  1.84it/s]Extractor Predicting: 101it [01:00,  1.79it/s]Extractor Predicting: 102it [01:01,  1.78it/s]Extractor Predicting: 103it [01:01,  1.80it/s]Extractor Predicting: 104it [01:02,  1.80it/s]Extractor Predicting: 105it [01:02,  1.81it/s]Extractor Predicting: 106it [01:03,  1.83it/s]Extractor Predicting: 107it [01:03,  1.77it/s]Extractor Predicting: 108it [01:04,  1.84it/s]Extractor Predicting: 109it [01:04,  1.83it/s]Extractor Predicting: 110it [01:05,  1.86it/s]Extractor Predicting: 111it [01:05,  1.86it/s]Extractor Predicting: 112it [01:06,  1.81it/s]Extractor Predicting: 113it [01:07,  1.79it/s]Extractor Predicting: 114it [01:07,  1.76it/s]Extractor Predicting: 115it [01:08,  1.82it/s]Extractor Predicting: 116it [01:08,  1.79it/s]Extractor Predicting: 117it [01:09,  1.77it/s]Extractor Predicting: 118it [01:09,  1.78it/s]Extractor Predicting: 119it [01:10,  1.79it/s]Extractor Predicting: 120it [01:10,  1.83it/s]Extractor Predicting: 121it [01:11,  1.82it/s]Extractor Predicting: 122it [01:12,  1.76it/s]Extractor Predicting: 123it [01:12,  1.82it/s]Extractor Predicting: 124it [01:13,  1.78it/s]Extractor Predicting: 125it [01:13,  1.81it/s]Extractor Predicting: 126it [01:14,  1.83it/s]Extractor Predicting: 127it [01:14,  1.85it/s]Extractor Predicting: 128it [01:15,  1.77it/s]Extractor Predicting: 129it [01:15,  1.82it/s]Extractor Predicting: 130it [01:16,  1.84it/s]Extractor Predicting: 131it [01:17,  1.85it/s]Extractor Predicting: 132it [01:17,  1.83it/s]Extractor Predicting: 133it [01:18,  1.83it/s]Extractor Predicting: 134it [01:18,  1.82it/s]Extractor Predicting: 135it [01:19,  1.83it/s]Extractor Predicting: 136it [01:19,  1.84it/s]Extractor Predicting: 137it [01:20,  1.82it/s]Extractor Predicting: 138it [01:20,  1.77it/s]Extractor Predicting: 139it [01:21,  1.74it/s]Extractor Predicting: 140it [01:22,  1.56it/s]Extractor Predicting: 141it [01:22,  1.63it/s]Extractor Predicting: 142it [01:23,  1.65it/s]Extractor Predicting: 143it [01:24,  1.69it/s]Extractor Predicting: 144it [01:24,  1.68it/s]Extractor Predicting: 145it [01:25,  1.72it/s]Extractor Predicting: 146it [01:25,  1.76it/s]Extractor Predicting: 147it [01:26,  1.76it/s]Extractor Predicting: 148it [01:26,  1.77it/s]Extractor Predicting: 149it [01:27,  1.81it/s]Extractor Predicting: 150it [01:27,  1.73it/s]Extractor Predicting: 151it [01:28,  1.67it/s]Extractor Predicting: 152it [01:29,  1.62it/s]Extractor Predicting: 153it [01:29,  1.59it/s]Extractor Predicting: 154it [01:30,  1.48it/s]Extractor Predicting: 155it [01:31,  1.52it/s]Extractor Predicting: 156it [01:32,  1.49it/s]Extractor Predicting: 157it [01:32,  1.52it/s]Extractor Predicting: 158it [01:33,  1.52it/s]Extractor Predicting: 159it [01:34,  1.48it/s]Extractor Predicting: 160it [01:34,  1.47it/s]Extractor Predicting: 161it [01:35,  1.51it/s]Extractor Predicting: 162it [01:36,  1.47it/s]Extractor Predicting: 163it [01:36,  1.48it/s]Extractor Predicting: 164it [01:37,  1.44it/s]Extractor Predicting: 165it [01:38,  1.41it/s]Extractor Predicting: 166it [01:38,  1.43it/s]Extractor Predicting: 167it [01:39,  1.45it/s]Extractor Predicting: 168it [01:40,  1.46it/s]Extractor Predicting: 169it [01:41,  1.42it/s]Extractor Predicting: 170it [01:41,  1.44it/s]Extractor Predicting: 171it [01:42,  1.47it/s]Extractor Predicting: 172it [01:43,  1.45it/s]Extractor Predicting: 173it [01:43,  1.48it/s]Extractor Predicting: 174it [01:44,  1.49it/s]Extractor Predicting: 175it [01:45,  1.49it/s]Extractor Predicting: 176it [01:45,  1.48it/s]Extractor Predicting: 177it [01:46,  1.50it/s]Extractor Predicting: 178it [01:47,  1.50it/s]Extractor Predicting: 179it [01:47,  1.49it/s]Extractor Predicting: 180it [01:48,  1.45it/s]Extractor Predicting: 181it [01:49,  1.45it/s]Extractor Predicting: 182it [01:49,  1.47it/s]Extractor Predicting: 183it [01:50,  1.50it/s]Extractor Predicting: 184it [01:51,  1.52it/s]Extractor Predicting: 185it [01:51,  1.52it/s]Extractor Predicting: 186it [01:52,  1.54it/s]Extractor Predicting: 187it [01:53,  1.51it/s]Extractor Predicting: 188it [01:53,  1.55it/s]Extractor Predicting: 189it [01:54,  1.49it/s]Extractor Predicting: 190it [01:55,  1.47it/s]Extractor Predicting: 191it [01:55,  1.45it/s]Extractor Predicting: 192it [01:56,  1.41it/s]Extractor Predicting: 193it [01:57,  1.46it/s]Extractor Predicting: 194it [01:57,  1.50it/s]Extractor Predicting: 195it [01:58,  1.50it/s]Extractor Predicting: 196it [01:59,  1.53it/s]Extractor Predicting: 197it [01:59,  1.55it/s]Extractor Predicting: 198it [02:00,  1.57it/s]Extractor Predicting: 199it [02:00,  1.58it/s]Extractor Predicting: 200it [02:01,  1.58it/s]Extractor Predicting: 201it [02:02,  1.52it/s]Extractor Predicting: 202it [02:02,  1.54it/s]Extractor Predicting: 203it [02:03,  1.57it/s]Extractor Predicting: 204it [02:04,  1.56it/s]Extractor Predicting: 205it [02:04,  1.60it/s]Extractor Predicting: 206it [02:05,  1.57it/s]Extractor Predicting: 207it [02:06,  1.59it/s]Extractor Predicting: 208it [02:06,  1.60it/s]Extractor Predicting: 209it [02:07,  1.65it/s]Extractor Predicting: 210it [02:07,  1.55it/s]Extractor Predicting: 211it [02:08,  1.51it/s]Extractor Predicting: 212it [02:09,  1.55it/s]Extractor Predicting: 213it [02:09,  1.57it/s]Extractor Predicting: 214it [02:10,  1.60it/s]Extractor Predicting: 215it [02:11,  1.62it/s]Extractor Predicting: 216it [02:11,  1.63it/s]Extractor Predicting: 217it [02:12,  1.61it/s]Extractor Predicting: 218it [02:12,  1.66it/s]Extractor Predicting: 219it [02:13,  1.60it/s]Extractor Predicting: 220it [02:14,  1.57it/s]Extractor Predicting: 221it [02:14,  1.60it/s]Extractor Predicting: 222it [02:15,  1.61it/s]Extractor Predicting: 223it [02:16,  1.64it/s]Extractor Predicting: 224it [02:16,  1.46it/s]Extractor Predicting: 225it [02:17,  1.52it/s]Extractor Predicting: 226it [02:18,  1.54it/s]Extractor Predicting: 227it [02:18,  1.55it/s]Extractor Predicting: 228it [02:19,  1.55it/s]Extractor Predicting: 229it [02:19,  1.58it/s]Extractor Predicting: 230it [02:20,  1.59it/s]Extractor Predicting: 231it [02:21,  1.63it/s]Extractor Predicting: 232it [02:21,  1.65it/s]Extractor Predicting: 233it [02:22,  1.60it/s]Extractor Predicting: 234it [02:23,  1.60it/s]Extractor Predicting: 235it [02:23,  1.58it/s]Extractor Predicting: 236it [02:24,  1.60it/s]Extractor Predicting: 237it [02:25,  1.54it/s]Extractor Predicting: 237it [02:25,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:20,587 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:20,614 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:20,614 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:20,614 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:20,615 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:37:21,424 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:37:21,425 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:37:22,106 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:37:23,247 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:37:23,247 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:26,306 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:26,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:26,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:26,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:37:26,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:37:27,224 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:37:27,225 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:37:27,875 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:37:28,089 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:37:28,089 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.68it/s]Extractor Predicting: 6it [00:03,  1.76it/s]Extractor Predicting: 7it [00:04,  1.84it/s]Extractor Predicting: 8it [00:04,  1.83it/s]Extractor Predicting: 9it [00:05,  1.81it/s]Extractor Predicting: 10it [00:05,  1.82it/s]Extractor Predicting: 11it [00:06,  1.81it/s]Extractor Predicting: 12it [00:06,  1.82it/s]Extractor Predicting: 13it [00:07,  1.81it/s]Extractor Predicting: 14it [00:08,  1.74it/s]Extractor Predicting: 15it [00:08,  1.74it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:09,  1.74it/s]Extractor Predicting: 18it [00:10,  1.77it/s]Extractor Predicting: 19it [00:10,  1.78it/s]Extractor Predicting: 20it [00:11,  1.77it/s]Extractor Predicting: 21it [00:11,  1.81it/s]Extractor Predicting: 22it [00:12,  1.83it/s]Extractor Predicting: 23it [00:13,  1.87it/s]Extractor Predicting: 24it [00:13,  1.90it/s]Extractor Predicting: 25it [00:14,  1.89it/s]Extractor Predicting: 26it [00:14,  1.87it/s]Extractor Predicting: 27it [00:15,  1.85it/s]Extractor Predicting: 28it [00:15,  1.85it/s]Extractor Predicting: 29it [00:16,  1.82it/s]Extractor Predicting: 30it [00:16,  1.81it/s]Extractor Predicting: 31it [00:17,  1.82it/s]Extractor Predicting: 32it [00:17,  1.81it/s]Extractor Predicting: 33it [00:18,  1.77it/s]Extractor Predicting: 34it [00:19,  1.75it/s]Extractor Predicting: 35it [00:19,  1.76it/s]Extractor Predicting: 36it [00:20,  1.78it/s]Extractor Predicting: 37it [00:20,  1.85it/s]Extractor Predicting: 38it [00:21,  1.86it/s]Extractor Predicting: 39it [00:21,  1.81it/s]Extractor Predicting: 40it [00:22,  1.77it/s]Extractor Predicting: 41it [00:23,  1.70it/s]Extractor Predicting: 42it [00:23,  1.70it/s]Extractor Predicting: 43it [00:24,  1.72it/s]Extractor Predicting: 44it [00:24,  1.68it/s]Extractor Predicting: 45it [00:25,  1.69it/s]Extractor Predicting: 46it [00:26,  1.64it/s]Extractor Predicting: 47it [00:26,  1.65it/s]Extractor Predicting: 48it [00:27,  1.68it/s]Extractor Predicting: 49it [00:27,  1.66it/s]Extractor Predicting: 50it [00:28,  1.67it/s]Extractor Predicting: 51it [00:29,  1.69it/s]Extractor Predicting: 52it [00:29,  1.69it/s]Extractor Predicting: 53it [00:30,  1.70it/s]Extractor Predicting: 54it [00:30,  1.70it/s]Extractor Predicting: 55it [00:31,  1.69it/s]Extractor Predicting: 56it [00:31,  1.72it/s]Extractor Predicting: 57it [00:32,  1.69it/s]Extractor Predicting: 58it [00:33,  1.72it/s]Extractor Predicting: 59it [00:33,  1.70it/s]Extractor Predicting: 60it [00:34,  1.72it/s]Extractor Predicting: 61it [00:34,  1.68it/s]Extractor Predicting: 62it [00:35,  1.68it/s]Extractor Predicting: 63it [00:36,  1.69it/s]Extractor Predicting: 64it [00:36,  1.74it/s]Extractor Predicting: 65it [00:37,  1.74it/s]Extractor Predicting: 66it [00:37,  1.74it/s]Extractor Predicting: 67it [00:38,  1.74it/s]Extractor Predicting: 68it [00:38,  1.72it/s]Extractor Predicting: 69it [00:39,  1.73it/s]Extractor Predicting: 70it [00:40,  1.71it/s]Extractor Predicting: 71it [00:40,  1.73it/s]Extractor Predicting: 72it [00:41,  1.73it/s]Extractor Predicting: 73it [00:41,  1.70it/s]Extractor Predicting: 74it [00:42,  1.73it/s]Extractor Predicting: 75it [00:43,  1.73it/s]Extractor Predicting: 76it [00:43,  1.71it/s]Extractor Predicting: 77it [00:44,  1.71it/s]Extractor Predicting: 78it [00:44,  1.72it/s]Extractor Predicting: 79it [00:45,  1.74it/s]Extractor Predicting: 80it [00:45,  1.75it/s]Extractor Predicting: 81it [00:46,  1.73it/s]Extractor Predicting: 82it [00:47,  1.68it/s]Extractor Predicting: 83it [00:47,  1.74it/s]Extractor Predicting: 84it [00:48,  1.72it/s]Extractor Predicting: 85it [00:48,  1.70it/s]Extractor Predicting: 86it [00:49,  1.54it/s]Extractor Predicting: 87it [00:50,  1.61it/s]Extractor Predicting: 88it [00:50,  1.61it/s]Extractor Predicting: 89it [00:51,  1.62it/s]Extractor Predicting: 90it [00:52,  1.65it/s]Extractor Predicting: 91it [00:52,  1.58it/s]Extractor Predicting: 92it [00:53,  1.64it/s]Extractor Predicting: 93it [00:53,  1.70it/s]Extractor Predicting: 94it [00:54,  1.65it/s]Extractor Predicting: 95it [00:55,  1.65it/s]Extractor Predicting: 96it [00:55,  1.66it/s]Extractor Predicting: 97it [00:56,  1.66it/s]Extractor Predicting: 98it [00:56,  1.68it/s]Extractor Predicting: 99it [00:57,  1.71it/s]Extractor Predicting: 100it [00:58,  1.60it/s]Extractor Predicting: 101it [00:58,  1.65it/s]Extractor Predicting: 102it [00:59,  1.64it/s]Extractor Predicting: 103it [00:59,  1.68it/s]Extractor Predicting: 104it [01:00,  1.72it/s]Extractor Predicting: 105it [01:00,  1.71it/s]Extractor Predicting: 106it [01:01,  1.75it/s]Extractor Predicting: 107it [01:02,  1.75it/s]Extractor Predicting: 108it [01:02,  1.78it/s]Extractor Predicting: 109it [01:03,  1.73it/s]Extractor Predicting: 110it [01:03,  1.74it/s]Extractor Predicting: 111it [01:04,  1.72it/s]Extractor Predicting: 112it [01:04,  1.71it/s]Extractor Predicting: 113it [01:05,  1.76it/s]Extractor Predicting: 114it [01:06,  1.70it/s]Extractor Predicting: 115it [01:06,  1.68it/s]Extractor Predicting: 116it [01:07,  1.72it/s]Extractor Predicting: 117it [01:07,  1.72it/s]Extractor Predicting: 118it [01:08,  1.76it/s]Extractor Predicting: 119it [01:09,  1.74it/s]Extractor Predicting: 120it [01:09,  1.71it/s]Extractor Predicting: 121it [01:10,  1.65it/s]Extractor Predicting: 122it [01:10,  1.63it/s]Extractor Predicting: 123it [01:11,  1.67it/s]Extractor Predicting: 124it [01:12,  1.71it/s]Extractor Predicting: 125it [01:12,  1.70it/s]Extractor Predicting: 126it [01:13,  1.72it/s]Extractor Predicting: 127it [01:13,  1.70it/s]Extractor Predicting: 128it [01:14,  1.76it/s]Extractor Predicting: 129it [01:14,  1.76it/s]Extractor Predicting: 130it [01:15,  1.78it/s]Extractor Predicting: 131it [01:16,  1.75it/s]Extractor Predicting: 132it [01:16,  1.71it/s]Extractor Predicting: 133it [01:17,  1.70it/s]Extractor Predicting: 134it [01:17,  1.74it/s]Extractor Predicting: 135it [01:18,  1.74it/s]Extractor Predicting: 136it [01:18,  1.75it/s]Extractor Predicting: 137it [01:19,  1.72it/s]Extractor Predicting: 138it [01:20,  1.70it/s]Extractor Predicting: 139it [01:20,  1.68it/s]Extractor Predicting: 140it [01:21,  1.75it/s]Extractor Predicting: 141it [01:21,  1.74it/s]Extractor Predicting: 142it [01:22,  1.70it/s]Extractor Predicting: 143it [01:23,  1.65it/s]Extractor Predicting: 144it [01:23,  1.66it/s]Extractor Predicting: 145it [01:24,  1.69it/s]Extractor Predicting: 146it [01:24,  1.68it/s]Extractor Predicting: 147it [01:25,  1.69it/s]Extractor Predicting: 148it [01:26,  1.63it/s]Extractor Predicting: 149it [01:26,  1.60it/s]Extractor Predicting: 150it [01:27,  1.63it/s]Extractor Predicting: 151it [01:27,  1.66it/s]Extractor Predicting: 152it [01:28,  1.66it/s]Extractor Predicting: 153it [01:29,  1.68it/s]Extractor Predicting: 154it [01:29,  1.65it/s]Extractor Predicting: 155it [01:30,  1.65it/s]Extractor Predicting: 156it [01:30,  1.67it/s]Extractor Predicting: 157it [01:31,  1.64it/s]Extractor Predicting: 158it [01:32,  1.62it/s]Extractor Predicting: 159it [01:32,  1.62it/s]Extractor Predicting: 160it [01:33,  1.64it/s]Extractor Predicting: 161it [01:34,  1.66it/s]Extractor Predicting: 162it [01:34,  1.64it/s]Extractor Predicting: 163it [01:35,  1.60it/s]Extractor Predicting: 164it [01:35,  1.61it/s]Extractor Predicting: 165it [01:36,  1.65it/s]Extractor Predicting: 166it [01:37,  1.60it/s]Extractor Predicting: 167it [01:37,  1.63it/s]Extractor Predicting: 168it [01:38,  1.76it/s]Extractor Predicting: 168it [01:38,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:17,071 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:17,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:17,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:17,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:17,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:39:17,731 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:39:17,732 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:39:18,327 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:39:19,378 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:39:19,378 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:22,303 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:22,306 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:22,306 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:22,306 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:39:22,306 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:39:22,975 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:39:23,109 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:39:23,688 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:39:23,847 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:39:23,847 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.67it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.72it/s]Extractor Predicting: 8it [00:04,  1.74it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.68it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:10,  1.59it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 19it [00:11,  1.66it/s]
[INFO|configuration_utils.py:515] 2023-08-28 06:39:37,722 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:39:37,723 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:39:37,782 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:39:37,783 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 06:39:37,801 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:39:46,822 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 06:39:46,845 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 06:39:46,934 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:39:46,935 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:39:46,994 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:47,027 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:47,027 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:47,027 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:47,027 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:47,027 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:47,027 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 06:39:47,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:48,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:48,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:49,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:50,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:50,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:51,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:52,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:52,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:53,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:54,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:54,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:55,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:56,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:57,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:57,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:58,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:59,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:39:59,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:00,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:01,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:10, 14.50s/it][WARNING|generation_utils.py:914] 2023-08-28 06:40:01,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:02,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:03,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:03,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:04,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:05,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:05,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:06,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:07,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:07,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:08,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:09,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:09,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:10,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:11,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:11,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:12,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:13,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:13,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:14,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:15,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:15,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:16,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:17,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:18,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:18,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:19,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:20,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:16, 17.08s/it][WARNING|generation_utils.py:914] 2023-08-28 06:40:20,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:21,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:22,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:22,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:23,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:24,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:25,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:25,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:26,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:27,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:27,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:28,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:29,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:29,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:30,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:31,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:31,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:32,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:32,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:33,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:34,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:34,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:35,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:49<01:55, 16.47s/it][WARNING|generation_utils.py:914] 2023-08-28 06:40:36,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:37,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:37,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:38,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:39,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:39,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:40,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:41,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:41,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:42,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:42,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:43,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:44,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:44,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:45,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:45,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:46,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:47,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:47,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:48,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:49,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:49,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:50,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:03<01:33, 15.65s/it][WARNING|generation_utils.py:914] 2023-08-28 06:40:50,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:51,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:52,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:52,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:53,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:54,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:54,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:55,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:56,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:56,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:57,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:58,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:58,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:40:59,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:00,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:00,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:01,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:02,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:02,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:03,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:03,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:04,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:05,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:18<01:17, 15.41s/it][WARNING|generation_utils.py:914] 2023-08-28 06:41:05,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:06,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:07,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:07,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:08,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:08,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:09,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:10,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:10,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:11,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:12,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:12,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:13,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:13,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:14,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:15,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:15,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:16,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:17,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:17,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:18,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:19,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:19,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:32<01:00, 15.07s/it][WARNING|generation_utils.py:914] 2023-08-28 06:41:20,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:20,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:21,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:22,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:22,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:23,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:23,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:24,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:25,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:26,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:26,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:27,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:28,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:28,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:29,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:30,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:30,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:31,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:32,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:32,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:33,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:34,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:34,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:35,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:35,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:49<00:46, 15.48s/it][WARNING|generation_utils.py:914] 2023-08-28 06:41:36,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:37,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:38,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:38,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:39,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:40,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:40,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:41,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:42,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:42,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:43,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:44,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:44,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:45,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:46,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:46,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:47,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:48,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:48,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:49,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:50,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:50,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:04<00:30, 15.26s/it][WARNING|generation_utils.py:914] 2023-08-28 06:41:51,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:52,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:52,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:53,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:53,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:54,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:54,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:55,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:55,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:56,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:57,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:57,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:58,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:58,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:41:59,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:00,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:00,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:01,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:01,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:02,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:02,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:03,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:04,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:05,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:18<00:14, 14.93s/it][WARNING|generation_utils.py:914] 2023-08-28 06:42:05,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:06,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:06,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:07,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:08,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:09,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:09,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:10,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:10,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:11,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:12,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:13,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:13,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:14,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:15,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:15,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:16,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:17,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:18,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:19,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:19,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:20,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:42:21,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:34<00:00, 15.31s/it]Generating: 100%|██████████| 10/10 [02:34<00:00, 15.44s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:29,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:29,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:29,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:29,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:29,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:42:30,486 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:42:30,487 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:42:31,156 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:42:32,349 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:42:32,349 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:35,516 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:35,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:35,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:35,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:42:35,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:42:36,537 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:42:36,539 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:42:37,164 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:42:37,424 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:42:37,424 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9166666666666666, 'errors': {'', "('', 'composer', 'Jérémie Léger', 'The title track of the album was a track Iced Tea made by Jérémie Léger and written by Pierre Bousquet and Bernard Barrou , among others .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 397, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 443, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 489, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 555, 'raw': 800}
{'target': 600, 'success': 574, 'raw': 832}
{'target': 600, 'success': 593, 'raw': 864}
{'target': 600, 'success': 614, 'raw': 896}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.6852678571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : opposite of . Context : Later in the year ( 1143 ) , Ptolemaeus , the founder of the Macedonian Republic , married Ariadne , daughter of Ariadne , founder of Macedonian kings , Thebes and Ileani . Head Entity : Ileani , Tail Entity : Ileanius .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8491847826086957, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : creator .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.75375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Lothar', 'twinned administrative body', '', 'He died in the Battle of Mervyn at Rheinmetall in 645 along with his daughter Lothar , D.')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 12521
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12621, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.44it/s]Extractor Estimating: 3it [00:02,  1.50it/s]Extractor Estimating: 4it [00:02,  1.53it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.54it/s]Extractor Estimating: 7it [00:04,  1.60it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.56it/s]Extractor Estimating: 10it [00:06,  1.50it/s]Extractor Estimating: 11it [00:07,  1.47it/s]Extractor Estimating: 12it [00:07,  1.48it/s]Extractor Estimating: 13it [00:08,  1.53it/s]Extractor Estimating: 14it [00:09,  1.54it/s]Extractor Estimating: 15it [00:09,  1.56it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:11,  1.45it/s]Extractor Estimating: 18it [00:11,  1.47it/s]Extractor Estimating: 19it [00:12,  1.48it/s]Extractor Estimating: 20it [00:13,  1.51it/s]Extractor Estimating: 21it [00:13,  1.49it/s]Extractor Estimating: 22it [00:14,  1.54it/s]Extractor Estimating: 23it [00:15,  1.57it/s]Extractor Estimating: 24it [00:15,  1.51it/s]Extractor Estimating: 25it [00:16,  1.53it/s]Extractor Estimating: 26it [00:17,  1.54it/s]Extractor Estimating: 27it [00:17,  1.54it/s]Extractor Estimating: 28it [00:18,  1.53it/s]Extractor Estimating: 29it [00:18,  1.56it/s]Extractor Estimating: 30it [00:19,  1.42it/s]Extractor Estimating: 31it [00:20,  1.48it/s]Extractor Estimating: 32it [00:21,  1.49it/s]Extractor Estimating: 33it [00:21,  1.47it/s]Extractor Estimating: 34it [00:22,  1.50it/s]Extractor Estimating: 35it [00:23,  1.48it/s]Extractor Estimating: 36it [00:23,  1.50it/s]Extractor Estimating: 37it [00:24,  1.46it/s]Extractor Estimating: 38it [00:25,  1.49it/s]Extractor Estimating: 39it [00:25,  1.48it/s]Extractor Estimating: 40it [00:26,  1.50it/s]Extractor Estimating: 41it [00:27,  1.53it/s]Extractor Estimating: 42it [00:27,  1.51it/s]Extractor Estimating: 43it [00:28,  1.51it/s]Extractor Estimating: 44it [00:29,  1.52it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:30,  1.52it/s]Extractor Estimating: 47it [00:31,  1.52it/s]Extractor Estimating: 48it [00:31,  1.50it/s]Extractor Estimating: 49it [00:32,  1.45it/s]Extractor Estimating: 50it [00:33,  1.48it/s]Extractor Estimating: 51it [00:33,  1.50it/s]Extractor Estimating: 52it [00:34,  1.52it/s]Extractor Estimating: 53it [00:35,  1.49it/s]Extractor Estimating: 54it [00:35,  1.49it/s]Extractor Estimating: 55it [00:36,  1.49it/s]Extractor Estimating: 56it [00:37,  1.52it/s]Extractor Estimating: 57it [00:37,  1.50it/s]Extractor Estimating: 58it [00:38,  1.48it/s]Extractor Estimating: 59it [00:39,  1.48it/s]Extractor Estimating: 60it [00:39,  1.54it/s]Extractor Estimating: 61it [00:41,  1.06it/s]Extractor Estimating: 62it [00:42,  1.16it/s]Extractor Estimating: 63it [00:42,  1.25it/s]Extractor Estimating: 64it [00:43,  1.30it/s]Extractor Estimating: 65it [00:44,  1.37it/s]Extractor Estimating: 66it [00:44,  1.47it/s]Extractor Estimating: 67it [00:45,  1.47it/s]Extractor Estimating: 68it [00:45,  1.56it/s]Extractor Estimating: 69it [00:46,  1.59it/s]Extractor Estimating: 70it [00:46,  1.66it/s]Extractor Estimating: 71it [00:47,  1.66it/s]Extractor Estimating: 72it [00:48,  1.61it/s]Extractor Estimating: 73it [00:48,  1.64it/s]Extractor Estimating: 74it [00:49,  1.62it/s]Extractor Estimating: 75it [00:50,  1.56it/s]Extractor Estimating: 76it [00:50,  1.58it/s]Extractor Estimating: 77it [00:51,  1.45it/s]Extractor Estimating: 78it [00:52,  1.47it/s]Extractor Estimating: 79it [00:52,  1.51it/s]Extractor Estimating: 80it [00:53,  1.44it/s]Extractor Estimating: 81it [00:54,  1.46it/s]Extractor Estimating: 82it [00:54,  1.49it/s]Extractor Estimating: 83it [00:55,  1.48it/s]Extractor Estimating: 84it [00:56,  1.54it/s]Extractor Estimating: 85it [00:56,  1.56it/s]Extractor Estimating: 86it [00:57,  1.58it/s]Extractor Estimating: 87it [00:58,  1.59it/s]Extractor Estimating: 88it [00:58,  1.62it/s]Extractor Estimating: 89it [00:59,  1.59it/s]Extractor Estimating: 90it [00:59,  1.57it/s]Extractor Estimating: 91it [01:00,  1.57it/s]Extractor Estimating: 92it [01:01,  1.56it/s]Extractor Estimating: 93it [01:01,  1.59it/s]Extractor Estimating: 94it [01:02,  1.60it/s]Extractor Estimating: 95it [01:03,  1.62it/s]Extractor Estimating: 96it [01:03,  1.63it/s]Extractor Estimating: 97it [01:04,  1.67it/s]Extractor Estimating: 98it [01:04,  1.67it/s]Extractor Estimating: 99it [01:05,  1.68it/s]Extractor Estimating: 100it [01:06,  1.67it/s]Extractor Estimating: 101it [01:06,  1.59it/s]Extractor Estimating: 102it [01:07,  1.59it/s]Extractor Estimating: 103it [01:08,  1.55it/s]Extractor Estimating: 104it [01:08,  1.55it/s]Extractor Estimating: 105it [01:09,  1.59it/s]Extractor Estimating: 106it [01:09,  1.56it/s]Extractor Estimating: 107it [01:10,  1.59it/s]Extractor Estimating: 108it [01:11,  1.59it/s]Extractor Estimating: 109it [01:11,  1.61it/s]Extractor Estimating: 110it [01:12,  1.64it/s]Extractor Estimating: 111it [01:13,  1.59it/s]Extractor Estimating: 112it [01:13,  1.63it/s]Extractor Estimating: 113it [01:14,  1.61it/s]Extractor Estimating: 114it [01:14,  1.58it/s]Extractor Estimating: 115it [01:15,  1.58it/s]Extractor Estimating: 116it [01:16,  1.61it/s]Extractor Estimating: 117it [01:16,  1.60it/s]Extractor Estimating: 118it [01:17,  1.55it/s]Extractor Estimating: 119it [01:18,  1.55it/s]Extractor Estimating: 120it [01:18,  1.57it/s]Extractor Estimating: 121it [01:19,  1.58it/s]Extractor Estimating: 122it [01:19,  1.60it/s]Extractor Estimating: 123it [01:20,  1.60it/s]Extractor Estimating: 124it [01:21,  1.60it/s]Extractor Estimating: 125it [01:21,  1.58it/s]Extractor Estimating: 126it [01:22,  1.55it/s]Extractor Estimating: 127it [01:23,  1.59it/s]Extractor Estimating: 128it [01:23,  1.61it/s]Extractor Estimating: 129it [01:24,  1.58it/s]Extractor Estimating: 130it [01:24,  1.64it/s]Extractor Estimating: 131it [01:25,  1.57it/s]Extractor Estimating: 132it [01:26,  1.55it/s]Extractor Estimating: 133it [01:26,  1.57it/s]Extractor Estimating: 134it [01:27,  1.61it/s]Extractor Estimating: 135it [01:28,  1.61it/s]Extractor Estimating: 136it [01:28,  1.59it/s]Extractor Estimating: 137it [01:29,  1.57it/s]Extractor Estimating: 138it [01:30,  1.59it/s]Extractor Estimating: 139it [01:30,  1.60it/s]Extractor Estimating: 140it [01:31,  1.60it/s]Extractor Estimating: 141it [01:31,  1.57it/s]Extractor Estimating: 142it [01:32,  1.56it/s]Extractor Estimating: 143it [01:33,  1.57it/s]Extractor Estimating: 144it [01:33,  1.55it/s]Extractor Estimating: 145it [01:34,  1.55it/s]Extractor Estimating: 146it [01:35,  1.55it/s]Extractor Estimating: 147it [01:35,  1.56it/s]Extractor Estimating: 148it [01:36,  1.57it/s]Extractor Estimating: 149it [01:37,  1.47it/s]Extractor Estimating: 150it [01:37,  1.50it/s]Extractor Estimating: 151it [01:38,  1.55it/s]Extractor Estimating: 152it [01:39,  1.52it/s]Extractor Estimating: 153it [01:39,  1.54it/s]Extractor Estimating: 154it [01:40,  1.52it/s]Extractor Estimating: 155it [01:41,  1.52it/s]Extractor Estimating: 156it [01:41,  1.52it/s]Extractor Estimating: 157it [01:42,  1.54it/s]Extractor Estimating: 158it [01:43,  1.55it/s]Extractor Estimating: 159it [01:43,  1.53it/s]Extractor Estimating: 160it [01:44,  1.53it/s]Extractor Estimating: 161it [01:45,  1.52it/s]Extractor Estimating: 162it [01:45,  1.57it/s]Extractor Estimating: 163it [01:46,  1.53it/s]Extractor Estimating: 164it [01:47,  1.50it/s]Extractor Estimating: 165it [01:47,  1.50it/s]Extractor Estimating: 166it [01:48,  1.54it/s]Extractor Estimating: 167it [01:48,  1.56it/s]Extractor Estimating: 168it [01:49,  1.61it/s]Extractor Estimating: 169it [01:50,  1.63it/s]Extractor Estimating: 170it [01:50,  1.53it/s]Extractor Estimating: 171it [01:51,  1.52it/s]Extractor Estimating: 172it [01:52,  1.55it/s]Extractor Estimating: 173it [01:52,  1.57it/s]Extractor Estimating: 174it [01:53,  1.60it/s]Extractor Estimating: 175it [01:54,  1.55it/s]Extractor Estimating: 176it [01:54,  1.57it/s]Extractor Estimating: 177it [01:55,  1.49it/s]Extractor Estimating: 178it [01:56,  1.51it/s]Extractor Estimating: 179it [01:56,  1.52it/s]Extractor Estimating: 180it [01:57,  1.51it/s]Extractor Estimating: 181it [01:57,  1.56it/s]Extractor Estimating: 182it [01:58,  1.57it/s]Extractor Estimating: 183it [01:59,  1.59it/s]Extractor Estimating: 184it [01:59,  1.60it/s]Extractor Estimating: 185it [02:00,  1.55it/s]Extractor Estimating: 186it [02:01,  1.54it/s]Extractor Estimating: 187it [02:01,  1.58it/s]Extractor Estimating: 188it [02:02,  1.56it/s]Extractor Estimating: 189it [02:03,  1.54it/s]Extractor Estimating: 190it [02:03,  1.57it/s]Extractor Estimating: 191it [02:04,  1.60it/s]Extractor Estimating: 192it [02:04,  1.58it/s]Extractor Estimating: 193it [02:05,  1.54it/s]Extractor Estimating: 194it [02:06,  1.51it/s]Extractor Estimating: 195it [02:06,  1.53it/s]Extractor Estimating: 196it [02:07,  1.53it/s]Extractor Estimating: 197it [02:08,  1.55it/s]Extractor Estimating: 198it [02:08,  1.53it/s]Extractor Estimating: 199it [02:09,  1.55it/s]Extractor Estimating: 200it [02:10,  1.55it/s]Extractor Estimating: 201it [02:10,  1.58it/s]Extractor Estimating: 202it [02:11,  1.60it/s]Extractor Estimating: 203it [02:12,  1.59it/s]Extractor Estimating: 204it [02:12,  1.60it/s]Extractor Estimating: 205it [02:13,  1.60it/s]Extractor Estimating: 206it [02:13,  1.62it/s]Extractor Estimating: 207it [02:14,  1.64it/s]Extractor Estimating: 208it [02:15,  1.62it/s]Extractor Estimating: 209it [02:15,  1.62it/s]Extractor Estimating: 210it [02:16,  1.63it/s]Extractor Estimating: 211it [02:16,  1.64it/s]Extractor Estimating: 212it [02:17,  1.64it/s]Extractor Estimating: 213it [02:18,  1.65it/s]Extractor Estimating: 214it [02:18,  1.69it/s]Extractor Estimating: 215it [02:19,  1.71it/s]Extractor Estimating: 216it [02:19,  1.70it/s]Extractor Estimating: 217it [02:20,  1.67it/s]Extractor Estimating: 218it [02:21,  1.70it/s]Extractor Estimating: 219it [02:21,  1.65it/s]Extractor Estimating: 220it [02:22,  1.67it/s]Extractor Estimating: 221it [02:22,  1.66it/s]Extractor Estimating: 222it [02:23,  1.60it/s]Extractor Estimating: 223it [02:24,  1.60it/s]Extractor Estimating: 224it [02:24,  1.51it/s]Extractor Estimating: 225it [02:25,  1.54it/s]Extractor Estimating: 226it [02:26,  1.56it/s]Extractor Estimating: 227it [02:26,  1.55it/s]Extractor Estimating: 228it [02:27,  1.56it/s]Extractor Estimating: 229it [02:28,  1.52it/s]Extractor Estimating: 230it [02:28,  1.60it/s]Extractor Estimating: 231it [02:29,  1.55it/s]Extractor Estimating: 232it [02:29,  1.58it/s]Extractor Estimating: 233it [02:30,  1.47it/s]Extractor Estimating: 234it [02:31,  1.53it/s]Extractor Estimating: 235it [02:32,  1.52it/s]Extractor Estimating: 236it [02:32,  1.53it/s]Extractor Estimating: 237it [02:33,  1.52it/s]Extractor Estimating: 238it [02:33,  1.58it/s]Extractor Estimating: 239it [02:34,  1.55it/s]Extractor Estimating: 240it [02:35,  1.53it/s]Extractor Estimating: 241it [02:35,  1.57it/s]Extractor Estimating: 242it [02:36,  1.59it/s]Extractor Estimating: 243it [02:37,  1.58it/s]Extractor Estimating: 244it [02:38,  1.38it/s]Extractor Estimating: 245it [02:38,  1.46it/s]Extractor Estimating: 246it [02:39,  1.45it/s]Extractor Estimating: 247it [02:39,  1.50it/s]Extractor Estimating: 248it [02:40,  1.57it/s]Extractor Estimating: 249it [02:41,  1.53it/s]Extractor Estimating: 250it [02:41,  1.64it/s]Extractor Estimating: 250it [02:41,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:42,777 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:42,810 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:42,810 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:42,810 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:42,810 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:45:43,458 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:45:43,459 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:45:44,039 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:45:45,085 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:45:45,086 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:48,144 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:48,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:48,169 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:48,169 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:48,169 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:45:48,811 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:45:48,813 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:45:49,403 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:45:49,566 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:45:49,566 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 08:26:47,783 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 08:26:47,974 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5093 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 23039
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23139, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23139, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.163, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.070, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.089, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.071, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.081, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.883, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.073, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.075, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.068, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.084, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.864, loss:nan
g_step 1200, step 135, avg_time 1.081, loss:nan
g_step 1300, step 22, avg_time 1.080, loss:nan
g_step 1400, step 122, avg_time 1.084, loss:nan
g_step 1500, step 9, avg_time 1.075, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.887, loss:nan
g_step 1700, step 209, avg_time 1.081, loss:nan
g_step 1800, step 96, avg_time 1.090, loss:nan
g_step 1900, step 196, avg_time 1.072, loss:nan
g_step 2000, step 83, avg_time 1.074, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.892, loss:nan
g_step 2200, step 70, avg_time 1.061, loss:nan
g_step 2300, step 170, avg_time 1.092, loss:nan
g_step 2400, step 57, avg_time 1.069, loss:nan
g_step 2500, step 157, avg_time 1.090, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.879, loss:nan
g_step 2700, step 144, avg_time 1.081, loss:nan
g_step 2800, step 31, avg_time 1.095, loss:nan
g_step 2900, step 131, avg_time 1.096, loss:nan
g_step 3000, step 18, avg_time 1.067, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.887, loss:nan
g_step 3200, step 5, avg_time 1.083, loss:nan
g_step 3300, step 105, avg_time 1.091, loss:nan
g_step 3400, step 205, avg_time 1.082, loss:nan
g_step 3500, step 92, avg_time 1.075, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.887, loss:nan
g_step 3700, step 79, avg_time 1.085, loss:nan
g_step 3800, step 179, avg_time 1.080, loss:nan
g_step 3900, step 66, avg_time 1.079, loss:nan
g_step 4000, step 166, avg_time 1.095, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.876, loss:nan
g_step 4200, step 153, avg_time 1.091, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 08:26:47 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 08:26:47 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_08-26-47_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 08:26:49 - WARNING - datasets.builder -   Using custom data configuration default-be2c35f2808dd329
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-be2c35f2808dd329/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 08:26:50,645 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:26:50,646 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 08:26:50,647 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:26:50,648 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 08:26:50,735 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:26:50,793 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:26:50,824 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:26:50,824 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:26:50,824 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:26:50,824 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:26:50,824 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 08:26:51,205 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 08:26:54,342 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 08:26:54,366 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-be2c35f2808dd329/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.70ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.61ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.05ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.45ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.79ba/s]100%|██████████| 6/6 [00:01<00:00,  4.31ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.99ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.69ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.01ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  4.18ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.27ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.34ba/s]100%|██████████| 7/7 [00:01<00:00,  4.31ba/s]100%|██████████| 7/7 [00:01<00:00,  4.14ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  4.58ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.08ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.40ba/s]100%|██████████| 6/6 [00:00<00:00, 10.15ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.85ba/s] 43%|████▎     | 3/7 [00:00<00:00,  8.22ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.54ba/s]100%|██████████| 7/7 [00:00<00:00, 10.35ba/s]100%|██████████| 7/7 [00:00<00:00,  9.47ba/s]
[INFO|trainer.py:414] 2023-08-28 08:27:00,149 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 08:27:00,263 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 08:27:00,263 >>   Num examples = 5100
[INFO|trainer.py:1149] 2023-08-28 08:27:00,263 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 08:27:00,264 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 08:27:00,264 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 08:27:00,264 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 08:27:00,264 >>   Total optimization steps = 400
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<01:54,  3.49it/s]  0%|          | 2/400 [00:00<02:03,  3.22it/s]  1%|          | 3/400 [00:00<01:57,  3.39it/s]  1%|          | 4/400 [00:01<01:54,  3.45it/s]  1%|▏         | 5/400 [00:01<01:53,  3.49it/s]  2%|▏         | 6/400 [00:01<01:52,  3.51it/s]  2%|▏         | 7/400 [00:02<01:51,  3.53it/s]  2%|▏         | 8/400 [00:02<01:50,  3.54it/s]  2%|▏         | 9/400 [00:02<01:50,  3.54it/s]  2%|▎         | 10/400 [00:02<01:49,  3.55it/s]  3%|▎         | 11/400 [00:03<01:49,  3.55it/s]  3%|▎         | 12/400 [00:03<01:48,  3.56it/s]  3%|▎         | 13/400 [00:03<01:54,  3.39it/s]  4%|▎         | 14/400 [00:04<01:52,  3.43it/s]  4%|▍         | 15/400 [00:04<01:50,  3.47it/s]  4%|▍         | 16/400 [00:04<01:49,  3.49it/s]  4%|▍         | 17/400 [00:04<01:49,  3.51it/s]  4%|▍         | 18/400 [00:05<01:48,  3.52it/s]  5%|▍         | 19/400 [00:05<01:47,  3.54it/s]  5%|▌         | 20/400 [00:05<01:47,  3.54it/s]  5%|▌         | 21/400 [00:05<01:46,  3.55it/s]  6%|▌         | 22/400 [00:06<01:46,  3.55it/s]  6%|▌         | 23/400 [00:06<01:45,  3.56it/s]  6%|▌         | 24/400 [00:06<01:47,  3.48it/s]  6%|▋         | 25/400 [00:07<01:47,  3.50it/s]  6%|▋         | 26/400 [00:07<01:46,  3.51it/s]  7%|▋         | 27/400 [00:07<01:45,  3.52it/s]  7%|▋         | 28/400 [00:07<01:45,  3.53it/s]  7%|▋         | 29/400 [00:08<01:44,  3.53it/s]  8%|▊         | 30/400 [00:08<01:44,  3.54it/s]  8%|▊         | 31/400 [00:08<01:44,  3.54it/s]  8%|▊         | 32/400 [00:09<01:43,  3.55it/s]  8%|▊         | 33/400 [00:09<01:43,  3.55it/s]  8%|▊         | 34/400 [00:09<01:43,  3.55it/s]  9%|▉         | 35/400 [00:09<01:45,  3.47it/s]  9%|▉         | 36/400 [00:10<01:44,  3.49it/s]  9%|▉         | 37/400 [00:10<01:43,  3.51it/s] 10%|▉         | 38/400 [00:10<01:42,  3.52it/s] 10%|▉         | 39/400 [00:11<01:42,  3.53it/s] 10%|█         | 40/400 [00:11<01:41,  3.53it/s] 10%|█         | 41/400 [00:11<01:41,  3.54it/s] 10%|█         | 42/400 [00:11<01:41,  3.53it/s] 11%|█         | 43/400 [00:12<01:40,  3.54it/s] 11%|█         | 44/400 [00:12<01:40,  3.54it/s] 11%|█▏        | 45/400 [00:12<01:40,  3.55it/s] 12%|█▏        | 46/400 [00:13<01:42,  3.45it/s] 12%|█▏        | 47/400 [00:13<01:41,  3.48it/s] 12%|█▏        | 48/400 [00:13<01:40,  3.50it/s] 12%|█▏        | 49/400 [00:13<01:39,  3.51it/s] 12%|█▎        | 50/400 [00:14<01:39,  3.53it/s] 13%|█▎        | 51/400 [00:14<01:38,  3.53it/s] 13%|█▎        | 52/400 [00:14<01:38,  3.54it/s] 13%|█▎        | 53/400 [00:15<01:37,  3.54it/s] 14%|█▎        | 54/400 [00:15<01:37,  3.55it/s] 14%|█▍        | 55/400 [00:15<01:37,  3.55it/s] 14%|█▍        | 56/400 [00:15<01:36,  3.55it/s] 14%|█▍        | 57/400 [00:16<01:40,  3.40it/s] 14%|█▍        | 58/400 [00:16<01:39,  3.44it/s] 15%|█▍        | 59/400 [00:16<01:38,  3.47it/s] 15%|█▌        | 60/400 [00:17<01:37,  3.49it/s] 15%|█▌        | 61/400 [00:17<01:36,  3.51it/s] 16%|█▌        | 62/400 [00:17<01:35,  3.53it/s] 16%|█▌        | 63/400 [00:17<01:35,  3.54it/s] 16%|█▌        | 64/400 [00:18<01:34,  3.54it/s] 16%|█▋        | 65/400 [00:18<01:34,  3.54it/s] 16%|█▋        | 66/400 [00:18<01:34,  3.55it/s] 17%|█▋        | 67/400 [00:19<01:33,  3.54it/s] 17%|█▋        | 68/400 [00:19<01:33,  3.54it/s] 17%|█▋        | 69/400 [00:19<01:33,  3.54it/s] 18%|█▊        | 70/400 [00:19<01:33,  3.55it/s] 18%|█▊        | 71/400 [00:20<01:32,  3.55it/s] 18%|█▊        | 72/400 [00:20<01:32,  3.55it/s] 18%|█▊        | 73/400 [00:20<01:32,  3.55it/s] 18%|█▊        | 74/400 [00:21<01:31,  3.55it/s] 19%|█▉        | 75/400 [00:21<01:33,  3.46it/s] 19%|█▉        | 76/400 [00:21<01:32,  3.48it/s] 19%|█▉        | 77/400 [00:21<01:32,  3.50it/s] 20%|█▉        | 78/400 [00:22<01:31,  3.51it/s] 20%|█▉        | 79/400 [00:22<01:31,  3.52it/s] 20%|██        | 80/400 [00:22<01:22,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 08:27:22,939 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:27:22,939 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 08:27:22,939 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.42it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.58it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.20it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.27it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.56it/s][A
  4%|▎         | 32/861 [00:00<00:18, 45.00it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.48it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.23it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.35it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.39it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.45it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.69it/s][A
  8%|▊         | 67/861 [00:01<00:18, 43.40it/s][A
  8%|▊         | 72/861 [00:01<00:17, 43.88it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.00it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.98it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.83it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.06it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.20it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.42it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.55it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.61it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.59it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.59it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.40it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.29it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.30it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.41it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.55it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.53it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.53it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.54it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.56it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.46it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.29it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.25it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.47it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.46it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.60it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 43.64it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 43.95it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.04it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.12it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.13it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.08it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.27it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.47it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.38it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.49it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.49it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.42it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.31it/s][A
 31%|███       | 267/861 [00:05<00:13, 44.27it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.27it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.17it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.45it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.44it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.54it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.62it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.50it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.42it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.36it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.31it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.39it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.42it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.44it/s][A
 39%|███▉      | 337/861 [00:07<00:12, 42.45it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.27it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.65it/s][A
 41%|████      | 352/861 [00:07<00:11, 43.88it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.94it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.10it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.21it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.24it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.01it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.24it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.40it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.50it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.42it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.42it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.36it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.35it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.36it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.16it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.31it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.50it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.49it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.52it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.46it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.37it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.40it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.33it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.26it/s][A
 55%|█████▍    | 472/861 [00:10<00:09, 42.60it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.16it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 43.67it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.02it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.24it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.40it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.25it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.19it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 43.94it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.13it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.29it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.41it/s][A
 62%|██████▏   | 532/861 [00:11<00:07, 44.56it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.55it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.53it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.45it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.27it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.03it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.21it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.29it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.43it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.60it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.58it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.52it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.42it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.22it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 43.97it/s][A
 70%|███████   | 607/861 [00:13<00:05, 43.73it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.11it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.26it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.47it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.51it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.42it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.37it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.17it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.02it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.10it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.29it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.50it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.68it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.58it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.54it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.26it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.23it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.08it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.16it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.28it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.50it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.70it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.74it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.53it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.30it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.12it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.02it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.26it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.74it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 44.06it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.30it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.44it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.47it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.26it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.07it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 43.97it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.07it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.14it/s][A
 93%|█████████▎| 797/861 [00:17<00:01, 44.31it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.44it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.59it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.62it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.46it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.17it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.07it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.19it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.28it/s][A
 98%|█████████▊| 842/861 [00:18<00:00, 44.37it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.53it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.61it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.59it/s][A                                                
                                                 [A 20%|██        | 80/400 [00:42<01:22,  3.86it/s]
100%|██████████| 861/861 [00:19<00:00, 44.59it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:27:42,456 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-28 08:27:42,555 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:27:45,147 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:27:45,239 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:27:45,286 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80/special_tokens_map.json
 20%|██        | 81/400 [00:46<38:38,  7.27s/it] 20%|██        | 82/400 [00:46<27:24,  5.17s/it] 21%|██        | 83/400 [00:46<19:34,  3.70s/it] 21%|██        | 84/400 [00:47<14:06,  2.68s/it] 21%|██▏       | 85/400 [00:47<10:17,  1.96s/it] 22%|██▏       | 86/400 [00:47<08:01,  1.53s/it] 22%|██▏       | 87/400 [00:48<06:04,  1.17s/it] 22%|██▏       | 88/400 [00:48<04:40,  1.11it/s] 22%|██▏       | 89/400 [00:48<03:42,  1.40it/s] 22%|██▎       | 90/400 [00:49<03:01,  1.71it/s] 23%|██▎       | 91/400 [00:49<02:32,  2.03it/s] 23%|██▎       | 92/400 [00:49<02:12,  2.33it/s] 23%|██▎       | 93/400 [00:49<01:58,  2.60it/s] 24%|██▎       | 94/400 [00:50<01:48,  2.82it/s] 24%|██▍       | 95/400 [00:50<01:41,  3.01it/s] 24%|██▍       | 96/400 [00:50<01:36,  3.15it/s] 24%|██▍       | 97/400 [00:51<01:32,  3.26it/s] 24%|██▍       | 98/400 [00:51<01:30,  3.34it/s] 25%|██▍       | 99/400 [00:51<01:28,  3.40it/s] 25%|██▌       | 100/400 [00:51<01:27,  3.44it/s] 25%|██▌       | 101/400 [00:52<01:27,  3.43it/s] 26%|██▌       | 102/400 [00:52<01:26,  3.46it/s] 26%|██▌       | 103/400 [00:52<01:25,  3.49it/s] 26%|██▌       | 104/400 [00:53<01:24,  3.51it/s] 26%|██▋       | 105/400 [00:53<01:23,  3.52it/s] 26%|██▋       | 106/400 [00:53<01:23,  3.53it/s] 27%|██▋       | 107/400 [00:53<01:22,  3.54it/s] 27%|██▋       | 108/400 [00:54<01:22,  3.54it/s] 27%|██▋       | 109/400 [00:54<01:22,  3.54it/s] 28%|██▊       | 110/400 [00:54<01:21,  3.54it/s] 28%|██▊       | 111/400 [00:55<01:21,  3.55it/s] 28%|██▊       | 112/400 [00:55<01:22,  3.49it/s] 28%|██▊       | 113/400 [00:55<01:21,  3.52it/s] 28%|██▊       | 114/400 [00:55<01:20,  3.54it/s] 29%|██▉       | 115/400 [00:56<01:20,  3.56it/s] 29%|██▉       | 116/400 [00:56<01:19,  3.57it/s] 29%|██▉       | 117/400 [00:56<01:18,  3.58it/s] 30%|██▉       | 118/400 [00:56<01:18,  3.59it/s] 30%|██▉       | 119/400 [00:57<01:18,  3.60it/s] 30%|███       | 120/400 [00:57<01:17,  3.60it/s] 30%|███       | 121/400 [00:57<01:17,  3.60it/s] 30%|███       | 122/400 [00:58<01:17,  3.60it/s] 31%|███       | 123/400 [00:58<01:21,  3.41it/s] 31%|███       | 124/400 [00:58<01:19,  3.47it/s] 31%|███▏      | 125/400 [00:58<01:18,  3.51it/s] 32%|███▏      | 126/400 [00:59<01:17,  3.53it/s] 32%|███▏      | 127/400 [00:59<01:16,  3.55it/s] 32%|███▏      | 128/400 [00:59<01:16,  3.57it/s] 32%|███▏      | 129/400 [01:00<01:15,  3.58it/s] 32%|███▎      | 130/400 [01:00<01:15,  3.59it/s] 33%|███▎      | 131/400 [01:00<01:14,  3.59it/s] 33%|███▎      | 132/400 [01:00<01:14,  3.60it/s] 33%|███▎      | 133/400 [01:01<01:14,  3.60it/s] 34%|███▎      | 134/400 [01:01<01:18,  3.40it/s] 34%|███▍      | 135/400 [01:01<01:16,  3.46it/s] 34%|███▍      | 136/400 [01:02<01:15,  3.50it/s] 34%|███▍      | 137/400 [01:02<01:14,  3.53it/s] 34%|███▍      | 138/400 [01:02<01:13,  3.55it/s] 35%|███▍      | 139/400 [01:02<01:13,  3.57it/s] 35%|███▌      | 140/400 [01:03<01:12,  3.58it/s] 35%|███▌      | 141/400 [01:03<01:12,  3.59it/s] 36%|███▌      | 142/400 [01:03<01:11,  3.59it/s] 36%|███▌      | 143/400 [01:04<01:11,  3.59it/s] 36%|███▌      | 144/400 [01:04<01:11,  3.60it/s] 36%|███▋      | 145/400 [01:04<01:13,  3.47it/s] 36%|███▋      | 146/400 [01:04<01:12,  3.51it/s] 37%|███▋      | 147/400 [01:05<01:11,  3.53it/s] 37%|███▋      | 148/400 [01:05<01:10,  3.55it/s] 37%|███▋      | 149/400 [01:05<01:10,  3.57it/s] 38%|███▊      | 150/400 [01:06<01:09,  3.58it/s] 38%|███▊      | 151/400 [01:06<01:09,  3.59it/s] 38%|███▊      | 152/400 [01:06<01:09,  3.59it/s] 38%|███▊      | 153/400 [01:06<01:08,  3.59it/s] 38%|███▊      | 154/400 [01:07<01:08,  3.60it/s] 39%|███▉      | 155/400 [01:07<01:08,  3.60it/s] 39%|███▉      | 156/400 [01:07<01:10,  3.46it/s] 39%|███▉      | 157/400 [01:08<01:09,  3.50it/s] 40%|███▉      | 158/400 [01:08<01:08,  3.53it/s] 40%|███▉      | 159/400 [01:08<01:07,  3.56it/s] 40%|████      | 160/400 [01:08<01:01,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 08:28:09,024 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:28:09,024 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 08:28:09,024 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.4406, 'eval_samples_per_second': 354.104, 'eval_steps_per_second': 44.289, 'epoch': 1.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.77it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.68it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.12it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.85it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.11it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.66it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.54it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.24it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.28it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.45it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.55it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.70it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.53it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.48it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.26it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.90it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.90it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.11it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.26it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.43it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.56it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.46it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.47it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.23it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.06it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.01it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.17it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.32it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.50it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.60it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.57it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.40it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.32it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.15it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.14it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.11it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.38it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.48it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.57it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.54it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.30it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.23it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.79it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 43.92it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.96it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.25it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.34it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.40it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.44it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.39it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.21it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.12it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.16it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.13it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.31it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.38it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.51it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.49it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.41it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.25it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.18it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.19it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.19it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.20it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.40it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.54it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.50it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.41it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.25it/s][A
 41%|████      | 352/861 [00:07<00:11, 42.49it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.08it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.42it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.73it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.03it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.21it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.29it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.21it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.01it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 43.98it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.16it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.25it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.32it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.38it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.43it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.39it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.20it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.14it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.14it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.16it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.34it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.38it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.47it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.51it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.42it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.16it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.15it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.15it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.23it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.28it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.39it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.53it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.45it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.33it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.24it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.13it/s][A
 62%|██████▏   | 532/861 [00:11<00:07, 44.22it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.23it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.23it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.40it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.44it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.44it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.33it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.26it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.20it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.17it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.28it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.25it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.40it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.44it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.34it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.29it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.40it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 43.62it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.81it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.93it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.11it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.33it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.37it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.36it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.16it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.15it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.22it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.29it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.13it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.29it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.41it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.44it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.40it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.32it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.27it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.21it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.21it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.18it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.33it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.34it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.35it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.31it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.32it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 41.97it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 42.84it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.22it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.65it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.88it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.11it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.13it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.13it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 43.90it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.02it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.18it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.19it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.37it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.49it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.34it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.31it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.15it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.02it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.13it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.21it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.26it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.39it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.49it/s][A                                                 
                                                 [A 40%|████      | 160/400 [01:28<01:01,  3.90it/s]
100%|██████████| 861/861 [00:19<00:00, 44.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:28:28,749 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-28 08:28:28,921 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:28:32,288 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:28:32,437 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:28:32,516 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160/special_tokens_map.json
 40%|████      | 161/400 [01:33<30:13,  7.59s/it] 40%|████      | 162/400 [01:33<21:24,  5.40s/it] 41%|████      | 163/400 [01:34<15:15,  3.86s/it] 41%|████      | 164/400 [01:34<10:57,  2.79s/it] 41%|████▏     | 165/400 [01:34<07:58,  2.04s/it] 42%|████▏     | 166/400 [01:34<05:53,  1.51s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 42%|████▏     | 167/400 [01:35<04:25,  1.14s/it] 42%|████▏     | 168/400 [01:35<03:26,  1.12it/s] 42%|████▏     | 169/400 [01:35<02:43,  1.41it/s] 42%|████▎     | 170/400 [01:36<02:13,  1.72it/s] 43%|████▎     | 171/400 [01:36<01:52,  2.04it/s] 43%|████▎     | 172/400 [01:36<01:37,  2.34it/s] 43%|████▎     | 173/400 [01:36<01:26,  2.61it/s] 44%|████▎     | 174/400 [01:37<01:19,  2.85it/s] 44%|████▍     | 175/400 [01:37<01:14,  3.04it/s] 44%|████▍     | 176/400 [01:37<01:10,  3.19it/s] 44%|████▍     | 177/400 [01:37<01:07,  3.30it/s] 44%|████▍     | 178/400 [01:38<01:05,  3.39it/s] 45%|████▍     | 179/400 [01:38<01:04,  3.43it/s] 45%|████▌     | 180/400 [01:38<01:03,  3.48it/s] 45%|████▌     | 181/400 [01:39<01:02,  3.51it/s] 46%|████▌     | 182/400 [01:39<01:01,  3.54it/s] 46%|████▌     | 183/400 [01:39<01:00,  3.56it/s] 46%|████▌     | 184/400 [01:39<01:00,  3.58it/s] 46%|████▋     | 185/400 [01:40<00:59,  3.59it/s] 46%|████▋     | 186/400 [01:40<00:59,  3.59it/s] 47%|████▋     | 187/400 [01:40<00:59,  3.59it/s] 47%|████▋     | 188/400 [01:41<00:58,  3.60it/s] 47%|████▋     | 189/400 [01:41<00:58,  3.60it/s] 48%|████▊     | 190/400 [01:41<01:00,  3.49it/s] 48%|████▊     | 191/400 [01:41<00:59,  3.53it/s] 48%|████▊     | 192/400 [01:42<00:58,  3.55it/s] 48%|████▊     | 193/400 [01:42<00:58,  3.56it/s] 48%|████▊     | 194/400 [01:42<00:57,  3.58it/s] 49%|████▉     | 195/400 [01:42<00:57,  3.59it/s] 49%|████▉     | 196/400 [01:43<00:56,  3.59it/s] 49%|████▉     | 197/400 [01:43<00:56,  3.60it/s] 50%|████▉     | 198/400 [01:43<00:56,  3.60it/s] 50%|████▉     | 199/400 [01:44<00:55,  3.60it/s] 50%|█████     | 200/400 [01:44<00:55,  3.60it/s] 50%|█████     | 201/400 [01:44<00:55,  3.56it/s] 50%|█████     | 202/400 [01:44<00:55,  3.57it/s] 51%|█████     | 203/400 [01:45<00:54,  3.58it/s] 51%|█████     | 204/400 [01:45<00:54,  3.59it/s] 51%|█████▏    | 205/400 [01:45<00:54,  3.59it/s] 52%|█████▏    | 206/400 [01:46<00:54,  3.54it/s] 52%|█████▏    | 207/400 [01:46<00:54,  3.55it/s] 52%|█████▏    | 208/400 [01:46<00:53,  3.57it/s] 52%|█████▏    | 209/400 [01:46<00:53,  3.58it/s] 52%|█████▎    | 210/400 [01:47<00:53,  3.58it/s] 53%|█████▎    | 211/400 [01:47<00:52,  3.59it/s] 53%|█████▎    | 212/400 [01:47<01:05,  2.86it/s] 53%|█████▎    | 213/400 [01:48<01:01,  3.02it/s] 54%|█████▎    | 214/400 [01:48<00:58,  3.18it/s] 54%|█████▍    | 215/400 [01:48<00:56,  3.29it/s] 54%|█████▍    | 216/400 [01:49<00:54,  3.38it/s] 54%|█████▍    | 217/400 [01:49<00:53,  3.44it/s] 55%|█████▍    | 218/400 [01:49<00:52,  3.49it/s] 55%|█████▍    | 219/400 [01:49<00:51,  3.52it/s] 55%|█████▌    | 220/400 [01:50<00:50,  3.55it/s] 55%|█████▌    | 221/400 [01:50<00:50,  3.56it/s] 56%|█████▌    | 222/400 [01:50<00:49,  3.58it/s] 56%|█████▌    | 223/400 [01:51<00:49,  3.58it/s] 56%|█████▌    | 224/400 [01:51<00:49,  3.58it/s] 56%|█████▋    | 225/400 [01:51<00:48,  3.59it/s] 56%|█████▋    | 226/400 [01:51<00:48,  3.59it/s] 57%|█████▋    | 227/400 [01:52<00:48,  3.59it/s] 57%|█████▋    | 228/400 [01:52<00:47,  3.60it/s] 57%|█████▋    | 229/400 [01:52<00:47,  3.60it/s] 57%|█████▊    | 230/400 [01:52<00:47,  3.60it/s] 58%|█████▊    | 231/400 [01:53<00:46,  3.60it/s] 58%|█████▊    | 232/400 [01:53<00:47,  3.53it/s] 58%|█████▊    | 233/400 [01:53<00:47,  3.55it/s] 58%|█████▊    | 234/400 [01:54<00:46,  3.56it/s] 59%|█████▉    | 235/400 [01:54<00:46,  3.57it/s] 59%|█████▉    | 236/400 [01:54<00:45,  3.58it/s] 59%|█████▉    | 237/400 [01:54<00:45,  3.59it/s] 60%|█████▉    | 238/400 [01:55<00:45,  3.59it/s] 60%|█████▉    | 239/400 [01:55<00:44,  3.60it/s] 60%|██████    | 240/400 [01:55<00:40,  3.94it/s][INFO|trainer.py:2140] 2023-08-28 08:28:55,966 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:28:55,966 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 08:28:55,966 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.4661, 'eval_samples_per_second': 353.64, 'eval_steps_per_second': 44.231, 'epoch': 2.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.83it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.77it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.92it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.00it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.41it/s][A
  4%|▎         | 32/861 [00:00<00:18, 43.97it/s][A
  4%|▍         | 37/861 [00:00<00:18, 43.91it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.68it/s][A
  5%|▌         | 47/861 [00:01<00:18, 43.92it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.15it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.34it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.48it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.65it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.43it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.25it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.07it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.91it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.04it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.23it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.37it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.51it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.55it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.45it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.28it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.12it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.98it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.98it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.16it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.33it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.49it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.57it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.47it/s][A
 19%|█▉        | 167/861 [00:03<00:17, 40.72it/s][A
 20%|█▉        | 172/861 [00:03<00:16, 41.93it/s][A
 21%|██        | 177/861 [00:04<00:16, 42.67it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.00it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.50it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 43.83it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.13it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.27it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 43.88it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.98it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.11it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.22it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.08it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.26it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.43it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.53it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.39it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.10it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.09it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.26it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.31it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.24it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.23it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.41it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.49it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.30it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.19it/s][A
 35%|███▌      | 302/861 [00:06<00:13, 42.10it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 42.88it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.48it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.79it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.00it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.18it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.16it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.14it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.83it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.93it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.06it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.07it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.45it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.48it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.47it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.38it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.18it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.03it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.15it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.12it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.34it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.42it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.56it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.49it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.34it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.18it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.08it/s][A
 51%|█████     | 437/861 [00:09<00:10, 41.83it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 42.80it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.37it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.76it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.94it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.02it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.03it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.97it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.77it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 43.91it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.12it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.24it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.41it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.53it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.40it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.27it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.02it/s][A
 61%|██████    | 522/861 [00:11<00:07, 43.97it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.04it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.13it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.38it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.50it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.52it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.49it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.23it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.08it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.05it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 42.03it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 42.85it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.29it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.92it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.02it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.16it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.12it/s][A
 70%|███████   | 607/861 [00:13<00:05, 43.92it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.80it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 43.83it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.14it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.30it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.52it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.59it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.39it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.28it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.00it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 43.94it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.97it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.20it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.35it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.54it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.56it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.51it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.31it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.00it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 43.96it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.73it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.03it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.22it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.45it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.45it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.30it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.23it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.02it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.95it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.11it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.20it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.39it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.51it/s][A
 90%|████████▉ | 772/861 [00:17<00:01, 44.54it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.42it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.15it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.08it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.02it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.08it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.23it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.38it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.48it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.44it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.40it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.24it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.16it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.11it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.85it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.08it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.26it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.35it/s][A                                                 
                                                 [A 60%|██████    | 240/400 [02:15<00:40,  3.94it/s]
100%|██████████| 861/861 [00:19<00:00, 44.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:29:15,562 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 08:29:15,690 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:29:20,028 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:29:20,159 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:29:20,235 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240/special_tokens_map.json
 60%|██████    | 241/400 [02:21<20:40,  7.80s/it] 60%|██████    | 242/400 [02:21<14:36,  5.54s/it] 61%|██████    | 243/400 [02:21<10:22,  3.97s/it] 61%|██████    | 244/400 [02:21<07:26,  2.86s/it] 61%|██████▏   | 245/400 [02:22<05:23,  2.09s/it] 62%|██████▏   | 246/400 [02:22<03:57,  1.54s/it] 62%|██████▏   | 247/400 [02:22<02:58,  1.17s/it] 62%|██████▏   | 248/400 [02:23<02:18,  1.10it/s] 62%|██████▏   | 249/400 [02:23<01:49,  1.38it/s] 62%|██████▎   | 250/400 [02:23<01:28,  1.69it/s] 63%|██████▎   | 251/400 [02:23<01:14,  2.01it/s] 63%|██████▎   | 252/400 [02:24<01:04,  2.31it/s] 63%|██████▎   | 253/400 [02:24<00:56,  2.58it/s] 64%|██████▎   | 254/400 [02:24<00:51,  2.81it/s] 64%|██████▍   | 255/400 [02:25<00:48,  3.00it/s] 64%|██████▍   | 256/400 [02:25<00:45,  3.15it/s] 64%|██████▍   | 257/400 [02:25<00:43,  3.25it/s] 64%|██████▍   | 258/400 [02:25<00:42,  3.33it/s] 65%|██████▍   | 259/400 [02:26<00:42,  3.35it/s] 65%|██████▌   | 260/400 [02:26<00:41,  3.41it/s] 65%|██████▌   | 261/400 [02:26<00:40,  3.45it/s] 66%|██████▌   | 262/400 [02:27<00:39,  3.47it/s] 66%|██████▌   | 263/400 [02:27<00:39,  3.50it/s] 66%|██████▌   | 264/400 [02:27<00:38,  3.51it/s] 66%|██████▋   | 265/400 [02:27<00:38,  3.52it/s] 66%|██████▋   | 266/400 [02:28<00:37,  3.53it/s] 67%|██████▋   | 267/400 [02:28<00:37,  3.54it/s] 67%|██████▋   | 268/400 [02:28<00:37,  3.54it/s] 67%|██████▋   | 269/400 [02:29<00:37,  3.54it/s] 68%|██████▊   | 270/400 [02:29<00:37,  3.44it/s] 68%|██████▊   | 271/400 [02:29<00:37,  3.47it/s] 68%|██████▊   | 272/400 [02:29<00:36,  3.50it/s] 68%|██████▊   | 273/400 [02:30<00:36,  3.51it/s] 68%|██████▊   | 274/400 [02:30<00:35,  3.52it/s] 69%|██████▉   | 275/400 [02:30<00:35,  3.53it/s] 69%|██████▉   | 276/400 [02:31<00:35,  3.53it/s] 69%|██████▉   | 277/400 [02:31<00:34,  3.53it/s] 70%|██████▉   | 278/400 [02:31<00:34,  3.54it/s] 70%|██████▉   | 279/400 [02:31<00:34,  3.54it/s] 70%|███████   | 280/400 [02:32<00:33,  3.55it/s] 70%|███████   | 281/400 [02:32<00:34,  3.46it/s] 70%|███████   | 282/400 [02:32<00:33,  3.49it/s] 71%|███████   | 283/400 [02:33<00:33,  3.50it/s] 71%|███████   | 284/400 [02:33<00:32,  3.52it/s] 71%|███████▏  | 285/400 [02:33<00:32,  3.52it/s] 72%|███████▏  | 286/400 [02:33<00:32,  3.53it/s] 72%|███████▏  | 287/400 [02:34<00:31,  3.53it/s] 72%|███████▏  | 288/400 [02:34<00:31,  3.54it/s] 72%|███████▏  | 289/400 [02:34<00:31,  3.54it/s] 72%|███████▎  | 290/400 [02:35<00:31,  3.55it/s] 73%|███████▎  | 291/400 [02:35<00:30,  3.55it/s] 73%|███████▎  | 292/400 [02:35<00:30,  3.49it/s] 73%|███████▎  | 293/400 [02:35<00:30,  3.51it/s] 74%|███████▎  | 294/400 [02:36<00:30,  3.52it/s] 74%|███████▍  | 295/400 [02:36<00:29,  3.53it/s] 74%|███████▍  | 296/400 [02:36<00:29,  3.53it/s] 74%|███████▍  | 297/400 [02:37<00:29,  3.53it/s] 74%|███████▍  | 298/400 [02:37<00:28,  3.54it/s] 75%|███████▍  | 299/400 [02:37<00:28,  3.54it/s] 75%|███████▌  | 300/400 [02:37<00:28,  3.54it/s] 75%|███████▌  | 301/400 [02:38<00:27,  3.55it/s] 76%|███████▌  | 302/400 [02:38<00:27,  3.55it/s] 76%|███████▌  | 303/400 [02:38<00:28,  3.42it/s] 76%|███████▌  | 304/400 [02:39<00:27,  3.46it/s] 76%|███████▋  | 305/400 [02:39<00:27,  3.48it/s] 76%|███████▋  | 306/400 [02:39<00:26,  3.50it/s] 77%|███████▋  | 307/400 [02:39<00:26,  3.52it/s] 77%|███████▋  | 308/400 [02:40<00:26,  3.52it/s] 77%|███████▋  | 309/400 [02:40<00:25,  3.53it/s] 78%|███████▊  | 310/400 [02:40<00:25,  3.53it/s] 78%|███████▊  | 311/400 [02:40<00:25,  3.54it/s] 78%|███████▊  | 312/400 [02:41<00:24,  3.54it/s] 78%|███████▊  | 313/400 [02:41<00:24,  3.54it/s] 78%|███████▊  | 314/400 [02:41<00:25,  3.42it/s] 79%|███████▉  | 315/400 [02:42<00:24,  3.46it/s] 79%|███████▉  | 316/400 [02:42<00:24,  3.49it/s] 79%|███████▉  | 317/400 [02:42<00:23,  3.50it/s] 80%|███████▉  | 318/400 [02:42<00:23,  3.52it/s] 80%|███████▉  | 319/400 [02:43<00:22,  3.53it/s] 80%|████████  | 320/400 [02:43<00:20,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 08:29:43,743 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:29:43,743 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 08:29:43,743 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.5275, 'eval_samples_per_second': 352.528, 'eval_steps_per_second': 44.092, 'epoch': 3.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.96it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.61it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.99it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.01it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.32it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.75it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.24it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.16it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.31it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.46it/s][A
  7%|▋         | 57/861 [00:01<00:19, 41.92it/s][A
  7%|▋         | 62/861 [00:01<00:18, 42.80it/s][A
  8%|▊         | 67/861 [00:01<00:18, 43.48it/s][A
  8%|▊         | 72/861 [00:01<00:18, 43.71it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.72it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.80it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.90it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.04it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.95it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.17it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.32it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.57it/s][A
 14%|█▎        | 117/861 [00:02<00:17, 43.69it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.89it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.94it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.03it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.98it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.08it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.24it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.39it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.48it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.40it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.33it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.21it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.27it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.17it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.20it/s][A
 22%|██▏       | 192/861 [00:04<00:20, 32.40it/s][A
 23%|██▎       | 198/861 [00:04<00:18, 36.68it/s][A
 24%|██▎       | 203/861 [00:04<00:16, 38.78it/s][A
 24%|██▍       | 208/861 [00:04<00:16, 40.33it/s][A
 25%|██▍       | 213/861 [00:04<00:15, 41.52it/s][A
 25%|██▌       | 218/861 [00:05<00:15, 42.49it/s][A
 26%|██▌       | 223/861 [00:05<00:14, 43.18it/s][A
 26%|██▋       | 228/861 [00:05<00:14, 43.53it/s][A
 27%|██▋       | 233/861 [00:05<00:14, 43.39it/s][A
 28%|██▊       | 238/861 [00:05<00:14, 43.25it/s][A
 28%|██▊       | 243/861 [00:05<00:14, 43.46it/s][A
 29%|██▉       | 248/861 [00:05<00:13, 43.85it/s][A
 29%|██▉       | 253/861 [00:05<00:14, 43.36it/s][A
 30%|██▉       | 258/861 [00:05<00:13, 43.91it/s][A
 31%|███       | 263/861 [00:06<00:13, 44.21it/s][A
 31%|███       | 268/861 [00:06<00:13, 43.02it/s][A
 32%|███▏      | 273/861 [00:06<00:13, 43.46it/s][A
 32%|███▏      | 278/861 [00:06<00:13, 43.52it/s][A
 33%|███▎      | 283/861 [00:06<00:13, 43.45it/s][A
 33%|███▎      | 288/861 [00:06<00:13, 43.69it/s][A
 34%|███▍      | 293/861 [00:06<00:12, 43.92it/s][A
 35%|███▍      | 298/861 [00:06<00:12, 44.12it/s][A
 35%|███▌      | 303/861 [00:06<00:12, 44.32it/s][A
 36%|███▌      | 308/861 [00:07<00:12, 44.26it/s][A
 36%|███▋      | 313/861 [00:07<00:12, 44.38it/s][A
 37%|███▋      | 318/861 [00:07<00:12, 44.45it/s][A
 38%|███▊      | 323/861 [00:07<00:12, 44.29it/s][A
 38%|███▊      | 328/861 [00:07<00:12, 44.13it/s][A
 39%|███▊      | 333/861 [00:07<00:11, 44.00it/s][A
 39%|███▉      | 338/861 [00:07<00:11, 44.21it/s][A
 40%|███▉      | 343/861 [00:07<00:11, 44.39it/s][A
 40%|████      | 348/861 [00:07<00:11, 44.43it/s][A
 41%|████      | 353/861 [00:08<00:11, 44.40it/s][A
 42%|████▏     | 358/861 [00:08<00:11, 44.39it/s][A
 42%|████▏     | 363/861 [00:08<00:11, 44.34it/s][A
 43%|████▎     | 368/861 [00:08<00:11, 44.26it/s][A
 43%|████▎     | 373/861 [00:08<00:11, 44.16it/s][A
 44%|████▍     | 378/861 [00:08<00:10, 44.09it/s][A
 44%|████▍     | 383/861 [00:08<00:10, 44.22it/s][A
 45%|████▌     | 388/861 [00:08<00:10, 44.43it/s][A
 46%|████▌     | 393/861 [00:09<00:10, 44.41it/s][A
 46%|████▌     | 398/861 [00:09<00:10, 44.41it/s][A
 47%|████▋     | 403/861 [00:09<00:10, 42.98it/s][A
 47%|████▋     | 408/861 [00:09<00:10, 43.40it/s][A
 48%|████▊     | 413/861 [00:09<00:10, 43.72it/s][A
 49%|████▊     | 418/861 [00:09<00:10, 43.86it/s][A
 49%|████▉     | 423/861 [00:09<00:09, 43.93it/s][A
 50%|████▉     | 428/861 [00:09<00:09, 44.12it/s][A
 50%|█████     | 433/861 [00:09<00:09, 44.31it/s][A
 51%|█████     | 438/861 [00:10<00:09, 44.37it/s][A
 51%|█████▏    | 443/861 [00:10<00:09, 44.15it/s][A
 52%|█████▏    | 448/861 [00:10<00:09, 44.27it/s][A
 53%|█████▎    | 453/861 [00:10<00:09, 44.21it/s][A
 53%|█████▎    | 458/861 [00:10<00:09, 44.20it/s][A
 54%|█████▍    | 463/861 [00:10<00:08, 44.25it/s][A
 54%|█████▍    | 468/861 [00:10<00:08, 44.21it/s][A
 55%|█████▍    | 473/861 [00:10<00:08, 44.28it/s][A
 56%|█████▌    | 478/861 [00:10<00:08, 44.41it/s][A
 56%|█████▌    | 483/861 [00:11<00:08, 44.30it/s][A
 57%|█████▋    | 488/861 [00:11<00:08, 44.31it/s][A
 57%|█████▋    | 493/861 [00:11<00:08, 44.34it/s][A
 58%|█████▊    | 498/861 [00:11<00:08, 44.27it/s][A
 58%|█████▊    | 503/861 [00:11<00:08, 44.28it/s][A
 59%|█████▉    | 508/861 [00:11<00:07, 44.38it/s][A
 60%|█████▉    | 513/861 [00:11<00:07, 44.29it/s][A
 60%|██████    | 518/861 [00:11<00:07, 44.30it/s][A
 61%|██████    | 523/861 [00:11<00:07, 44.34it/s][A
 61%|██████▏   | 528/861 [00:12<00:07, 44.33it/s][A
 62%|██████▏   | 533/861 [00:12<00:07, 44.26it/s][A
 62%|██████▏   | 538/861 [00:12<00:07, 44.09it/s][A
 63%|██████▎   | 543/861 [00:12<00:07, 44.17it/s][A
 64%|██████▎   | 548/861 [00:12<00:07, 44.26it/s][A
 64%|██████▍   | 553/861 [00:12<00:06, 44.27it/s][A
 65%|██████▍   | 558/861 [00:12<00:06, 44.32it/s][A
 65%|██████▌   | 563/861 [00:12<00:06, 44.24it/s][A
 66%|██████▌   | 568/861 [00:12<00:06, 44.32it/s][A
 67%|██████▋   | 573/861 [00:13<00:06, 44.29it/s][A
 67%|██████▋   | 578/861 [00:13<00:06, 44.27it/s][A
 68%|██████▊   | 583/861 [00:13<00:06, 44.27it/s][A
 68%|██████▊   | 588/861 [00:13<00:06, 44.35it/s][A
 69%|██████▉   | 593/861 [00:13<00:06, 44.32it/s][A
 69%|██████▉   | 598/861 [00:13<00:05, 44.32it/s][A
 70%|███████   | 603/861 [00:13<00:05, 44.39it/s][A
 71%|███████   | 608/861 [00:13<00:05, 44.37it/s][A
 71%|███████   | 613/861 [00:13<00:05, 44.25it/s][A
 72%|███████▏  | 618/861 [00:14<00:05, 44.29it/s][A
 72%|███████▏  | 623/861 [00:14<00:05, 44.29it/s][A
 73%|███████▎  | 628/861 [00:14<00:05, 44.31it/s][A
 74%|███████▎  | 633/861 [00:14<00:05, 44.24it/s][A
 74%|███████▍  | 638/861 [00:14<00:05, 44.32it/s][A
 75%|███████▍  | 643/861 [00:14<00:04, 44.29it/s][A
 75%|███████▌  | 648/861 [00:14<00:04, 44.38it/s][A
 76%|███████▌  | 653/861 [00:14<00:04, 44.37it/s][A
 76%|███████▋  | 658/861 [00:15<00:04, 44.24it/s][A
 77%|███████▋  | 663/861 [00:15<00:04, 44.21it/s][A
 78%|███████▊  | 668/861 [00:15<00:04, 44.27it/s][A
 78%|███████▊  | 673/861 [00:15<00:04, 43.36it/s][A
 79%|███████▊  | 678/861 [00:15<00:04, 43.75it/s][A
 79%|███████▉  | 683/861 [00:15<00:04, 43.92it/s][A
 80%|███████▉  | 688/861 [00:15<00:03, 43.92it/s][A
 80%|████████  | 693/861 [00:15<00:03, 44.30it/s][A
 81%|████████  | 698/861 [00:15<00:03, 44.34it/s][A
 82%|████████▏ | 703/861 [00:16<00:03, 44.28it/s][A
 82%|████████▏ | 708/861 [00:16<00:03, 44.26it/s][A
 83%|████████▎ | 713/861 [00:16<00:03, 44.05it/s][A
 83%|████████▎ | 718/861 [00:16<00:03, 44.05it/s][A
 84%|████████▍ | 723/861 [00:16<00:03, 44.24it/s][A
 85%|████████▍ | 728/861 [00:16<00:03, 44.31it/s][A
 85%|████████▌ | 733/861 [00:16<00:02, 44.39it/s][A
 86%|████████▌ | 738/861 [00:16<00:02, 44.46it/s][A
 86%|████████▋ | 743/861 [00:16<00:02, 44.39it/s][A
 87%|████████▋ | 748/861 [00:17<00:02, 44.33it/s][A
 87%|████████▋ | 753/861 [00:17<00:02, 44.22it/s][A
 88%|████████▊ | 758/861 [00:17<00:02, 44.09it/s][A
 89%|████████▊ | 763/861 [00:17<00:02, 44.09it/s][A
 89%|████████▉ | 768/861 [00:17<00:02, 44.23it/s][A
 90%|████████▉ | 773/861 [00:17<00:01, 44.30it/s][A
 90%|█████████ | 778/861 [00:17<00:01, 44.35it/s][A
 91%|█████████ | 783/861 [00:17<00:01, 44.35it/s][A
 92%|█████████▏| 788/861 [00:17<00:01, 44.41it/s][A
 92%|█████████▏| 793/861 [00:18<00:01, 44.30it/s][A
 93%|█████████▎| 798/861 [00:18<00:01, 44.16it/s][A
 93%|█████████▎| 803/861 [00:18<00:01, 44.10it/s][A
 94%|█████████▍| 808/861 [00:18<00:01, 41.92it/s][A
 94%|█████████▍| 813/861 [00:18<00:01, 42.81it/s][A
 95%|█████████▌| 818/861 [00:18<00:00, 43.39it/s][A
 96%|█████████▌| 823/861 [00:18<00:00, 43.67it/s][A
 96%|█████████▌| 828/861 [00:18<00:00, 43.86it/s][A
 97%|█████████▋| 833/861 [00:18<00:00, 43.95it/s][A
 97%|█████████▋| 838/861 [00:19<00:00, 44.08it/s][A
 98%|█████████▊| 843/861 [00:19<00:00, 43.92it/s][A
 98%|█████████▊| 848/861 [00:19<00:00, 43.78it/s][A
 99%|█████████▉| 853/861 [00:19<00:00, 43.96it/s][A
100%|█████████▉| 858/861 [00:19<00:00, 44.23it/s][A                                                 
                                                 [A 80%|████████  | 320/400 [03:03<00:20,  3.86it/s]
100%|██████████| 861/861 [00:19<00:00, 44.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:30:03,639 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320
[INFO|configuration_utils.py:351] 2023-08-28 08:30:03,860 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:30:07,307 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:30:07,493 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:30:07,592 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320/special_tokens_map.json
 80%|████████  | 321/400 [03:09<10:26,  7.93s/it] 80%|████████  | 322/400 [03:09<07:19,  5.64s/it] 81%|████████  | 323/400 [03:09<05:10,  4.03s/it] 81%|████████  | 324/400 [03:10<03:40,  2.91s/it] 81%|████████▏ | 325/400 [03:10<02:39,  2.13s/it] 82%|████████▏ | 326/400 [03:10<01:56,  1.57s/it] 82%|████████▏ | 327/400 [03:11<01:26,  1.19s/it] 82%|████████▏ | 328/400 [03:11<01:05,  1.09it/s] 82%|████████▏ | 329/400 [03:11<00:51,  1.38it/s] 82%|████████▎ | 330/400 [03:11<00:41,  1.69it/s] 83%|████████▎ | 331/400 [03:12<00:34,  2.00it/s] 83%|████████▎ | 332/400 [03:12<00:29,  2.30it/s] 83%|████████▎ | 333/400 [03:12<00:26,  2.57it/s] 84%|████████▎ | 334/400 [03:13<00:23,  2.80it/s] 84%|████████▍ | 335/400 [03:13<00:21,  2.99it/s] 84%|████████▍ | 336/400 [03:13<00:20,  3.10it/s] 84%|████████▍ | 337/400 [03:13<00:19,  3.22it/s] 84%|████████▍ | 338/400 [03:14<00:18,  3.31it/s] 85%|████████▍ | 339/400 [03:14<00:18,  3.38it/s] 85%|████████▌ | 340/400 [03:14<00:17,  3.43it/s] 85%|████████▌ | 341/400 [03:15<00:17,  3.46it/s] 86%|████████▌ | 342/400 [03:15<00:16,  3.49it/s] 86%|████████▌ | 343/400 [03:15<00:16,  3.50it/s] 86%|████████▌ | 344/400 [03:15<00:15,  3.52it/s] 86%|████████▋ | 345/400 [03:16<00:15,  3.52it/s] 86%|████████▋ | 346/400 [03:16<00:15,  3.53it/s] 87%|████████▋ | 347/400 [03:16<00:15,  3.42it/s] 87%|████████▋ | 348/400 [03:17<00:15,  3.46it/s] 87%|████████▋ | 349/400 [03:17<00:14,  3.49it/s] 88%|████████▊ | 350/400 [03:17<00:14,  3.50it/s] 88%|████████▊ | 351/400 [03:17<00:13,  3.52it/s] 88%|████████▊ | 352/400 [03:18<00:13,  3.53it/s] 88%|████████▊ | 353/400 [03:18<00:13,  3.54it/s] 88%|████████▊ | 354/400 [03:18<00:12,  3.54it/s] 89%|████████▉ | 355/400 [03:18<00:12,  3.54it/s] 89%|████████▉ | 356/400 [03:19<00:12,  3.54it/s] 89%|████████▉ | 357/400 [03:19<00:12,  3.55it/s] 90%|████████▉ | 358/400 [03:19<00:11,  3.54it/s] 90%|████████▉ | 359/400 [03:20<00:11,  3.54it/s] 90%|█████████ | 360/400 [03:20<00:11,  3.54it/s] 90%|█████████ | 361/400 [03:20<00:11,  3.48it/s] 90%|█████████ | 362/400 [03:20<00:10,  3.50it/s] 91%|█████████ | 363/400 [03:21<00:10,  3.52it/s] 91%|█████████ | 364/400 [03:21<00:10,  3.53it/s] 91%|█████████▏| 365/400 [03:21<00:09,  3.53it/s] 92%|█████████▏| 366/400 [03:22<00:09,  3.54it/s] 92%|█████████▏| 367/400 [03:22<00:09,  3.54it/s] 92%|█████████▏| 368/400 [03:22<00:09,  3.54it/s] 92%|█████████▏| 369/400 [03:22<00:08,  3.54it/s] 92%|█████████▎| 370/400 [03:23<00:08,  3.54it/s] 93%|█████████▎| 371/400 [03:23<00:08,  3.54it/s] 93%|█████████▎| 372/400 [03:23<00:08,  3.49it/s] 93%|█████████▎| 373/400 [03:24<00:07,  3.50it/s] 94%|█████████▎| 374/400 [03:24<00:07,  3.52it/s] 94%|█████████▍| 375/400 [03:24<00:07,  3.53it/s] 94%|█████████▍| 376/400 [03:24<00:06,  3.53it/s] 94%|█████████▍| 377/400 [03:25<00:06,  3.54it/s] 94%|█████████▍| 378/400 [03:25<00:06,  3.54it/s] 95%|█████████▍| 379/400 [03:25<00:05,  3.54it/s] 95%|█████████▌| 380/400 [03:26<00:05,  3.54it/s] 95%|█████████▌| 381/400 [03:26<00:05,  3.54it/s] 96%|█████████▌| 382/400 [03:26<00:05,  3.54it/s] 96%|█████████▌| 383/400 [03:26<00:04,  3.48it/s] 96%|█████████▌| 384/400 [03:27<00:04,  3.50it/s] 96%|█████████▋| 385/400 [03:27<00:04,  3.52it/s] 96%|█████████▋| 386/400 [03:27<00:03,  3.53it/s] 97%|█████████▋| 387/400 [03:28<00:03,  3.54it/s] 97%|█████████▋| 388/400 [03:28<00:03,  3.54it/s] 97%|█████████▋| 389/400 [03:28<00:03,  3.54it/s] 98%|█████████▊| 390/400 [03:28<00:02,  3.54it/s] 98%|█████████▊| 391/400 [03:29<00:02,  3.55it/s] 98%|█████████▊| 392/400 [03:29<00:02,  3.54it/s] 98%|█████████▊| 393/400 [03:29<00:01,  3.54it/s] 98%|█████████▊| 394/400 [03:30<00:01,  3.46it/s] 99%|█████████▉| 395/400 [03:30<00:01,  3.49it/s] 99%|█████████▉| 396/400 [03:30<00:01,  3.51it/s] 99%|█████████▉| 397/400 [03:30<00:00,  3.52it/s]100%|█████████▉| 398/400 [03:31<00:00,  3.54it/s]100%|█████████▉| 399/400 [03:31<00:00,  3.56it/s]100%|██████████| 400/400 [03:31<00:00,  3.91it/s][INFO|trainer.py:2140] 2023-08-28 08:30:31,916 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:30:31,916 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 08:30:31,916 >>   Batch size = 8
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.6288, 'eval_samples_per_second': 350.71, 'eval_steps_per_second': 43.864, 'epoch': 4.0}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.50it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.80it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.06it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.04it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.25it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.78it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.54it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.17it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.18it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.28it/s][A
  7%|▋         | 57/861 [00:01<00:19, 41.07it/s][A
  7%|▋         | 62/861 [00:01<00:18, 42.14it/s][A
  8%|▊         | 67/861 [00:01<00:18, 42.97it/s][A
  8%|▊         | 72/861 [00:01<00:18, 43.49it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.90it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.86it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.89it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.97it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.76it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.96it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.02it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.30it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.39it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.54it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.47it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.28it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.18it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.05it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.18it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.21it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.34it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.44it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.52it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.49it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.35it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.16it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.06it/s][A
 22%|██▏       | 192/861 [00:04<00:16, 40.69it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 41.83it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 42.68it/s][A
 24%|██▍       | 207/861 [00:04<00:15, 43.30it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.78it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.13it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.17it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.07it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 43.81it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 43.69it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 43.94it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.18it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.43it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.55it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.54it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.49it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.27it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.95it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 43.87it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 44.09it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.23it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.39it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.63it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.62it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.47it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.32it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.07it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 42.87it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 43.45it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 43.78it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.10it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.23it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.35it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.26it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.18it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.89it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 43.98it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.25it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.39it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.55it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.52it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.47it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.38it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.16it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.08it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.05it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.26it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.43it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.54it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.58it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.54it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.34it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.18it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.14it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 43.72it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.00it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.20it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.37it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.38it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.41it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.29it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.10it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.02it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.07it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.18it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.40it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.51it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.45it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.34it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.24it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.13it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 43.90it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.16it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.31it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.44it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.40it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.50it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.37it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.17it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.11it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.05it/s][A
 69%|██████▉   | 597/861 [00:13<00:06, 43.04it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 43.54it/s][A
 70%|███████   | 607/861 [00:13<00:05, 43.83it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.11it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.21it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.17it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.18it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 43.99it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 41.63it/s][A
 75%|███████▍  | 642/861 [00:14<00:05, 43.12it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 43.68it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.03it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.30it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.35it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.29it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.19it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 43.94it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 43.83it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 43.94it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.15it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.35it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.50it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 44.56it/s][A
 83%|████████▎ | 712/861 [00:16<00:06, 24.47it/s][A
 83%|████████▎ | 717/861 [00:16<00:05, 28.47it/s][A
 84%|████████▍ | 722/861 [00:16<00:04, 31.94it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 34.93it/s][A
 85%|████████▌ | 732/861 [00:16<00:03, 37.42it/s][A
 86%|████████▌ | 737/861 [00:16<00:03, 39.37it/s][A
 86%|████████▌ | 742/861 [00:17<00:02, 40.83it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 42.00it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 42.39it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 42.61it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 42.79it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 40.80it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 42.11it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 42.87it/s][A
 91%|█████████ | 782/861 [00:18<00:01, 43.32it/s][A
 91%|█████████▏| 787/861 [00:18<00:01, 43.79it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.13it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.17it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.04it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 43.89it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 43.83it/s][A
 95%|█████████▍| 817/861 [00:18<00:01, 43.98it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.22it/s][A
 96%|█████████▌| 827/861 [00:19<00:00, 44.26it/s][A
 97%|█████████▋| 832/861 [00:19<00:00, 44.40it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 44.51it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 42.74it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.14it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.33it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.50it/s][A                                                 
                                                 [A100%|██████████| 400/400 [03:51<00:00,  3.91it/s]
100%|██████████| 861/861 [00:19<00:00, 43.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:30:51,834 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400
[INFO|configuration_utils.py:351] 2023-08-28 08:30:51,961 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:30:55,763 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:30:55,953 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:30:56,028 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 08:30:56,926 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 08:30:56,927 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80 (score: 1.0493918657302856).
                                                 100%|██████████| 400/400 [04:05<00:00,  3.91it/s]100%|██████████| 400/400 [04:05<00:00,  1.63it/s]
[INFO|trainer.py:1894] 2023-08-28 08:31:06,109 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 08:31:06,288 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:31:09,693 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:31:09,967 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:31:10,090 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:31:10,753 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:10,753 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:10,753 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:10,754 >>   train_runtime            = 0:04:05.63
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:10,754 >>   train_samples            =       5100
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:10,754 >>   train_samples_per_second =    103.813
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:10,754 >>   train_steps_per_second   =      1.628
{'eval_loss': 1.0493918657302856, 'eval_runtime': 19.8558, 'eval_samples_per_second': 346.699, 'eval_steps_per_second': 43.363, 'epoch': 5.0}
{'train_runtime': 245.6344, 'train_samples_per_second': 103.813, 'train_steps_per_second': 1.628, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 08:31:11 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 08:31:11,079 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:31:11,079 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 08:31:11,079 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 55.29it/s]  1%|▏         | 12/861 [00:00<00:17, 48.87it/s]  2%|▏         | 17/861 [00:00<00:17, 47.20it/s]  3%|▎         | 22/861 [00:00<00:18, 46.41it/s]  3%|▎         | 27/861 [00:00<00:18, 45.84it/s]  4%|▎         | 32/861 [00:00<00:18, 45.58it/s]  4%|▍         | 37/861 [00:00<00:18, 45.35it/s]  5%|▍         | 42/861 [00:00<00:18, 44.78it/s]  5%|▌         | 47/861 [00:01<00:18, 44.20it/s]  6%|▌         | 52/861 [00:01<00:18, 44.07it/s]  7%|▋         | 57/861 [00:01<00:18, 44.27it/s]  7%|▋         | 62/861 [00:01<00:17, 44.45it/s]  8%|▊         | 67/861 [00:01<00:17, 44.63it/s]  8%|▊         | 72/861 [00:01<00:17, 44.68it/s]  9%|▉         | 77/861 [00:01<00:17, 44.82it/s] 10%|▉         | 82/861 [00:01<00:18, 42.51it/s] 10%|█         | 87/861 [00:01<00:18, 42.87it/s] 11%|█         | 92/861 [00:02<00:17, 43.01it/s] 11%|█▏        | 97/861 [00:02<00:17, 43.40it/s] 12%|█▏        | 102/861 [00:02<00:17, 43.72it/s] 12%|█▏        | 107/861 [00:02<00:17, 44.04it/s] 13%|█▎        | 112/861 [00:02<00:16, 44.24it/s] 14%|█▎        | 117/861 [00:02<00:16, 44.38it/s] 14%|█▍        | 122/861 [00:02<00:16, 44.25it/s] 15%|█▍        | 127/861 [00:02<00:16, 44.29it/s] 15%|█▌        | 132/861 [00:02<00:16, 44.24it/s] 16%|█▌        | 137/861 [00:03<00:16, 44.12it/s] 16%|█▋        | 142/861 [00:03<00:16, 44.12it/s] 17%|█▋        | 147/861 [00:03<00:16, 44.14it/s] 18%|█▊        | 152/861 [00:03<00:15, 44.35it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.43it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.52it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.55it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.46it/s] 21%|██        | 177/861 [00:03<00:15, 44.29it/s] 21%|██        | 182/861 [00:04<00:15, 44.17it/s] 22%|██▏       | 187/861 [00:04<00:15, 44.28it/s] 22%|██▏       | 192/861 [00:04<00:15, 44.28it/s] 23%|██▎       | 197/861 [00:04<00:14, 44.37it/s] 23%|██▎       | 202/861 [00:04<00:14, 44.44it/s] 24%|██▍       | 207/861 [00:04<00:14, 44.55it/s] 25%|██▍       | 212/861 [00:04<00:14, 44.43it/s] 25%|██▌       | 217/861 [00:04<00:14, 43.10it/s] 26%|██▌       | 222/861 [00:05<00:14, 43.46it/s] 26%|██▋       | 227/861 [00:05<00:14, 43.71it/s] 27%|██▋       | 232/861 [00:05<00:14, 43.89it/s] 28%|██▊       | 237/861 [00:05<00:14, 44.03it/s] 28%|██▊       | 242/861 [00:05<00:14, 44.14it/s] 29%|██▊       | 247/861 [00:05<00:13, 44.35it/s] 29%|██▉       | 252/861 [00:05<00:13, 44.39it/s] 30%|██▉       | 257/861 [00:05<00:13, 44.37it/s] 30%|███       | 262/861 [00:05<00:13, 44.40it/s] 31%|███       | 267/861 [00:06<00:13, 44.41it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.38it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.46it/s] 33%|███▎      | 282/861 [00:06<00:12, 44.56it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.58it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.71it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.64it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.56it/s] 36%|███▌      | 307/861 [00:06<00:12, 44.56it/s] 36%|███▌      | 312/861 [00:07<00:12, 44.54it/s] 37%|███▋      | 317/861 [00:07<00:12, 44.44it/s] 37%|███▋      | 322/861 [00:07<00:12, 44.48it/s] 38%|███▊      | 327/861 [00:07<00:11, 44.51it/s] 39%|███▊      | 332/861 [00:07<00:11, 44.60it/s] 39%|███▉      | 337/861 [00:07<00:11, 44.65it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.62it/s] 40%|████      | 347/861 [00:07<00:11, 44.51it/s] 41%|████      | 352/861 [00:07<00:13, 38.06it/s] 41%|████▏     | 357/861 [00:08<00:12, 40.02it/s] 42%|████▏     | 362/861 [00:08<00:12, 41.35it/s] 43%|████▎     | 367/861 [00:08<00:11, 42.44it/s] 43%|████▎     | 372/861 [00:08<00:11, 43.11it/s] 44%|████▍     | 377/861 [00:08<00:11, 43.67it/s] 44%|████▍     | 382/861 [00:08<00:10, 44.18it/s] 45%|████▍     | 387/861 [00:08<00:10, 44.25it/s] 46%|████▌     | 392/861 [00:08<00:10, 43.97it/s] 46%|████▌     | 397/861 [00:08<00:10, 43.78it/s] 47%|████▋     | 402/861 [00:09<00:10, 44.04it/s] 47%|████▋     | 407/861 [00:09<00:10, 44.20it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.50it/s] 48%|████▊     | 417/861 [00:09<00:09, 44.60it/s] 49%|████▉     | 422/861 [00:09<00:09, 44.72it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.79it/s] 50%|█████     | 432/861 [00:09<00:09, 44.68it/s] 51%|█████     | 437/861 [00:09<00:09, 44.33it/s] 51%|█████▏    | 442/861 [00:10<00:09, 44.07it/s] 52%|█████▏    | 447/861 [00:10<00:09, 44.15it/s] 52%|█████▏    | 452/861 [00:10<00:09, 44.29it/s] 53%|█████▎    | 457/861 [00:10<00:09, 44.50it/s] 54%|█████▎    | 462/861 [00:10<00:08, 44.67it/s] 54%|█████▍    | 467/861 [00:10<00:08, 44.79it/s] 55%|█████▍    | 472/861 [00:10<00:08, 44.81it/s] 55%|█████▌    | 477/861 [00:10<00:08, 44.64it/s] 56%|█████▌    | 482/861 [00:10<00:08, 44.36it/s] 57%|█████▋    | 487/861 [00:11<00:08, 43.24it/s] 57%|█████▋    | 492/861 [00:11<00:08, 43.56it/s] 58%|█████▊    | 497/861 [00:11<00:08, 43.91it/s] 58%|█████▊    | 502/861 [00:11<00:08, 44.18it/s] 59%|█████▉    | 507/861 [00:11<00:07, 44.53it/s] 59%|█████▉    | 512/861 [00:11<00:07, 44.56it/s] 60%|██████    | 517/861 [00:11<00:07, 44.67it/s] 61%|██████    | 522/861 [00:11<00:07, 44.51it/s] 61%|██████    | 527/861 [00:11<00:07, 44.17it/s] 62%|██████▏   | 532/861 [00:12<00:07, 44.13it/s] 62%|██████▏   | 537/861 [00:12<00:07, 44.18it/s] 63%|██████▎   | 542/861 [00:12<00:07, 44.41it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.54it/s] 64%|██████▍   | 552/861 [00:12<00:06, 44.76it/s] 65%|██████▍   | 557/861 [00:12<00:06, 44.77it/s] 65%|██████▌   | 562/861 [00:12<00:06, 44.66it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.42it/s] 66%|██████▋   | 572/861 [00:12<00:06, 44.30it/s] 67%|██████▋   | 577/861 [00:13<00:06, 44.23it/s] 68%|██████▊   | 582/861 [00:13<00:06, 44.14it/s] 68%|██████▊   | 587/861 [00:13<00:06, 44.33it/s] 69%|██████▉   | 592/861 [00:13<00:06, 44.50it/s] 69%|██████▉   | 597/861 [00:13<00:05, 44.76it/s] 70%|██████▉   | 602/861 [00:13<00:05, 44.81it/s] 70%|███████   | 607/861 [00:13<00:05, 44.61it/s] 71%|███████   | 612/861 [00:13<00:05, 44.39it/s] 72%|███████▏  | 617/861 [00:13<00:05, 44.34it/s] 72%|███████▏  | 622/861 [00:14<00:05, 44.23it/s] 73%|███████▎  | 627/861 [00:14<00:05, 44.39it/s] 73%|███████▎  | 632/861 [00:14<00:05, 44.39it/s] 74%|███████▍  | 637/861 [00:14<00:05, 44.47it/s] 75%|███████▍  | 642/861 [00:14<00:04, 44.65it/s] 75%|███████▌  | 647/861 [00:14<00:04, 44.72it/s] 76%|███████▌  | 652/861 [00:14<00:04, 44.63it/s] 76%|███████▋  | 657/861 [00:14<00:04, 44.48it/s] 77%|███████▋  | 662/861 [00:14<00:04, 44.23it/s] 77%|███████▋  | 667/861 [00:15<00:04, 44.29it/s] 78%|███████▊  | 672/861 [00:15<00:04, 44.40it/s] 79%|███████▊  | 677/861 [00:15<00:04, 44.49it/s] 79%|███████▉  | 682/861 [00:15<00:04, 44.47it/s] 80%|███████▉  | 687/861 [00:15<00:03, 44.65it/s] 80%|████████  | 692/861 [00:15<00:03, 44.69it/s] 81%|████████  | 697/861 [00:15<00:03, 44.64it/s] 82%|████████▏ | 702/861 [00:15<00:03, 44.42it/s] 82%|████████▏ | 707/861 [00:15<00:03, 44.31it/s] 83%|████████▎ | 712/861 [00:16<00:03, 44.27it/s] 83%|████████▎ | 717/861 [00:16<00:03, 44.41it/s] 84%|████████▍ | 722/861 [00:16<00:03, 44.46it/s] 84%|████████▍ | 727/861 [00:16<00:03, 44.50it/s] 85%|████████▌ | 732/861 [00:16<00:02, 44.67it/s] 86%|████████▌ | 737/861 [00:16<00:02, 44.60it/s] 86%|████████▌ | 742/861 [00:16<00:02, 44.57it/s] 87%|████████▋ | 747/861 [00:16<00:02, 44.42it/s] 87%|████████▋ | 752/861 [00:16<00:02, 44.24it/s] 88%|████████▊ | 757/861 [00:17<00:02, 44.19it/s] 89%|████████▊ | 762/861 [00:17<00:02, 44.41it/s] 89%|████████▉ | 767/861 [00:17<00:02, 44.55it/s] 90%|████████▉ | 772/861 [00:17<00:01, 44.52it/s] 90%|█████████ | 777/861 [00:17<00:01, 44.59it/s] 91%|█████████ | 782/861 [00:17<00:01, 44.58it/s] 91%|█████████▏| 787/861 [00:17<00:01, 44.57it/s] 92%|█████████▏| 792/861 [00:17<00:01, 44.42it/s] 93%|█████████▎| 797/861 [00:17<00:01, 44.31it/s] 93%|█████████▎| 802/861 [00:18<00:01, 44.25it/s] 94%|█████████▎| 807/861 [00:18<00:01, 44.33it/s] 94%|█████████▍| 812/861 [00:18<00:01, 44.44it/s] 95%|█████████▍| 817/861 [00:18<00:00, 44.50it/s] 95%|█████████▌| 822/861 [00:18<00:00, 44.65it/s] 96%|█████████▌| 827/861 [00:18<00:00, 44.42it/s] 97%|█████████▋| 832/861 [00:18<00:00, 44.50it/s] 97%|█████████▋| 837/861 [00:18<00:00, 44.46it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.40it/s] 98%|█████████▊| 847/861 [00:19<00:00, 44.32it/s] 99%|█████████▉| 852/861 [00:19<00:00, 44.40it/s]100%|█████████▉| 857/861 [00:19<00:00, 44.47it/s]100%|██████████| 861/861 [00:19<00:00, 44.30it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:31:30,531 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:30,531 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:30,531 >>   eval_loss               =     1.0494
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:30,531 >>   eval_runtime            = 0:00:19.45
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:30,531 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:30,531 >>   eval_samples_per_second =    353.901
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:30,532 >>   eval_steps_per_second   =     44.263
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:31:30,532 >>   perplexity              =     2.8559
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:39,826 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:39,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:39,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:39,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:39,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:31:40,477 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:31:40,478 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:31:40,919 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:31:42,017 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:31:42,017 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:44,110 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:44,124 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:44,124 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:44,124 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:31:44,124 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:31:44,763 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:31:44,764 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:31:45,203 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:31:45,418 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:31:45,418 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 3it [00:02,  1.43it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.59it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.66it/s]Extractor Predicting: 14it [00:08,  1.72it/s]Extractor Predicting: 15it [00:09,  1.74it/s]Extractor Predicting: 16it [00:10,  1.73it/s]Extractor Predicting: 17it [00:10,  1.71it/s]Extractor Predicting: 18it [00:11,  1.71it/s]Extractor Predicting: 19it [00:11,  1.72it/s]Extractor Predicting: 20it [00:12,  1.68it/s]Extractor Predicting: 21it [00:13,  1.64it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:14,  1.69it/s]Extractor Predicting: 25it [00:15,  1.69it/s]Extractor Predicting: 26it [00:16,  1.69it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.65it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:19,  1.64it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:20,  1.64it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:22,  1.64it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:23,  1.66it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:25,  1.66it/s]Extractor Predicting: 42it [00:25,  1.66it/s]Extractor Predicting: 43it [00:26,  1.68it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:27,  1.71it/s]Extractor Predicting: 46it [00:28,  1.72it/s]Extractor Predicting: 47it [00:28,  1.68it/s]Extractor Predicting: 48it [00:29,  1.69it/s]Extractor Predicting: 49it [00:29,  1.67it/s]Extractor Predicting: 50it [00:30,  1.66it/s]Extractor Predicting: 51it [00:31,  1.64it/s]Extractor Predicting: 52it [00:31,  1.64it/s]Extractor Predicting: 53it [00:32,  1.61it/s]Extractor Predicting: 54it [00:33,  1.63it/s]Extractor Predicting: 55it [00:33,  1.62it/s]Extractor Predicting: 56it [00:34,  1.64it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:36,  1.61it/s]Extractor Predicting: 60it [00:36,  1.61it/s]Extractor Predicting: 61it [00:37,  1.65it/s]Extractor Predicting: 62it [00:37,  1.62it/s]Extractor Predicting: 63it [00:38,  1.62it/s]Extractor Predicting: 64it [00:39,  1.63it/s]Extractor Predicting: 65it [00:39,  1.65it/s]Extractor Predicting: 66it [00:40,  1.66it/s]Extractor Predicting: 67it [00:40,  1.68it/s]Extractor Predicting: 68it [00:41,  1.68it/s]Extractor Predicting: 69it [00:42,  1.66it/s]Extractor Predicting: 70it [00:42,  1.64it/s]Extractor Predicting: 71it [00:43,  1.63it/s]Extractor Predicting: 72it [00:44,  1.61it/s]Extractor Predicting: 73it [00:44,  1.62it/s]Extractor Predicting: 74it [00:45,  1.65it/s]Extractor Predicting: 75it [00:45,  1.68it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:47,  1.61it/s]Extractor Predicting: 78it [00:47,  1.64it/s]Extractor Predicting: 79it [00:48,  1.69it/s]Extractor Predicting: 80it [00:48,  1.72it/s]Extractor Predicting: 81it [00:49,  1.74it/s]Extractor Predicting: 82it [00:49,  1.81it/s]Extractor Predicting: 83it [00:50,  1.74it/s]Extractor Predicting: 84it [00:51,  1.75it/s]Extractor Predicting: 85it [00:51,  1.77it/s]Extractor Predicting: 86it [00:52,  1.79it/s]Extractor Predicting: 87it [00:52,  1.77it/s]Extractor Predicting: 88it [00:53,  1.78it/s]Extractor Predicting: 89it [00:53,  1.75it/s]Extractor Predicting: 90it [00:54,  1.80it/s]Extractor Predicting: 91it [00:54,  1.81it/s]Extractor Predicting: 92it [00:55,  1.65it/s]Extractor Predicting: 93it [00:56,  1.66it/s]Extractor Predicting: 94it [00:56,  1.71it/s]Extractor Predicting: 95it [00:57,  1.74it/s]Extractor Predicting: 96it [00:57,  1.72it/s]Extractor Predicting: 97it [00:58,  1.79it/s]Extractor Predicting: 98it [00:58,  1.79it/s]Extractor Predicting: 99it [00:59,  1.80it/s]Extractor Predicting: 100it [01:00,  1.80it/s]Extractor Predicting: 101it [01:00,  1.75it/s]Extractor Predicting: 102it [01:01,  1.75it/s]Extractor Predicting: 103it [01:01,  1.77it/s]Extractor Predicting: 104it [01:02,  1.77it/s]Extractor Predicting: 105it [01:02,  1.76it/s]Extractor Predicting: 106it [01:03,  1.79it/s]Extractor Predicting: 107it [01:04,  1.76it/s]Extractor Predicting: 108it [01:04,  1.82it/s]Extractor Predicting: 109it [01:05,  1.80it/s]Extractor Predicting: 110it [01:05,  1.80it/s]Extractor Predicting: 111it [01:06,  1.82it/s]Extractor Predicting: 112it [01:06,  1.77it/s]Extractor Predicting: 113it [01:07,  1.77it/s]Extractor Predicting: 114it [01:08,  1.73it/s]Extractor Predicting: 115it [01:08,  1.79it/s]Extractor Predicting: 116it [01:09,  1.73it/s]Extractor Predicting: 117it [01:09,  1.73it/s]Extractor Predicting: 118it [01:10,  1.74it/s]Extractor Predicting: 119it [01:10,  1.77it/s]Extractor Predicting: 120it [01:11,  1.81it/s]Extractor Predicting: 121it [01:11,  1.80it/s]Extractor Predicting: 122it [01:12,  1.73it/s]Extractor Predicting: 123it [01:13,  1.79it/s]Extractor Predicting: 124it [01:13,  1.75it/s]Extractor Predicting: 125it [01:14,  1.80it/s]Extractor Predicting: 126it [01:14,  1.82it/s]Extractor Predicting: 127it [01:15,  1.85it/s]Extractor Predicting: 128it [01:15,  1.71it/s]Extractor Predicting: 129it [01:16,  1.77it/s]Extractor Predicting: 130it [01:16,  1.79it/s]Extractor Predicting: 131it [01:17,  1.81it/s]Extractor Predicting: 132it [01:18,  1.86it/s]Extractor Predicting: 133it [01:18,  1.84it/s]Extractor Predicting: 134it [01:19,  1.80it/s]Extractor Predicting: 135it [01:19,  1.81it/s]Extractor Predicting: 136it [01:20,  1.81it/s]Extractor Predicting: 137it [01:20,  1.79it/s]Extractor Predicting: 138it [01:21,  1.80it/s]Extractor Predicting: 139it [01:22,  1.75it/s]Extractor Predicting: 140it [01:22,  1.71it/s]Extractor Predicting: 141it [01:23,  1.75it/s]Extractor Predicting: 142it [01:23,  1.75it/s]Extractor Predicting: 143it [01:24,  1.80it/s]Extractor Predicting: 144it [01:24,  1.75it/s]Extractor Predicting: 145it [01:25,  1.77it/s]Extractor Predicting: 146it [01:26,  1.74it/s]Extractor Predicting: 147it [01:26,  1.75it/s]Extractor Predicting: 148it [01:27,  1.76it/s]Extractor Predicting: 149it [01:27,  1.83it/s]Extractor Predicting: 150it [01:28,  1.74it/s]Extractor Predicting: 151it [01:28,  1.69it/s]Extractor Predicting: 152it [01:29,  1.63it/s]Extractor Predicting: 153it [01:30,  1.60it/s]Extractor Predicting: 154it [01:30,  1.51it/s]Extractor Predicting: 155it [01:31,  1.55it/s]Extractor Predicting: 156it [01:32,  1.52it/s]Extractor Predicting: 157it [01:32,  1.54it/s]Extractor Predicting: 158it [01:33,  1.54it/s]Extractor Predicting: 159it [01:34,  1.53it/s]Extractor Predicting: 160it [01:34,  1.50it/s]Extractor Predicting: 161it [01:35,  1.54it/s]Extractor Predicting: 162it [01:36,  1.47it/s]Extractor Predicting: 163it [01:36,  1.48it/s]Extractor Predicting: 164it [01:37,  1.48it/s]Extractor Predicting: 165it [01:38,  1.45it/s]Extractor Predicting: 166it [01:39,  1.46it/s]Extractor Predicting: 167it [01:39,  1.45it/s]Extractor Predicting: 168it [01:40,  1.47it/s]Extractor Predicting: 169it [01:41,  1.46it/s]Extractor Predicting: 170it [01:41,  1.47it/s]Extractor Predicting: 171it [01:42,  1.48it/s]Extractor Predicting: 172it [01:43,  1.45it/s]Extractor Predicting: 173it [01:43,  1.47it/s]Extractor Predicting: 174it [01:44,  1.52it/s]Extractor Predicting: 175it [01:45,  1.51it/s]Extractor Predicting: 176it [01:45,  1.49it/s]Extractor Predicting: 177it [01:46,  1.50it/s]Extractor Predicting: 178it [01:47,  1.50it/s]Extractor Predicting: 179it [01:47,  1.50it/s]Extractor Predicting: 180it [01:48,  1.49it/s]Extractor Predicting: 181it [01:49,  1.48it/s]Extractor Predicting: 182it [01:49,  1.46it/s]Extractor Predicting: 183it [01:50,  1.49it/s]Extractor Predicting: 184it [01:51,  1.52it/s]Extractor Predicting: 185it [01:51,  1.54it/s]Extractor Predicting: 186it [01:52,  1.39it/s]Extractor Predicting: 187it [01:53,  1.39it/s]Extractor Predicting: 188it [01:53,  1.46it/s]Extractor Predicting: 189it [01:54,  1.42it/s]Extractor Predicting: 190it [01:55,  1.43it/s]Extractor Predicting: 191it [01:56,  1.41it/s]Extractor Predicting: 192it [01:56,  1.37it/s]Extractor Predicting: 193it [01:57,  1.42it/s]Extractor Predicting: 194it [01:58,  1.47it/s]Extractor Predicting: 195it [01:58,  1.47it/s]Extractor Predicting: 196it [01:59,  1.50it/s]Extractor Predicting: 197it [02:00,  1.51it/s]Extractor Predicting: 198it [02:00,  1.53it/s]Extractor Predicting: 199it [02:01,  1.56it/s]Extractor Predicting: 200it [02:01,  1.56it/s]Extractor Predicting: 201it [02:02,  1.51it/s]Extractor Predicting: 202it [02:03,  1.50it/s]Extractor Predicting: 203it [02:03,  1.54it/s]Extractor Predicting: 204it [02:04,  1.54it/s]Extractor Predicting: 205it [02:05,  1.59it/s]Extractor Predicting: 206it [02:05,  1.56it/s]Extractor Predicting: 207it [02:06,  1.58it/s]Extractor Predicting: 208it [02:07,  1.56it/s]Extractor Predicting: 209it [02:07,  1.62it/s]Extractor Predicting: 210it [02:08,  1.61it/s]Extractor Predicting: 211it [02:09,  1.55it/s]Extractor Predicting: 212it [02:09,  1.57it/s]Extractor Predicting: 213it [02:10,  1.57it/s]Extractor Predicting: 214it [02:10,  1.59it/s]Extractor Predicting: 215it [02:11,  1.63it/s]Extractor Predicting: 216it [02:12,  1.63it/s]Extractor Predicting: 217it [02:12,  1.61it/s]Extractor Predicting: 218it [02:13,  1.64it/s]Extractor Predicting: 219it [02:14,  1.58it/s]Extractor Predicting: 220it [02:14,  1.57it/s]Extractor Predicting: 221it [02:15,  1.59it/s]Extractor Predicting: 222it [02:15,  1.60it/s]Extractor Predicting: 223it [02:16,  1.61it/s]Extractor Predicting: 224it [02:17,  1.60it/s]Extractor Predicting: 225it [02:17,  1.63it/s]Extractor Predicting: 226it [02:18,  1.62it/s]Extractor Predicting: 227it [02:18,  1.62it/s]Extractor Predicting: 228it [02:19,  1.57it/s]Extractor Predicting: 229it [02:20,  1.60it/s]Extractor Predicting: 230it [02:20,  1.60it/s]Extractor Predicting: 231it [02:21,  1.65it/s]Extractor Predicting: 232it [02:22,  1.66it/s]Extractor Predicting: 233it [02:22,  1.60it/s]Extractor Predicting: 234it [02:23,  1.60it/s]Extractor Predicting: 235it [02:23,  1.59it/s]Extractor Predicting: 236it [02:24,  1.61it/s]Extractor Predicting: 237it [02:25,  1.55it/s]Extractor Predicting: 237it [02:25,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:23,572 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:23,575 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:23,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:23,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:23,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:34:24,410 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:34:24,411 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:34:25,044 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:34:26,177 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:34:26,178 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:29,187 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:29,190 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:29,190 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:29,190 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:34:29,190 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:34:29,882 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:34:29,883 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:34:30,520 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:34:30,732 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:34:30,732 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.75it/s]Extractor Predicting: 6it [00:03,  1.80it/s]Extractor Predicting: 7it [00:03,  1.85it/s]Extractor Predicting: 8it [00:04,  1.86it/s]Extractor Predicting: 9it [00:05,  1.83it/s]Extractor Predicting: 10it [00:05,  1.83it/s]Extractor Predicting: 11it [00:06,  1.81it/s]Extractor Predicting: 12it [00:06,  1.82it/s]Extractor Predicting: 13it [00:07,  1.81it/s]Extractor Predicting: 14it [00:07,  1.78it/s]Extractor Predicting: 15it [00:08,  1.77it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:09,  1.74it/s]Extractor Predicting: 18it [00:10,  1.76it/s]Extractor Predicting: 19it [00:10,  1.78it/s]Extractor Predicting: 20it [00:11,  1.78it/s]Extractor Predicting: 21it [00:11,  1.82it/s]Extractor Predicting: 22it [00:12,  1.83it/s]Extractor Predicting: 23it [00:12,  1.87it/s]Extractor Predicting: 24it [00:13,  1.78it/s]Extractor Predicting: 25it [00:14,  1.80it/s]Extractor Predicting: 26it [00:14,  1.81it/s]Extractor Predicting: 27it [00:15,  1.80it/s]Extractor Predicting: 28it [00:15,  1.81it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:16,  1.79it/s]Extractor Predicting: 31it [00:17,  1.80it/s]Extractor Predicting: 32it [00:17,  1.83it/s]Extractor Predicting: 33it [00:18,  1.78it/s]Extractor Predicting: 34it [00:19,  1.75it/s]Extractor Predicting: 35it [00:19,  1.75it/s]Extractor Predicting: 36it [00:20,  1.77it/s]Extractor Predicting: 37it [00:20,  1.82it/s]Extractor Predicting: 38it [00:21,  1.87it/s]Extractor Predicting: 39it [00:21,  1.81it/s]Extractor Predicting: 40it [00:22,  1.77it/s]Extractor Predicting: 41it [00:23,  1.70it/s]Extractor Predicting: 42it [00:23,  1.69it/s]Extractor Predicting: 43it [00:24,  1.71it/s]Extractor Predicting: 44it [00:24,  1.68it/s]Extractor Predicting: 45it [00:25,  1.69it/s]Extractor Predicting: 46it [00:26,  1.64it/s]Extractor Predicting: 47it [00:26,  1.64it/s]Extractor Predicting: 48it [00:27,  1.67it/s]Extractor Predicting: 49it [00:27,  1.67it/s]Extractor Predicting: 50it [00:28,  1.67it/s]Extractor Predicting: 51it [00:29,  1.68it/s]Extractor Predicting: 52it [00:29,  1.65it/s]Extractor Predicting: 53it [00:30,  1.67it/s]Extractor Predicting: 54it [00:30,  1.67it/s]Extractor Predicting: 55it [00:31,  1.68it/s]Extractor Predicting: 56it [00:31,  1.71it/s]Extractor Predicting: 57it [00:32,  1.68it/s]Extractor Predicting: 58it [00:33,  1.70it/s]Extractor Predicting: 59it [00:33,  1.68it/s]Extractor Predicting: 60it [00:34,  1.69it/s]Extractor Predicting: 61it [00:35,  1.61it/s]Extractor Predicting: 62it [00:35,  1.65it/s]Extractor Predicting: 63it [00:36,  1.68it/s]Extractor Predicting: 64it [00:36,  1.72it/s]Extractor Predicting: 65it [00:37,  1.72it/s]Extractor Predicting: 66it [00:37,  1.72it/s]Extractor Predicting: 67it [00:38,  1.73it/s]Extractor Predicting: 68it [00:39,  1.72it/s]Extractor Predicting: 69it [00:39,  1.73it/s]Extractor Predicting: 70it [00:40,  1.66it/s]Extractor Predicting: 71it [00:40,  1.69it/s]Extractor Predicting: 72it [00:41,  1.70it/s]Extractor Predicting: 73it [00:42,  1.67it/s]Extractor Predicting: 74it [00:42,  1.72it/s]Extractor Predicting: 75it [00:43,  1.72it/s]Extractor Predicting: 76it [00:43,  1.70it/s]Extractor Predicting: 77it [00:44,  1.70it/s]Extractor Predicting: 78it [00:45,  1.67it/s]Extractor Predicting: 79it [00:45,  1.70it/s]Extractor Predicting: 80it [00:46,  1.73it/s]Extractor Predicting: 81it [00:46,  1.71it/s]Extractor Predicting: 82it [00:47,  1.66it/s]Extractor Predicting: 83it [00:47,  1.72it/s]Extractor Predicting: 84it [00:48,  1.70it/s]Extractor Predicting: 85it [00:49,  1.68it/s]Extractor Predicting: 86it [00:49,  1.71it/s]Extractor Predicting: 87it [00:50,  1.72it/s]Extractor Predicting: 88it [00:50,  1.68it/s]Extractor Predicting: 89it [00:51,  1.68it/s]Extractor Predicting: 90it [00:52,  1.69it/s]Extractor Predicting: 91it [00:52,  1.68it/s]Extractor Predicting: 92it [00:53,  1.71it/s]Extractor Predicting: 93it [00:53,  1.76it/s]Extractor Predicting: 94it [00:54,  1.69it/s]Extractor Predicting: 95it [00:55,  1.68it/s]Extractor Predicting: 96it [00:55,  1.70it/s]Extractor Predicting: 97it [00:56,  1.69it/s]Extractor Predicting: 98it [00:56,  1.71it/s]Extractor Predicting: 99it [00:57,  1.73it/s]Extractor Predicting: 100it [00:58,  1.61it/s]Extractor Predicting: 101it [00:58,  1.69it/s]Extractor Predicting: 102it [00:59,  1.67it/s]Extractor Predicting: 103it [00:59,  1.70it/s]Extractor Predicting: 104it [01:00,  1.71it/s]Extractor Predicting: 105it [01:00,  1.70it/s]Extractor Predicting: 106it [01:01,  1.75it/s]Extractor Predicting: 107it [01:02,  1.78it/s]Extractor Predicting: 108it [01:02,  1.81it/s]Extractor Predicting: 109it [01:03,  1.75it/s]Extractor Predicting: 110it [01:03,  1.76it/s]Extractor Predicting: 111it [01:04,  1.72it/s]Extractor Predicting: 112it [01:04,  1.72it/s]Extractor Predicting: 113it [01:05,  1.76it/s]Extractor Predicting: 114it [01:06,  1.71it/s]Extractor Predicting: 115it [01:06,  1.70it/s]Extractor Predicting: 116it [01:07,  1.74it/s]Extractor Predicting: 117it [01:07,  1.73it/s]Extractor Predicting: 118it [01:08,  1.77it/s]Extractor Predicting: 119it [01:08,  1.76it/s]Extractor Predicting: 120it [01:09,  1.75it/s]Extractor Predicting: 121it [01:10,  1.68it/s]Extractor Predicting: 122it [01:10,  1.64it/s]Extractor Predicting: 123it [01:11,  1.67it/s]Extractor Predicting: 124it [01:11,  1.71it/s]Extractor Predicting: 125it [01:12,  1.74it/s]Extractor Predicting: 126it [01:13,  1.75it/s]Extractor Predicting: 127it [01:13,  1.73it/s]Extractor Predicting: 128it [01:14,  1.78it/s]Extractor Predicting: 129it [01:14,  1.78it/s]Extractor Predicting: 130it [01:15,  1.79it/s]Extractor Predicting: 131it [01:15,  1.77it/s]Extractor Predicting: 132it [01:16,  1.73it/s]Extractor Predicting: 133it [01:17,  1.72it/s]Extractor Predicting: 134it [01:17,  1.75it/s]Extractor Predicting: 135it [01:18,  1.76it/s]Extractor Predicting: 136it [01:18,  1.59it/s]Extractor Predicting: 137it [01:19,  1.61it/s]Extractor Predicting: 138it [01:20,  1.62it/s]Extractor Predicting: 139it [01:20,  1.61it/s]Extractor Predicting: 140it [01:21,  1.68it/s]Extractor Predicting: 141it [01:21,  1.68it/s]Extractor Predicting: 142it [01:22,  1.66it/s]Extractor Predicting: 143it [01:23,  1.65it/s]Extractor Predicting: 144it [01:23,  1.65it/s]Extractor Predicting: 145it [01:24,  1.68it/s]Extractor Predicting: 146it [01:24,  1.67it/s]Extractor Predicting: 147it [01:25,  1.68it/s]Extractor Predicting: 148it [01:26,  1.58it/s]Extractor Predicting: 149it [01:26,  1.58it/s]Extractor Predicting: 150it [01:27,  1.61it/s]Extractor Predicting: 151it [01:27,  1.65it/s]Extractor Predicting: 152it [01:28,  1.65it/s]Extractor Predicting: 153it [01:29,  1.67it/s]Extractor Predicting: 154it [01:29,  1.63it/s]Extractor Predicting: 155it [01:30,  1.65it/s]Extractor Predicting: 156it [01:31,  1.65it/s]Extractor Predicting: 157it [01:31,  1.62it/s]Extractor Predicting: 158it [01:32,  1.61it/s]Extractor Predicting: 159it [01:32,  1.60it/s]Extractor Predicting: 160it [01:33,  1.64it/s]Extractor Predicting: 161it [01:34,  1.66it/s]Extractor Predicting: 162it [01:34,  1.64it/s]Extractor Predicting: 163it [01:35,  1.59it/s]Extractor Predicting: 164it [01:36,  1.59it/s]Extractor Predicting: 165it [01:36,  1.58it/s]Extractor Predicting: 166it [01:37,  1.58it/s]Extractor Predicting: 167it [01:37,  1.60it/s]Extractor Predicting: 168it [01:38,  1.74it/s]Extractor Predicting: 168it [01:38,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,503 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,503 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,503 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:36:20,102 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:36:20,103 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:36:20,421 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:36:21,541 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:36:21,541 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:23,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:23,391 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:23,391 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:23,391 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:23,391 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:36:24,241 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:36:24,242 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:36:24,953 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:36:25,158 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:36:25,158 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.72it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 19it [00:11,  1.65it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_1', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:17, 15.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:18, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:49<01:55, 16.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:04<01:34, 15.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:18<01:16, 15.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:33<01:00, 15.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:49<00:46, 15.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:04<00:30, 15.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:18<00:14, 14.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:34<00:00, 15.27s/it]Generating: 100%|██████████| 10/10 [02:34<00:00, 15.45s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9166666666666666, 'errors': {'', "('', 'composer', 'Jérémie Léger', 'The title track of the album was a track Iced Tea made by Jérémie Léger and written by Pierre Bousquet and Bernard Barrou , among others .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 397, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 443, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 489, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 555, 'raw': 800}
{'target': 600, 'success': 574, 'raw': 832}
{'target': 600, 'success': 593, 'raw': 864}
{'target': 600, 'success': 614, 'raw': 896}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.6852678571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : opposite of . Context : Later in the year ( 1143 ) , Ptolemaeus , the founder of the Macedonian Republic , married Ariadne , daughter of Ariadne , founder of Macedonian kings , Thebes and Ileani . Head Entity : Ileani , Tail Entity : Ileanius .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8491847826086957, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'not enough values to unpack (expected 2, got 1)', ''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : creator .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.75375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', "('Lothar', 'twinned administrative body', '', 'He died in the Battle of Mervyn at Rheinmetall in 645 along with his daughter Lothar , D.')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 12521
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12621, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_5_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:14, 14.89s/it]Extractor Estimating: 2it [00:17,  7.82s/it]Extractor Estimating: 3it [00:18,  4.54s/it]Extractor Estimating: 4it [00:19,  2.99s/it]Extractor Estimating: 5it [00:19,  2.13s/it]Extractor Estimating: 6it [00:20,  1.61s/it]Extractor Estimating: 7it [00:20,  1.27s/it]Extractor Estimating: 8it [00:21,  1.11s/it]Extractor Estimating: 9it [00:22,  1.04it/s]Extractor Estimating: 10it [00:22,  1.17it/s]Extractor Estimating: 11it [00:23,  1.05it/s]Extractor Estimating: 12it [00:24,  1.15it/s]Extractor Estimating: 13it [00:25,  1.27it/s]Extractor Estimating: 14it [00:25,  1.36it/s]Extractor Estimating: 15it [00:26,  1.44it/s]Extractor Estimating: 16it [00:27,  1.45it/s]Extractor Estimating: 17it [00:29,  1.20s/it]Extractor Estimating: 18it [00:30,  1.03s/it]Extractor Estimating: 19it [00:30,  1.09it/s]Extractor Estimating: 20it [00:31,  1.21it/s]Extractor Estimating: 21it [00:32,  1.28it/s]Extractor Estimating: 22it [00:32,  1.38it/s]Extractor Estimating: 23it [00:33,  1.47it/s]Extractor Estimating: 24it [00:34,  1.43it/s]Extractor Estimating: 25it [00:34,  1.48it/s]Extractor Estimating: 26it [00:35,  1.51it/s]Extractor Estimating: 27it [00:35,  1.54it/s]Extractor Estimating: 28it [00:36,  1.53it/s]Extractor Estimating: 29it [00:37,  1.56it/s]Extractor Estimating: 30it [00:37,  1.57it/s]Extractor Estimating: 31it [00:38,  1.58it/s]Extractor Estimating: 32it [00:39,  1.56it/s]Extractor Estimating: 33it [00:39,  1.53it/s]Extractor Estimating: 34it [00:40,  1.53it/s]Extractor Estimating: 35it [00:41,  1.53it/s]Extractor Estimating: 36it [00:41,  1.54it/s]Extractor Estimating: 37it [00:42,  1.49it/s]Extractor Estimating: 38it [00:43,  1.52it/s]Extractor Estimating: 39it [00:43,  1.49it/s]Extractor Estimating: 40it [00:44,  1.51it/s]Extractor Estimating: 41it [00:45,  1.54it/s]Extractor Estimating: 42it [00:45,  1.52it/s]Extractor Estimating: 43it [00:46,  1.54it/s]Extractor Estimating: 44it [00:46,  1.54it/s]Extractor Estimating: 45it [00:47,  1.54it/s]Extractor Estimating: 46it [00:48,  1.52it/s]Extractor Estimating: 47it [00:48,  1.52it/s]Extractor Estimating: 48it [00:49,  1.51it/s]Extractor Estimating: 49it [00:50,  1.46it/s]Extractor Estimating: 50it [00:50,  1.51it/s]Extractor Estimating: 51it [00:51,  1.50it/s]Extractor Estimating: 52it [00:52,  1.53it/s]Extractor Estimating: 53it [00:52,  1.54it/s]Extractor Estimating: 54it [00:53,  1.52it/s]Extractor Estimating: 55it [00:54,  1.52it/s]Extractor Estimating: 56it [00:54,  1.53it/s]Extractor Estimating: 57it [00:55,  1.51it/s]Extractor Estimating: 58it [00:56,  1.51it/s]Extractor Estimating: 59it [00:56,  1.51it/s]Extractor Estimating: 60it [00:57,  1.57it/s]Extractor Estimating: 61it [00:58,  1.60it/s]Extractor Estimating: 62it [00:58,  1.57it/s]Extractor Estimating: 63it [00:59,  1.55it/s]Extractor Estimating: 64it [01:00,  1.54it/s]Extractor Estimating: 65it [01:00,  1.54it/s]Extractor Estimating: 66it [01:01,  1.59it/s]Extractor Estimating: 67it [01:01,  1.56it/s]Extractor Estimating: 68it [01:02,  1.63it/s]Extractor Estimating: 69it [01:03,  1.66it/s]Extractor Estimating: 70it [01:03,  1.72it/s]Extractor Estimating: 71it [01:04,  1.70it/s]Extractor Estimating: 72it [01:04,  1.63it/s]Extractor Estimating: 73it [01:05,  1.66it/s]Extractor Estimating: 74it [01:06,  1.64it/s]Extractor Estimating: 75it [01:06,  1.50it/s]Extractor Estimating: 76it [01:07,  1.54it/s]Extractor Estimating: 77it [01:08,  1.55it/s]Extractor Estimating: 78it [01:08,  1.55it/s]Extractor Estimating: 79it [01:09,  1.58it/s]Extractor Estimating: 80it [01:10,  1.55it/s]Extractor Estimating: 81it [01:10,  1.55it/s]Extractor Estimating: 82it [01:11,  1.54it/s]Extractor Estimating: 83it [01:12,  1.53it/s]Extractor Estimating: 84it [01:12,  1.59it/s]Extractor Estimating: 85it [01:13,  1.64it/s]Extractor Estimating: 86it [01:13,  1.65it/s]Extractor Estimating: 87it [01:14,  1.61it/s]Extractor Estimating: 88it [01:14,  1.65it/s]Extractor Estimating: 89it [01:15,  1.62it/s]Extractor Estimating: 90it [01:16,  1.62it/s]Extractor Estimating: 91it [01:16,  1.61it/s]Extractor Estimating: 92it [01:17,  1.61it/s]Extractor Estimating: 93it [01:18,  1.63it/s]Extractor Estimating: 94it [01:18,  1.63it/s]Extractor Estimating: 95it [01:19,  1.64it/s]Extractor Estimating: 96it [01:19,  1.65it/s]Extractor Estimating: 97it [01:20,  1.69it/s]Extractor Estimating: 98it [01:21,  1.70it/s]Extractor Estimating: 99it [01:21,  1.71it/s]Extractor Estimating: 100it [01:22,  1.70it/s]Extractor Estimating: 101it [01:22,  1.61it/s]Extractor Estimating: 102it [01:23,  1.61it/s]Extractor Estimating: 103it [01:24,  1.57it/s]Extractor Estimating: 104it [01:24,  1.57it/s]Extractor Estimating: 105it [01:25,  1.61it/s]Extractor Estimating: 106it [01:26,  1.57it/s]Extractor Estimating: 107it [01:26,  1.61it/s]Extractor Estimating: 108it [01:27,  1.61it/s]Extractor Estimating: 109it [01:27,  1.63it/s]Extractor Estimating: 110it [01:28,  1.66it/s]Extractor Estimating: 111it [01:29,  1.60it/s]Extractor Estimating: 112it [01:29,  1.64it/s]Extractor Estimating: 113it [01:30,  1.64it/s]Extractor Estimating: 114it [01:31,  1.61it/s]Extractor Estimating: 115it [01:31,  1.61it/s]Extractor Estimating: 116it [01:32,  1.61it/s]Extractor Estimating: 117it [01:32,  1.60it/s]Extractor Estimating: 118it [01:33,  1.56it/s]Extractor Estimating: 119it [01:34,  1.56it/s]Extractor Estimating: 120it [01:34,  1.58it/s]Extractor Estimating: 121it [01:35,  1.58it/s]Extractor Estimating: 122it [01:36,  1.60it/s]Extractor Estimating: 123it [01:36,  1.61it/s]Extractor Estimating: 124it [01:37,  1.61it/s]Extractor Estimating: 125it [01:37,  1.60it/s]Extractor Estimating: 126it [01:38,  1.55it/s]Extractor Estimating: 127it [01:39,  1.59it/s]Extractor Estimating: 128it [01:39,  1.62it/s]Extractor Estimating: 129it [01:40,  1.62it/s]Extractor Estimating: 130it [01:40,  1.67it/s]Extractor Estimating: 131it [01:41,  1.57it/s]Extractor Estimating: 132it [01:42,  1.56it/s]Extractor Estimating: 133it [01:42,  1.58it/s]Extractor Estimating: 134it [01:43,  1.63it/s]Extractor Estimating: 135it [01:44,  1.63it/s]Extractor Estimating: 136it [01:44,  1.58it/s]Extractor Estimating: 137it [01:45,  1.59it/s]Extractor Estimating: 138it [01:46,  1.60it/s]Extractor Estimating: 139it [01:46,  1.61it/s]Extractor Estimating: 140it [01:47,  1.61it/s]Extractor Estimating: 141it [01:47,  1.59it/s]Extractor Estimating: 142it [01:48,  1.58it/s]Extractor Estimating: 143it [01:49,  1.46it/s]Extractor Estimating: 144it [01:50,  1.46it/s]Extractor Estimating: 145it [01:50,  1.51it/s]Extractor Estimating: 146it [01:51,  1.53it/s]Extractor Estimating: 147it [01:51,  1.55it/s]Extractor Estimating: 148it [01:52,  1.56it/s]Extractor Estimating: 149it [01:53,  1.59it/s]Extractor Estimating: 150it [01:53,  1.61it/s]Extractor Estimating: 151it [01:54,  1.64it/s]Extractor Estimating: 152it [01:55,  1.59it/s]Extractor Estimating: 153it [01:55,  1.60it/s]Extractor Estimating: 154it [01:56,  1.55it/s]Extractor Estimating: 155it [01:56,  1.56it/s]Extractor Estimating: 156it [01:57,  1.56it/s]Extractor Estimating: 157it [01:58,  1.58it/s]Extractor Estimating: 158it [01:58,  1.60it/s]Extractor Estimating: 159it [01:59,  1.55it/s]Extractor Estimating: 160it [02:00,  1.59it/s]Extractor Estimating: 161it [02:00,  1.57it/s]Extractor Estimating: 162it [02:01,  1.61it/s]Extractor Estimating: 163it [02:02,  1.57it/s]Extractor Estimating: 164it [02:02,  1.52it/s]Extractor Estimating: 165it [02:03,  1.54it/s]Extractor Estimating: 166it [02:03,  1.58it/s]Extractor Estimating: 167it [02:04,  1.59it/s]Extractor Estimating: 168it [02:05,  1.65it/s]Extractor Estimating: 169it [02:05,  1.64it/s]Extractor Estimating: 170it [02:06,  1.59it/s]Extractor Estimating: 171it [02:07,  1.56it/s]Extractor Estimating: 172it [02:07,  1.60it/s]Extractor Estimating: 173it [02:08,  1.60it/s]Extractor Estimating: 174it [02:08,  1.61it/s]Extractor Estimating: 175it [02:09,  1.60it/s]Extractor Estimating: 176it [02:10,  1.62it/s]Extractor Estimating: 177it [02:10,  1.53it/s]Extractor Estimating: 178it [02:11,  1.55it/s]Extractor Estimating: 179it [02:12,  1.54it/s]Extractor Estimating: 180it [02:12,  1.56it/s]Extractor Estimating: 181it [02:13,  1.61it/s]Extractor Estimating: 182it [02:14,  1.61it/s]Extractor Estimating: 183it [02:14,  1.63it/s]Extractor Estimating: 184it [02:15,  1.60it/s]Extractor Estimating: 185it [02:15,  1.57it/s]Extractor Estimating: 186it [02:16,  1.56it/s]Extractor Estimating: 187it [02:17,  1.60it/s]Extractor Estimating: 188it [02:17,  1.58it/s]Extractor Estimating: 189it [02:18,  1.56it/s]Extractor Estimating: 190it [02:19,  1.60it/s]Extractor Estimating: 191it [02:19,  1.62it/s]Extractor Estimating: 192it [02:20,  1.57it/s]Extractor Estimating: 193it [02:20,  1.56it/s]Extractor Estimating: 194it [02:21,  1.54it/s]Extractor Estimating: 195it [02:22,  1.56it/s]Extractor Estimating: 196it [02:22,  1.56it/s]Extractor Estimating: 197it [02:23,  1.56it/s]Extractor Estimating: 198it [02:24,  1.56it/s]Extractor Estimating: 199it [02:24,  1.58it/s]Extractor Estimating: 200it [02:25,  1.57it/s]Extractor Estimating: 201it [02:26,  1.60it/s]Extractor Estimating: 202it [02:26,  1.58it/s]Extractor Estimating: 203it [02:27,  1.60it/s]Extractor Estimating: 204it [02:27,  1.61it/s]Extractor Estimating: 205it [02:28,  1.62it/s]Extractor Estimating: 206it [02:29,  1.64it/s]Extractor Estimating: 207it [02:29,  1.64it/s]Extractor Estimating: 208it [02:30,  1.64it/s]Extractor Estimating: 209it [02:30,  1.65it/s]Extractor Estimating: 210it [02:31,  1.65it/s]Extractor Estimating: 211it [02:32,  1.66it/s]Extractor Estimating: 212it [02:32,  1.64it/s]Extractor Estimating: 213it [02:33,  1.68it/s]Extractor Estimating: 214it [02:33,  1.72it/s]Extractor Estimating: 215it [02:34,  1.73it/s]Extractor Estimating: 216it [02:35,  1.72it/s]Extractor Estimating: 217it [02:35,  1.69it/s]Extractor Estimating: 218it [02:36,  1.68it/s]Extractor Estimating: 219it [02:36,  1.70it/s]Extractor Estimating: 220it [02:37,  1.70it/s]Extractor Estimating: 221it [02:38,  1.55it/s]Extractor Estimating: 222it [02:38,  1.53it/s]Extractor Estimating: 223it [02:39,  1.52it/s]Extractor Estimating: 224it [02:40,  1.55it/s]Extractor Estimating: 225it [02:40,  1.57it/s]Extractor Estimating: 226it [02:41,  1.59it/s]Extractor Estimating: 227it [02:42,  1.59it/s]Extractor Estimating: 228it [02:42,  1.58it/s]Extractor Estimating: 229it [02:43,  1.56it/s]Extractor Estimating: 230it [02:43,  1.63it/s]Extractor Estimating: 231it [02:44,  1.58it/s]Extractor Estimating: 232it [02:45,  1.61it/s]Extractor Estimating: 233it [02:45,  1.62it/s]Extractor Estimating: 234it [02:46,  1.67it/s]Extractor Estimating: 235it [02:46,  1.63it/s]Extractor Estimating: 236it [02:47,  1.63it/s]Extractor Estimating: 237it [02:48,  1.60it/s]Extractor Estimating: 238it [02:48,  1.66it/s]Extractor Estimating: 239it [02:49,  1.62it/s]Extractor Estimating: 240it [02:50,  1.59it/s]Extractor Estimating: 241it [02:50,  1.64it/s]Extractor Estimating: 242it [02:51,  1.63it/s]Extractor Estimating: 243it [02:51,  1.63it/s]Extractor Estimating: 244it [02:55,  1.50s/it]Extractor Estimating: 245it [02:56,  1.23s/it]Extractor Estimating: 246it [02:56,  1.06s/it]Extractor Estimating: 247it [02:57,  1.09it/s]Extractor Estimating: 248it [02:57,  1.23it/s]Extractor Estimating: 249it [02:58,  1.30it/s]Extractor Estimating: 250it [02:59,  1.46it/s]Extractor Estimating: 250it [02:59,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5005 mean pseudo reward: 0.8828492202896392
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 26877
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26977, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_filtered_large/unseen_5_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26977, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.269, loss:1577.6706
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.993, loss:1479.5702
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.986, loss:1429.2883
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.992, loss:1397.2134
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.993, loss:1377.0999
>> valid entity prec:0.4876, rec:0.4477, f1:0.4668
>> valid relation prec:0.2308, rec:0.0004, f1:0.0009
>> valid relation with NER prec:0.2308, rec:0.0004, f1:0.0009
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.111, loss:1339.4718
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 0.988, loss:1292.6637
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.991, loss:1289.2251
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.994, loss:1285.1703
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.994, loss:1230.3287
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4168, rec:0.5881, f1:0.4878
>> valid relation prec:0.0488, rec:0.0006, f1:0.0011
>> valid relation with NER prec:0.0488, rec:0.0006, f1:0.0011
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 55, avg_time 3.141, loss:1189.6180
g_step 1200, step 155, avg_time 0.996, loss:1181.0041
g_step 1300, step 46, avg_time 0.985, loss:1175.1545
g_step 1400, step 146, avg_time 0.998, loss:1141.4758
g_step 1500, step 37, avg_time 0.991, loss:1072.3661
>> valid entity prec:0.5092, rec:0.2641, f1:0.3478
>> valid relation prec:0.4753, rec:0.0112, f1:0.0219
>> valid relation with NER prec:0.4753, rec:0.0112, f1:0.0219
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 137, avg_time 3.045, loss:1086.6616
g_step 1700, step 28, avg_time 0.993, loss:1085.3833
g_step 1800, step 128, avg_time 0.994, loss:1026.0789
g_step 1900, step 19, avg_time 0.995, loss:1056.2285
g_step 2000, step 119, avg_time 0.994, loss:998.6027
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4325, rec:0.4387, f1:0.4355
>> valid relation prec:0.3837, rec:0.0144, f1:0.0277
>> valid relation with NER prec:0.3837, rec:0.0144, f1:0.0277
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 10, avg_time 3.096, loss:1024.9218
g_step 2200, step 110, avg_time 0.989, loss:939.4659
g_step 2300, step 1, avg_time 0.984, loss:989.0907
g_step 2400, step 101, avg_time 0.993, loss:914.3362
g_step 2500, step 201, avg_time 0.971, loss:953.4689
>> valid entity prec:0.4754, rec:0.4865, f1:0.4809
>> valid relation prec:0.3816, rec:0.0126, f1:0.0245
>> valid relation with NER prec:0.3816, rec:0.0126, f1:0.0245
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 92, avg_time 3.114, loss:891.7392
g_step 2700, step 192, avg_time 0.985, loss:918.9744
g_step 2800, step 83, avg_time 0.982, loss:884.2886
g_step 2900, step 183, avg_time 0.995, loss:886.8784
g_step 3000, step 74, avg_time 0.985, loss:855.7432
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4589, rec:0.4629, f1:0.4609
>> valid relation prec:0.2834, rec:0.0201, f1:0.0375
>> valid relation with NER prec:0.2834, rec:0.0201, f1:0.0375
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 174, avg_time 3.099, loss:843.3501
g_step 3200, step 65, avg_time 0.983, loss:808.8324
g_step 3300, step 165, avg_time 0.992, loss:826.3566
g_step 3400, step 56, avg_time 0.987, loss:802.2284
g_step 3500, step 156, avg_time 0.989, loss:808.4431
>> valid entity prec:0.4811, rec:0.4127, f1:0.4443
>> valid relation prec:0.1530, rec:0.0081, f1:0.0155
>> valid relation with NER prec:0.1530, rec:0.0081, f1:0.0155
g_step 3600, step 47, avg_time 3.088, loss:752.4843
g_step 3700, step 147, avg_time 0.982, loss:746.0112
g_step 3800, step 38, avg_time 0.998, loss:774.7378
g_step 3900, step 138, avg_time 0.983, loss:720.8263
g_step 4000, step 29, avg_time 0.986, loss:741.1491
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4560, rec:0.4190, f1:0.4367
>> valid relation prec:0.2394, rec:0.0173, f1:0.0323
>> valid relation with NER prec:0.2394, rec:0.0173, f1:0.0323
g_step 4100, step 129, avg_time 3.091, loss:701.6923
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:25:09 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:25:09 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-25-09_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:25:10 - WARNING - datasets.builder -   Using custom data configuration default-632937eb510c6531
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-632937eb510c6531/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:25:12,441 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:25:12,443 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:25:12,443 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:25:12,444 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:25:12,509 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:25:12,540 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:25:12,540 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:25:12,541 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:25:12,541 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:25:12,541 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:25:12,541 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:25:12,799 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:25:15,936 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:25:15,936 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-632937eb510c6531/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 10:25:15 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14e8e73d25f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:03,  1.32ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.29ba/s] 50%|█████     | 3/6 [00:01<00:01,  2.97ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.44ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.78ba/s]100%|██████████| 6/6 [00:01<00:00,  3.64ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.52ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.01ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.22ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.33ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.39ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.25ba/s]100%|██████████| 7/7 [00:01<00:00,  4.49ba/s]100%|██████████| 7/7 [00:01<00:00,  4.31ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  6.52ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.19ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.32ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.50ba/s]100%|██████████| 6/6 [00:00<00:00, 10.73ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  5.92ba/s] 43%|████▎     | 3/7 [00:00<00:00,  8.87ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.79ba/s]100%|██████████| 7/7 [00:00<00:00, 10.49ba/s]100%|██████████| 7/7 [00:00<00:00,  9.84ba/s]
[INFO|trainer.py:414] 2023-08-28 10:25:21,446 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:25:21,454 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:25:21,454 >>   Num examples = 5061
[INFO|trainer.py:1149] 2023-08-28 10:25:21,454 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:25:21,454 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:25:21,454 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:25:21,454 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:25:21,454 >>   Total optimization steps = 395
  0%|          | 0/395 [00:00<?, ?it/s]  0%|          | 1/395 [00:00<01:58,  3.32it/s]  1%|          | 2/395 [00:00<01:55,  3.39it/s]  1%|          | 3/395 [00:00<01:54,  3.42it/s]  1%|          | 4/395 [00:01<01:53,  3.43it/s]  1%|▏         | 5/395 [00:01<01:53,  3.44it/s]  2%|▏         | 6/395 [00:01<01:52,  3.45it/s]  2%|▏         | 7/395 [00:02<01:52,  3.45it/s]  2%|▏         | 8/395 [00:02<01:52,  3.45it/s]  2%|▏         | 9/395 [00:02<01:52,  3.43it/s]  3%|▎         | 10/395 [00:02<01:52,  3.42it/s]  3%|▎         | 11/395 [00:03<01:52,  3.42it/s]  3%|▎         | 12/395 [00:03<01:52,  3.41it/s]  3%|▎         | 13/395 [00:03<01:52,  3.41it/s]  4%|▎         | 14/395 [00:04<01:51,  3.40it/s]  4%|▍         | 15/395 [00:04<01:51,  3.40it/s]  4%|▍         | 16/395 [00:04<01:51,  3.40it/s]  4%|▍         | 17/395 [00:04<01:51,  3.40it/s]  5%|▍         | 18/395 [00:05<01:50,  3.40it/s]  5%|▍         | 19/395 [00:05<01:50,  3.40it/s]  5%|▌         | 20/395 [00:05<01:50,  3.40it/s]  5%|▌         | 21/395 [00:06<01:50,  3.40it/s]  6%|▌         | 22/395 [00:06<01:49,  3.40it/s]  6%|▌         | 23/395 [00:06<01:49,  3.40it/s]  6%|▌         | 24/395 [00:07<01:49,  3.40it/s]  6%|▋         | 25/395 [00:07<01:48,  3.40it/s]  7%|▋         | 26/395 [00:07<01:48,  3.40it/s]  7%|▋         | 27/395 [00:07<01:48,  3.40it/s]  7%|▋         | 28/395 [00:08<01:48,  3.40it/s]  7%|▋         | 29/395 [00:08<01:47,  3.40it/s]  8%|▊         | 30/395 [00:08<01:47,  3.40it/s]  8%|▊         | 31/395 [00:09<01:47,  3.40it/s]  8%|▊         | 32/395 [00:09<01:46,  3.40it/s]  8%|▊         | 33/395 [00:09<01:46,  3.40it/s]  9%|▊         | 34/395 [00:10<01:48,  3.32it/s]  9%|▉         | 35/395 [00:10<01:47,  3.35it/s]  9%|▉         | 36/395 [00:10<01:46,  3.36it/s]  9%|▉         | 37/395 [00:10<01:46,  3.37it/s] 10%|▉         | 38/395 [00:11<01:45,  3.38it/s] 10%|▉         | 39/395 [00:11<01:45,  3.38it/s] 10%|█         | 40/395 [00:11<01:44,  3.39it/s] 10%|█         | 41/395 [00:12<01:44,  3.39it/s] 11%|█         | 42/395 [00:12<01:44,  3.39it/s] 11%|█         | 43/395 [00:12<01:43,  3.40it/s] 11%|█         | 44/395 [00:12<01:43,  3.40it/s] 11%|█▏        | 45/395 [00:13<01:42,  3.40it/s] 12%|█▏        | 46/395 [00:13<01:42,  3.40it/s] 12%|█▏        | 47/395 [00:13<01:42,  3.40it/s] 12%|█▏        | 48/395 [00:14<01:42,  3.40it/s] 12%|█▏        | 49/395 [00:14<01:41,  3.40it/s] 13%|█▎        | 50/395 [00:14<01:41,  3.40it/s] 13%|█▎        | 51/395 [00:15<01:41,  3.40it/s] 13%|█▎        | 52/395 [00:15<01:40,  3.40it/s] 13%|█▎        | 53/395 [00:15<01:40,  3.40it/s] 14%|█▎        | 54/395 [00:15<01:40,  3.40it/s] 14%|█▍        | 55/395 [00:16<01:40,  3.40it/s] 14%|█▍        | 56/395 [00:16<01:39,  3.40it/s] 14%|█▍        | 57/395 [00:16<01:39,  3.40it/s] 15%|█▍        | 58/395 [00:17<01:39,  3.40it/s] 15%|█▍        | 59/395 [00:17<01:38,  3.40it/s] 15%|█▌        | 60/395 [00:17<01:38,  3.39it/s] 15%|█▌        | 61/395 [00:17<01:38,  3.39it/s] 16%|█▌        | 62/395 [00:18<01:38,  3.40it/s] 16%|█▌        | 63/395 [00:18<01:37,  3.39it/s] 16%|█▌        | 64/395 [00:18<01:39,  3.31it/s] 16%|█▋        | 65/395 [00:19<01:38,  3.34it/s] 17%|█▋        | 66/395 [00:19<01:38,  3.35it/s] 17%|█▋        | 67/395 [00:19<01:37,  3.37it/s] 17%|█▋        | 68/395 [00:20<01:36,  3.38it/s] 17%|█▋        | 69/395 [00:20<01:36,  3.38it/s] 18%|█▊        | 70/395 [00:20<01:35,  3.39it/s] 18%|█▊        | 71/395 [00:20<01:35,  3.39it/s] 18%|█▊        | 72/395 [00:21<01:35,  3.39it/s] 18%|█▊        | 73/395 [00:21<01:34,  3.39it/s] 19%|█▊        | 74/395 [00:21<01:34,  3.39it/s] 19%|█▉        | 75/395 [00:22<01:34,  3.40it/s] 19%|█▉        | 76/395 [00:22<01:33,  3.39it/s] 19%|█▉        | 77/395 [00:22<01:33,  3.39it/s] 20%|█▉        | 78/395 [00:22<01:33,  3.40it/s] 20%|██        | 79/395 [00:23<01:33,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 10:25:44,767 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:25:44,768 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 10:25:44,768 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.36it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.87it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.84it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.99it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.21it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.95it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.77it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.40it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.42it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.54it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.63it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.75it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.70it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.55it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.31it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.19it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.23it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.28it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.38it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.60it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.60it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.65it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.51it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.21it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.20it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.22it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.27it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.39it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.59it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.69it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.59it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.29it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.22it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.23it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.29it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.27it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.45it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.59it/s][A
 23%|██▎       | 197/861 [00:04<00:16, 39.22it/s][A
 23%|██▎       | 202/861 [00:04<00:16, 41.13it/s][A
 24%|██▍       | 207/861 [00:04<00:15, 42.16it/s][A
 25%|██▍       | 212/861 [00:04<00:15, 42.87it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.45it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.79it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.07it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.11it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.00it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 43.84it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.03it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.21it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.46it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.53it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.48it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.46it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.53it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.23it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.19it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.20it/s][A
 34%|███▍      | 297/861 [00:06<00:17, 31.84it/s][A
 35%|███▌      | 302/861 [00:06<00:15, 35.09it/s][A
 36%|███▌      | 307/861 [00:07<00:17, 31.42it/s][A
 36%|███▌      | 311/861 [00:07<00:23, 23.30it/s][A
 37%|███▋      | 316/861 [00:07<00:19, 27.50it/s][A
 37%|███▋      | 321/861 [00:07<00:17, 31.26it/s][A
 38%|███▊      | 326/861 [00:07<00:15, 34.50it/s][A
 38%|███▊      | 331/861 [00:07<00:14, 37.12it/s][A
 39%|███▉      | 336/861 [00:08<00:13, 39.15it/s][A
 40%|███▉      | 341/861 [00:08<00:12, 40.84it/s][A
 40%|████      | 346/861 [00:08<00:12, 41.96it/s][A
 41%|████      | 351/861 [00:08<00:12, 42.47it/s][A
 41%|████▏     | 356/861 [00:08<00:11, 42.66it/s][A
 42%|████▏     | 361/861 [00:08<00:11, 42.95it/s][A
 43%|████▎     | 366/861 [00:08<00:11, 43.46it/s][A
 43%|████▎     | 371/861 [00:08<00:11, 43.94it/s][A
 44%|████▎     | 376/861 [00:08<00:10, 44.28it/s][A
 44%|████▍     | 381/861 [00:09<00:10, 44.56it/s][A
 45%|████▍     | 386/861 [00:09<00:10, 44.68it/s][A
 45%|████▌     | 391/861 [00:09<00:10, 44.61it/s][A
 46%|████▌     | 396/861 [00:09<00:10, 44.30it/s][A
 47%|████▋     | 401/861 [00:09<00:10, 44.01it/s][A
 47%|████▋     | 406/861 [00:09<00:10, 44.02it/s][A
 48%|████▊     | 411/861 [00:09<00:10, 44.18it/s][A
 48%|████▊     | 416/861 [00:09<00:10, 44.29it/s][A
 49%|████▉     | 421/861 [00:09<00:09, 44.57it/s][A
 49%|████▉     | 426/861 [00:10<00:09, 44.77it/s][A
 50%|█████     | 431/861 [00:10<00:09, 44.87it/s][A
 51%|█████     | 436/861 [00:10<00:09, 44.64it/s][A
 51%|█████     | 441/861 [00:10<00:09, 44.38it/s][A
 52%|█████▏    | 446/861 [00:10<00:09, 44.21it/s][A
 52%|█████▏    | 451/861 [00:10<00:09, 44.20it/s][A
 53%|█████▎    | 456/861 [00:10<00:09, 44.31it/s][A
 54%|█████▎    | 461/861 [00:10<00:09, 44.38it/s][A
 54%|█████▍    | 466/861 [00:10<00:08, 44.51it/s][A
 55%|█████▍    | 471/861 [00:11<00:08, 44.66it/s][A
 55%|█████▌    | 476/861 [00:11<00:08, 44.78it/s][A
 56%|█████▌    | 481/861 [00:11<00:08, 44.58it/s][A
 56%|█████▋    | 486/861 [00:11<00:08, 44.45it/s][A
 57%|█████▋    | 491/861 [00:11<00:08, 44.23it/s][A
 58%|█████▊    | 496/861 [00:11<00:08, 44.11it/s][A
 58%|█████▊    | 501/861 [00:11<00:08, 44.06it/s][A
 59%|█████▉    | 506/861 [00:11<00:08, 44.16it/s][A
 59%|█████▉    | 511/861 [00:11<00:07, 44.22it/s][A
 60%|█████▉    | 516/861 [00:12<00:07, 44.58it/s][A
 61%|██████    | 521/861 [00:12<00:07, 44.68it/s][A
 61%|██████    | 526/861 [00:12<00:07, 44.61it/s][A
 62%|██████▏   | 531/861 [00:12<00:07, 44.49it/s][A
 62%|██████▏   | 536/861 [00:12<00:07, 44.28it/s][A
 63%|██████▎   | 541/861 [00:12<00:07, 44.20it/s][A
 63%|██████▎   | 546/861 [00:12<00:07, 44.27it/s][A
 64%|██████▍   | 551/861 [00:12<00:06, 44.44it/s][A
 65%|██████▍   | 556/861 [00:12<00:06, 44.55it/s][A
 65%|██████▌   | 561/861 [00:13<00:06, 44.66it/s][A
 66%|██████▌   | 566/861 [00:13<00:06, 44.65it/s][A
 66%|██████▋   | 571/861 [00:13<00:06, 44.72it/s][A
 67%|██████▋   | 576/861 [00:13<00:06, 44.55it/s][A
 67%|██████▋   | 581/861 [00:13<00:06, 44.36it/s][A
 68%|██████▊   | 586/861 [00:13<00:06, 44.24it/s][A
 69%|██████▊   | 591/861 [00:13<00:06, 44.28it/s][A
 69%|██████▉   | 596/861 [00:13<00:05, 44.38it/s][A
 70%|██████▉   | 601/861 [00:13<00:05, 44.59it/s][A
 70%|███████   | 606/861 [00:14<00:05, 44.70it/s][A
 71%|███████   | 611/861 [00:14<00:05, 44.74it/s][A
 72%|███████▏  | 616/861 [00:14<00:05, 44.48it/s][A
 72%|███████▏  | 621/861 [00:14<00:05, 44.40it/s][A
 73%|███████▎  | 626/861 [00:14<00:05, 44.36it/s][A
 73%|███████▎  | 631/861 [00:14<00:05, 44.35it/s][A
 74%|███████▍  | 636/861 [00:14<00:05, 44.37it/s][A
 74%|███████▍  | 641/861 [00:14<00:04, 44.37it/s][A
 75%|███████▌  | 646/861 [00:14<00:04, 44.51it/s][A
 76%|███████▌  | 651/861 [00:15<00:04, 44.66it/s][A
 76%|███████▌  | 656/861 [00:15<00:04, 44.65it/s][A
 77%|███████▋  | 661/861 [00:15<00:04, 44.51it/s][A
 77%|███████▋  | 666/861 [00:15<00:04, 44.39it/s][A
 78%|███████▊  | 671/861 [00:15<00:04, 44.31it/s][A
 79%|███████▊  | 676/861 [00:15<00:04, 44.32it/s][A
 79%|███████▉  | 681/861 [00:15<00:04, 44.36it/s][A
 80%|███████▉  | 686/861 [00:15<00:03, 44.39it/s][A
 80%|████████  | 691/861 [00:16<00:03, 44.46it/s][A
 81%|████████  | 696/861 [00:16<00:03, 44.61it/s][A
 81%|████████▏ | 701/861 [00:16<00:03, 44.61it/s][A
 82%|████████▏ | 706/861 [00:16<00:03, 44.49it/s][A
 83%|████████▎ | 711/861 [00:16<00:03, 44.42it/s][A
 83%|████████▎ | 716/861 [00:16<00:03, 44.26it/s][A
 84%|████████▎ | 721/861 [00:16<00:03, 44.34it/s][A
 84%|████████▍ | 726/861 [00:16<00:03, 44.37it/s][A
 85%|████████▍ | 731/861 [00:16<00:02, 44.56it/s][A
 85%|████████▌ | 736/861 [00:17<00:02, 44.62it/s][A
 86%|████████▌ | 741/861 [00:17<00:02, 44.64it/s][A
 87%|████████▋ | 746/861 [00:17<00:02, 44.57it/s][A
 87%|████████▋ | 751/861 [00:17<00:02, 44.47it/s][A
 88%|████████▊ | 756/861 [00:17<00:02, 44.39it/s][A
 88%|████████▊ | 761/861 [00:17<00:02, 44.27it/s][A
 89%|████████▉ | 766/861 [00:17<00:02, 44.36it/s][A
 90%|████████▉ | 771/861 [00:17<00:02, 44.41it/s][A
 90%|█████████ | 776/861 [00:17<00:01, 44.53it/s][A
 91%|█████████ | 781/861 [00:18<00:01, 44.68it/s][A
 91%|█████████▏| 786/861 [00:18<00:01, 44.64it/s][A
 92%|█████████▏| 791/861 [00:18<00:01, 44.64it/s][A
 92%|█████████▏| 796/861 [00:18<00:01, 44.48it/s][A
 93%|█████████▎| 801/861 [00:18<00:01, 44.41it/s][A
 94%|█████████▎| 806/861 [00:18<00:01, 44.29it/s][A
 94%|█████████▍| 811/861 [00:18<00:01, 44.39it/s][A
 95%|█████████▍| 816/861 [00:18<00:01, 44.49it/s][A
 95%|█████████▌| 821/861 [00:18<00:00, 44.58it/s][A
 96%|█████████▌| 826/861 [00:19<00:00, 44.57it/s][A
 97%|█████████▋| 831/861 [00:19<00:00, 44.57it/s][A
 97%|█████████▋| 836/861 [00:19<00:00, 44.55it/s][A
 98%|█████████▊| 841/861 [00:19<00:00, 41.31it/s][A
 98%|█████████▊| 846/861 [00:19<00:00, 42.25it/s][A
 99%|█████████▉| 851/861 [00:19<00:00, 42.99it/s][A
 99%|█████████▉| 856/861 [00:19<00:00, 43.50it/s][A
100%|██████████| 861/861 [00:19<00:00, 43.86it/s][A                                                
                                                 [A 20%|██        | 79/395 [00:43<01:33,  3.39it/s]
100%|██████████| 861/861 [00:19<00:00, 43.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:26:04,959 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79
[INFO|configuration_utils.py:351] 2023-08-28 10:26:05,137 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:26:08,246 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:26:08,340 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:26:08,413 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79/special_tokens_map.json
 20%|██        | 80/395 [00:54<49:57,  9.51s/it] 21%|██        | 81/395 [00:54<35:21,  6.76s/it] 21%|██        | 82/395 [00:54<25:08,  4.82s/it] 21%|██        | 83/395 [00:55<17:59,  3.46s/it] 21%|██▏       | 84/395 [00:55<13:00,  2.51s/it] 22%|██▏       | 85/395 [00:55<09:32,  1.85s/it] 22%|██▏       | 86/395 [00:56<07:06,  1.38s/it] 22%|██▏       | 87/395 [00:56<05:24,  1.05s/it] 22%|██▏       | 88/395 [00:56<04:13,  1.21it/s] 23%|██▎       | 89/395 [00:56<03:24,  1.50it/s] 23%|██▎       | 90/395 [00:57<02:49,  1.80it/s] 23%|██▎       | 91/395 [00:57<02:24,  2.10it/s] 23%|██▎       | 92/395 [00:57<02:08,  2.35it/s] 24%|██▎       | 93/395 [00:58<01:56,  2.59it/s] 24%|██▍       | 94/395 [00:58<01:47,  2.79it/s] 24%|██▍       | 95/395 [00:58<01:41,  2.95it/s] 24%|██▍       | 96/395 [00:59<01:37,  3.07it/s] 25%|██▍       | 97/395 [00:59<01:34,  3.16it/s] 25%|██▍       | 98/395 [00:59<01:32,  3.23it/s] 25%|██▌       | 99/395 [00:59<01:30,  3.28it/s] 25%|██▌       | 100/395 [01:00<01:29,  3.31it/s] 26%|██▌       | 101/395 [01:00<01:28,  3.34it/s] 26%|██▌       | 102/395 [01:00<01:27,  3.36it/s] 26%|██▌       | 103/395 [01:01<01:28,  3.30it/s] 26%|██▋       | 104/395 [01:01<01:27,  3.33it/s] 27%|██▋       | 105/395 [01:01<01:26,  3.34it/s] 27%|██▋       | 106/395 [01:02<01:25,  3.36it/s] 27%|██▋       | 107/395 [01:02<01:25,  3.37it/s] 27%|██▋       | 108/395 [01:02<01:24,  3.38it/s] 28%|██▊       | 109/395 [01:02<01:24,  3.38it/s] 28%|██▊       | 110/395 [01:03<01:24,  3.38it/s] 28%|██▊       | 111/395 [01:03<01:23,  3.39it/s] 28%|██▊       | 112/395 [01:03<01:23,  3.39it/s] 29%|██▊       | 113/395 [01:04<01:23,  3.39it/s] 29%|██▉       | 114/395 [01:04<01:23,  3.35it/s] 29%|██▉       | 115/395 [01:04<01:23,  3.36it/s] 29%|██▉       | 116/395 [01:04<01:22,  3.37it/s] 30%|██▉       | 117/395 [01:05<01:22,  3.38it/s] 30%|██▉       | 118/395 [01:05<01:21,  3.38it/s] 30%|███       | 119/395 [01:05<01:21,  3.39it/s] 30%|███       | 120/395 [01:06<01:21,  3.39it/s] 31%|███       | 121/395 [01:06<01:20,  3.39it/s] 31%|███       | 122/395 [01:06<01:20,  3.39it/s] 31%|███       | 123/395 [01:07<01:20,  3.39it/s] 31%|███▏      | 124/395 [01:07<01:19,  3.39it/s] 32%|███▏      | 125/395 [01:07<01:22,  3.26it/s] 32%|███▏      | 126/395 [01:07<01:21,  3.30it/s] 32%|███▏      | 127/395 [01:08<01:20,  3.32it/s] 32%|███▏      | 128/395 [01:08<01:19,  3.35it/s] 33%|███▎      | 129/395 [01:08<01:19,  3.36it/s] 33%|███▎      | 130/395 [01:09<01:18,  3.37it/s] 33%|███▎      | 131/395 [01:09<01:18,  3.38it/s] 33%|███▎      | 132/395 [01:09<01:17,  3.38it/s] 34%|███▎      | 133/395 [01:10<01:17,  3.38it/s] 34%|███▍      | 134/395 [01:10<01:17,  3.39it/s] 34%|███▍      | 135/395 [01:10<01:16,  3.38it/s] 34%|███▍      | 136/395 [01:10<01:19,  3.27it/s] 35%|███▍      | 137/395 [01:11<01:18,  3.30it/s] 35%|███▍      | 138/395 [01:11<01:17,  3.33it/s] 35%|███▌      | 139/395 [01:11<01:16,  3.35it/s] 35%|███▌      | 140/395 [01:12<01:15,  3.37it/s] 36%|███▌      | 141/395 [01:12<01:15,  3.38it/s] 36%|███▌      | 142/395 [01:12<01:14,  3.38it/s] 36%|███▌      | 143/395 [01:13<01:14,  3.39it/s] 36%|███▋      | 144/395 [01:13<01:14,  3.39it/s] 37%|███▋      | 145/395 [01:13<01:13,  3.39it/s] 37%|███▋      | 146/395 [01:13<01:13,  3.39it/s] 37%|███▋      | 147/395 [01:14<01:14,  3.35it/s] 37%|███▋      | 148/395 [01:14<01:13,  3.36it/s] 38%|███▊      | 149/395 [01:14<01:13,  3.37it/s] 38%|███▊      | 150/395 [01:15<01:12,  3.38it/s] 38%|███▊      | 151/395 [01:15<01:12,  3.38it/s] 38%|███▊      | 152/395 [01:15<01:11,  3.39it/s] 39%|███▊      | 153/395 [01:15<01:11,  3.39it/s] 39%|███▉      | 154/395 [01:16<01:11,  3.39it/s] 39%|███▉      | 155/395 [01:16<01:10,  3.39it/s] 39%|███▉      | 156/395 [01:16<01:10,  3.40it/s] 40%|███▉      | 157/395 [01:17<01:10,  3.40it/s] 40%|████      | 158/395 [01:17<01:11,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 10:26:38,952 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:26:38,952 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 10:26:38,952 >>   Batch size = 8
{'eval_loss': 0.987551748752594, 'eval_runtime': 19.8697, 'eval_samples_per_second': 346.458, 'eval_steps_per_second': 43.332, 'epoch': 0.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.14it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.34it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.92it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.33it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.61it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.87it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.54it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.17it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.12it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.29it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.27it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.33it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.44it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.57it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.44it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.25it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.15it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.22it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.25it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.45it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.66it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.64it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.64it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.39it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.31it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.28it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.29it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.35it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.48it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.57it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.56it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.49it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.38it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.35it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.35it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.28it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.33it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.40it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.58it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.56it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.33it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.41it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.36it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.30it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.35it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.27it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.37it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.61it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.39it/s][A
 29%|██▉       | 252/861 [00:05<00:15, 40.37it/s][A
 30%|██▉       | 257/861 [00:05<00:14, 41.71it/s][A
 30%|███       | 262/861 [00:05<00:14, 42.54it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.16it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.53it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.84it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.16it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.26it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 43.94it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.02it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.03it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.20it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.38it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.57it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.69it/s][A
 38%|███▊      | 327/861 [00:07<00:11, 44.61it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.51it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.25it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.20it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.15it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.26it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.42it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.61it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.48it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.72it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.49it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.36it/s][A
 45%|████▍     | 387/861 [00:08<00:11, 39.69it/s][A
 46%|████▌     | 392/861 [00:08<00:11, 41.14it/s][A
 46%|████▌     | 397/861 [00:08<00:11, 42.18it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 42.92it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.51it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.95it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.29it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.38it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 43.89it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.77it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.95it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.24it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.42it/s][A
 52%|█████▏    | 452/861 [00:10<00:10, 40.73it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 42.17it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 43.08it/s][A
 54%|█████▍    | 467/861 [00:10<00:09, 43.66it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.77it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.82it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.01it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.27it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.21it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 43.96it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.14it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.46it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.63it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.57it/s][A
 61%|██████    | 522/861 [00:11<00:08, 42.02it/s][A
 61%|██████    | 527/861 [00:11<00:07, 42.86it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.34it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 43.60it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 43.87it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.01it/s][A
 64%|██████▍   | 552/861 [00:12<00:09, 31.99it/s][A
 65%|██████▍   | 557/861 [00:12<00:08, 35.60it/s][A
 65%|██████▌   | 562/861 [00:12<00:07, 37.94it/s][A
 66%|██████▌   | 567/861 [00:13<00:12, 23.59it/s][A
 66%|██████▋   | 572/861 [00:13<00:10, 27.55it/s][A
 67%|██████▋   | 577/861 [00:13<00:09, 31.18it/s][A
 68%|██████▊   | 582/861 [00:13<00:08, 34.30it/s][A
 68%|██████▊   | 587/861 [00:13<00:07, 36.90it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 39.02it/s][A
 69%|██████▉   | 597/861 [00:13<00:06, 40.65it/s][A
 70%|██████▉   | 602/861 [00:14<00:06, 41.82it/s][A
 70%|███████   | 607/861 [00:14<00:06, 42.19it/s][A
 71%|███████   | 612/861 [00:14<00:05, 42.52it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 42.98it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.53it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.92it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.15it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 41.76it/s][A
 75%|███████▍  | 642/861 [00:14<00:05, 42.72it/s][A
 75%|███████▌  | 647/861 [00:15<00:04, 43.37it/s][A
 76%|███████▌  | 652/861 [00:15<00:04, 43.40it/s][A
 76%|███████▋  | 657/861 [00:15<00:04, 43.66it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.66it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.02it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.18it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.18it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.29it/s][A
 80%|███████▉  | 687/861 [00:16<00:03, 44.53it/s][A
 80%|████████  | 692/861 [00:16<00:03, 44.51it/s][A
 81%|████████  | 697/861 [00:16<00:03, 44.43it/s][A
 82%|████████▏ | 702/861 [00:16<00:03, 44.38it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 44.27it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.44it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.35it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.35it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.40it/s][A
 85%|████████▌ | 732/861 [00:17<00:02, 44.47it/s][A
 86%|████████▌ | 737/861 [00:17<00:02, 44.50it/s][A
 86%|████████▌ | 742/861 [00:17<00:02, 44.47it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 44.43it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.47it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.41it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.31it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.32it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 42.62it/s][A
 90%|█████████ | 777/861 [00:18<00:01, 43.30it/s][A
 91%|█████████ | 782/861 [00:18<00:01, 43.68it/s][A
 91%|█████████▏| 787/861 [00:18<00:01, 43.87it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.00it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.05it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.04it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.07it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 43.98it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.15it/s][A
 95%|█████████▌| 822/861 [00:19<00:00, 44.38it/s][A
 96%|█████████▌| 827/861 [00:19<00:00, 44.58it/s][A
 97%|█████████▋| 832/861 [00:19<00:00, 44.52it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 44.47it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.50it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.44it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.35it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.25it/s][A                                                 
                                                 [A 40%|████      | 158/395 [01:37<01:11,  3.31it/s]
100%|██████████| 861/861 [00:19<00:00, 44.25it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:26:59,200 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-28 10:26:59,448 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:27:01,883 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:27:01,990 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:27:02,049 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158/special_tokens_map.json
 40%|████      | 159/395 [01:47<36:14,  9.21s/it] 41%|████      | 160/395 [01:47<25:38,  6.55s/it] 41%|████      | 161/395 [01:48<18:13,  4.67s/it] 41%|████      | 162/395 [01:48<13:02,  3.36s/it] 41%|████▏     | 163/395 [01:48<09:25,  2.44s/it] 42%|████▏     | 164/395 [01:48<06:54,  1.80s/it] 42%|████▏     | 165/395 [01:49<05:09,  1.35s/it] 42%|████▏     | 166/395 [01:49<03:55,  1.03s/it] 42%|████▏     | 167/395 [01:49<03:04,  1.24it/s] 43%|████▎     | 168/395 [01:50<02:28,  1.53it/s] 43%|████▎     | 169/395 [01:50<02:03,  1.83it/s] 43%|████▎     | 170/395 [01:50<01:46,  2.12it/s] 43%|████▎     | 171/395 [01:51<01:33,  2.39it/s] 44%|████▎     | 172/395 [01:51<01:24,  2.62it/s] 44%|████▍     | 173/395 [01:51<01:18,  2.82it/s] 44%|████▍     | 174/395 [01:51<01:14,  2.97it/s] 44%|████▍     | 175/395 [01:52<01:11,  3.08it/s] 45%|████▍     | 176/395 [01:52<01:09,  3.17it/s] 45%|████▍     | 177/395 [01:52<01:07,  3.23it/s] 45%|████▌     | 178/395 [01:53<01:06,  3.28it/s] 45%|████▌     | 179/395 [01:53<01:05,  3.31it/s] 46%|████▌     | 180/395 [01:53<01:04,  3.34it/s] 46%|████▌     | 181/395 [01:54<01:05,  3.25it/s] 46%|████▌     | 182/395 [01:54<01:04,  3.29it/s] 46%|████▋     | 183/395 [01:54<01:03,  3.32it/s] 47%|████▋     | 184/395 [01:54<01:03,  3.34it/s] 47%|████▋     | 185/395 [01:55<01:02,  3.36it/s] 47%|████▋     | 186/395 [01:55<01:02,  3.37it/s] 47%|████▋     | 187/395 [01:55<01:01,  3.38it/s] 48%|████▊     | 188/395 [01:56<01:01,  3.38it/s] 48%|████▊     | 189/395 [01:56<01:00,  3.39it/s] 48%|████▊     | 190/395 [01:56<01:00,  3.39it/s] 48%|████▊     | 191/395 [01:56<01:00,  3.39it/s] 49%|████▊     | 192/395 [01:57<01:01,  3.30it/s] 49%|████▉     | 193/395 [01:57<01:00,  3.33it/s] 49%|████▉     | 194/395 [01:57<00:59,  3.35it/s] 49%|████▉     | 195/395 [01:58<00:59,  3.36it/s] 50%|████▉     | 196/395 [01:58<00:58,  3.37it/s] 50%|████▉     | 197/395 [01:58<00:58,  3.38it/s] 50%|█████     | 198/395 [01:59<00:58,  3.39it/s] 50%|█████     | 199/395 [01:59<00:57,  3.39it/s] 51%|█████     | 200/395 [01:59<00:57,  3.39it/s] 51%|█████     | 201/395 [01:59<00:57,  3.39it/s] 51%|█████     | 202/395 [02:00<00:56,  3.39it/s] 51%|█████▏    | 203/395 [02:00<00:57,  3.36it/s] 52%|█████▏    | 204/395 [02:00<00:56,  3.37it/s] 52%|█████▏    | 205/395 [02:01<00:56,  3.37it/s] 52%|█████▏    | 206/395 [02:01<00:55,  3.38it/s] 52%|█████▏    | 207/395 [02:01<00:55,  3.38it/s] 53%|█████▎    | 208/395 [02:02<00:55,  3.39it/s] 53%|█████▎    | 209/395 [02:02<00:54,  3.39it/s] 53%|█████▎    | 210/395 [02:02<00:54,  3.39it/s] 53%|█████▎    | 211/395 [02:02<00:54,  3.39it/s] 54%|█████▎    | 212/395 [02:03<00:53,  3.39it/s] 54%|█████▍    | 213/395 [02:03<00:53,  3.39it/s] 54%|█████▍    | 214/395 [02:03<00:54,  3.32it/s] 54%|█████▍    | 215/395 [02:04<00:53,  3.34it/s] 55%|█████▍    | 216/395 [02:04<00:53,  3.36it/s] 55%|█████▍    | 217/395 [02:04<00:52,  3.36it/s] 55%|█████▌    | 218/395 [02:04<00:52,  3.37it/s] 55%|█████▌    | 219/395 [02:05<00:52,  3.38it/s] 56%|█████▌    | 220/395 [02:05<00:51,  3.38it/s] 56%|█████▌    | 221/395 [02:05<00:51,  3.38it/s] 56%|█████▌    | 222/395 [02:06<00:51,  3.38it/s] 56%|█████▋    | 223/395 [02:06<00:50,  3.38it/s] 57%|█████▋    | 224/395 [02:06<00:50,  3.39it/s] 57%|█████▋    | 225/395 [02:07<00:52,  3.25it/s] 57%|█████▋    | 226/395 [02:07<00:51,  3.29it/s] 57%|█████▋    | 227/395 [02:07<00:50,  3.32it/s] 58%|█████▊    | 228/395 [02:07<00:49,  3.35it/s] 58%|█████▊    | 229/395 [02:08<00:49,  3.36it/s] 58%|█████▊    | 230/395 [02:08<00:49,  3.37it/s] 58%|█████▊    | 231/395 [02:08<00:48,  3.37it/s] 59%|█████▊    | 232/395 [02:09<00:48,  3.38it/s] 59%|█████▉    | 233/395 [02:09<00:47,  3.38it/s] 59%|█████▉    | 234/395 [02:09<00:47,  3.38it/s] 59%|█████▉    | 235/395 [02:10<00:47,  3.39it/s] 60%|█████▉    | 236/395 [02:10<00:48,  3.25it/s] 60%|██████    | 237/395 [02:10<00:47,  3.29it/s][INFO|trainer.py:2140] 2023-08-28 10:27:32,154 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:27:32,154 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 10:27:32,154 >>   Batch size = 8
{'eval_loss': 0.965660572052002, 'eval_runtime': 19.9572, 'eval_samples_per_second': 344.938, 'eval_steps_per_second': 43.142, 'epoch': 1.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.03it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.68it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.92it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.96it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.30it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.84it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.69it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.28it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.40it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.57it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.59it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.68it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.65it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.54it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.33it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.22it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.12it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.22it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.34it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.54it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.61it/s][A
 13%|█▎        | 112/861 [00:02<00:17, 42.75it/s][A
 14%|█▎        | 117/861 [00:02<00:17, 43.31it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.60it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.77it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.79it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.97it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.22it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.38it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.20it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.24it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.26it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.34it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.36it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.29it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.37it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.42it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.36it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.34it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.31it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.35it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.40it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.34it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.36it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.46it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.41it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.48it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.39it/s][A
 29%|██▊       | 247/861 [00:05<00:14, 41.89it/s][A
 29%|██▉       | 252/861 [00:05<00:14, 42.56it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.40it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.72it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.00it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.16it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.31it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.24it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 43.88it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 43.96it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.23it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.45it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.49it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.34it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.51it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.42it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.36it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.14it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.18it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.32it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.51it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.51it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.54it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.46it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.38it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.23it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.24it/s][A
 44%|████▍     | 382/861 [00:08<00:11, 42.79it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 43.43it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 43.84it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 43.98it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.18it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.35it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.25it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.14it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.00it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.09it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.26it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.50it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.49it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.58it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.49it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.34it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.29it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.07it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.12it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.20it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.47it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.57it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.58it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.44it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.35it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.35it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.18it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.23it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.35it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.50it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.55it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.53it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.53it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.31it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.25it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.19it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.39it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.41it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.53it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.57it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.46it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.48it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.25it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.25it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.25it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.34it/s][A
 71%|███████   | 612/861 [00:13<00:05, 42.68it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 43.11it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.88it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.06it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.13it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.11it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.18it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.15it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 43.99it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.12it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.37it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.55it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.49it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.31it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.32it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.23it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.18it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.12it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.21it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.27it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.51it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.47it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.43it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.35it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.20it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.17it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.11it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 40.88it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 38.35it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 40.75it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 41.94it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 42.81it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.42it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 43.80it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 43.83it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 43.67it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 43.68it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.70it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 43.89it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.30it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.45it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.63it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.59it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.33it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.16it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.02it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.04it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.12it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.30it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 29.37it/s][A                                                 
                                                 [A 60%|██████    | 237/395 [02:30<00:47,  3.29it/s]
100%|██████████| 861/861 [00:19<00:00, 29.37it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:27:52,015 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237
[INFO|configuration_utils.py:351] 2023-08-28 10:27:52,456 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:28:03,015 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:28:03,333 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:28:03,413 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237/special_tokens_map.json
 60%|██████    | 238/395 [02:49<30:53, 11.80s/it] 61%|██████    | 239/395 [02:49<21:44,  8.36s/it] 61%|██████    | 240/395 [02:49<15:20,  5.94s/it] 61%|██████    | 241/395 [02:50<10:53,  4.25s/it] 61%|██████▏   | 242/395 [02:50<07:48,  3.06s/it] 62%|██████▏   | 243/395 [02:50<05:39,  2.23s/it] 62%|██████▏   | 244/395 [02:51<04:09,  1.65s/it] 62%|██████▏   | 245/395 [02:51<03:06,  1.24s/it] 62%|██████▏   | 246/395 [02:51<02:22,  1.04it/s] 63%|██████▎   | 247/395 [02:51<01:52,  1.32it/s] 63%|██████▎   | 248/395 [02:52<01:31,  1.61it/s] 63%|██████▎   | 249/395 [02:52<01:16,  1.91it/s] 63%|██████▎   | 250/395 [02:52<01:07,  2.16it/s] 64%|██████▎   | 251/395 [02:53<00:59,  2.43it/s] 64%|██████▍   | 252/395 [02:53<00:53,  2.66it/s] 64%|██████▍   | 253/395 [02:53<00:49,  2.84it/s] 64%|██████▍   | 254/395 [02:54<00:47,  2.99it/s] 65%|██████▍   | 255/395 [02:54<00:45,  3.10it/s] 65%|██████▍   | 256/395 [02:54<00:43,  3.18it/s] 65%|██████▌   | 257/395 [02:54<00:42,  3.24it/s] 65%|██████▌   | 258/395 [02:55<00:41,  3.28it/s] 66%|██████▌   | 259/395 [02:55<00:40,  3.32it/s] 66%|██████▌   | 260/395 [02:55<00:40,  3.34it/s] 66%|██████▌   | 261/395 [02:56<00:41,  3.21it/s] 66%|██████▋   | 262/395 [02:56<00:40,  3.26it/s] 67%|██████▋   | 263/395 [02:56<00:39,  3.30it/s] 67%|██████▋   | 264/395 [02:57<00:39,  3.33it/s] 67%|██████▋   | 265/395 [02:57<00:38,  3.35it/s] 67%|██████▋   | 266/395 [02:57<00:38,  3.36it/s] 68%|██████▊   | 267/395 [02:57<00:37,  3.37it/s] 68%|██████▊   | 268/395 [02:58<00:37,  3.38it/s] 68%|██████▊   | 269/395 [02:58<00:37,  3.38it/s] 68%|██████▊   | 270/395 [02:58<00:36,  3.39it/s] 69%|██████▊   | 271/395 [02:59<00:36,  3.39it/s] 69%|██████▉   | 272/395 [02:59<00:37,  3.30it/s] 69%|██████▉   | 273/395 [02:59<00:36,  3.33it/s] 69%|██████▉   | 274/395 [03:00<00:36,  3.35it/s] 70%|██████▉   | 275/395 [03:00<00:35,  3.36it/s] 70%|██████▉   | 276/395 [03:00<00:35,  3.37it/s] 70%|███████   | 277/395 [03:00<00:34,  3.38it/s] 70%|███████   | 278/395 [03:01<00:34,  3.39it/s] 71%|███████   | 279/395 [03:01<00:34,  3.39it/s] 71%|███████   | 280/395 [03:01<00:33,  3.39it/s] 71%|███████   | 281/395 [03:02<00:33,  3.39it/s] 71%|███████▏  | 282/395 [03:02<00:33,  3.39it/s] 72%|███████▏  | 283/395 [03:02<00:33,  3.37it/s] 72%|███████▏  | 284/395 [03:02<00:32,  3.38it/s] 72%|███████▏  | 285/395 [03:03<00:32,  3.38it/s] 72%|███████▏  | 286/395 [03:03<00:32,  3.39it/s] 73%|███████▎  | 287/395 [03:03<00:31,  3.39it/s] 73%|███████▎  | 288/395 [03:04<00:31,  3.39it/s] 73%|███████▎  | 289/395 [03:04<00:31,  3.39it/s] 73%|███████▎  | 290/395 [03:04<00:30,  3.39it/s] 74%|███████▎  | 291/395 [03:05<00:30,  3.39it/s] 74%|███████▍  | 292/395 [03:05<00:30,  3.40it/s] 74%|███████▍  | 293/395 [03:05<00:30,  3.40it/s] 74%|███████▍  | 294/395 [03:05<00:30,  3.35it/s] 75%|███████▍  | 295/395 [03:06<00:29,  3.36it/s] 75%|███████▍  | 296/395 [03:06<00:29,  3.37it/s] 75%|███████▌  | 297/395 [03:06<00:29,  3.38it/s] 75%|███████▌  | 298/395 [03:07<00:28,  3.38it/s] 76%|███████▌  | 299/395 [03:07<00:28,  3.38it/s] 76%|███████▌  | 300/395 [03:07<00:28,  3.39it/s] 76%|███████▌  | 301/395 [03:07<00:27,  3.39it/s] 76%|███████▋  | 302/395 [03:08<00:27,  3.39it/s] 77%|███████▋  | 303/395 [03:08<00:27,  3.39it/s] 77%|███████▋  | 304/395 [03:08<00:26,  3.39it/s] 77%|███████▋  | 305/395 [03:09<00:27,  3.26it/s] 77%|███████▋  | 306/395 [03:09<00:26,  3.30it/s] 78%|███████▊  | 307/395 [03:09<00:26,  3.33it/s] 78%|███████▊  | 308/395 [03:10<00:25,  3.35it/s] 78%|███████▊  | 309/395 [03:10<00:25,  3.36it/s] 78%|███████▊  | 310/395 [03:10<00:25,  3.37it/s] 79%|███████▊  | 311/395 [03:10<00:24,  3.38it/s] 79%|███████▉  | 312/395 [03:11<00:24,  3.38it/s] 79%|███████▉  | 313/395 [03:11<00:24,  3.38it/s] 79%|███████▉  | 314/395 [03:11<00:23,  3.39it/s] 80%|███████▉  | 315/395 [03:12<00:23,  3.39it/s] 80%|████████  | 316/395 [03:12<00:23,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 10:28:33,973 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:28:33,973 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 10:28:33,974 >>   Batch size = 8
{'eval_loss': 0.9699513912200928, 'eval_runtime': 19.7086, 'eval_samples_per_second': 349.288, 'eval_steps_per_second': 43.686, 'epoch': 2.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.28it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.12it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.86it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.47it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.70it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.92it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.45it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.10it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.13it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.38it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.55it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.70it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.79it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.72it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.57it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.18it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.05it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.10it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.25it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.46it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.61it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.64it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.73it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.49it/s][A
 15%|█▍        | 127/861 [00:02<00:17, 42.28it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 42.94it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.26it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 43.68it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 43.99it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.24it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.46it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.47it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.10it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.05it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.14it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.27it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.43it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.56it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.69it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.69it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.27it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.01it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.03it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.18it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.32it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.37it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.56it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.60it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.46it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.18it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.11it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.11it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.14it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.29it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.43it/s][A
 33%|███▎      | 282/861 [00:06<00:12, 44.66it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.71it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.63it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.34it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.26it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.23it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.31it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.40it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.56it/s][A
 38%|███▊      | 327/861 [00:07<00:11, 44.70it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.67it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.63it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.43it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.23it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.10it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.27it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.49it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.58it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.66it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.47it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.55it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.35it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.26it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.19it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.37it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.43it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.57it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.60it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.50it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.55it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.32it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.30it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.30it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.34it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.50it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.62it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.51it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.48it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.42it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.41it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.31it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.20it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.29it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.53it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.55it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.62it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.48it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.34it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.31it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.21it/s][A
 62%|██████▏   | 532/861 [00:11<00:07, 44.23it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.35it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.41it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.54it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.53it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.45it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.45it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.29it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.20it/s][A
 67%|██████▋   | 577/861 [00:12<00:06, 44.26it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.38it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.49it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.55it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.52it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.39it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.35it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.31it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.23it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.28it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.34it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.43it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.51it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.45it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.49it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.49it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.19it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.29it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.33it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.36it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 40.04it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 41.66it/s][A
 80%|███████▉  | 687/861 [00:15<00:04, 42.62it/s][A
 80%|████████  | 692/861 [00:15<00:03, 43.26it/s][A
 81%|████████  | 697/861 [00:15<00:03, 43.74it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 43.98it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.11it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.17it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.96it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.80it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.00it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.20it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.32it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.31it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.62it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 44.67it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.39it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.26it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.15it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.19it/s][A
 90%|█████████ | 777/861 [00:17<00:02, 34.65it/s][A
 91%|█████████ | 781/861 [00:17<00:02, 34.83it/s][A
 91%|█████████▏| 786/861 [00:17<00:01, 37.56it/s][A
 92%|█████████▏| 790/861 [00:17<00:01, 37.44it/s][A
 92%|█████████▏| 794/861 [00:18<00:02, 25.33it/s][A
 93%|█████████▎| 798/861 [00:18<00:02, 26.76it/s][A
 93%|█████████▎| 803/861 [00:18<00:01, 31.05it/s][A
 94%|█████████▍| 808/861 [00:18<00:01, 34.44it/s][A
 94%|█████████▍| 813/861 [00:18<00:01, 37.16it/s][A
 95%|█████████▌| 818/861 [00:18<00:01, 39.21it/s][A
 96%|█████████▌| 823/861 [00:18<00:00, 40.82it/s][A
 96%|█████████▌| 828/861 [00:19<00:00, 42.02it/s][A
 97%|█████████▋| 833/861 [00:19<00:00, 42.71it/s][A
 97%|█████████▋| 838/861 [00:19<00:00, 42.86it/s][A
 98%|█████████▊| 843/861 [00:19<00:00, 43.06it/s][A
 98%|█████████▊| 848/861 [00:19<00:00, 43.37it/s][A
 99%|█████████▉| 853/861 [00:19<00:00, 43.81it/s][A
100%|█████████▉| 858/861 [00:19<00:00, 44.08it/s][A                                                 
                                                 [A 80%|████████  | 316/395 [03:32<00:23,  3.32it/s]
100%|██████████| 861/861 [00:19<00:00, 44.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:28:53,948 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-28 10:28:54,388 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:28:58,703 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:28:58,914 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:28:59,003 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316/special_tokens_map.json
 80%|████████  | 317/395 [03:45<13:02, 10.03s/it] 81%|████████  | 318/395 [03:45<09:08,  7.12s/it] 81%|████████  | 319/395 [03:45<06:25,  5.07s/it] 81%|████████  | 320/395 [03:46<04:32,  3.64s/it] 81%|████████▏ | 321/395 [03:46<03:15,  2.64s/it] 82%|████████▏ | 322/395 [03:46<02:21,  1.93s/it] 82%|████████▏ | 323/395 [03:47<01:43,  1.44s/it] 82%|████████▏ | 324/395 [03:47<01:17,  1.10s/it] 82%|████████▏ | 325/395 [03:47<00:59,  1.17it/s] 83%|████████▎ | 326/395 [03:47<00:47,  1.45it/s] 83%|████████▎ | 327/395 [03:48<00:38,  1.75it/s] 83%|████████▎ | 328/395 [03:48<00:32,  2.05it/s] 83%|████████▎ | 329/395 [03:48<00:28,  2.29it/s] 84%|████████▎ | 330/395 [03:49<00:25,  2.54it/s] 84%|████████▍ | 331/395 [03:49<00:23,  2.75it/s] 84%|████████▍ | 332/395 [03:49<00:21,  2.91it/s] 84%|████████▍ | 333/395 [03:49<00:20,  3.04it/s] 85%|████████▍ | 334/395 [03:50<00:19,  3.14it/s] 85%|████████▍ | 335/395 [03:50<00:18,  3.22it/s] 85%|████████▌ | 336/395 [03:50<00:18,  3.27it/s] 85%|████████▌ | 337/395 [03:51<00:17,  3.31it/s] 86%|████████▌ | 338/395 [03:51<00:17,  3.33it/s] 86%|████████▌ | 339/395 [03:51<00:16,  3.35it/s] 86%|████████▌ | 340/395 [03:52<00:16,  3.26it/s] 86%|████████▋ | 341/395 [03:52<00:16,  3.30it/s] 87%|████████▋ | 342/395 [03:52<00:15,  3.33it/s] 87%|████████▋ | 343/395 [03:52<00:15,  3.35it/s] 87%|████████▋ | 344/395 [03:53<00:15,  3.36it/s] 87%|████████▋ | 345/395 [03:53<00:14,  3.37it/s] 88%|████████▊ | 346/395 [03:53<00:14,  3.38it/s] 88%|████████▊ | 347/395 [03:54<00:14,  3.39it/s] 88%|████████▊ | 348/395 [03:54<00:13,  3.39it/s] 88%|████████▊ | 349/395 [03:54<00:13,  3.39it/s] 89%|████████▊ | 350/395 [03:55<00:13,  3.39it/s] 89%|████████▉ | 351/395 [03:55<00:13,  3.32it/s] 89%|████████▉ | 352/395 [03:55<00:12,  3.34it/s] 89%|████████▉ | 353/395 [03:55<00:12,  3.36it/s] 90%|████████▉ | 354/395 [03:56<00:12,  3.37it/s] 90%|████████▉ | 355/395 [03:56<00:11,  3.38it/s] 90%|█████████ | 356/395 [03:56<00:11,  3.38it/s] 90%|█████████ | 357/395 [03:57<00:11,  3.38it/s] 91%|█████████ | 358/395 [03:57<00:10,  3.39it/s] 91%|█████████ | 359/395 [03:57<00:10,  3.39it/s] 91%|█████████ | 360/395 [03:57<00:10,  3.39it/s] 91%|█████████▏| 361/395 [03:58<00:10,  3.39it/s] 92%|█████████▏| 362/395 [03:58<00:09,  3.31it/s] 92%|█████████▏| 363/395 [03:58<00:09,  3.33it/s] 92%|█████████▏| 364/395 [03:59<00:09,  3.35it/s] 92%|█████████▏| 365/395 [03:59<00:08,  3.36it/s] 93%|█████████▎| 366/395 [03:59<00:08,  3.37it/s] 93%|█████████▎| 367/395 [04:00<00:08,  3.38it/s] 93%|█████████▎| 368/395 [04:00<00:07,  3.38it/s] 93%|█████████▎| 369/395 [04:00<00:07,  3.38it/s] 94%|█████████▎| 370/395 [04:00<00:07,  3.38it/s] 94%|█████████▍| 371/395 [04:01<00:07,  3.39it/s] 94%|█████████▍| 372/395 [04:01<00:06,  3.39it/s] 94%|█████████▍| 373/395 [04:01<00:06,  3.30it/s] 95%|█████████▍| 374/395 [04:02<00:06,  3.33it/s] 95%|█████████▍| 375/395 [04:02<00:05,  3.35it/s] 95%|█████████▌| 376/395 [04:02<00:05,  3.36it/s] 95%|█████████▌| 377/395 [04:03<00:05,  3.37it/s] 96%|█████████▌| 378/395 [04:03<00:05,  3.38it/s] 96%|█████████▌| 379/395 [04:03<00:04,  3.38it/s] 96%|█████████▌| 380/395 [04:03<00:04,  3.39it/s] 96%|█████████▋| 381/395 [04:04<00:04,  3.39it/s] 97%|█████████▋| 382/395 [04:04<00:03,  3.39it/s] 97%|█████████▋| 383/395 [04:04<00:03,  3.39it/s] 97%|█████████▋| 384/395 [04:05<00:03,  3.39it/s] 97%|█████████▋| 385/395 [04:05<00:02,  3.39it/s] 98%|█████████▊| 386/395 [04:05<00:02,  3.39it/s] 98%|█████████▊| 387/395 [04:05<00:02,  3.39it/s] 98%|█████████▊| 388/395 [04:06<00:02,  3.39it/s] 98%|█████████▊| 389/395 [04:06<00:01,  3.39it/s] 99%|█████████▊| 390/395 [04:06<00:01,  3.39it/s] 99%|█████████▉| 391/395 [04:07<00:01,  3.28it/s] 99%|█████████▉| 392/395 [04:07<00:00,  3.31it/s] 99%|█████████▉| 393/395 [04:07<00:00,  3.33it/s]100%|█████████▉| 394/395 [04:08<00:00,  3.35it/s]100%|██████████| 395/395 [04:08<00:00,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 10:29:29,845 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:29:29,845 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 10:29:29,845 >>   Batch size = 8
{'eval_loss': 0.9732990264892578, 'eval_runtime': 19.8172, 'eval_samples_per_second': 347.375, 'eval_steps_per_second': 43.447, 'epoch': 3.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.54it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.44it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.70it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.86it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.37it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.74it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.67it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.52it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.58it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.70it/s][A
  7%|▋         | 57/861 [00:01<00:17, 44.74it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.59it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.54it/s][A
  8%|▊         | 72/861 [00:01<00:17, 43.87it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.99it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.07it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.10it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.32it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.46it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.56it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.57it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.41it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.28it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.32it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.19it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.23it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.30it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.38it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.44it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.59it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.41it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.38it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.36it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.23it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.19it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.25it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.36it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.51it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.54it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.46it/s][A
 24%|██▍       | 207/861 [00:04<00:15, 42.45it/s][A
 25%|██▍       | 212/861 [00:04<00:15, 43.07it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.41it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 43.73it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.93it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.25it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.38it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.40it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.16it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.26it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.21it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.13it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.31it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.35it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.55it/s][A
 33%|███▎      | 282/861 [00:06<00:12, 44.60it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.43it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.22it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.26it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.34it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.35it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.34it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.38it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.54it/s][A
 38%|███▊      | 327/861 [00:07<00:11, 44.55it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.44it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.31it/s][A
 40%|███▉      | 342/861 [00:07<00:12, 42.62it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.31it/s][A
 41%|████      | 352/861 [00:07<00:11, 43.64it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.92it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.10it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.29it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.36it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.29it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.11it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.16it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.23it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.30it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.46it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.53it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.61it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.51it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.30it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.18it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.12it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.26it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.42it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.54it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.47it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.51it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.51it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.36it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.19it/s][A
 55%|█████▌    | 477/861 [00:10<00:09, 41.70it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 42.63it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 43.30it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.71it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.03it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.22it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.16it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.08it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.94it/s][A
 61%|██████    | 522/861 [00:11<00:07, 43.92it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.16it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.33it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.46it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.60it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.52it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.53it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.24it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.19it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.09it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.20it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.43it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.50it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.67it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.55it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.49it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.28it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.22it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.12it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 43.59it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.93it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.27it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.28it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.30it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.18it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.03it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 43.93it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.13it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.36it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.44it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.65it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.62it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.48it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.36it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.17it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.05it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.16it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.28it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.52it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.68it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.63it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.53it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.25it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.19it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.18it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.39it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 43.87it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.21it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.34it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.41it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.42it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.27it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.21it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 43.96it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.13it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.30it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.54it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.74it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.59it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.39it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.37it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.15it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.05it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.10it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.30it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.48it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.56it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 39.81it/s][A                                                 
                                                 [A100%|██████████| 395/395 [04:27<00:00,  3.36it/s]
100%|██████████| 861/861 [00:19<00:00, 39.81it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:29:49,571 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395
[INFO|configuration_utils.py:351] 2023-08-28 10:29:49,881 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:29:54,166 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:29:54,517 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:29:54,673 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:30:05,525 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:30:05,564 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158 (score: 0.965660572052002).
                                                 100%|██████████| 395/395 [04:55<00:00,  3.36it/s]100%|██████████| 395/395 [04:55<00:00,  1.34it/s]
[INFO|trainer.py:1894] 2023-08-28 10:30:16,997 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 10:30:17,213 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:30:21,506 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:30:21,661 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:30:21,756 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:30:22,318 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:22,318 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:22,318 >>   train_loss               =     0.7408
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:22,319 >>   train_runtime            = 0:04:55.45
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:22,319 >>   train_samples            =       5061
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:22,319 >>   train_samples_per_second =     85.646
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:22,319 >>   train_steps_per_second   =      1.337
{'eval_loss': 0.9763361811637878, 'eval_runtime': 19.4977, 'eval_samples_per_second': 353.068, 'eval_steps_per_second': 44.159, 'epoch': 4.99}
{'train_runtime': 295.4596, 'train_samples_per_second': 85.646, 'train_steps_per_second': 1.337, 'train_loss': 0.7407735703866694, 'epoch': 4.99}
08/28/2023 10:30:22 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:30:22,592 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:30:22,592 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 10:30:22,592 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 55.76it/s]  1%|▏         | 12/861 [00:00<00:17, 48.80it/s]  2%|▏         | 17/861 [00:00<00:17, 47.15it/s]  3%|▎         | 22/861 [00:00<00:18, 46.26it/s]  3%|▎         | 27/861 [00:00<00:18, 45.71it/s]  4%|▎         | 32/861 [00:00<00:18, 45.51it/s]  4%|▍         | 37/861 [00:00<00:18, 45.36it/s]  5%|▍         | 42/861 [00:00<00:18, 45.03it/s]  5%|▌         | 47/861 [00:01<00:18, 44.44it/s]  6%|▌         | 52/861 [00:01<00:18, 44.14it/s]  7%|▋         | 57/861 [00:01<00:18, 44.29it/s]  7%|▋         | 62/861 [00:01<00:17, 44.55it/s]  8%|▊         | 67/861 [00:01<00:17, 44.69it/s]  8%|▊         | 72/861 [00:01<00:17, 44.66it/s]  9%|▉         | 77/861 [00:01<00:17, 44.61it/s] 10%|▉         | 82/861 [00:01<00:17, 44.82it/s] 10%|█         | 87/861 [00:01<00:17, 44.65it/s] 11%|█         | 92/861 [00:02<00:18, 40.99it/s] 11%|█▏        | 97/861 [00:02<00:18, 42.09it/s] 12%|█▏        | 102/861 [00:02<00:17, 42.86it/s] 12%|█▏        | 107/861 [00:02<00:17, 43.43it/s] 13%|█▎        | 112/861 [00:02<00:17, 43.77it/s] 14%|█▎        | 117/861 [00:02<00:16, 44.23it/s] 14%|█▍        | 122/861 [00:02<00:16, 44.46it/s] 15%|█▍        | 127/861 [00:02<00:16, 44.39it/s] 15%|█▌        | 132/861 [00:02<00:16, 44.11it/s] 16%|█▌        | 137/861 [00:03<00:16, 44.04it/s] 16%|█▋        | 142/861 [00:03<00:16, 44.19it/s] 17%|█▋        | 147/861 [00:03<00:16, 44.38it/s] 18%|█▊        | 152/861 [00:03<00:15, 44.43it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.59it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.69it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.83it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.59it/s] 21%|██        | 177/861 [00:03<00:15, 44.27it/s] 21%|██        | 182/861 [00:04<00:15, 44.26it/s] 22%|██▏       | 187/861 [00:04<00:16, 39.97it/s] 22%|██▏       | 192/861 [00:04<00:16, 41.46it/s] 23%|██▎       | 197/861 [00:04<00:15, 42.45it/s] 23%|██▎       | 202/861 [00:04<00:15, 43.24it/s] 24%|██▍       | 207/861 [00:04<00:14, 43.80it/s] 25%|██▍       | 212/861 [00:04<00:14, 43.91it/s] 25%|██▌       | 217/861 [00:04<00:14, 44.32it/s] 26%|██▌       | 222/861 [00:05<00:14, 44.12it/s] 26%|██▋       | 227/861 [00:05<00:14, 43.86it/s] 27%|██▋       | 232/861 [00:05<00:14, 43.79it/s] 28%|██▊       | 237/861 [00:05<00:14, 44.01it/s] 28%|██▊       | 242/861 [00:05<00:13, 44.28it/s] 29%|██▊       | 247/861 [00:05<00:13, 44.39it/s] 29%|██▉       | 252/861 [00:05<00:13, 44.64it/s] 30%|██▉       | 257/861 [00:05<00:13, 44.79it/s] 30%|███       | 262/861 [00:05<00:13, 44.77it/s] 31%|███       | 267/861 [00:06<00:13, 44.49it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.15it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.16it/s] 33%|███▎      | 282/861 [00:06<00:13, 44.15it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.37it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.47it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.54it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.77it/s] 36%|███▌      | 307/861 [00:06<00:12, 44.71it/s] 36%|███▌      | 312/861 [00:07<00:12, 44.56it/s] 37%|███▋      | 317/861 [00:07<00:12, 44.31it/s] 37%|███▋      | 322/861 [00:07<00:12, 42.46it/s] 38%|███▊      | 327/861 [00:07<00:12, 43.28it/s] 39%|███▊      | 332/861 [00:07<00:12, 43.68it/s] 39%|███▉      | 337/861 [00:07<00:11, 44.09it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.41it/s] 40%|████      | 347/861 [00:07<00:11, 44.42it/s] 41%|████      | 352/861 [00:07<00:11, 44.41it/s] 41%|████▏     | 357/861 [00:08<00:11, 44.26it/s] 42%|████▏     | 362/861 [00:08<00:11, 43.99it/s] 43%|████▎     | 367/861 [00:08<00:11, 44.05it/s] 43%|████▎     | 372/861 [00:08<00:11, 44.15it/s] 44%|████▍     | 377/861 [00:08<00:10, 44.33it/s] 44%|████▍     | 382/861 [00:08<00:10, 44.49it/s] 45%|████▍     | 387/861 [00:08<00:10, 44.62it/s] 46%|████▌     | 392/861 [00:08<00:10, 44.77it/s] 46%|████▌     | 397/861 [00:08<00:10, 44.71it/s] 47%|████▋     | 402/861 [00:09<00:10, 44.42it/s] 47%|████▋     | 407/861 [00:09<00:10, 44.24it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.23it/s] 48%|████▊     | 417/861 [00:09<00:10, 44.39it/s] 49%|████▉     | 422/861 [00:09<00:09, 44.48it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.41it/s] 50%|█████     | 432/861 [00:09<00:09, 44.57it/s] 51%|█████     | 437/861 [00:09<00:09, 44.59it/s] 51%|█████▏    | 442/861 [00:09<00:09, 44.61it/s] 52%|█████▏    | 447/861 [00:10<00:09, 44.39it/s] 52%|█████▏    | 452/861 [00:10<00:09, 44.23it/s] 53%|█████▎    | 457/861 [00:10<00:09, 43.55it/s] 54%|█████▎    | 462/861 [00:10<00:09, 43.96it/s] 54%|█████▍    | 467/861 [00:10<00:08, 44.16it/s] 55%|█████▍    | 472/861 [00:10<00:08, 44.29it/s] 55%|█████▌    | 477/861 [00:10<00:08, 44.52it/s] 56%|█████▌    | 482/861 [00:10<00:08, 44.52it/s] 57%|█████▋    | 487/861 [00:11<00:08, 44.47it/s] 57%|█████▋    | 492/861 [00:11<00:08, 44.23it/s] 58%|█████▊    | 497/861 [00:11<00:08, 44.05it/s] 58%|█████▊    | 502/861 [00:11<00:08, 44.12it/s] 59%|█████▉    | 507/861 [00:11<00:07, 44.32it/s] 59%|█████▉    | 512/861 [00:11<00:07, 44.43it/s] 60%|██████    | 517/861 [00:11<00:07, 44.58it/s] 61%|██████    | 522/861 [00:11<00:07, 44.52it/s] 61%|██████    | 527/861 [00:11<00:07, 44.61it/s] 62%|██████▏   | 532/861 [00:12<00:07, 44.59it/s] 62%|██████▏   | 537/861 [00:12<00:07, 44.34it/s] 63%|██████▎   | 542/861 [00:12<00:07, 44.11it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.03it/s] 64%|██████▍   | 552/861 [00:12<00:06, 44.31it/s] 65%|██████▍   | 557/861 [00:12<00:06, 44.51it/s] 65%|██████▌   | 562/861 [00:12<00:06, 44.56it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.57it/s] 66%|██████▋   | 572/861 [00:12<00:06, 44.61it/s] 67%|██████▋   | 577/861 [00:13<00:06, 44.50it/s] 68%|██████▊   | 582/861 [00:13<00:06, 44.30it/s] 68%|██████▊   | 587/861 [00:13<00:06, 44.17it/s] 69%|██████▉   | 592/861 [00:13<00:06, 43.86it/s] 69%|██████▉   | 597/861 [00:13<00:05, 44.12it/s] 70%|██████▉   | 602/861 [00:13<00:05, 44.37it/s] 70%|███████   | 607/861 [00:13<00:05, 44.39it/s] 71%|███████   | 612/861 [00:13<00:05, 44.45it/s] 72%|███████▏  | 617/861 [00:13<00:05, 44.48it/s] 72%|███████▏  | 622/861 [00:14<00:05, 44.44it/s] 73%|███████▎  | 627/861 [00:14<00:05, 44.23it/s] 73%|███████▎  | 632/861 [00:14<00:05, 44.08it/s] 74%|███████▍  | 637/861 [00:14<00:05, 44.23it/s] 75%|███████▍  | 642/861 [00:14<00:04, 44.37it/s] 75%|███████▌  | 647/861 [00:14<00:04, 44.41it/s] 76%|███████▌  | 652/861 [00:14<00:04, 44.51it/s] 76%|███████▋  | 657/861 [00:14<00:04, 44.50it/s] 77%|███████▋  | 662/861 [00:14<00:04, 44.45it/s] 77%|███████▋  | 667/861 [00:15<00:04, 44.49it/s] 78%|███████▊  | 672/861 [00:15<00:04, 44.29it/s] 79%|███████▊  | 677/861 [00:15<00:04, 44.19it/s] 79%|███████▉  | 682/861 [00:15<00:04, 44.22it/s] 80%|███████▉  | 687/861 [00:15<00:03, 44.33it/s] 80%|████████  | 692/861 [00:15<00:03, 44.41it/s] 81%|████████  | 697/861 [00:15<00:03, 44.52it/s] 82%|████████▏ | 702/861 [00:15<00:03, 44.55it/s] 82%|████████▏ | 707/861 [00:15<00:03, 44.48it/s] 83%|████████▎ | 712/861 [00:16<00:03, 44.41it/s] 83%|████████▎ | 717/861 [00:16<00:03, 44.22it/s] 84%|████████▍ | 722/861 [00:16<00:03, 44.18it/s] 84%|████████▍ | 727/861 [00:16<00:03, 42.86it/s] 85%|████████▌ | 732/861 [00:16<00:02, 43.49it/s] 86%|████████▌ | 737/861 [00:16<00:02, 43.87it/s] 86%|████████▌ | 742/861 [00:16<00:02, 44.17it/s] 87%|████████▋ | 747/861 [00:16<00:02, 44.33it/s] 87%|████████▋ | 752/861 [00:16<00:02, 44.30it/s] 88%|████████▊ | 757/861 [00:17<00:02, 44.41it/s] 89%|████████▊ | 762/861 [00:17<00:02, 44.30it/s] 89%|████████▉ | 767/861 [00:17<00:02, 44.07it/s] 90%|████████▉ | 772/861 [00:17<00:02, 44.24it/s] 90%|█████████ | 777/861 [00:17<00:01, 44.37it/s] 91%|█████████ | 782/861 [00:17<00:01, 44.67it/s] 91%|█████████▏| 787/861 [00:17<00:01, 44.77it/s] 92%|█████████▏| 792/861 [00:17<00:01, 44.77it/s] 93%|█████████▎| 797/861 [00:17<00:01, 44.75it/s] 93%|█████████▎| 802/861 [00:18<00:01, 44.60it/s] 94%|█████████▎| 807/861 [00:18<00:01, 44.40it/s] 94%|█████████▍| 812/861 [00:18<00:01, 44.28it/s] 95%|█████████▍| 817/861 [00:18<00:00, 44.31it/s] 95%|█████████▌| 822/861 [00:18<00:00, 44.41it/s] 96%|█████████▌| 827/861 [00:18<00:00, 44.65it/s] 97%|█████████▋| 832/861 [00:18<00:00, 44.77it/s] 97%|█████████▋| 837/861 [00:18<00:00, 44.78it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.60it/s] 98%|█████████▊| 847/861 [00:19<00:00, 44.61it/s] 99%|█████████▉| 852/861 [00:19<00:00, 44.41it/s]100%|█████████▉| 857/861 [00:19<00:00, 44.32it/s]100%|██████████| 861/861 [00:19<00:00, 44.22it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:30:42,081 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:42,082 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:42,082 >>   eval_loss               =     0.9657
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:42,082 >>   eval_runtime            = 0:00:19.48
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:42,082 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:42,082 >>   eval_samples_per_second =    353.227
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:42,082 >>   eval_steps_per_second   =     44.179
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:42,082 >>   perplexity              =     2.6265
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:52,232 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:52,273 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:52,273 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:52,273 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:52,273 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:30:53,185 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:30:53,186 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:30:53,771 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:30:54,859 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:30:54,859 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,896 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,896 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,896 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:57,896 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:30:58,572 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:30:58,574 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:30:59,172 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:30:59,347 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:30:59,347 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-395
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-237
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-316
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-79
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-158
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:04,  1.41it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.61it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:11,  1.60it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:13,  1.57it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.58it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.56it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.56it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.57it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.52it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:38,  1.51it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:39,  1.53it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:43,  1.53it/s]Extractor Predicting: 67it [00:43,  1.57it/s]Extractor Predicting: 68it [00:44,  1.57it/s]Extractor Predicting: 69it [00:44,  1.55it/s]Extractor Predicting: 70it [00:45,  1.52it/s]Extractor Predicting: 71it [00:46,  1.52it/s]Extractor Predicting: 72it [00:46,  1.51it/s]Extractor Predicting: 73it [00:47,  1.52it/s]Extractor Predicting: 74it [00:48,  1.55it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:49,  1.52it/s]Extractor Predicting: 77it [00:50,  1.38it/s]Extractor Predicting: 78it [00:51,  1.43it/s]Extractor Predicting: 79it [00:51,  1.50it/s]Extractor Predicting: 80it [00:52,  1.52it/s]Extractor Predicting: 81it [00:52,  1.57it/s]Extractor Predicting: 82it [00:53,  1.64it/s]Extractor Predicting: 83it [00:54,  1.61it/s]Extractor Predicting: 84it [00:54,  1.62it/s]Extractor Predicting: 85it [00:55,  1.62it/s]Extractor Predicting: 86it [00:55,  1.65it/s]Extractor Predicting: 87it [00:56,  1.63it/s]Extractor Predicting: 88it [00:57,  1.64it/s]Extractor Predicting: 89it [00:57,  1.63it/s]Extractor Predicting: 90it [00:58,  1.67it/s]Extractor Predicting: 91it [00:58,  1.67it/s]Extractor Predicting: 92it [00:59,  1.70it/s]Extractor Predicting: 93it [01:00,  1.66it/s]Extractor Predicting: 94it [01:00,  1.67it/s]Extractor Predicting: 95it [01:01,  1.71it/s]Extractor Predicting: 96it [01:01,  1.67it/s]Extractor Predicting: 97it [01:02,  1.72it/s]Extractor Predicting: 98it [01:03,  1.70it/s]Extractor Predicting: 99it [01:03,  1.69it/s]Extractor Predicting: 100it [01:04,  1.69it/s]Extractor Predicting: 101it [01:04,  1.66it/s]Extractor Predicting: 102it [01:05,  1.65it/s]Extractor Predicting: 103it [01:06,  1.67it/s]Extractor Predicting: 104it [01:06,  1.61it/s]Extractor Predicting: 105it [01:07,  1.62it/s]Extractor Predicting: 106it [01:07,  1.65it/s]Extractor Predicting: 107it [01:08,  1.64it/s]Extractor Predicting: 108it [01:09,  1.69it/s]Extractor Predicting: 109it [01:09,  1.64it/s]Extractor Predicting: 110it [01:10,  1.67it/s]Extractor Predicting: 111it [01:10,  1.69it/s]Extractor Predicting: 112it [01:11,  1.65it/s]Extractor Predicting: 113it [01:12,  1.64it/s]Extractor Predicting: 114it [01:12,  1.61it/s]Extractor Predicting: 115it [01:13,  1.66it/s]Extractor Predicting: 116it [01:13,  1.64it/s]Extractor Predicting: 117it [01:14,  1.63it/s]Extractor Predicting: 118it [01:15,  1.63it/s]Extractor Predicting: 119it [01:15,  1.64it/s]Extractor Predicting: 120it [01:16,  1.67it/s]Extractor Predicting: 121it [01:16,  1.67it/s]Extractor Predicting: 122it [01:17,  1.62it/s]Extractor Predicting: 123it [01:18,  1.67it/s]Extractor Predicting: 124it [01:18,  1.64it/s]Extractor Predicting: 125it [01:19,  1.65it/s]Extractor Predicting: 126it [01:19,  1.67it/s]Extractor Predicting: 127it [01:20,  1.69it/s]Extractor Predicting: 128it [01:21,  1.62it/s]Extractor Predicting: 129it [01:21,  1.67it/s]Extractor Predicting: 130it [01:22,  1.68it/s]Extractor Predicting: 131it [01:22,  1.67it/s]Extractor Predicting: 132it [01:23,  1.70it/s]Extractor Predicting: 133it [01:24,  1.69it/s]Extractor Predicting: 134it [01:24,  1.69it/s]Extractor Predicting: 135it [01:25,  1.69it/s]Extractor Predicting: 136it [01:25,  1.69it/s]Extractor Predicting: 137it [01:26,  1.66it/s]Extractor Predicting: 138it [01:27,  1.67it/s]Extractor Predicting: 139it [01:27,  1.63it/s]Extractor Predicting: 140it [01:28,  1.62it/s]Extractor Predicting: 141it [01:28,  1.65it/s]Extractor Predicting: 142it [01:29,  1.64it/s]Extractor Predicting: 143it [01:30,  1.68it/s]Extractor Predicting: 144it [01:30,  1.63it/s]Extractor Predicting: 145it [01:31,  1.65it/s]Extractor Predicting: 146it [01:32,  1.66it/s]Extractor Predicting: 147it [01:32,  1.66it/s]Extractor Predicting: 148it [01:33,  1.66it/s]Extractor Predicting: 149it [01:33,  1.70it/s]Extractor Predicting: 150it [01:34,  1.62it/s]Extractor Predicting: 151it [01:35,  1.57it/s]Extractor Predicting: 152it [01:35,  1.53it/s]Extractor Predicting: 153it [01:36,  1.50it/s]Extractor Predicting: 154it [01:37,  1.41it/s]Extractor Predicting: 155it [01:37,  1.45it/s]Extractor Predicting: 156it [01:38,  1.42it/s]Extractor Predicting: 157it [01:39,  1.45it/s]Extractor Predicting: 158it [01:40,  1.44it/s]Extractor Predicting: 159it [01:40,  1.42it/s]Extractor Predicting: 160it [01:41,  1.41it/s]Extractor Predicting: 161it [01:42,  1.44it/s]Extractor Predicting: 162it [01:42,  1.40it/s]Extractor Predicting: 163it [01:43,  1.40it/s]Extractor Predicting: 164it [01:44,  1.38it/s]Extractor Predicting: 165it [01:45,  1.36it/s]Extractor Predicting: 166it [01:45,  1.37it/s]Extractor Predicting: 167it [01:46,  1.38it/s]Extractor Predicting: 168it [01:47,  1.39it/s]Extractor Predicting: 169it [01:48,  1.36it/s]Extractor Predicting: 170it [01:48,  1.37it/s]Extractor Predicting: 171it [01:49,  1.38it/s]Extractor Predicting: 172it [01:50,  1.37it/s]Extractor Predicting: 173it [01:50,  1.38it/s]Extractor Predicting: 174it [01:51,  1.40it/s]Extractor Predicting: 175it [01:52,  1.39it/s]Extractor Predicting: 176it [01:53,  1.26it/s]Extractor Predicting: 177it [01:54,  1.32it/s]Extractor Predicting: 178it [01:54,  1.32it/s]Extractor Predicting: 179it [01:55,  1.34it/s]Extractor Predicting: 180it [01:56,  1.35it/s]Extractor Predicting: 181it [01:56,  1.36it/s]Extractor Predicting: 182it [01:57,  1.37it/s]Extractor Predicting: 183it [01:58,  1.39it/s]Extractor Predicting: 184it [01:59,  1.42it/s]Extractor Predicting: 185it [01:59,  1.44it/s]Extractor Predicting: 186it [02:00,  1.44it/s]Extractor Predicting: 187it [02:01,  1.42it/s]Extractor Predicting: 188it [02:01,  1.43it/s]Extractor Predicting: 189it [02:02,  1.38it/s]Extractor Predicting: 190it [02:03,  1.38it/s]Extractor Predicting: 191it [02:04,  1.36it/s]Extractor Predicting: 192it [02:04,  1.33it/s]Extractor Predicting: 193it [02:05,  1.36it/s]Extractor Predicting: 194it [02:06,  1.40it/s]Extractor Predicting: 195it [02:06,  1.40it/s]Extractor Predicting: 196it [02:07,  1.43it/s]Extractor Predicting: 197it [02:08,  1.45it/s]Extractor Predicting: 198it [02:09,  1.43it/s]Extractor Predicting: 199it [02:09,  1.46it/s]Extractor Predicting: 200it [02:10,  1.47it/s]Extractor Predicting: 201it [02:11,  1.42it/s]Extractor Predicting: 202it [02:11,  1.44it/s]Extractor Predicting: 203it [02:12,  1.46it/s]Extractor Predicting: 204it [02:13,  1.46it/s]Extractor Predicting: 205it [02:13,  1.50it/s]Extractor Predicting: 206it [02:14,  1.48it/s]Extractor Predicting: 207it [02:15,  1.49it/s]Extractor Predicting: 208it [02:15,  1.47it/s]Extractor Predicting: 209it [02:16,  1.52it/s]Extractor Predicting: 210it [02:17,  1.51it/s]Extractor Predicting: 211it [02:17,  1.46it/s]Extractor Predicting: 212it [02:18,  1.47it/s]Extractor Predicting: 213it [02:19,  1.48it/s]Extractor Predicting: 214it [02:19,  1.49it/s]Extractor Predicting: 215it [02:20,  1.52it/s]Extractor Predicting: 216it [02:21,  1.53it/s]Extractor Predicting: 217it [02:21,  1.51it/s]Extractor Predicting: 218it [02:22,  1.53it/s]Extractor Predicting: 219it [02:23,  1.49it/s]Extractor Predicting: 220it [02:23,  1.48it/s]Extractor Predicting: 221it [02:24,  1.50it/s]Extractor Predicting: 222it [02:25,  1.51it/s]Extractor Predicting: 223it [02:25,  1.51it/s]Extractor Predicting: 224it [02:26,  1.50it/s]Extractor Predicting: 225it [02:27,  1.52it/s]Extractor Predicting: 226it [02:27,  1.52it/s]Extractor Predicting: 227it [02:28,  1.51it/s]Extractor Predicting: 228it [02:29,  1.51it/s]Extractor Predicting: 229it [02:29,  1.52it/s]Extractor Predicting: 230it [02:30,  1.52it/s]Extractor Predicting: 231it [02:30,  1.55it/s]Extractor Predicting: 232it [02:31,  1.56it/s]Extractor Predicting: 233it [02:32,  1.51it/s]Extractor Predicting: 234it [02:32,  1.51it/s]Extractor Predicting: 235it [02:33,  1.49it/s]Extractor Predicting: 236it [02:34,  1.51it/s]Extractor Predicting: 237it [02:35,  1.45it/s]Extractor Predicting: 237it [02:35,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:44,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:44,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:44,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:44,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:44,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:33:45,266 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:33:45,267 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:33:45,537 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:33:46,615 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:33:46,615 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:49,542 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:49,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:49,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:49,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:33:49,565 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:33:50,208 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:33:50,209 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:33:50,797 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:33:50,953 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:33:50,953 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6114649681528662,
  "recall": 0.013945380592678676,
  "score": 0.027268853855986364,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.75it/s]Extractor Predicting: 8it [00:04,  1.75it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:08,  1.65it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.65it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:11,  1.66it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.70it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:14,  1.75it/s]Extractor Predicting: 25it [00:14,  1.74it/s]Extractor Predicting: 26it [00:15,  1.75it/s]Extractor Predicting: 27it [00:15,  1.73it/s]Extractor Predicting: 28it [00:16,  1.72it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:17,  1.55it/s]Extractor Predicting: 31it [00:18,  1.60it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:19,  1.63it/s]Extractor Predicting: 34it [00:20,  1.62it/s]Extractor Predicting: 35it [00:20,  1.64it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.71it/s]Extractor Predicting: 38it [00:22,  1.74it/s]Extractor Predicting: 39it [00:23,  1.69it/s]Extractor Predicting: 40it [00:23,  1.65it/s]Extractor Predicting: 41it [00:24,  1.56it/s]Extractor Predicting: 42it [00:25,  1.57it/s]Extractor Predicting: 43it [00:25,  1.59it/s]Extractor Predicting: 44it [00:26,  1.58it/s]Extractor Predicting: 45it [00:27,  1.59it/s]Extractor Predicting: 46it [00:27,  1.53it/s]Extractor Predicting: 47it [00:28,  1.53it/s]Extractor Predicting: 48it [00:29,  1.56it/s]Extractor Predicting: 49it [00:29,  1.56it/s]Extractor Predicting: 50it [00:30,  1.57it/s]Extractor Predicting: 51it [00:30,  1.58it/s]Extractor Predicting: 52it [00:31,  1.58it/s]Extractor Predicting: 53it [00:32,  1.58it/s]Extractor Predicting: 54it [00:32,  1.55it/s]Extractor Predicting: 55it [00:33,  1.57it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:34,  1.55it/s]Extractor Predicting: 58it [00:35,  1.58it/s]Extractor Predicting: 59it [00:36,  1.57it/s]Extractor Predicting: 60it [00:36,  1.58it/s]Extractor Predicting: 61it [00:37,  1.55it/s]Extractor Predicting: 62it [00:37,  1.57it/s]Extractor Predicting: 63it [00:38,  1.58it/s]Extractor Predicting: 64it [00:39,  1.61it/s]Extractor Predicting: 65it [00:39,  1.61it/s]Extractor Predicting: 66it [00:40,  1.62it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:41,  1.62it/s]Extractor Predicting: 69it [00:42,  1.61it/s]Extractor Predicting: 70it [00:42,  1.58it/s]Extractor Predicting: 71it [00:43,  1.59it/s]Extractor Predicting: 72it [00:44,  1.60it/s]Extractor Predicting: 73it [00:44,  1.56it/s]Extractor Predicting: 74it [00:45,  1.61it/s]Extractor Predicting: 75it [00:46,  1.60it/s]Extractor Predicting: 76it [00:46,  1.58it/s]Extractor Predicting: 77it [00:47,  1.58it/s]Extractor Predicting: 78it [00:48,  1.58it/s]Extractor Predicting: 79it [00:48,  1.60it/s]Extractor Predicting: 80it [00:49,  1.63it/s]Extractor Predicting: 81it [00:49,  1.60it/s]Extractor Predicting: 82it [00:50,  1.56it/s]Extractor Predicting: 83it [00:51,  1.60it/s]Extractor Predicting: 84it [00:51,  1.59it/s]Extractor Predicting: 85it [00:52,  1.57it/s]Extractor Predicting: 86it [00:53,  1.60it/s]Extractor Predicting: 87it [00:53,  1.62it/s]Extractor Predicting: 88it [00:54,  1.58it/s]Extractor Predicting: 89it [00:54,  1.57it/s]Extractor Predicting: 90it [00:55,  1.59it/s]Extractor Predicting: 91it [00:56,  1.58it/s]Extractor Predicting: 92it [00:56,  1.61it/s]Extractor Predicting: 93it [00:57,  1.64it/s]Extractor Predicting: 94it [00:58,  1.59it/s]Extractor Predicting: 95it [00:58,  1.57it/s]Extractor Predicting: 96it [00:59,  1.61it/s]Extractor Predicting: 97it [00:59,  1.60it/s]Extractor Predicting: 98it [01:00,  1.61it/s]Extractor Predicting: 99it [01:01,  1.62it/s]Extractor Predicting: 100it [01:01,  1.53it/s]Extractor Predicting: 101it [01:02,  1.59it/s]Extractor Predicting: 102it [01:03,  1.57it/s]Extractor Predicting: 103it [01:03,  1.56it/s]Extractor Predicting: 104it [01:04,  1.61it/s]Extractor Predicting: 105it [01:04,  1.60it/s]Extractor Predicting: 106it [01:05,  1.63it/s]Extractor Predicting: 107it [01:06,  1.66it/s]Extractor Predicting: 108it [01:06,  1.68it/s]Extractor Predicting: 109it [01:07,  1.63it/s]Extractor Predicting: 110it [01:07,  1.63it/s]Extractor Predicting: 111it [01:08,  1.58it/s]Extractor Predicting: 112it [01:09,  1.59it/s]Extractor Predicting: 113it [01:09,  1.64it/s]Extractor Predicting: 114it [01:10,  1.60it/s]Extractor Predicting: 115it [01:11,  1.59it/s]Extractor Predicting: 116it [01:11,  1.63it/s]Extractor Predicting: 117it [01:12,  1.62it/s]Extractor Predicting: 118it [01:12,  1.65it/s]Extractor Predicting: 119it [01:13,  1.64it/s]Extractor Predicting: 120it [01:14,  1.58it/s]Extractor Predicting: 121it [01:14,  1.53it/s]Extractor Predicting: 122it [01:15,  1.52it/s]Extractor Predicting: 123it [01:16,  1.54it/s]Extractor Predicting: 124it [01:16,  1.58it/s]Extractor Predicting: 125it [01:17,  1.60it/s]Extractor Predicting: 126it [01:18,  1.61it/s]Extractor Predicting: 127it [01:18,  1.60it/s]Extractor Predicting: 128it [01:19,  1.64it/s]Extractor Predicting: 129it [01:19,  1.64it/s]Extractor Predicting: 130it [01:20,  1.65it/s]Extractor Predicting: 131it [01:21,  1.64it/s]Extractor Predicting: 132it [01:21,  1.60it/s]Extractor Predicting: 133it [01:22,  1.59it/s]Extractor Predicting: 134it [01:23,  1.49it/s]Extractor Predicting: 135it [01:23,  1.52it/s]Extractor Predicting: 136it [01:24,  1.55it/s]Extractor Predicting: 137it [01:24,  1.56it/s]Extractor Predicting: 138it [01:25,  1.56it/s]Extractor Predicting: 139it [01:26,  1.55it/s]Extractor Predicting: 140it [01:26,  1.60it/s]Extractor Predicting: 141it [01:27,  1.60it/s]Extractor Predicting: 142it [01:28,  1.57it/s]Extractor Predicting: 143it [01:28,  1.56it/s]Extractor Predicting: 144it [01:29,  1.56it/s]Extractor Predicting: 145it [01:30,  1.57it/s]Extractor Predicting: 146it [01:30,  1.56it/s]Extractor Predicting: 147it [01:31,  1.57it/s]Extractor Predicting: 148it [01:32,  1.51it/s]Extractor Predicting: 149it [01:32,  1.50it/s]Extractor Predicting: 150it [01:33,  1.51it/s]Extractor Predicting: 151it [01:34,  1.54it/s]Extractor Predicting: 152it [01:34,  1.54it/s]Extractor Predicting: 153it [01:35,  1.56it/s]Extractor Predicting: 154it [01:35,  1.52it/s]Extractor Predicting: 155it [01:36,  1.52it/s]Extractor Predicting: 156it [01:37,  1.54it/s]Extractor Predicting: 157it [01:37,  1.52it/s]Extractor Predicting: 158it [01:38,  1.50it/s]Extractor Predicting: 159it [01:39,  1.50it/s]Extractor Predicting: 160it [01:39,  1.52it/s]Extractor Predicting: 161it [01:40,  1.55it/s]Extractor Predicting: 162it [01:41,  1.52it/s]Extractor Predicting: 163it [01:41,  1.49it/s]Extractor Predicting: 164it [01:42,  1.49it/s]Extractor Predicting: 165it [01:43,  1.52it/s]Extractor Predicting: 166it [01:43,  1.50it/s]Extractor Predicting: 167it [01:44,  1.51it/s]Extractor Predicting: 168it [01:45,  1.63it/s]Extractor Predicting: 168it [01:45,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:45,111 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:45,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:45,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:45,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:45,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:35:45,777 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:35:45,778 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:35:46,342 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:35:47,415 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:35:47,415 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:50,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:50,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:50,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:50,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:50,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:35:51,039 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:35:51,041 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:35:51,659 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:35:51,827 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:35:51,827 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.44545454545454544,
  "recall": 0.012176938369781311,
  "score": 0.02370585389453314,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.75it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.54it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.52it/s]Extractor Predicting: 18it [00:11,  1.47it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 19it [00:12,  1.58it/s]
[INFO|configuration_utils.py:515] 2023-08-28 10:36:05,770 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:36:05,771 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:36:05,903 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:36:05,904 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 10:36:05,950 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:36:15,362 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 10:36:15,373 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 10:36:15,452 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:36:15,452 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:36:15,491 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:36:15,518 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:36:15,518 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:36:15,518 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:36:15,518 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:36:15,518 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:36:15,518 >> loading file outputs/wrapper/wiki/unseen_5_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.0009652509652509653,
  "score": 0.0019267822736030826,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 10:36:15,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:16,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:16,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:17,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:18,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:18,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:19,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:19,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:20,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:21,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:21,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:22,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:23,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:23,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:24,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:25,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:25,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:26,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:26,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:27,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:28,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:12<01:55, 12.82s/it][WARNING|generation_utils.py:914] 2023-08-28 10:36:28,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:29,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:29,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:30,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:31,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:31,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:32,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:33,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:33,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:34,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:35,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:35,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:36,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:37,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:37,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:38,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:39,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:39,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:40,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:41,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:41,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:42,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:42,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:43,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:44,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:44,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:45,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:46,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:30<02:07, 15.97s/it][WARNING|generation_utils.py:914] 2023-08-28 10:36:46,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:47,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:48,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:48,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:49,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:50,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:51,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:51,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:52,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:52,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:53,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:54,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:54,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:55,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:55,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:56,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:57,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:57,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:58,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:59,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:59,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:00,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:45<01:46, 15.20s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:01,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:01,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:02,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:02,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:03,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:04,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:04,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:05,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:05,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:06,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:06,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:07,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:08,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:08,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:09,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:09,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:10,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:11,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:11,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:12,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:13,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:13,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:14,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:59<01:28, 14.69s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:14,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:15,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:16,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:16,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:17,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:17,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:18,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:19,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:19,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:20,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:21,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:21,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:22,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:22,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:23,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:24,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:24,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:25,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:25,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:26,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:27,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:27,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:12<01:11, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:28,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:28,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:29,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:30,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:30,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:31,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:32,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:32,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:33,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:33,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:34,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:34,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:35,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:36,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:36,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:37,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:37,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:38,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:39,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:39,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:40,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:40,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:25<00:55, 13.89s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:41,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:42,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:42,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:43,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:44,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:44,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:45,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:45,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:46,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:47,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:47,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:48,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:49,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:49,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:50,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:50,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:51,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:52,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:52,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:53,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:53,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:54,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:54,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:39<00:41, 13.88s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:55,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:56,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:56,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:57,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:57,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:58,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:59,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:59,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:00,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:01,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:01,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:02,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:03,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:03,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:04,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:04,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:05,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:06,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:06,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:07,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:08,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:08,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:09,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:54<00:28, 14.13s/it][WARNING|generation_utils.py:914] 2023-08-28 10:38:10,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:10,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:11,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:12,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:12,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:13,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:13,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:14,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:14,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:15,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:16,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:16,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:17,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:18,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:18,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:19,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:19,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:20,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:20,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:21,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:06<00:13, 13.45s/it][WARNING|generation_utils.py:914] 2023-08-28 10:38:22,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:22,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:23,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:24,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:24,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:25,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:26,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:26,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:27,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:28,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:28,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:29,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:29,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:30,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:31,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:32,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:32,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:33,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:34,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:34,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:35,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:36,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:21<00:00, 13.85s/it]Generating: 100%|██████████| 10/10 [02:21<00:00, 14.11s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:42,772 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:42,826 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:42,826 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:42,826 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:42,826 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:38:43,492 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:38:43,493 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:38:44,106 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:38:45,208 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:38:45,208 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:48,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:48,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:48,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:48,239 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:38:48,239 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:38:48,890 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:38:48,891 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:38:49,467 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:38:49,641 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:38:49,641 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 217, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 259, 'raw': 384}
{'target': 600, 'success': 279, 'raw': 416}
{'target': 600, 'success': 295, 'raw': 448}
{'target': 600, 'success': 316, 'raw': 480}
{'target': 600, 'success': 339, 'raw': 512}
{'target': 600, 'success': 359, 'raw': 544}
{'target': 600, 'success': 381, 'raw': 576}
{'target': 600, 'success': 403, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 445, 'raw': 672}
{'target': 600, 'success': 467, 'raw': 704}
{'target': 600, 'success': 491, 'raw': 736}
{'target': 600, 'success': 510, 'raw': 768}
{'target': 600, 'success': 531, 'raw': 800}
{'target': 600, 'success': 551, 'raw': 832}
{'target': 600, 'success': 574, 'raw': 864}
{'target': 600, 'success': 602, 'raw': 896}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.671875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.842391304347826, 'errors': {'', "('General Motors', 'shares border with', '', 'In 1994 , Lutte formed the company Lusco as a result of the merger of Lusco and General Motors .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8693181818181818, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 11223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.43it/s]Extractor Estimating: 2it [00:01,  1.51it/s]Extractor Estimating: 3it [00:01,  1.55it/s]Extractor Estimating: 4it [00:02,  1.58it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.60it/s]Extractor Estimating: 7it [00:04,  1.61it/s]Extractor Estimating: 8it [00:05,  1.62it/s]Extractor Estimating: 9it [00:05,  1.62it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.64it/s]Extractor Estimating: 12it [00:07,  1.62it/s]Extractor Estimating: 13it [00:08,  1.64it/s]Extractor Estimating: 14it [00:08,  1.58it/s]Extractor Estimating: 15it [00:09,  1.56it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:10,  1.63it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:11,  1.62it/s]Extractor Estimating: 20it [00:12,  1.66it/s]Extractor Estimating: 21it [00:13,  1.59it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.62it/s]Extractor Estimating: 24it [00:14,  1.66it/s]Extractor Estimating: 25it [00:15,  1.69it/s]Extractor Estimating: 26it [00:16,  1.67it/s]Extractor Estimating: 27it [00:16,  1.60it/s]Extractor Estimating: 28it [00:17,  1.59it/s]Extractor Estimating: 29it [00:17,  1.65it/s]Extractor Estimating: 30it [00:18,  1.62it/s]Extractor Estimating: 31it [00:19,  1.60it/s]Extractor Estimating: 32it [00:19,  1.61it/s]Extractor Estimating: 33it [00:20,  1.59it/s]Extractor Estimating: 34it [00:21,  1.50it/s]Extractor Estimating: 35it [00:21,  1.52it/s]Extractor Estimating: 36it [00:22,  1.50it/s]Extractor Estimating: 37it [00:23,  1.48it/s]Extractor Estimating: 38it [00:23,  1.53it/s]Extractor Estimating: 39it [00:24,  1.50it/s]Extractor Estimating: 40it [00:25,  1.50it/s]Extractor Estimating: 41it [00:25,  1.52it/s]Extractor Estimating: 42it [00:26,  1.52it/s]Extractor Estimating: 43it [00:27,  1.55it/s]Extractor Estimating: 44it [00:27,  1.56it/s]Extractor Estimating: 45it [00:28,  1.58it/s]Extractor Estimating: 46it [00:29,  1.59it/s]Extractor Estimating: 47it [00:29,  1.45it/s]Extractor Estimating: 48it [00:30,  1.48it/s]Extractor Estimating: 49it [00:31,  1.50it/s]Extractor Estimating: 50it [00:31,  1.49it/s]Extractor Estimating: 51it [00:32,  1.51it/s]Extractor Estimating: 52it [00:33,  1.49it/s]Extractor Estimating: 53it [00:33,  1.52it/s]Extractor Estimating: 54it [00:34,  1.56it/s]Extractor Estimating: 55it [00:35,  1.55it/s]Extractor Estimating: 56it [00:35,  1.56it/s]Extractor Estimating: 57it [00:36,  1.59it/s]Extractor Estimating: 58it [00:36,  1.56it/s]Extractor Estimating: 59it [00:37,  1.53it/s]Extractor Estimating: 60it [00:38,  1.55it/s]Extractor Estimating: 61it [00:38,  1.58it/s]Extractor Estimating: 62it [00:39,  1.62it/s]Extractor Estimating: 63it [00:40,  1.58it/s]Extractor Estimating: 64it [00:40,  1.57it/s]Extractor Estimating: 65it [00:41,  1.58it/s]Extractor Estimating: 66it [00:42,  1.58it/s]Extractor Estimating: 67it [00:42,  1.62it/s]Extractor Estimating: 68it [00:43,  1.60it/s]Extractor Estimating: 69it [00:43,  1.57it/s]Extractor Estimating: 70it [00:44,  1.60it/s]Extractor Estimating: 71it [00:45,  1.58it/s]Extractor Estimating: 72it [00:45,  1.60it/s]Extractor Estimating: 73it [00:46,  1.61it/s]Extractor Estimating: 74it [00:47,  1.56it/s]Extractor Estimating: 75it [00:47,  1.61it/s]Extractor Estimating: 76it [00:48,  1.60it/s]Extractor Estimating: 77it [00:48,  1.61it/s]Extractor Estimating: 78it [00:49,  1.63it/s]Extractor Estimating: 79it [00:50,  1.67it/s]Extractor Estimating: 80it [00:50,  1.71it/s]Extractor Estimating: 81it [00:51,  1.73it/s]Extractor Estimating: 82it [00:51,  1.74it/s]Extractor Estimating: 83it [00:52,  1.76it/s]Extractor Estimating: 84it [00:52,  1.77it/s]Extractor Estimating: 85it [00:53,  1.74it/s]Extractor Estimating: 86it [00:54,  1.73it/s]Extractor Estimating: 87it [00:54,  1.76it/s]Extractor Estimating: 88it [00:55,  1.76it/s]Extractor Estimating: 89it [00:55,  1.75it/s]Extractor Estimating: 90it [00:56,  1.74it/s]Extractor Estimating: 91it [00:56,  1.69it/s]Extractor Estimating: 92it [00:57,  1.64it/s]Extractor Estimating: 93it [00:58,  1.64it/s]Extractor Estimating: 94it [00:58,  1.66it/s]Extractor Estimating: 95it [00:59,  1.62it/s]Extractor Estimating: 96it [01:00,  1.65it/s]Extractor Estimating: 97it [01:00,  1.63it/s]Extractor Estimating: 98it [01:01,  1.65it/s]Extractor Estimating: 99it [01:01,  1.72it/s]Extractor Estimating: 100it [01:02,  1.74it/s]Extractor Estimating: 101it [01:03,  1.60it/s]Extractor Estimating: 102it [01:03,  1.60it/s]Extractor Estimating: 103it [01:04,  1.59it/s]Extractor Estimating: 104it [01:04,  1.58it/s]Extractor Estimating: 105it [01:05,  1.62it/s]Extractor Estimating: 106it [01:06,  1.52it/s]Extractor Estimating: 107it [01:06,  1.57it/s]Extractor Estimating: 108it [01:07,  1.58it/s]Extractor Estimating: 109it [01:08,  1.55it/s]Extractor Estimating: 110it [01:08,  1.56it/s]Extractor Estimating: 111it [01:09,  1.57it/s]Extractor Estimating: 112it [01:10,  1.59it/s]Extractor Estimating: 113it [01:10,  1.64it/s]Extractor Estimating: 114it [01:11,  1.58it/s]Extractor Estimating: 115it [01:11,  1.62it/s]Extractor Estimating: 116it [01:12,  1.61it/s]Extractor Estimating: 117it [01:13,  1.68it/s]Extractor Estimating: 118it [01:13,  1.71it/s]Extractor Estimating: 119it [01:14,  1.69it/s]Extractor Estimating: 120it [01:14,  1.64it/s]Extractor Estimating: 121it [01:15,  1.61it/s]Extractor Estimating: 122it [01:16,  1.63it/s]Extractor Estimating: 123it [01:16,  1.65it/s]Extractor Estimating: 124it [01:17,  1.64it/s]Extractor Estimating: 125it [01:17,  1.60it/s]Extractor Estimating: 126it [01:18,  1.60it/s]Extractor Estimating: 127it [01:19,  1.65it/s]Extractor Estimating: 128it [01:19,  1.62it/s]Extractor Estimating: 129it [01:20,  1.66it/s]Extractor Estimating: 130it [01:20,  1.72it/s]Extractor Estimating: 131it [01:21,  1.70it/s]Extractor Estimating: 132it [01:22,  1.65it/s]Extractor Estimating: 133it [01:22,  1.62it/s]Extractor Estimating: 134it [01:23,  1.65it/s]Extractor Estimating: 135it [01:23,  1.65it/s]Extractor Estimating: 136it [01:24,  1.62it/s]Extractor Estimating: 137it [01:25,  1.64it/s]Extractor Estimating: 138it [01:25,  1.64it/s]Extractor Estimating: 139it [01:26,  1.65it/s]Extractor Estimating: 140it [01:27,  1.66it/s]Extractor Estimating: 141it [01:27,  1.64it/s]Extractor Estimating: 142it [01:28,  1.60it/s]Extractor Estimating: 143it [01:28,  1.62it/s]Extractor Estimating: 144it [01:29,  1.66it/s]Extractor Estimating: 145it [01:30,  1.68it/s]Extractor Estimating: 146it [01:30,  1.65it/s]Extractor Estimating: 147it [01:31,  1.63it/s]Extractor Estimating: 148it [01:31,  1.59it/s]Extractor Estimating: 149it [01:32,  1.60it/s]Extractor Estimating: 150it [01:33,  1.61it/s]Extractor Estimating: 151it [01:33,  1.58it/s]Extractor Estimating: 152it [01:34,  1.59it/s]Extractor Estimating: 153it [01:35,  1.60it/s]Extractor Estimating: 154it [01:35,  1.58it/s]Extractor Estimating: 155it [01:36,  1.62it/s]Extractor Estimating: 156it [01:36,  1.61it/s]Extractor Estimating: 157it [01:37,  1.64it/s]Extractor Estimating: 158it [01:38,  1.61it/s]Extractor Estimating: 159it [01:38,  1.59it/s]Extractor Estimating: 160it [01:39,  1.62it/s]Extractor Estimating: 161it [01:40,  1.59it/s]Extractor Estimating: 162it [01:40,  1.57it/s]Extractor Estimating: 163it [01:41,  1.58it/s]Extractor Estimating: 164it [01:41,  1.62it/s]Extractor Estimating: 165it [01:42,  1.62it/s]Extractor Estimating: 166it [01:43,  1.57it/s]Extractor Estimating: 167it [01:43,  1.64it/s]Extractor Estimating: 168it [01:44,  1.67it/s]Extractor Estimating: 169it [01:45,  1.65it/s]Extractor Estimating: 170it [01:45,  1.61it/s]Extractor Estimating: 171it [01:46,  1.62it/s]Extractor Estimating: 172it [01:46,  1.62it/s]Extractor Estimating: 173it [01:47,  1.65it/s]Extractor Estimating: 174it [01:48,  1.66it/s]Extractor Estimating: 175it [01:48,  1.65it/s]Extractor Estimating: 176it [01:49,  1.68it/s]Extractor Estimating: 177it [01:49,  1.64it/s]Extractor Estimating: 178it [01:50,  1.63it/s]Extractor Estimating: 179it [01:51,  1.66it/s]Extractor Estimating: 180it [01:51,  1.67it/s]Extractor Estimating: 181it [01:52,  1.64it/s]Extractor Estimating: 182it [01:52,  1.64it/s]Extractor Estimating: 183it [01:53,  1.62it/s]Extractor Estimating: 184it [01:54,  1.62it/s]Extractor Estimating: 185it [01:54,  1.63it/s]Extractor Estimating: 186it [01:55,  1.48it/s]Extractor Estimating: 187it [01:56,  1.52it/s]Extractor Estimating: 188it [01:56,  1.53it/s]Extractor Estimating: 189it [01:57,  1.54it/s]Extractor Estimating: 190it [01:58,  1.52it/s]Extractor Estimating: 191it [01:58,  1.54it/s]Extractor Estimating: 192it [01:59,  1.56it/s]Extractor Estimating: 193it [02:00,  1.57it/s]Extractor Estimating: 194it [02:00,  1.58it/s]Extractor Estimating: 195it [02:01,  1.59it/s]Extractor Estimating: 196it [02:01,  1.61it/s]Extractor Estimating: 197it [02:02,  1.61it/s]Extractor Estimating: 198it [02:03,  1.60it/s]Extractor Estimating: 199it [02:03,  1.56it/s]Extractor Estimating: 200it [02:04,  1.59it/s]Extractor Estimating: 201it [02:05,  1.59it/s]Extractor Estimating: 202it [02:05,  1.56it/s]Extractor Estimating: 203it [02:06,  1.59it/s]Extractor Estimating: 204it [02:06,  1.60it/s]Extractor Estimating: 205it [02:07,  1.61it/s]Extractor Estimating: 206it [02:08,  1.62it/s]Extractor Estimating: 207it [02:08,  1.60it/s]Extractor Estimating: 208it [02:09,  1.62it/s]Extractor Estimating: 209it [02:10,  1.63it/s]Extractor Estimating: 210it [02:10,  1.61it/s]Extractor Estimating: 211it [02:11,  1.63it/s]Extractor Estimating: 212it [02:11,  1.62it/s]Extractor Estimating: 213it [02:12,  1.65it/s]Extractor Estimating: 214it [02:13,  1.66it/s]Extractor Estimating: 215it [02:13,  1.63it/s]Extractor Estimating: 216it [02:14,  1.55it/s]Extractor Estimating: 217it [02:15,  1.57it/s]Extractor Estimating: 218it [02:15,  1.63it/s]Extractor Estimating: 219it [02:16,  1.61it/s]Extractor Estimating: 220it [02:16,  1.64it/s]Extractor Estimating: 221it [02:17,  1.63it/s]Extractor Estimating: 222it [02:18,  1.66it/s]Extractor Estimating: 223it [02:18,  1.65it/s]Extractor Estimating: 224it [02:19,  1.60it/s]Extractor Estimating: 225it [02:19,  1.59it/s]Extractor Estimating: 226it [02:20,  1.61it/s]Extractor Estimating: 227it [02:21,  1.59it/s]Extractor Estimating: 228it [02:21,  1.58it/s]Extractor Estimating: 229it [02:22,  1.59it/s]Extractor Estimating: 230it [02:23,  1.61it/s]Extractor Estimating: 231it [02:23,  1.61it/s]Extractor Estimating: 232it [02:24,  1.59it/s]Extractor Estimating: 233it [02:24,  1.60it/s]Extractor Estimating: 234it [02:25,  1.65it/s]Extractor Estimating: 235it [02:26,  1.65it/s]Extractor Estimating: 236it [02:26,  1.64it/s]Extractor Estimating: 237it [02:27,  1.63it/s]Extractor Estimating: 238it [02:27,  1.66it/s]Extractor Estimating: 239it [02:28,  1.67it/s]Extractor Estimating: 240it [02:29,  1.65it/s]Extractor Estimating: 241it [02:29,  1.65it/s]Extractor Estimating: 242it [02:30,  1.66it/s]Extractor Estimating: 243it [02:30,  1.69it/s]Extractor Estimating: 244it [02:31,  1.63it/s]Extractor Estimating: 245it [02:32,  1.59it/s]Extractor Estimating: 246it [02:32,  1.62it/s]Extractor Estimating: 247it [02:33,  1.59it/s]Extractor Estimating: 248it [02:34,  1.63it/s]Extractor Estimating: 249it [02:34,  1.59it/s]Extractor Estimating: 250it [02:35,  1.61it/s]Extractor Estimating: 250it [02:35,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:44,644 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:44,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:44,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:44,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:44,687 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:41:44,998 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:41:44,999 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:41:45,286 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:41:46,391 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:41:46,391 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:48,199 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:48,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:48,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:48,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:48,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:41:48,523 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:41:48,524 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:41:48,811 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:41:48,957 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:41:48,957 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 12:18:10,094 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 12:18:10,408 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 4992 mean pseudo reward: 0.9114329379103788
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 25502
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25602, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25602, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.984, loss:834.2291
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.986, loss:789.9952
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 0.992, loss:790.0760
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 0.988, loss:776.9342
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.993, loss:755.3956
>> valid entity prec:0.5191, rec:0.3073, f1:0.3860
>> valid relation prec:0.1914, rec:0.0071, f1:0.0137
>> valid relation with NER prec:0.1914, rec:0.0071, f1:0.0137
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 3.072, loss:775.4525
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.994, loss:757.3745
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.987, loss:784.1008
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.996, loss:746.8050
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.981, loss:771.5999
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4679, rec:0.2945, f1:0.3615
>> valid relation prec:0.2035, rec:0.0119, f1:0.0225
>> valid relation with NER prec:0.2035, rec:0.0119, f1:0.0225
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 60, avg_time 3.081, loss:752.1787
g_step 1200, step 160, avg_time 0.992, loss:750.0671
g_step 1300, step 52, avg_time 0.991, loss:734.0339
g_step 1400, step 152, avg_time 0.989, loss:717.6064
g_step 1500, step 44, avg_time 0.984, loss:700.2120
>> valid entity prec:0.4673, rec:0.4320, f1:0.4490
>> valid relation prec:0.1965, rec:0.0276, f1:0.0484
>> valid relation with NER prec:0.1965, rec:0.0276, f1:0.0484
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 144, avg_time 3.106, loss:698.1032
g_step 1700, step 36, avg_time 0.988, loss:708.7264
g_step 1800, step 136, avg_time 0.991, loss:667.6066
g_step 1900, step 28, avg_time 0.989, loss:695.2714
g_step 2000, step 128, avg_time 0.989, loss:648.2448
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4319, rec:0.3714, f1:0.3994
>> valid relation prec:0.2076, rec:0.0142, f1:0.0267
>> valid relation with NER prec:0.2076, rec:0.0142, f1:0.0267
g_step 2100, step 20, avg_time 3.091, loss:635.9077
g_step 2200, step 120, avg_time 0.985, loss:622.9477
g_step 2300, step 12, avg_time 0.995, loss:638.6302
g_step 2400, step 112, avg_time 0.982, loss:587.8603
g_step 2500, step 4, avg_time 0.992, loss:620.7268
>> valid entity prec:0.4369, rec:0.3272, f1:0.3742
>> valid relation prec:0.1066, rec:0.0118, f1:0.0212
>> valid relation with NER prec:0.1066, rec:0.0118, f1:0.0212
g_step 2600, step 104, avg_time 3.078, loss:565.2290
g_step 2700, step 204, avg_time 0.983, loss:600.7841
g_step 2800, step 96, avg_time 0.981, loss:545.3702
g_step 2900, step 196, avg_time 0.993, loss:575.1506
g_step 3000, step 88, avg_time 0.995, loss:548.9765
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4957, rec:0.3163, f1:0.3862
>> valid relation prec:0.1860, rec:0.0128, f1:0.0239
>> valid relation with NER prec:0.1860, rec:0.0128, f1:0.0239
g_step 3100, step 188, avg_time 3.055, loss:539.5688
g_step 3200, step 80, avg_time 0.975, loss:518.5235
g_step 3300, step 180, avg_time 0.984, loss:534.1422
g_step 3400, step 72, avg_time 0.984, loss:502.9830
g_step 3500, step 172, avg_time 0.975, loss:515.0725
>> valid entity prec:0.4791, rec:0.2431, f1:0.3225
>> valid relation prec:0.1593, rec:0.0125, f1:0.0232
>> valid relation with NER prec:0.1593, rec:0.0125, f1:0.0232
g_step 3600, step 64, avg_time 3.044, loss:494.6192
g_step 3700, step 164, avg_time 0.974, loss:492.4598
g_step 3800, step 56, avg_time 0.980, loss:476.0103
g_step 3900, step 156, avg_time 0.977, loss:486.0420
g_step 4000, step 48, avg_time 0.984, loss:472.8229
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4625, rec:0.3894, f1:0.4228
>> valid relation prec:0.1847, rec:0.0246, f1:0.0434
>> valid relation with NER prec:0.1847, rec:0.0246, f1:0.0434
g_step 4100, step 148, avg_time 3.081, loss:456.5404
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 12:18:10 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 12:18:10 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_12-18-10_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 12:18:11 - WARNING - datasets.builder -   Using custom data configuration default-133ea321560ba9a3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-133ea321560ba9a3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  1.06 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 12:18:15,050 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:18:15,051 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:18:15,052 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:18:15,053 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:18:15,146 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:15,188 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:15,189 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:15,189 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:15,189 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:15,189 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:15,189 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 12:18:15,521 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:18:18,661 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 12:18:18,662 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-133ea321560ba9a3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.41ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.72ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.35ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.76ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.84ba/s]100%|██████████| 6/6 [00:01<00:00,  4.16ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.97ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.70ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.03ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  4.19ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.30ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.37ba/s]100%|██████████| 7/7 [00:01<00:00,  4.58ba/s]100%|██████████| 7/7 [00:01<00:00,  4.25ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.71ba/s] 50%|█████     | 3/6 [00:00<00:00,  7.27ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  8.64ba/s]100%|██████████| 6/6 [00:00<00:00,  9.28ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.28ba/s] 43%|████▎     | 3/7 [00:00<00:00,  6.79ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  8.47ba/s]100%|██████████| 7/7 [00:00<00:00,  9.46ba/s]100%|██████████| 7/7 [00:00<00:00,  8.25ba/s]
[INFO|trainer.py:414] 2023-08-28 12:18:25,280 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 12:18:25,634 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 12:18:25,634 >>   Num examples = 5023
[INFO|trainer.py:1149] 2023-08-28 12:18:25,634 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 12:18:25,634 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 12:18:25,634 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 12:18:25,634 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 12:18:25,634 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<05:41,  1.14it/s]  1%|          | 2/390 [00:01<03:27,  1.87it/s]  1%|          | 3/390 [00:01<02:44,  2.36it/s]  1%|          | 4/390 [00:01<02:23,  2.68it/s]  1%|▏         | 5/390 [00:02<02:12,  2.91it/s]  2%|▏         | 6/390 [00:02<02:05,  3.06it/s]  2%|▏         | 7/390 [00:02<02:00,  3.17it/s]  2%|▏         | 8/390 [00:02<01:57,  3.24it/s]  2%|▏         | 9/390 [00:03<01:55,  3.29it/s]  3%|▎         | 10/390 [00:03<01:54,  3.33it/s]  3%|▎         | 11/390 [00:03<01:53,  3.35it/s]  3%|▎         | 12/390 [00:04<01:52,  3.37it/s]  3%|▎         | 13/390 [00:04<01:51,  3.38it/s]  4%|▎         | 14/390 [00:04<01:50,  3.39it/s]  4%|▍         | 15/390 [00:04<01:50,  3.40it/s]  4%|▍         | 16/390 [00:05<01:50,  3.40it/s]  4%|▍         | 17/390 [00:05<01:49,  3.40it/s]  5%|▍         | 18/390 [00:05<01:49,  3.40it/s]  5%|▍         | 19/390 [00:06<01:48,  3.41it/s]  5%|▌         | 20/390 [00:06<01:48,  3.41it/s]  5%|▌         | 21/390 [00:06<01:53,  3.24it/s]  6%|▌         | 22/390 [00:07<01:51,  3.29it/s]  6%|▌         | 23/390 [00:07<01:50,  3.32it/s]  6%|▌         | 24/390 [00:07<01:49,  3.34it/s]  6%|▋         | 25/390 [00:07<01:48,  3.36it/s]  7%|▋         | 26/390 [00:08<01:47,  3.37it/s]  7%|▋         | 27/390 [00:08<01:47,  3.38it/s]  7%|▋         | 28/390 [00:08<01:47,  3.38it/s]  7%|▋         | 29/390 [00:09<01:46,  3.39it/s]  8%|▊         | 30/390 [00:09<01:46,  3.39it/s]  8%|▊         | 31/390 [00:09<01:45,  3.39it/s]  8%|▊         | 32/390 [00:10<01:45,  3.39it/s]  8%|▊         | 33/390 [00:10<01:45,  3.39it/s]  9%|▊         | 34/390 [00:10<01:44,  3.39it/s]  9%|▉         | 35/390 [00:10<01:44,  3.39it/s]  9%|▉         | 36/390 [00:11<01:44,  3.39it/s]  9%|▉         | 37/390 [00:11<01:43,  3.40it/s] 10%|▉         | 38/390 [00:11<01:43,  3.39it/s] 10%|█         | 39/390 [00:12<01:43,  3.39it/s] 10%|█         | 40/390 [00:12<01:43,  3.39it/s] 11%|█         | 41/390 [00:12<01:42,  3.40it/s] 11%|█         | 42/390 [00:12<01:42,  3.40it/s] 11%|█         | 43/390 [00:13<01:42,  3.40it/s] 11%|█▏        | 44/390 [00:13<01:41,  3.40it/s] 12%|█▏        | 45/390 [00:13<01:41,  3.40it/s] 12%|█▏        | 46/390 [00:14<01:41,  3.40it/s] 12%|█▏        | 47/390 [00:14<01:41,  3.39it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.40it/s] 13%|█▎        | 49/390 [00:15<01:49,  3.11it/s] 13%|█▎        | 50/390 [00:15<01:46,  3.19it/s] 13%|█▎        | 51/390 [00:15<01:44,  3.25it/s] 13%|█▎        | 52/390 [00:16<01:42,  3.28it/s] 14%|█▎        | 53/390 [00:16<01:41,  3.32it/s] 14%|█▍        | 54/390 [00:16<01:40,  3.34it/s] 14%|█▍        | 55/390 [00:16<01:39,  3.35it/s] 14%|█▍        | 56/390 [00:17<01:39,  3.36it/s] 15%|█▍        | 57/390 [00:17<01:38,  3.37it/s] 15%|█▍        | 58/390 [00:17<01:38,  3.38it/s] 15%|█▌        | 59/390 [00:18<01:37,  3.38it/s] 15%|█▌        | 60/390 [00:18<01:37,  3.38it/s] 16%|█▌        | 61/390 [00:18<01:37,  3.39it/s] 16%|█▌        | 62/390 [00:18<01:36,  3.39it/s] 16%|█▌        | 63/390 [00:19<01:36,  3.39it/s] 16%|█▋        | 64/390 [00:19<01:36,  3.39it/s] 17%|█▋        | 65/390 [00:19<01:35,  3.39it/s] 17%|█▋        | 66/390 [00:20<01:35,  3.39it/s] 17%|█▋        | 67/390 [00:20<01:35,  3.39it/s] 17%|█▋        | 68/390 [00:20<01:34,  3.39it/s] 18%|█▊        | 69/390 [00:21<01:34,  3.39it/s] 18%|█▊        | 70/390 [00:21<01:34,  3.39it/s] 18%|█▊        | 71/390 [00:21<01:33,  3.39it/s] 18%|█▊        | 72/390 [00:21<01:33,  3.39it/s] 19%|█▊        | 73/390 [00:22<01:33,  3.39it/s] 19%|█▉        | 74/390 [00:22<01:33,  3.39it/s] 19%|█▉        | 75/390 [00:22<01:32,  3.39it/s] 19%|█▉        | 76/390 [00:23<01:32,  3.39it/s] 20%|█▉        | 77/390 [00:23<01:32,  3.39it/s] 20%|██        | 78/390 [00:23<01:31,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 12:18:49,452 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:18:49,452 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 12:18:49,452 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.72it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.27it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.91it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.96it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.41it/s][A
  4%|▎         | 32/861 [00:00<00:18, 45.00it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.60it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.48it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.45it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.62it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.62it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.70it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.77it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.57it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.47it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.31it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.26it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.40it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.47it/s][A
 12%|█▏        | 102/861 [00:02<00:16, 44.69it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.64it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.68it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.59it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.36it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.22it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.30it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.33it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.57it/s][A
 17%|█▋        | 147/861 [00:03<00:15, 44.65it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.69it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.70it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.54it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.34it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.19it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.17it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.32it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.48it/s][A
 22%|██▏       | 192/861 [00:04<00:14, 44.64it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.75it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.69it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.54it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.10it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.13it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.28it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.40it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.46it/s][A
 28%|██▊       | 237/861 [00:05<00:13, 44.67it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.70it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.69it/s][A
 29%|██▉       | 252/861 [00:05<00:14, 42.38it/s][A
 30%|██▉       | 257/861 [00:05<00:14, 43.07it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.48it/s][A
 31%|███       | 267/861 [00:05<00:13, 43.79it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.02it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.18it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.44it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.59it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.28it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.15it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.17it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.29it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.44it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.47it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.54it/s][A
 38%|███▊      | 327/861 [00:07<00:11, 44.62it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.57it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.38it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.18it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.22it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.38it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.53it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.52it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.63it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.63it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.52it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.39it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.30it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.24it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.37it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.52it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.59it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.58it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.64it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.57it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.39it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.29it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.31it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 44.34it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.37it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.54it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.49it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.59it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.53it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.36it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.39it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.25it/s][A
 57%|█████▋    | 487/861 [00:10<00:08, 44.33it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.42it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.57it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.66it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.52it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.50it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.43it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.32it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.26it/s][A
 62%|██████▏   | 532/861 [00:11<00:07, 44.32it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.39it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.51it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.60it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.46it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.54it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.42it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.36it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.28it/s][A
 67%|██████▋   | 577/861 [00:12<00:06, 44.30it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.38it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.49it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.58it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.54it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.46it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.32it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.37it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.33it/s][A
 72%|███████▏  | 622/861 [00:13<00:05, 44.38it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.25it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.37it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.56it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.57it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.44it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.46it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.37it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.43it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.36it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 40.68it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 41.88it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 42.72it/s][A
 80%|███████▉  | 687/861 [00:15<00:04, 43.36it/s][A
 80%|████████  | 692/861 [00:15<00:03, 43.80it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.00it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.14it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.20it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.97it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.88it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.08it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.29it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.50it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.57it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.62it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.56it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 44.44it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.18it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.13it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.22it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.37it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.47it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.58it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.67it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.60it/s][A
 93%|█████████▎| 797/861 [00:17<00:01, 44.41it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.26it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 42.26it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 43.03it/s][A
 95%|█████████▍| 817/861 [00:18<00:01, 43.48it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 43.91it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.09it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 42.43it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.25it/s][A
 98%|█████████▊| 842/861 [00:18<00:00, 44.21it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.95it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.06it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.18it/s][A
                                                 [A                                                
100%|██████████| 861/861 [00:19<00:00, 44.18it/s][A 20%|██        | 78/390 [00:43<01:31,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:19:09,044 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 12:19:09,216 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:19:12,618 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:19:12,870 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:19:12,999 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:55<51:06,  9.86s/it] 21%|██        | 80/390 [00:56<36:09,  7.00s/it] 21%|██        | 81/390 [00:56<25:40,  4.99s/it] 21%|██        | 82/390 [00:56<18:22,  3.58s/it] 21%|██▏       | 83/390 [00:57<13:16,  2.59s/it] 22%|██▏       | 84/390 [00:57<09:42,  1.90s/it] 22%|██▏       | 85/390 [00:57<07:13,  1.42s/it] 22%|██▏       | 86/390 [00:57<05:29,  1.08s/it] 22%|██▏       | 87/390 [00:58<04:16,  1.18it/s] 23%|██▎       | 88/390 [00:58<03:25,  1.47it/s] 23%|██▎       | 89/390 [00:58<02:49,  1.77it/s] 23%|██▎       | 90/390 [00:59<02:24,  2.08it/s] 23%|██▎       | 91/390 [00:59<02:13,  2.24it/s] 24%|██▎       | 92/390 [00:59<01:59,  2.50it/s] 24%|██▍       | 93/390 [01:00<01:48,  2.73it/s] 24%|██▍       | 94/390 [01:00<01:41,  2.91it/s] 24%|██▍       | 95/390 [01:00<01:36,  3.05it/s] 25%|██▍       | 96/390 [01:00<01:33,  3.16it/s] 25%|██▍       | 97/390 [01:01<01:30,  3.24it/s] 25%|██▌       | 98/390 [01:01<01:28,  3.30it/s] 25%|██▌       | 99/390 [01:01<01:27,  3.34it/s] 26%|██▌       | 100/390 [01:02<01:26,  3.37it/s] 26%|██▌       | 101/390 [01:02<01:25,  3.39it/s] 26%|██▌       | 102/390 [01:02<01:30,  3.17it/s] 26%|██▋       | 103/390 [01:03<01:28,  3.25it/s] 27%|██▋       | 104/390 [01:03<01:26,  3.30it/s] 27%|██▋       | 105/390 [01:03<01:25,  3.34it/s] 27%|██▋       | 106/390 [01:03<01:24,  3.37it/s] 27%|██▋       | 107/390 [01:04<01:23,  3.39it/s] 28%|██▊       | 108/390 [01:04<01:22,  3.41it/s] 28%|██▊       | 109/390 [01:04<01:22,  3.42it/s] 28%|██▊       | 110/390 [01:05<01:21,  3.43it/s] 28%|██▊       | 111/390 [01:05<01:21,  3.43it/s] 29%|██▊       | 112/390 [01:05<01:20,  3.43it/s] 29%|██▉       | 113/390 [01:05<01:24,  3.28it/s] 29%|██▉       | 114/390 [01:06<01:23,  3.32it/s] 29%|██▉       | 115/390 [01:06<01:21,  3.36it/s] 30%|██▉       | 116/390 [01:06<01:20,  3.38it/s] 30%|███       | 117/390 [01:07<01:20,  3.40it/s] 30%|███       | 118/390 [01:07<01:19,  3.41it/s] 31%|███       | 119/390 [01:07<01:19,  3.42it/s] 31%|███       | 120/390 [01:08<01:18,  3.43it/s] 31%|███       | 121/390 [01:08<01:18,  3.43it/s] 31%|███▏      | 122/390 [01:08<01:18,  3.43it/s] 32%|███▏      | 123/390 [01:08<01:17,  3.44it/s] 32%|███▏      | 124/390 [01:09<01:21,  3.25it/s] 32%|███▏      | 125/390 [01:09<01:20,  3.30it/s] 32%|███▏      | 126/390 [01:09<01:18,  3.34it/s] 33%|███▎      | 127/390 [01:10<01:17,  3.37it/s] 33%|███▎      | 128/390 [01:10<01:17,  3.39it/s] 33%|███▎      | 129/390 [01:10<01:16,  3.41it/s] 33%|███▎      | 130/390 [01:10<01:16,  3.42it/s] 34%|███▎      | 131/390 [01:11<01:15,  3.42it/s] 34%|███▍      | 132/390 [01:11<01:15,  3.43it/s] 34%|███▍      | 133/390 [01:11<01:14,  3.43it/s] 34%|███▍      | 134/390 [01:12<01:14,  3.44it/s] 35%|███▍      | 135/390 [01:12<01:16,  3.35it/s] 35%|███▍      | 136/390 [01:12<01:15,  3.38it/s] 35%|███▌      | 137/390 [01:13<01:14,  3.39it/s] 35%|███▌      | 138/390 [01:13<01:14,  3.40it/s] 36%|███▌      | 139/390 [01:13<01:13,  3.42it/s] 36%|███▌      | 140/390 [01:13<01:12,  3.43it/s] 36%|███▌      | 141/390 [01:14<01:12,  3.43it/s] 36%|███▋      | 142/390 [01:14<01:12,  3.43it/s] 37%|███▋      | 143/390 [01:14<01:11,  3.44it/s] 37%|███▋      | 144/390 [01:15<01:11,  3.44it/s] 37%|███▋      | 145/390 [01:15<01:11,  3.44it/s] 37%|███▋      | 146/390 [01:15<01:12,  3.35it/s] 38%|███▊      | 147/390 [01:15<01:12,  3.37it/s] 38%|███▊      | 148/390 [01:16<01:11,  3.40it/s] 38%|███▊      | 149/390 [01:16<01:10,  3.41it/s] 38%|███▊      | 150/390 [01:16<01:10,  3.42it/s] 39%|███▊      | 151/390 [01:17<01:09,  3.42it/s] 39%|███▉      | 152/390 [01:17<01:09,  3.43it/s] 39%|███▉      | 153/390 [01:17<01:09,  3.43it/s] 39%|███▉      | 154/390 [01:18<01:08,  3.44it/s] 40%|███▉      | 155/390 [01:18<01:08,  3.44it/s] 40%|████      | 156/390 [01:18<01:08,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 12:19:44,360 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:19:44,360 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 12:19:44,360 >>   Batch size = 8
{'eval_loss': 0.9702323079109192, 'eval_runtime': 19.4363, 'eval_samples_per_second': 354.183, 'eval_steps_per_second': 44.299, 'epoch': 0.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.86it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.68it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.03it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.78it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.26it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.29it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.13it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.17it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.42it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.58it/s][A
  7%|▋         | 57/861 [00:01<00:17, 44.74it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.78it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.45it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.47it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.24it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.22it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.27it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.34it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.57it/s][A
 12%|█▏        | 102/861 [00:02<00:16, 44.71it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.68it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.57it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.43it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.35it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.27it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.17it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.28it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.46it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.58it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.67it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.60it/s][A
 19%|█▉        | 162/861 [00:03<00:16, 43.16it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 43.45it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.68it/s][A
 21%|██        | 177/861 [00:03<00:15, 43.77it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.04it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.24it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.45it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.47it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.33it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.37it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.46it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.36it/s][A
 26%|██▌       | 222/861 [00:04<00:14, 44.33it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.43it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.52it/s][A
 28%|██▊       | 237/861 [00:05<00:13, 44.58it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.54it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.40it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.45it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.43it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.34it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.26it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.42it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.50it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.51it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.50it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.40it/s][A
 34%|███▍      | 297/861 [00:06<00:13, 41.00it/s][A
 35%|███▌      | 302/861 [00:06<00:13, 42.19it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 42.89it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.33it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.79it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.07it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.12it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.20it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 43.93it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.95it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.17it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.37it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.45it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.61it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.66it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.60it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.45it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.16it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.12it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.12it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.38it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.49it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.51it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.57it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.62it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.44it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.26it/s][A
 50%|█████     | 432/861 [00:09<00:10, 42.56it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.17it/s][A
 51%|█████▏    | 442/861 [00:09<00:09, 43.55it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.88it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.13it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.38it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.30it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.30it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.00it/s][A
 55%|█████▌    | 477/861 [00:10<00:09, 42.23it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 43.14it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.59it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.89it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.12it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.30it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.36it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.24it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.92it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.01it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.22it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.44it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.61it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.60it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.54it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.47it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.27it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.18it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 43.20it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 43.77it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.14it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.33it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.42it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.44it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.29it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.17it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.02it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.17it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.34it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.49it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.68it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.72it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.61it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.46it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.28it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.07it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.21it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.34it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.56it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.62it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.63it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.64it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.49it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.28it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.06it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 40.67it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 41.90it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 42.82it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.42it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.86it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.07it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.10it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.02it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.84it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.95it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.17it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.38it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.61it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.65it/s][A
 90%|████████▉ | 772/861 [00:17<00:01, 44.62it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.45it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.31it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.20it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.12it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.37it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.49it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.59it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.65it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.64it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.46it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.37it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.24it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 40.76it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 41.95it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 42.81it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.54it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.98it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.98it/s][A 40%|████      | 156/390 [01:38<01:08,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:20:04,033 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 12:20:04,285 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:20:08,421 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:20:08,605 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:20:08,710 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:50<38:19,  9.87s/it] 41%|████      | 158/390 [01:51<27:05,  7.01s/it] 41%|████      | 159/390 [01:51<19:13,  4.99s/it] 41%|████      | 160/390 [01:51<13:44,  3.58s/it] 41%|████▏     | 161/390 [01:52<09:54,  2.60s/it] 42%|████▏     | 162/390 [01:52<07:14,  1.91s/it] 42%|████▏     | 163/390 [01:52<05:22,  1.42s/it] 42%|████▏     | 164/390 [01:52<04:05,  1.08s/it] 42%|████▏     | 165/390 [01:53<03:10,  1.18it/s] 43%|████▎     | 166/390 [01:53<02:34,  1.45it/s] 43%|████▎     | 167/390 [01:53<02:07,  1.75it/s] 43%|████▎     | 168/390 [01:54<01:50,  2.01it/s] 43%|████▎     | 169/390 [01:54<01:36,  2.29it/s] 44%|████▎     | 170/390 [01:54<01:27,  2.51it/s] 44%|████▍     | 171/390 [01:55<01:20,  2.73it/s] 44%|████▍     | 172/390 [01:55<01:14,  2.91it/s] 44%|████▍     | 173/390 [01:55<01:11,  3.05it/s] 45%|████▍     | 174/390 [01:55<01:08,  3.16it/s] 45%|████▍     | 175/390 [01:56<01:06,  3.24it/s] 45%|████▌     | 176/390 [01:56<01:04,  3.30it/s] 45%|████▌     | 177/390 [01:56<01:03,  3.34it/s] 46%|████▌     | 178/390 [01:57<01:02,  3.37it/s] 46%|████▌     | 179/390 [01:57<01:07,  3.14it/s] 46%|████▌     | 180/390 [01:57<01:05,  3.23it/s] 46%|████▋     | 181/390 [01:58<01:03,  3.29it/s] 47%|████▋     | 182/390 [01:58<01:02,  3.34it/s] 47%|████▋     | 183/390 [01:58<01:01,  3.37it/s] 47%|████▋     | 184/390 [01:58<01:00,  3.39it/s] 47%|████▋     | 185/390 [01:59<01:00,  3.41it/s] 48%|████▊     | 186/390 [01:59<00:59,  3.42it/s] 48%|████▊     | 187/390 [01:59<00:59,  3.43it/s] 48%|████▊     | 188/390 [02:00<00:58,  3.43it/s] 48%|████▊     | 189/390 [02:00<00:58,  3.43it/s] 49%|████▊     | 190/390 [02:00<01:01,  3.23it/s] 49%|████▉     | 191/390 [02:00<01:00,  3.29it/s] 49%|████▉     | 192/390 [02:01<00:59,  3.34it/s] 49%|████▉     | 193/390 [02:01<00:58,  3.37it/s] 50%|████▉     | 194/390 [02:01<00:57,  3.39it/s] 50%|█████     | 195/390 [02:02<00:57,  3.41it/s] 50%|█████     | 196/390 [02:02<00:56,  3.42it/s] 51%|█████     | 197/390 [02:02<00:56,  3.43it/s] 51%|█████     | 198/390 [02:03<00:55,  3.43it/s] 51%|█████     | 199/390 [02:03<00:55,  3.44it/s] 51%|█████▏    | 200/390 [02:03<00:55,  3.44it/s] 52%|█████▏    | 201/390 [02:03<00:58,  3.24it/s] 52%|█████▏    | 202/390 [02:04<00:57,  3.30it/s] 52%|█████▏    | 203/390 [02:04<00:55,  3.34it/s] 52%|█████▏    | 204/390 [02:04<00:55,  3.37it/s] 53%|█████▎    | 205/390 [02:05<00:54,  3.39it/s] 53%|█████▎    | 206/390 [02:05<00:54,  3.41it/s] 53%|█████▎    | 207/390 [02:05<00:53,  3.42it/s] 53%|█████▎    | 208/390 [02:05<00:53,  3.43it/s] 54%|█████▎    | 209/390 [02:06<00:52,  3.44it/s] 54%|█████▍    | 210/390 [02:06<00:52,  3.44it/s] 54%|█████▍    | 211/390 [02:06<00:52,  3.44it/s] 54%|█████▍    | 212/390 [02:07<00:54,  3.29it/s] 55%|█████▍    | 213/390 [02:07<00:53,  3.33it/s] 55%|█████▍    | 214/390 [02:07<00:52,  3.37it/s] 55%|█████▌    | 215/390 [02:08<00:51,  3.39it/s] 55%|█████▌    | 216/390 [02:08<00:51,  3.41it/s] 56%|█████▌    | 217/390 [02:08<00:50,  3.42it/s] 56%|█████▌    | 218/390 [02:08<00:50,  3.43it/s] 56%|█████▌    | 219/390 [02:09<00:49,  3.43it/s] 56%|█████▋    | 220/390 [02:09<00:49,  3.44it/s] 57%|█████▋    | 221/390 [02:09<00:49,  3.44it/s] 57%|█████▋    | 222/390 [02:10<00:48,  3.44it/s] 57%|█████▋    | 223/390 [02:10<00:50,  3.30it/s] 57%|█████▋    | 224/390 [02:10<00:49,  3.34it/s] 58%|█████▊    | 225/390 [02:10<00:48,  3.37it/s] 58%|█████▊    | 226/390 [02:11<00:48,  3.40it/s] 58%|█████▊    | 227/390 [02:11<00:47,  3.41it/s] 58%|█████▊    | 228/390 [02:11<00:47,  3.42it/s] 59%|█████▊    | 229/390 [02:12<00:46,  3.43it/s] 59%|█████▉    | 230/390 [02:12<00:46,  3.44it/s] 59%|█████▉    | 231/390 [02:12<00:46,  3.44it/s] 59%|█████▉    | 232/390 [02:13<00:45,  3.44it/s] 60%|█████▉    | 233/390 [02:13<00:45,  3.44it/s] 60%|██████    | 234/390 [02:13<00:46,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 12:20:39,411 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:20:39,411 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 12:20:39,411 >>   Batch size = 8
{'eval_loss': 0.9705470204353333, 'eval_runtime': 19.5122, 'eval_samples_per_second': 352.806, 'eval_steps_per_second': 44.126, 'epoch': 1.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.59it/s][A
  1%|▏         | 12/861 [00:00<00:17, 49.12it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.31it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.23it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.20it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.60it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.36it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.21it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.38it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.56it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.62it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.81it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.74it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.61it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.31it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.18it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.20it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.24it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.42it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.61it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.69it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.66it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.52it/s][A
 14%|█▍        | 122/861 [00:02<00:17, 42.77it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.32it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.55it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.86it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.10it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.31it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.45it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.42it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.16it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.15it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.18it/s][A
 21%|██        | 177/861 [00:03<00:16, 42.41it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.13it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.67it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 43.92it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.09it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.22it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.09it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.16it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.00it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.17it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.34it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.48it/s][A
 28%|██▊       | 237/861 [00:05<00:13, 44.58it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.60it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.48it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.30it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.25it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.23it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.23it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.38it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.50it/s][A
 33%|███▎      | 282/861 [00:06<00:12, 44.60it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.61it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.44it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.28it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.17it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.15it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 42.88it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.54it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.93it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.24it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.41it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.28it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.27it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.11it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.03it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.05it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.36it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.54it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.63it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.62it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.54it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.36it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.14it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.07it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.16it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.29it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.51it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.64it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.63it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.58it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.43it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.24it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.16it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 41.99it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 42.80it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.47it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 43.87it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.10it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.17it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.97it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.22it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.95it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.92it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.17it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.34it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.58it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.68it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.63it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.50it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.33it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.14it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.17it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.26it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.39it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.56it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.65it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.73it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.47it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.29it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.18it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.00it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.50it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 43.81it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.13it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.34it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.39it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.33it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.22it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.94it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.07it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.28it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.44it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.54it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.57it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.49it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.33it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.22it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.19it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.13it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.36it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.54it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.60it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.64it/s][A
 81%|████████  | 697/861 [00:15<00:03, 42.38it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 43.15it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 43.45it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.53it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 42.25it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.02it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.63it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 43.98it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.03it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.11it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.26it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.16it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.99it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.06it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.22it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.45it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.53it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.49it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.48it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.40it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.19it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.12it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.22it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.44it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.53it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.64it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.55it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.53it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 44.37it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.24it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.16it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 42.69it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.32it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.32it/s][A 60%|██████    | 234/390 [02:33<00:46,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:20:59,007 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 12:20:59,164 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:21:02,133 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:21:02,279 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:21:02,380 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:43<23:42,  9.18s/it] 61%|██████    | 236/390 [02:43<16:43,  6.51s/it] 61%|██████    | 237/390 [02:44<11:51,  4.65s/it] 61%|██████    | 238/390 [02:44<08:27,  3.34s/it] 61%|██████▏   | 239/390 [02:44<06:06,  2.43s/it] 62%|██████▏   | 240/390 [02:45<04:28,  1.79s/it] 62%|██████▏   | 241/390 [02:45<03:20,  1.34s/it] 62%|██████▏   | 242/390 [02:45<02:32,  1.03s/it] 62%|██████▏   | 243/390 [02:45<01:58,  1.24it/s] 63%|██████▎   | 244/390 [02:46<01:35,  1.53it/s] 63%|██████▎   | 245/390 [02:46<01:19,  1.83it/s] 63%|██████▎   | 246/390 [02:46<01:07,  2.13it/s] 63%|██████▎   | 247/390 [02:47<01:00,  2.35it/s] 64%|██████▎   | 248/390 [02:47<00:54,  2.59it/s] 64%|██████▍   | 249/390 [02:47<00:51,  2.73it/s] 64%|██████▍   | 250/390 [02:48<00:48,  2.90it/s] 64%|██████▍   | 251/390 [02:48<00:45,  3.04it/s] 65%|██████▍   | 252/390 [02:48<00:43,  3.14it/s] 65%|██████▍   | 253/390 [02:48<00:44,  3.10it/s] 65%|██████▌   | 254/390 [02:49<00:42,  3.19it/s] 65%|██████▌   | 255/390 [02:49<00:41,  3.25it/s] 66%|██████▌   | 256/390 [02:49<00:40,  3.29it/s] 66%|██████▌   | 257/390 [02:50<00:40,  3.32it/s] 66%|██████▌   | 258/390 [02:50<00:39,  3.34it/s] 66%|██████▋   | 259/390 [02:50<00:39,  3.36it/s] 67%|██████▋   | 260/390 [02:50<00:38,  3.37it/s] 67%|██████▋   | 261/390 [02:51<00:38,  3.38it/s] 67%|██████▋   | 262/390 [02:51<00:37,  3.38it/s] 67%|██████▋   | 263/390 [02:51<00:38,  3.29it/s] 68%|██████▊   | 264/390 [02:52<00:37,  3.32it/s] 68%|██████▊   | 265/390 [02:52<00:37,  3.34it/s] 68%|██████▊   | 266/390 [02:52<00:36,  3.36it/s] 68%|██████▊   | 267/390 [02:53<00:36,  3.37it/s] 69%|██████▊   | 268/390 [02:53<00:36,  3.38it/s] 69%|██████▉   | 269/390 [02:53<00:38,  3.12it/s] 69%|██████▉   | 270/390 [02:54<00:37,  3.20it/s] 69%|██████▉   | 271/390 [02:54<00:36,  3.25it/s] 70%|██████▉   | 272/390 [02:54<00:35,  3.30it/s] 70%|███████   | 273/390 [02:54<00:37,  3.13it/s] 70%|███████   | 274/390 [02:55<00:36,  3.21it/s] 71%|███████   | 275/390 [02:55<00:35,  3.26it/s] 71%|███████   | 276/390 [02:55<00:34,  3.30it/s] 71%|███████   | 277/390 [02:56<00:33,  3.33it/s] 71%|███████▏  | 278/390 [02:56<00:33,  3.35it/s] 72%|███████▏  | 279/390 [02:56<00:33,  3.36it/s] 72%|███████▏  | 280/390 [02:57<00:32,  3.37it/s] 72%|███████▏  | 281/390 [02:57<00:32,  3.38it/s] 72%|███████▏  | 282/390 [02:57<00:31,  3.38it/s] 73%|███████▎  | 283/390 [02:57<00:32,  3.33it/s] 73%|███████▎  | 284/390 [02:58<00:31,  3.35it/s] 73%|███████▎  | 285/390 [02:58<00:31,  3.36it/s] 73%|███████▎  | 286/390 [02:58<00:30,  3.37it/s] 74%|███████▎  | 287/390 [02:59<00:30,  3.38it/s] 74%|███████▍  | 288/390 [02:59<00:30,  3.38it/s] 74%|███████▍  | 289/390 [02:59<00:29,  3.39it/s] 74%|███████▍  | 290/390 [03:00<00:29,  3.39it/s] 75%|███████▍  | 291/390 [03:00<00:29,  3.39it/s] 75%|███████▍  | 292/390 [03:00<00:28,  3.39it/s] 75%|███████▌  | 293/390 [03:00<00:28,  3.39it/s] 75%|███████▌  | 294/390 [03:01<00:29,  3.24it/s] 76%|███████▌  | 295/390 [03:01<00:28,  3.29it/s] 76%|███████▌  | 296/390 [03:01<00:28,  3.32it/s] 76%|███████▌  | 297/390 [03:02<00:27,  3.34it/s] 76%|███████▋  | 298/390 [03:02<00:27,  3.36it/s] 77%|███████▋  | 299/390 [03:02<00:27,  3.37it/s] 77%|███████▋  | 300/390 [03:03<00:26,  3.38it/s] 77%|███████▋  | 301/390 [03:03<00:26,  3.38it/s] 77%|███████▋  | 302/390 [03:03<00:25,  3.39it/s] 78%|███████▊  | 303/390 [03:03<00:25,  3.39it/s] 78%|███████▊  | 304/390 [03:04<00:25,  3.39it/s] 78%|███████▊  | 305/390 [03:04<00:26,  3.25it/s] 78%|███████▊  | 306/390 [03:04<00:25,  3.29it/s] 79%|███████▊  | 307/390 [03:05<00:24,  3.32it/s] 79%|███████▉  | 308/390 [03:05<00:24,  3.34it/s] 79%|███████▉  | 309/390 [03:05<00:24,  3.36it/s] 79%|███████▉  | 310/390 [03:06<00:23,  3.37it/s] 80%|███████▉  | 311/390 [03:06<00:23,  3.37it/s] 80%|████████  | 312/390 [03:06<00:23,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 12:21:32,362 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:21:32,362 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 12:21:32,362 >>   Batch size = 8
{'eval_loss': 0.9738979339599609, 'eval_runtime': 19.4939, 'eval_samples_per_second': 353.136, 'eval_steps_per_second': 44.168, 'epoch': 2.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.78it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.22it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.57it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.66it/s][A
  3%|▎         | 27/861 [00:00<00:19, 42.99it/s][A
  4%|▎         | 32/861 [00:00<00:19, 43.59it/s][A
  4%|▍         | 37/861 [00:00<00:18, 43.71it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.90it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.16it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.45it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.47it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.47it/s][A
  8%|▊         | 67/861 [00:01<00:18, 44.09it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.23it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.31it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.28it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.29it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.47it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.55it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.62it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.39it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.38it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.37it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.35it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.31it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.44it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.47it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.57it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.50it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.35it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.22it/s][A
 19%|█▉        | 162/861 [00:03<00:17, 40.48it/s][A
 19%|█▉        | 167/861 [00:03<00:16, 41.65it/s][A
 20%|█▉        | 172/861 [00:03<00:16, 42.59it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.23it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.76it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.11it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.26it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.12it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 43.91it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 43.86it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.12it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.19it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.46it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.61it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.73it/s][A
 28%|██▊       | 237/861 [00:05<00:13, 44.58it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.29it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.07it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.10it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.19it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.38it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.46it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.69it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.75it/s][A
 33%|███▎      | 282/861 [00:06<00:12, 44.57it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.38it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.07it/s][A
 34%|███▍      | 297/861 [00:06<00:13, 41.50it/s][A
 35%|███▌      | 302/861 [00:06<00:13, 42.39it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 43.22it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.65it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.07it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.25it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.22it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 44.07it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 43.82it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.91it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.18it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.41it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.51it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.62it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.64it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.55it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.27it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.11it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.03it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.28it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.38it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.55it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.64it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.72it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.57it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.10it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.10it/s][A
 50%|█████     | 432/861 [00:09<00:10, 42.62it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.26it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.70it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.08it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.29it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.37it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.28it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.25it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.01it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.16it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.31it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.50it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.64it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.59it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.59it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.43it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.32it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.23it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.35it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.44it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.52it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.53it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.60it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.51it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.39it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.31it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.34it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.30it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.58it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.53it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.58it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.61it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.47it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.42it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.33it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.30it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.40it/s][A
 72%|███████▏  | 617/861 [00:13<00:05, 44.51it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.48it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.52it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.57it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.47it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.43it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.31it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.33it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.35it/s][A
 77%|███████▋  | 662/861 [00:14<00:04, 44.49it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.53it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.58it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.47it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.48it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 43.89it/s][A
 80%|████████  | 692/861 [00:15<00:03, 43.98it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.12it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.19it/s][A
 82%|████████▏ | 707/861 [00:15<00:03, 44.37it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.47it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.50it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.43it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.34it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.26it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.37it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.33it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.43it/s][A
 87%|████████▋ | 752/861 [00:16<00:02, 44.55it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.57it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.58it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.43it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.28it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.34it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.33it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.41it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.34it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.42it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.56it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.51it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.47it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.38it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 41.30it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 42.31it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 43.04it/s][A
 97%|█████████▋| 837/861 [00:18<00:00, 43.44it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.83it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.08it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.11it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.07it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 44.07it/s][A 80%|████████  | 312/390 [03:26<00:23,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:21:52,048 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 12:21:52,259 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:21:55,816 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:21:56,029 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:21:56,105 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:38<12:30,  9.75s/it] 81%|████████  | 314/390 [03:38<08:46,  6.93s/it] 81%|████████  | 315/390 [03:39<06:10,  4.94s/it] 81%|████████  | 316/390 [03:39<04:22,  3.54s/it] 81%|████████▏ | 317/390 [03:39<03:07,  2.57s/it] 82%|████████▏ | 318/390 [03:39<02:15,  1.89s/it] 82%|████████▏ | 319/390 [03:40<01:40,  1.41s/it] 82%|████████▏ | 320/390 [03:40<01:15,  1.07s/it] 82%|████████▏ | 321/390 [03:40<00:57,  1.19it/s] 83%|████████▎ | 322/390 [03:41<00:46,  1.48it/s] 83%|████████▎ | 323/390 [03:41<00:37,  1.78it/s] 83%|████████▎ | 324/390 [03:41<00:31,  2.08it/s] 83%|████████▎ | 325/390 [03:42<00:28,  2.27it/s] 84%|████████▎ | 326/390 [03:42<00:25,  2.52it/s] 84%|████████▍ | 327/390 [03:42<00:23,  2.73it/s] 84%|████████▍ | 328/390 [03:42<00:21,  2.90it/s] 84%|████████▍ | 329/390 [03:43<00:20,  3.03it/s] 85%|████████▍ | 330/390 [03:43<00:19,  3.14it/s] 85%|████████▍ | 331/390 [03:43<00:18,  3.21it/s] 85%|████████▌ | 332/390 [03:44<00:17,  3.26it/s] 85%|████████▌ | 333/390 [03:44<00:17,  3.30it/s] 86%|████████▌ | 334/390 [03:44<00:16,  3.33it/s] 86%|████████▌ | 335/390 [03:44<00:16,  3.35it/s] 86%|████████▌ | 336/390 [03:45<00:16,  3.20it/s] 86%|████████▋ | 337/390 [03:45<00:16,  3.26it/s] 87%|████████▋ | 338/390 [03:45<00:15,  3.30it/s] 87%|████████▋ | 339/390 [03:46<00:15,  3.33it/s] 87%|████████▋ | 340/390 [03:46<00:14,  3.35it/s] 87%|████████▋ | 341/390 [03:46<00:14,  3.36it/s] 88%|████████▊ | 342/390 [03:47<00:14,  3.37it/s] 88%|████████▊ | 343/390 [03:47<00:13,  3.38it/s] 88%|████████▊ | 344/390 [03:47<00:13,  3.38it/s] 88%|████████▊ | 345/390 [03:47<00:13,  3.39it/s] 89%|████████▊ | 346/390 [03:48<00:12,  3.39it/s] 89%|████████▉ | 347/390 [03:48<00:13,  3.26it/s] 89%|████████▉ | 348/390 [03:48<00:12,  3.30it/s] 89%|████████▉ | 349/390 [03:49<00:12,  3.33it/s] 90%|████████▉ | 350/390 [03:49<00:11,  3.35it/s] 90%|█████████ | 351/390 [03:49<00:11,  3.36it/s] 90%|█████████ | 352/390 [03:50<00:11,  3.37it/s] 91%|█████████ | 353/390 [03:50<00:10,  3.38it/s] 91%|█████████ | 354/390 [03:50<00:10,  3.38it/s] 91%|█████████ | 355/390 [03:50<00:10,  3.38it/s] 91%|█████████▏| 356/390 [03:51<00:10,  3.39it/s] 92%|█████████▏| 357/390 [03:51<00:09,  3.39it/s] 92%|█████████▏| 358/390 [03:51<00:09,  3.40it/s] 92%|█████████▏| 359/390 [03:52<00:09,  3.39it/s] 92%|█████████▏| 360/390 [03:52<00:08,  3.39it/s] 93%|█████████▎| 361/390 [03:52<00:08,  3.39it/s] 93%|█████████▎| 362/390 [03:53<00:08,  3.39it/s] 93%|█████████▎| 363/390 [03:53<00:08,  3.26it/s] 93%|█████████▎| 364/390 [03:53<00:07,  3.30it/s] 94%|█████████▎| 365/390 [03:53<00:07,  3.33it/s] 94%|█████████▍| 366/390 [03:54<00:07,  3.35it/s] 94%|█████████▍| 367/390 [03:54<00:06,  3.36it/s] 94%|█████████▍| 368/390 [03:54<00:06,  3.37it/s] 95%|█████████▍| 369/390 [03:55<00:06,  3.38it/s] 95%|█████████▍| 370/390 [03:55<00:05,  3.38it/s] 95%|█████████▌| 371/390 [03:55<00:05,  3.38it/s] 95%|█████████▌| 372/390 [03:55<00:05,  3.39it/s] 96%|█████████▌| 373/390 [03:56<00:05,  3.39it/s] 96%|█████████▌| 374/390 [03:56<00:04,  3.28it/s] 96%|█████████▌| 375/390 [03:56<00:04,  3.31it/s] 96%|█████████▋| 376/390 [03:57<00:04,  3.34it/s] 97%|█████████▋| 377/390 [03:57<00:03,  3.35it/s] 97%|█████████▋| 378/390 [03:57<00:03,  3.37it/s] 97%|█████████▋| 379/390 [03:58<00:03,  3.37it/s] 97%|█████████▋| 380/390 [03:58<00:02,  3.38it/s] 98%|█████████▊| 381/390 [03:58<00:02,  3.38it/s] 98%|█████████▊| 382/390 [03:58<00:02,  3.38it/s] 98%|█████████▊| 383/390 [03:59<00:02,  3.39it/s] 98%|█████████▊| 384/390 [03:59<00:01,  3.38it/s] 99%|█████████▊| 385/390 [03:59<00:01,  3.30it/s] 99%|█████████▉| 386/390 [04:00<00:01,  3.32it/s] 99%|█████████▉| 387/390 [04:00<00:00,  3.34it/s] 99%|█████████▉| 388/390 [04:00<00:00,  3.36it/s]100%|█████████▉| 389/390 [04:01<00:00,  3.37it/s]100%|██████████| 390/390 [04:01<00:00,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 12:22:27,005 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:22:27,005 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 12:22:27,005 >>   Batch size = 8
{'eval_loss': 0.9793479442596436, 'eval_runtime': 19.4936, 'eval_samples_per_second': 353.141, 'eval_steps_per_second': 44.168, 'epoch': 3.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.39it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.41it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.99it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.14it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.45it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.80it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.62it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.42it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.51it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.58it/s][A
  7%|▋         | 57/861 [00:01<00:17, 44.71it/s][A
  7%|▋         | 62/861 [00:01<00:19, 41.77it/s][A
  8%|▊         | 67/861 [00:01<00:18, 42.73it/s][A
  8%|▊         | 72/861 [00:01<00:18, 43.32it/s][A
  9%|▉         | 77/861 [00:01<00:18, 43.46it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.71it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.90it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.05it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.25it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.16it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.37it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.49it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.44it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.42it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.31it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.28it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.33it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.34it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.41it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.49it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.57it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.53it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.49it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.31it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.34it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.37it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.32it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.47it/s][A
 23%|██▎       | 197/861 [00:04<00:16, 39.55it/s][A
 23%|██▎       | 202/861 [00:04<00:16, 41.06it/s][A
 24%|██▍       | 207/861 [00:04<00:15, 42.16it/s][A
 25%|██▍       | 212/861 [00:04<00:15, 42.97it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.53it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.79it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.92it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.12it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 43.87it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 43.71it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.08it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.32it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.49it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.61it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.49it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.37it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.31it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.06it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 44.11it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.25it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.36it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.57it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.68it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.65it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.49it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.22it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.14it/s][A
 39%|███▊      | 332/861 [00:07<00:13, 40.15it/s][A
 39%|███▉      | 337/861 [00:07<00:12, 41.48it/s][A
 40%|███▉      | 342/861 [00:07<00:12, 42.39it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.10it/s][A
 41%|████      | 352/861 [00:08<00:11, 43.71it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.08it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.25it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.19it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 43.84it/s][A
 44%|████▍     | 377/861 [00:08<00:11, 43.73it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 43.98it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.25it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.52it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 44.50it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.70it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.63it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.46it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.12it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.04it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.13it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.28it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.53it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.65it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.67it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.63it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.42it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.18it/s][A
 54%|█████▍    | 467/861 [00:10<00:09, 40.02it/s][A
 55%|█████▍    | 472/861 [00:10<00:09, 41.38it/s][A
 55%|█████▌    | 477/861 [00:10<00:09, 42.42it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 43.05it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.60it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.01it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.17it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.21it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 43.87it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 43.78it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.98it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.17it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.43it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.60it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.64it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.58it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.43it/s][A
 64%|██████▍   | 552/861 [00:12<00:07, 44.12it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.03it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.04it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.18it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.43it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.60it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.69it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.73it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.53it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.30it/s][A
 70%|██████▉   | 602/861 [00:13<00:06, 40.85it/s][A
 70%|███████   | 607/861 [00:13<00:06, 42.00it/s][A
 71%|███████   | 612/861 [00:13<00:05, 42.87it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 43.51it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.95it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.16it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.27it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.09it/s][A
 75%|███████▍  | 642/861 [00:14<00:05, 43.79it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 43.82it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.05it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.25it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.37it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.57it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.74it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.62it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.43it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.11it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.09it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.30it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.40it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 44.42it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.63it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.76it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.71it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.32it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.09it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 42.61it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.28it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 43.69it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.02it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.27it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.54it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.49it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.20it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 43.83it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.08it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.28it/s][A
 92%|█████████▏| 792/861 [00:17<00:01, 44.38it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.54it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.65it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.67it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.63it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.31it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.15it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.19it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.32it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 44.38it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.59it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.61it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.62it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.48it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 44.48it/s][A100%|██████████| 390/390 [04:20<00:00,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:22:46,755 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 12:22:46,974 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:22:50,225 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:22:50,376 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:22:50,449 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 12:22:58,418 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 12:22:58,452 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78 (score: 0.9702323079109192).
                                                 100%|██████████| 390/390 [04:42<00:00,  3.38it/s]100%|██████████| 390/390 [04:42<00:00,  1.38it/s]
[INFO|trainer.py:1894] 2023-08-28 12:23:08,512 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 12:23:08,657 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:23:11,794 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:23:11,936 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:23:12,023 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:23:12,491 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:12,491 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:12,491 >>   train_loss               =     0.6536
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:12,491 >>   train_runtime            = 0:04:42.78
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:12,491 >>   train_samples            =       5023
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:12,491 >>   train_samples_per_second =     88.813
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:12,491 >>   train_steps_per_second   =      1.379
{'eval_loss': 0.9826636910438538, 'eval_runtime': 19.5605, 'eval_samples_per_second': 351.934, 'eval_steps_per_second': 44.017, 'epoch': 4.99}
{'train_runtime': 282.7838, 'train_samples_per_second': 88.813, 'train_steps_per_second': 1.379, 'train_loss': 0.6535819811698718, 'epoch': 4.99}
08/28/2023 12:23:12 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 12:23:12,761 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:23:12,761 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 12:23:12,761 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 55.65it/s]  1%|▏         | 12/861 [00:00<00:17, 48.99it/s]  2%|▏         | 17/861 [00:00<00:17, 47.13it/s]  3%|▎         | 22/861 [00:00<00:18, 46.37it/s]  3%|▎         | 27/861 [00:00<00:18, 45.87it/s]  4%|▎         | 32/861 [00:00<00:18, 45.69it/s]  4%|▍         | 37/861 [00:00<00:18, 45.49it/s]  5%|▍         | 42/861 [00:00<00:18, 45.04it/s]  5%|▌         | 47/861 [00:01<00:18, 44.52it/s]  6%|▌         | 52/861 [00:01<00:18, 44.24it/s]  7%|▋         | 57/861 [00:01<00:18, 44.31it/s]  7%|▋         | 62/861 [00:01<00:17, 44.48it/s]  8%|▊         | 67/861 [00:01<00:17, 44.61it/s]  8%|▊         | 72/861 [00:01<00:17, 44.77it/s]  9%|▉         | 77/861 [00:01<00:17, 44.80it/s] 10%|▉         | 82/861 [00:01<00:17, 44.85it/s] 10%|█         | 87/861 [00:01<00:18, 40.85it/s] 11%|█         | 92/861 [00:02<00:18, 42.04it/s] 11%|█▏        | 97/861 [00:02<00:17, 42.63it/s] 12%|█▏        | 102/861 [00:02<00:17, 43.25it/s] 12%|█▏        | 107/861 [00:02<00:17, 43.73it/s] 13%|█▎        | 112/861 [00:02<00:17, 44.02it/s] 14%|█▎        | 117/861 [00:02<00:16, 44.33it/s] 14%|█▍        | 122/861 [00:02<00:16, 44.42it/s] 15%|█▍        | 127/861 [00:02<00:16, 44.14it/s] 15%|█▌        | 132/861 [00:02<00:16, 43.89it/s] 16%|█▌        | 137/861 [00:03<00:16, 44.03it/s] 16%|█▋        | 142/861 [00:03<00:16, 44.15it/s] 17%|█▋        | 147/861 [00:03<00:16, 44.35it/s] 18%|█▊        | 152/861 [00:03<00:15, 44.44it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.69it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.77it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.72it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.50it/s] 21%|██        | 177/861 [00:03<00:15, 44.17it/s] 21%|██        | 182/861 [00:04<00:15, 44.12it/s] 22%|██▏       | 187/861 [00:04<00:17, 39.53it/s] 22%|██▏       | 192/861 [00:04<00:16, 41.06it/s] 23%|██▎       | 197/861 [00:04<00:15, 42.09it/s] 23%|██▎       | 202/861 [00:04<00:15, 42.97it/s] 24%|██▍       | 207/861 [00:04<00:15, 43.54it/s] 25%|██▍       | 212/861 [00:04<00:14, 44.06it/s] 25%|██▌       | 217/861 [00:04<00:14, 44.32it/s] 26%|██▌       | 222/861 [00:05<00:14, 44.23it/s] 26%|██▋       | 227/861 [00:05<00:14, 43.92it/s] 27%|██▋       | 232/861 [00:05<00:14, 43.80it/s] 28%|██▊       | 237/861 [00:05<00:14, 43.96it/s] 28%|██▊       | 242/861 [00:05<00:14, 44.08it/s] 29%|██▊       | 247/861 [00:05<00:13, 44.33it/s] 29%|██▉       | 252/861 [00:05<00:13, 44.52it/s] 30%|██▉       | 257/861 [00:05<00:13, 44.72it/s] 30%|███       | 262/861 [00:05<00:13, 44.78it/s] 31%|███       | 267/861 [00:06<00:13, 44.58it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.23it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.01it/s] 33%|███▎      | 282/861 [00:06<00:13, 44.18it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.24it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.51it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.64it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.76it/s] 36%|███▌      | 307/861 [00:06<00:12, 44.75it/s] 36%|███▌      | 312/861 [00:07<00:12, 44.53it/s] 37%|███▋      | 317/861 [00:07<00:12, 44.20it/s] 37%|███▋      | 322/861 [00:07<00:12, 42.43it/s] 38%|███▊      | 327/861 [00:07<00:12, 43.06it/s] 39%|███▊      | 332/861 [00:07<00:12, 43.62it/s] 39%|███▉      | 337/861 [00:07<00:11, 43.94it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.26it/s] 40%|████      | 347/861 [00:07<00:11, 44.45it/s] 41%|████      | 352/861 [00:07<00:11, 44.41it/s] 41%|████▏     | 357/861 [00:08<00:11, 44.33it/s] 42%|████▏     | 362/861 [00:08<00:11, 44.02it/s] 43%|████▎     | 367/861 [00:08<00:11, 43.96it/s] 43%|████▎     | 372/861 [00:08<00:11, 44.19it/s] 44%|████▍     | 377/861 [00:08<00:10, 44.42it/s] 44%|████▍     | 382/861 [00:08<00:10, 44.48it/s] 45%|████▍     | 387/861 [00:08<00:10, 44.69it/s] 46%|████▌     | 392/861 [00:08<00:10, 44.68it/s] 46%|████▌     | 397/861 [00:08<00:10, 44.58it/s] 47%|████▋     | 402/861 [00:09<00:10, 44.33it/s] 47%|████▋     | 407/861 [00:09<00:10, 44.09it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.10it/s] 48%|████▊     | 417/861 [00:09<00:10, 44.25it/s] 49%|████▉     | 422/861 [00:09<00:09, 44.33it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.52it/s] 50%|█████     | 432/861 [00:09<00:09, 44.64it/s] 51%|█████     | 437/861 [00:09<00:09, 44.61it/s] 51%|█████▏    | 442/861 [00:09<00:09, 44.51it/s] 52%|█████▏    | 447/861 [00:10<00:09, 44.31it/s] 52%|█████▏    | 452/861 [00:10<00:09, 44.17it/s] 53%|█████▎    | 457/861 [00:10<00:09, 42.82it/s] 54%|█████▎    | 462/861 [00:10<00:11, 35.33it/s] 54%|█████▍    | 467/861 [00:10<00:10, 37.89it/s] 55%|█████▍    | 472/861 [00:10<00:09, 39.69it/s] 55%|█████▌    | 477/861 [00:10<00:09, 41.17it/s] 56%|█████▌    | 482/861 [00:10<00:08, 42.18it/s] 57%|█████▋    | 487/861 [00:11<00:08, 42.97it/s] 57%|█████▋    | 492/861 [00:11<00:08, 43.40it/s] 58%|█████▊    | 497/861 [00:11<00:08, 43.85it/s] 58%|█████▊    | 502/861 [00:11<00:08, 43.63it/s] 59%|█████▉    | 507/861 [00:11<00:08, 43.48it/s] 59%|█████▉    | 512/861 [00:11<00:07, 43.72it/s] 60%|██████    | 517/861 [00:11<00:07, 43.96it/s] 61%|██████    | 522/861 [00:11<00:07, 44.03it/s] 61%|██████    | 527/861 [00:12<00:07, 44.28it/s] 62%|██████▏   | 532/861 [00:12<00:07, 44.44it/s] 62%|██████▏   | 537/861 [00:12<00:07, 44.59it/s] 63%|██████▎   | 542/861 [00:12<00:07, 44.66it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.35it/s] 64%|██████▍   | 552/861 [00:12<00:07, 44.14it/s] 65%|██████▍   | 557/861 [00:12<00:06, 44.17it/s] 65%|██████▌   | 562/861 [00:12<00:06, 44.26it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.27it/s] 66%|██████▋   | 572/861 [00:13<00:06, 44.45it/s] 67%|██████▋   | 577/861 [00:13<00:06, 44.44it/s] 68%|██████▊   | 582/861 [00:13<00:06, 44.55it/s] 68%|██████▊   | 587/861 [00:13<00:06, 41.54it/s] 69%|██████▉   | 592/861 [00:13<00:06, 42.51it/s] 69%|██████▉   | 597/861 [00:13<00:06, 43.07it/s] 70%|██████▉   | 602/861 [00:13<00:05, 43.42it/s] 70%|███████   | 607/861 [00:13<00:05, 43.68it/s] 71%|███████   | 612/861 [00:13<00:05, 43.85it/s] 72%|███████▏  | 617/861 [00:14<00:05, 44.14it/s] 72%|███████▏  | 622/861 [00:14<00:05, 44.23it/s] 73%|███████▎  | 627/861 [00:14<00:05, 44.09it/s] 73%|███████▎  | 632/861 [00:14<00:05, 44.17it/s] 74%|███████▍  | 637/861 [00:14<00:05, 44.35it/s] 75%|███████▍  | 642/861 [00:14<00:04, 44.42it/s] 75%|███████▌  | 647/861 [00:14<00:04, 44.34it/s] 76%|███████▌  | 652/861 [00:14<00:04, 44.31it/s] 76%|███████▋  | 657/861 [00:14<00:04, 44.29it/s] 77%|███████▋  | 662/861 [00:15<00:04, 44.44it/s] 77%|███████▋  | 667/861 [00:15<00:04, 44.38it/s] 78%|███████▊  | 672/861 [00:15<00:04, 44.26it/s] 79%|███████▊  | 677/861 [00:15<00:04, 44.33it/s] 79%|███████▉  | 682/861 [00:15<00:04, 44.38it/s] 80%|███████▉  | 687/861 [00:15<00:03, 44.46it/s] 80%|████████  | 692/861 [00:15<00:03, 44.38it/s] 81%|████████  | 697/861 [00:15<00:03, 44.39it/s] 82%|████████▏ | 702/861 [00:15<00:03, 44.44it/s] 82%|████████▏ | 707/861 [00:16<00:03, 44.41it/s] 83%|████████▎ | 712/861 [00:16<00:03, 44.36it/s] 83%|████████▎ | 717/861 [00:16<00:03, 44.26it/s] 84%|████████▍ | 722/861 [00:16<00:03, 40.25it/s] 84%|████████▍ | 727/861 [00:16<00:03, 41.65it/s] 85%|████████▌ | 732/861 [00:16<00:03, 42.58it/s] 86%|████████▌ | 737/861 [00:16<00:02, 43.30it/s] 86%|████████▌ | 742/861 [00:16<00:02, 43.75it/s] 87%|████████▋ | 747/861 [00:17<00:02, 44.01it/s] 87%|████████▋ | 752/861 [00:17<00:02, 44.08it/s] 88%|████████▊ | 757/861 [00:17<00:02, 44.02it/s] 89%|████████▊ | 762/861 [00:17<00:02, 43.67it/s] 89%|████████▉ | 767/861 [00:17<00:02, 43.71it/s] 90%|████████▉ | 772/861 [00:17<00:02, 44.04it/s] 90%|█████████ | 777/861 [00:17<00:01, 44.26it/s] 91%|█████████ | 782/861 [00:17<00:01, 44.35it/s] 91%|█████████▏| 787/861 [00:17<00:01, 44.57it/s] 92%|█████████▏| 792/861 [00:18<00:01, 44.62it/s] 93%|█████████▎| 797/861 [00:18<00:01, 44.53it/s] 93%|█████████▎| 802/861 [00:18<00:01, 44.30it/s] 94%|█████████▎| 807/861 [00:18<00:01, 44.08it/s] 94%|█████████▍| 812/861 [00:18<00:01, 44.00it/s] 95%|█████████▍| 817/861 [00:18<00:00, 44.13it/s] 95%|█████████▌| 822/861 [00:18<00:00, 44.27it/s] 96%|█████████▌| 827/861 [00:18<00:00, 44.47it/s] 97%|█████████▋| 832/861 [00:18<00:00, 44.66it/s] 97%|█████████▋| 837/861 [00:19<00:00, 44.65it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.54it/s] 98%|█████████▊| 847/861 [00:19<00:00, 44.28it/s] 99%|█████████▉| 852/861 [00:19<00:00, 44.10it/s]100%|█████████▉| 857/861 [00:19<00:00, 41.14it/s]100%|██████████| 861/861 [00:19<00:00, 43.86it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:23:32,410 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:32,410 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:32,410 >>   eval_loss               =     0.9702
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:32,410 >>   eval_runtime            = 0:00:19.64
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:32,410 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:32,411 >>   eval_samples_per_second =    350.351
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:32,411 >>   eval_steps_per_second   =     43.819
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:23:32,411 >>   perplexity              =     2.6386
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:23,167 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:23,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:23,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:23,212 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:23,212 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:24:24,303 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:24:24,304 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:24:24,955 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:25:07,700 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:25:07,736 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:25:09,282 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:25:09,285 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:25:09,285 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:25:09,285 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:25:09,285 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:25:09,924 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:25:09,925 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:25:10,384 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:25:13,485 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:25:13,507 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.62it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.62it/s]Extractor Predicting: 17it [00:11,  1.59it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:13,  1.57it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.58it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.49it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:26,  1.55it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:28,  1.57it/s]Extractor Predicting: 45it [00:29,  1.59it/s]Extractor Predicting: 46it [00:29,  1.60it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:31,  1.56it/s]Extractor Predicting: 50it [00:32,  1.56it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.52it/s]Extractor Predicting: 55it [00:35,  1.51it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:38,  1.37it/s]Extractor Predicting: 60it [00:39,  1.40it/s]Extractor Predicting: 61it [00:39,  1.46it/s]Extractor Predicting: 62it [00:40,  1.47it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:41,  1.48it/s]Extractor Predicting: 65it [00:42,  1.50it/s]Extractor Predicting: 66it [00:43,  1.52it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:44,  1.56it/s]Extractor Predicting: 69it [00:45,  1.52it/s]Extractor Predicting: 70it [00:45,  1.50it/s]Extractor Predicting: 71it [00:46,  1.50it/s]Extractor Predicting: 72it [00:47,  1.50it/s]Extractor Predicting: 73it [00:47,  1.51it/s]Extractor Predicting: 74it [00:48,  1.52it/s]Extractor Predicting: 75it [00:49,  1.55it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:50,  1.51it/s]Extractor Predicting: 78it [00:51,  1.51it/s]Extractor Predicting: 79it [00:51,  1.54it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:52,  1.60it/s]Extractor Predicting: 82it [00:53,  1.67it/s]Extractor Predicting: 83it [00:54,  1.62it/s]Extractor Predicting: 84it [00:54,  1.61it/s]Extractor Predicting: 85it [00:55,  1.63it/s]Extractor Predicting: 86it [00:55,  1.65it/s]Extractor Predicting: 87it [00:56,  1.63it/s]Extractor Predicting: 88it [00:57,  1.65it/s]Extractor Predicting: 89it [00:57,  1.61it/s]Extractor Predicting: 90it [00:58,  1.65it/s]Extractor Predicting: 91it [00:58,  1.66it/s]Extractor Predicting: 92it [00:59,  1.69it/s]Extractor Predicting: 93it [01:00,  1.66it/s]Extractor Predicting: 94it [01:00,  1.65it/s]Extractor Predicting: 95it [01:01,  1.70it/s]Extractor Predicting: 96it [01:01,  1.65it/s]Extractor Predicting: 97it [01:02,  1.70it/s]Extractor Predicting: 98it [01:03,  1.69it/s]Extractor Predicting: 99it [01:03,  1.70it/s]Extractor Predicting: 100it [01:04,  1.67it/s]Extractor Predicting: 101it [01:04,  1.65it/s]Extractor Predicting: 102it [01:05,  1.64it/s]Extractor Predicting: 103it [01:06,  1.66it/s]Extractor Predicting: 104it [01:06,  1.66it/s]Extractor Predicting: 105it [01:07,  1.62it/s]Extractor Predicting: 106it [01:07,  1.65it/s]Extractor Predicting: 107it [01:08,  1.63it/s]Extractor Predicting: 108it [01:09,  1.69it/s]Extractor Predicting: 109it [01:09,  1.69it/s]Extractor Predicting: 110it [01:10,  1.71it/s]Extractor Predicting: 111it [01:10,  1.72it/s]Extractor Predicting: 112it [01:11,  1.67it/s]Extractor Predicting: 113it [01:12,  1.64it/s]Extractor Predicting: 114it [01:12,  1.62it/s]Extractor Predicting: 115it [01:13,  1.67it/s]Extractor Predicting: 116it [01:13,  1.65it/s]Extractor Predicting: 117it [01:14,  1.64it/s]Extractor Predicting: 118it [01:15,  1.62it/s]Extractor Predicting: 119it [01:15,  1.66it/s]Extractor Predicting: 120it [01:16,  1.69it/s]Extractor Predicting: 121it [01:16,  1.68it/s]Extractor Predicting: 122it [01:17,  1.63it/s]Extractor Predicting: 123it [01:18,  1.65it/s]Extractor Predicting: 124it [01:18,  1.63it/s]Extractor Predicting: 125it [01:19,  1.67it/s]Extractor Predicting: 126it [01:19,  1.69it/s]Extractor Predicting: 127it [01:20,  1.71it/s]Extractor Predicting: 128it [01:21,  1.64it/s]Extractor Predicting: 129it [01:21,  1.66it/s]Extractor Predicting: 130it [01:22,  1.67it/s]Extractor Predicting: 131it [01:22,  1.69it/s]Extractor Predicting: 132it [01:23,  1.72it/s]Extractor Predicting: 133it [01:24,  1.71it/s]Extractor Predicting: 134it [01:24,  1.70it/s]Extractor Predicting: 135it [01:25,  1.68it/s]Extractor Predicting: 136it [01:25,  1.69it/s]Extractor Predicting: 137it [01:26,  1.67it/s]Extractor Predicting: 138it [01:27,  1.68it/s]Extractor Predicting: 139it [01:27,  1.64it/s]Extractor Predicting: 140it [01:28,  1.61it/s]Extractor Predicting: 141it [01:28,  1.64it/s]Extractor Predicting: 142it [01:29,  1.64it/s]Extractor Predicting: 143it [01:30,  1.68it/s]Extractor Predicting: 144it [01:30,  1.64it/s]Extractor Predicting: 145it [01:31,  1.63it/s]Extractor Predicting: 146it [01:32,  1.65it/s]Extractor Predicting: 147it [01:32,  1.66it/s]Extractor Predicting: 148it [01:33,  1.66it/s]Extractor Predicting: 149it [01:33,  1.71it/s]Extractor Predicting: 150it [01:34,  1.63it/s]Extractor Predicting: 151it [01:35,  1.56it/s]Extractor Predicting: 152it [01:35,  1.53it/s]Extractor Predicting: 153it [01:36,  1.50it/s]Extractor Predicting: 154it [01:37,  1.43it/s]Extractor Predicting: 155it [01:37,  1.46it/s]Extractor Predicting: 156it [01:38,  1.41it/s]Extractor Predicting: 157it [01:39,  1.45it/s]Extractor Predicting: 158it [01:40,  1.44it/s]Extractor Predicting: 159it [01:41,  1.30it/s]Extractor Predicting: 160it [01:41,  1.33it/s]Extractor Predicting: 161it [01:42,  1.38it/s]Extractor Predicting: 162it [01:43,  1.34it/s]Extractor Predicting: 163it [01:43,  1.36it/s]Extractor Predicting: 164it [01:44,  1.37it/s]Extractor Predicting: 165it [01:45,  1.35it/s]Extractor Predicting: 166it [01:46,  1.36it/s]Extractor Predicting: 167it [01:46,  1.36it/s]Extractor Predicting: 168it [01:47,  1.38it/s]Extractor Predicting: 169it [01:48,  1.37it/s]Extractor Predicting: 170it [01:48,  1.37it/s]Extractor Predicting: 171it [01:49,  1.39it/s]Extractor Predicting: 172it [01:50,  1.35it/s]Extractor Predicting: 173it [01:51,  1.38it/s]Extractor Predicting: 174it [01:51,  1.42it/s]Extractor Predicting: 175it [01:52,  1.41it/s]Extractor Predicting: 176it [01:53,  1.40it/s]Extractor Predicting: 177it [01:53,  1.41it/s]Extractor Predicting: 178it [01:54,  1.40it/s]Extractor Predicting: 179it [01:55,  1.40it/s]Extractor Predicting: 180it [01:56,  1.39it/s]Extractor Predicting: 181it [01:56,  1.39it/s]Extractor Predicting: 182it [01:57,  1.38it/s]Extractor Predicting: 183it [01:58,  1.40it/s]Extractor Predicting: 184it [01:58,  1.43it/s]Extractor Predicting: 185it [01:59,  1.45it/s]Extractor Predicting: 186it [02:00,  1.45it/s]Extractor Predicting: 187it [02:01,  1.42it/s]Extractor Predicting: 188it [02:01,  1.46it/s]Extractor Predicting: 189it [02:02,  1.41it/s]Extractor Predicting: 190it [02:03,  1.40it/s]Extractor Predicting: 191it [02:03,  1.37it/s]Extractor Predicting: 192it [02:04,  1.33it/s]Extractor Predicting: 193it [02:05,  1.37it/s]Extractor Predicting: 194it [02:06,  1.40it/s]Extractor Predicting: 195it [02:06,  1.41it/s]Extractor Predicting: 196it [02:07,  1.44it/s]Extractor Predicting: 197it [02:08,  1.45it/s]Extractor Predicting: 198it [02:08,  1.46it/s]Extractor Predicting: 199it [02:09,  1.48it/s]Extractor Predicting: 200it [02:10,  1.49it/s]Extractor Predicting: 201it [02:10,  1.44it/s]Extractor Predicting: 202it [02:11,  1.45it/s]Extractor Predicting: 203it [02:12,  1.48it/s]Extractor Predicting: 204it [02:12,  1.48it/s]Extractor Predicting: 205it [02:13,  1.51it/s]Extractor Predicting: 206it [02:14,  1.48it/s]Extractor Predicting: 207it [02:14,  1.49it/s]Extractor Predicting: 208it [02:15,  1.50it/s]Extractor Predicting: 209it [02:16,  1.54it/s]Extractor Predicting: 210it [02:16,  1.50it/s]Extractor Predicting: 211it [02:17,  1.45it/s]Extractor Predicting: 212it [02:18,  1.47it/s]Extractor Predicting: 213it [02:18,  1.49it/s]Extractor Predicting: 214it [02:19,  1.51it/s]Extractor Predicting: 215it [02:20,  1.51it/s]Extractor Predicting: 216it [02:20,  1.52it/s]Extractor Predicting: 217it [02:21,  1.50it/s]Extractor Predicting: 218it [02:22,  1.54it/s]Extractor Predicting: 219it [02:22,  1.50it/s]Extractor Predicting: 220it [02:23,  1.45it/s]Extractor Predicting: 221it [02:24,  1.49it/s]Extractor Predicting: 222it [02:24,  1.50it/s]Extractor Predicting: 223it [02:25,  1.53it/s]Extractor Predicting: 224it [02:26,  1.52it/s]Extractor Predicting: 225it [02:26,  1.49it/s]Extractor Predicting: 226it [02:27,  1.49it/s]Extractor Predicting: 227it [02:28,  1.50it/s]Extractor Predicting: 228it [02:28,  1.50it/s]Extractor Predicting: 229it [02:29,  1.51it/s]Extractor Predicting: 230it [02:30,  1.50it/s]Extractor Predicting: 231it [02:30,  1.54it/s]Extractor Predicting: 232it [02:31,  1.55it/s]Extractor Predicting: 233it [02:32,  1.53it/s]Extractor Predicting: 234it [02:32,  1.52it/s]Extractor Predicting: 235it [02:33,  1.49it/s]Extractor Predicting: 236it [02:34,  1.50it/s]Extractor Predicting: 237it [02:34,  1.45it/s]Extractor Predicting: 237it [02:34,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:02,125 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:02,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:02,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:02,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:02,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:28:02,923 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:28:02,924 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:28:03,543 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:28:04,686 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:28:04,687 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:07,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:07,732 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:07,732 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:07,733 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:28:07,733 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:28:08,530 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:28:08,531 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:28:09,152 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:28:09,396 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:28:09,396 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4558139534883721,
  "recall": 0.04270772806507844,
  "score": 0.07809802098552264,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.74it/s]Extractor Predicting: 8it [00:04,  1.75it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:08,  1.66it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:11,  1.68it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:13,  1.68it/s]Extractor Predicting: 24it [00:14,  1.73it/s]Extractor Predicting: 25it [00:14,  1.73it/s]Extractor Predicting: 26it [00:15,  1.74it/s]Extractor Predicting: 27it [00:16,  1.73it/s]Extractor Predicting: 28it [00:16,  1.71it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:17,  1.68it/s]Extractor Predicting: 31it [00:18,  1.70it/s]Extractor Predicting: 32it [00:18,  1.72it/s]Extractor Predicting: 33it [00:19,  1.68it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:20,  1.66it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:21,  1.71it/s]Extractor Predicting: 38it [00:22,  1.75it/s]Extractor Predicting: 39it [00:23,  1.70it/s]Extractor Predicting: 40it [00:23,  1.66it/s]Extractor Predicting: 41it [00:24,  1.60it/s]Extractor Predicting: 42it [00:25,  1.55it/s]Extractor Predicting: 43it [00:25,  1.58it/s]Extractor Predicting: 44it [00:26,  1.58it/s]Extractor Predicting: 45it [00:27,  1.58it/s]Extractor Predicting: 46it [00:27,  1.54it/s]Extractor Predicting: 47it [00:28,  1.51it/s]Extractor Predicting: 48it [00:29,  1.55it/s]Extractor Predicting: 49it [00:29,  1.55it/s]Extractor Predicting: 50it [00:30,  1.56it/s]Extractor Predicting: 51it [00:30,  1.57it/s]Extractor Predicting: 52it [00:31,  1.52it/s]Extractor Predicting: 53it [00:32,  1.55it/s]Extractor Predicting: 54it [00:32,  1.55it/s]Extractor Predicting: 55it [00:33,  1.57it/s]Extractor Predicting: 56it [00:34,  1.59it/s]Extractor Predicting: 57it [00:34,  1.51it/s]Extractor Predicting: 58it [00:35,  1.55it/s]Extractor Predicting: 59it [00:36,  1.55it/s]Extractor Predicting: 60it [00:36,  1.57it/s]Extractor Predicting: 61it [00:37,  1.54it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:38,  1.58it/s]Extractor Predicting: 64it [00:39,  1.62it/s]Extractor Predicting: 65it [00:39,  1.62it/s]Extractor Predicting: 66it [00:40,  1.62it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:41,  1.61it/s]Extractor Predicting: 69it [00:42,  1.61it/s]Extractor Predicting: 70it [00:42,  1.60it/s]Extractor Predicting: 71it [00:43,  1.61it/s]Extractor Predicting: 72it [00:44,  1.59it/s]Extractor Predicting: 73it [00:44,  1.57it/s]Extractor Predicting: 74it [00:45,  1.62it/s]Extractor Predicting: 75it [00:46,  1.61it/s]Extractor Predicting: 76it [00:46,  1.59it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:47,  1.59it/s]Extractor Predicting: 79it [00:48,  1.61it/s]Extractor Predicting: 80it [00:49,  1.63it/s]Extractor Predicting: 81it [00:49,  1.61it/s]Extractor Predicting: 82it [00:50,  1.57it/s]Extractor Predicting: 83it [00:51,  1.61it/s]Extractor Predicting: 84it [00:51,  1.60it/s]Extractor Predicting: 85it [00:52,  1.56it/s]Extractor Predicting: 86it [00:52,  1.59it/s]Extractor Predicting: 87it [00:53,  1.63it/s]Extractor Predicting: 88it [00:54,  1.59it/s]Extractor Predicting: 89it [00:54,  1.59it/s]Extractor Predicting: 90it [00:55,  1.58it/s]Extractor Predicting: 91it [00:56,  1.58it/s]Extractor Predicting: 92it [00:56,  1.61it/s]Extractor Predicting: 93it [00:57,  1.65it/s]Extractor Predicting: 94it [00:57,  1.59it/s]Extractor Predicting: 95it [00:58,  1.57it/s]Extractor Predicting: 96it [00:59,  1.61it/s]Extractor Predicting: 97it [00:59,  1.61it/s]Extractor Predicting: 98it [01:00,  1.62it/s]Extractor Predicting: 99it [01:01,  1.63it/s]Extractor Predicting: 100it [01:02,  1.38it/s]Extractor Predicting: 101it [01:02,  1.48it/s]Extractor Predicting: 102it [01:03,  1.50it/s]Extractor Predicting: 103it [01:03,  1.54it/s]Extractor Predicting: 104it [01:04,  1.59it/s]Extractor Predicting: 105it [01:05,  1.58it/s]Extractor Predicting: 106it [01:05,  1.63it/s]Extractor Predicting: 107it [01:06,  1.65it/s]Extractor Predicting: 108it [01:06,  1.68it/s]Extractor Predicting: 109it [01:07,  1.63it/s]Extractor Predicting: 110it [01:08,  1.61it/s]Extractor Predicting: 111it [01:08,  1.59it/s]Extractor Predicting: 112it [01:09,  1.60it/s]Extractor Predicting: 113it [01:09,  1.65it/s]Extractor Predicting: 114it [01:10,  1.61it/s]Extractor Predicting: 115it [01:11,  1.58it/s]Extractor Predicting: 116it [01:11,  1.62it/s]Extractor Predicting: 117it [01:12,  1.62it/s]Extractor Predicting: 118it [01:13,  1.65it/s]Extractor Predicting: 119it [01:13,  1.64it/s]Extractor Predicting: 120it [01:14,  1.61it/s]Extractor Predicting: 121it [01:14,  1.55it/s]Extractor Predicting: 122it [01:15,  1.54it/s]Extractor Predicting: 123it [01:16,  1.56it/s]Extractor Predicting: 124it [01:16,  1.59it/s]Extractor Predicting: 125it [01:17,  1.60it/s]Extractor Predicting: 126it [01:18,  1.61it/s]Extractor Predicting: 127it [01:18,  1.59it/s]Extractor Predicting: 128it [01:19,  1.63it/s]Extractor Predicting: 129it [01:19,  1.63it/s]Extractor Predicting: 130it [01:20,  1.63it/s]Extractor Predicting: 131it [01:21,  1.63it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.59it/s]Extractor Predicting: 134it [01:23,  1.59it/s]Extractor Predicting: 135it [01:23,  1.60it/s]Extractor Predicting: 136it [01:24,  1.62it/s]Extractor Predicting: 137it [01:24,  1.60it/s]Extractor Predicting: 138it [01:25,  1.58it/s]Extractor Predicting: 139it [01:26,  1.52it/s]Extractor Predicting: 140it [01:26,  1.59it/s]Extractor Predicting: 141it [01:27,  1.59it/s]Extractor Predicting: 142it [01:28,  1.57it/s]Extractor Predicting: 143it [01:28,  1.56it/s]Extractor Predicting: 144it [01:29,  1.54it/s]Extractor Predicting: 145it [01:30,  1.57it/s]Extractor Predicting: 146it [01:30,  1.57it/s]Extractor Predicting: 147it [01:31,  1.58it/s]Extractor Predicting: 148it [01:32,  1.52it/s]Extractor Predicting: 149it [01:32,  1.48it/s]Extractor Predicting: 150it [01:33,  1.52it/s]Extractor Predicting: 151it [01:34,  1.55it/s]Extractor Predicting: 152it [01:34,  1.55it/s]Extractor Predicting: 153it [01:35,  1.56it/s]Extractor Predicting: 154it [01:36,  1.50it/s]Extractor Predicting: 155it [01:36,  1.53it/s]Extractor Predicting: 156it [01:37,  1.55it/s]Extractor Predicting: 157it [01:37,  1.52it/s]Extractor Predicting: 158it [01:38,  1.51it/s]Extractor Predicting: 159it [01:39,  1.50it/s]Extractor Predicting: 160it [01:39,  1.54it/s]Extractor Predicting: 161it [01:40,  1.55it/s]Extractor Predicting: 162it [01:41,  1.53it/s]Extractor Predicting: 163it [01:41,  1.50it/s]Extractor Predicting: 164it [01:42,  1.49it/s]Extractor Predicting: 165it [01:43,  1.52it/s]Extractor Predicting: 166it [01:43,  1.51it/s]Extractor Predicting: 167it [01:44,  1.53it/s]Extractor Predicting: 168it [01:45,  1.65it/s]Extractor Predicting: 168it [01:45,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:04,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:04,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:04,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:04,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:04,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:30:05,551 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:30:05,553 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:30:06,185 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:30:07,313 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:30:07,313 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:10,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:10,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:10,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:10,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:10,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:30:11,395 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:30:11,396 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:30:12,001 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:30:12,197 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:30:12,197 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.39247830279652846,
  "recall": 0.10114314115308151,
  "score": 0.16083777909504052,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.75it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.54it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.47it/s]Extractor Predicting: 18it [00:11,  1.47it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 19it [00:12,  1.58it/s]
[INFO|configuration_utils.py:515] 2023-08-28 12:30:25,996 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:30:25,997 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:30:26,051 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:30:26,051 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 12:30:26,069 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:30:39,806 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 12:30:39,843 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 12:30:40,067 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:30:40,068 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:30:40,210 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:30:40,284 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:30:40,284 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:30:40,284 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:30:40,284 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:30:40,284 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:30:40,284 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6585365853658537,
  "recall": 0.026061776061776062,
  "score": 0.05013927576601672,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 12:30:40,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:41,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:41,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:42,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:43,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:43,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:44,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:44,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:45,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:46,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:46,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:47,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:47,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:48,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:49,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:49,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:50,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:51,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:51,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:52,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:52,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:12<01:55, 12.86s/it][WARNING|generation_utils.py:914] 2023-08-28 12:30:53,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:54,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:54,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:55,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:56,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:57,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:57,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:58,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:58,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:59,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:00,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:00,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:01,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:02,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:02,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:03,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:04,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:04,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:05,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:06,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:06,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:07,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:07,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:08,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:09,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:09,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:10,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:11,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:30<02:07, 15.95s/it][WARNING|generation_utils.py:914] 2023-08-28 12:31:11,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:12,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:12,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:13,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:14,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:14,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:15,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:15,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:16,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:17,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:17,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:18,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:18,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:19,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:20,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:20,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:21,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:21,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:22,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:23,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:23,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:24,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:44<01:43, 14.80s/it][WARNING|generation_utils.py:914] 2023-08-28 12:31:25,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:25,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:26,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:26,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:27,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:28,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:28,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:29,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:30,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:30,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:31,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:31,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:32,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:33,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:33,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:34,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:35,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:35,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:36,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:36,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:37,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:37,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:57<01:25, 14.20s/it][WARNING|generation_utils.py:914] 2023-08-28 12:31:38,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:39,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:39,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:40,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:41,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:41,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:42,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:42,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:43,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:43,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:44,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:45,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:45,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:46,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:47,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:47,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:48,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:48,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:49,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:50,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:50,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:10<01:08, 13.76s/it][WARNING|generation_utils.py:914] 2023-08-28 12:31:51,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:51,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:52,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:53,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:53,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:54,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:55,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:55,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:56,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:56,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:57,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:57,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:58,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:59,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:31:59,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:00,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:00,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:01,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:02,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:02,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:03,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:23<00:53, 13.29s/it][WARNING|generation_utils.py:914] 2023-08-28 12:32:03,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:04,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:04,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:05,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:06,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:06,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:07,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:08,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:08,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:09,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:10,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:10,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:11,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:12,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:12,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:13,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:13,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:14,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:15,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:15,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:16,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:16,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:36<00:40, 13.44s/it][WARNING|generation_utils.py:914] 2023-08-28 12:32:17,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:18,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:18,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:19,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:19,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:20,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:20,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:21,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:22,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:22,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:23,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:24,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:24,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:25,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:25,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:26,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:27,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:27,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:28,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:28,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:29,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:29,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:30,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:50<00:27, 13.54s/it][WARNING|generation_utils.py:914] 2023-08-28 12:32:31,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:31,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:32,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:32,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:33,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:34,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:34,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:35,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:36,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:36,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:37,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:37,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:38,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:39,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:39,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:40,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:40,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:41,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:42,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:42,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:02<00:13, 13.05s/it][WARNING|generation_utils.py:914] 2023-08-28 12:32:43,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:44,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:44,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:45,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:45,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:46,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:47,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:47,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:48,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:49,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:49,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:50,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:51,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:51,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:52,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:53,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:53,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:54,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:55,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:55,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:56,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:32:57,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:17<00:00, 13.51s/it]Generating: 100%|██████████| 10/10 [02:17<00:00, 13.71s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:05,168 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:05,205 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:05,205 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:05,205 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:05,205 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:33:05,828 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:33:05,829 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:33:06,429 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:33:07,527 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:33:07,527 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:10,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:10,517 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:10,517 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:10,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:33:10,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:33:11,197 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:33:11,198 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:33:11,788 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:33:11,964 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:33:11,964 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 82, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 191, 'raw': 288}
{'target': 600, 'success': 210, 'raw': 320}
{'target': 600, 'success': 229, 'raw': 352}
{'target': 600, 'success': 252, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 296, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 345, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 394, 'raw': 576}
{'target': 600, 'success': 414, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 461, 'raw': 672}
{'target': 600, 'success': 482, 'raw': 704}
{'target': 600, 'success': 500, 'raw': 736}
{'target': 600, 'success': 525, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 572, 'raw': 832}
{'target': 600, 'success': 597, 'raw': 864}
{'target': 600, 'success': 621, 'raw': 896}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.6930803571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8721590909090909, 'errors': {'', "('Perseus', 'opposite of', '', 'It was named when the Greek goddess of the region Perseus was at Jerusalem in the presence of her consort Aphrodite .')", 'not enough values to unpack (expected 2, got 1)', "('Senator Dianne Feinstein', 'opposite of', '', 'She was nominated to the United States Senate by her mother in November 2012 , while Senator Dianne Feinstein ( D-CA ) was still alive .')"}}
['Relation : shares border with . Context : Later in 2008 , the country became a part of a deal between Beijing and Shanghai at the end of 2010 , involving a 1,500 tonne deal , valued at $ 1 BnC . Head Entity : China , Tail Entity : Shanghai .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9196428571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : Later in life he studied at the Conservatory of Fine Arts at Loyola University in Rome and had a brief time at the Conservatory of Fine Arts in Paris , where he wrote many of his poems . Head Entity : Luca Saccone , Tail Entity : Conservatory of Fine Arts .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8707386363636364, 'errors': {'', "('Michael M. Skelton , Jr.', 'occupation', '', 'He received a Bachelor of Arts degree in English from Dartmouth College , where he studied drama , music and composition with Michael M. Skelton , Jr. ( 1869 1905 ) .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8464673913043478, 'errors': {'', "('William Jameson', 'residence', '', 'During the war , William G. Balsam , a Confederate colonel , joined William Jameson , a Republican , and he was named lieutenant colonel in August 1776 .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8664772727272727, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 10097
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10197, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.50it/s]Extractor Estimating: 3it [00:01,  1.57it/s]Extractor Estimating: 4it [00:02,  1.62it/s]Extractor Estimating: 5it [00:03,  1.63it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.65it/s]Extractor Estimating: 8it [00:04,  1.69it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:06,  1.55it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.63it/s]Extractor Estimating: 14it [00:08,  1.66it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:09,  1.56it/s]Extractor Estimating: 17it [00:10,  1.60it/s]Extractor Estimating: 18it [00:11,  1.55it/s]Extractor Estimating: 19it [00:11,  1.61it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:13,  1.60it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:14,  1.63it/s]Extractor Estimating: 25it [00:15,  1.65it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:16,  1.61it/s]Extractor Estimating: 28it [00:17,  1.58it/s]Extractor Estimating: 29it [00:18,  1.59it/s]Extractor Estimating: 30it [00:18,  1.59it/s]Extractor Estimating: 31it [00:19,  1.58it/s]Extractor Estimating: 32it [00:19,  1.55it/s]Extractor Estimating: 33it [00:20,  1.52it/s]Extractor Estimating: 34it [00:21,  1.58it/s]Extractor Estimating: 35it [00:21,  1.55it/s]Extractor Estimating: 36it [00:22,  1.56it/s]Extractor Estimating: 37it [00:23,  1.54it/s]Extractor Estimating: 38it [00:23,  1.57it/s]Extractor Estimating: 39it [00:24,  1.58it/s]Extractor Estimating: 40it [00:25,  1.60it/s]Extractor Estimating: 41it [00:25,  1.57it/s]Extractor Estimating: 42it [00:26,  1.57it/s]Extractor Estimating: 43it [00:27,  1.57it/s]Extractor Estimating: 44it [00:27,  1.54it/s]Extractor Estimating: 45it [00:28,  1.56it/s]Extractor Estimating: 46it [00:28,  1.55it/s]Extractor Estimating: 47it [00:29,  1.58it/s]Extractor Estimating: 48it [00:30,  1.59it/s]Extractor Estimating: 49it [00:30,  1.54it/s]Extractor Estimating: 50it [00:31,  1.53it/s]Extractor Estimating: 51it [00:32,  1.51it/s]Extractor Estimating: 52it [00:32,  1.54it/s]Extractor Estimating: 53it [00:33,  1.59it/s]Extractor Estimating: 54it [00:34,  1.63it/s]Extractor Estimating: 55it [00:34,  1.65it/s]Extractor Estimating: 56it [00:35,  1.57it/s]Extractor Estimating: 57it [00:35,  1.59it/s]Extractor Estimating: 58it [00:36,  1.63it/s]Extractor Estimating: 59it [00:37,  1.68it/s]Extractor Estimating: 60it [00:37,  1.66it/s]Extractor Estimating: 61it [00:38,  1.59it/s]Extractor Estimating: 62it [00:38,  1.63it/s]Extractor Estimating: 63it [00:39,  1.63it/s]Extractor Estimating: 64it [00:40,  1.63it/s]Extractor Estimating: 65it [00:40,  1.65it/s]Extractor Estimating: 66it [00:41,  1.61it/s]Extractor Estimating: 67it [00:41,  1.65it/s]Extractor Estimating: 68it [00:42,  1.69it/s]Extractor Estimating: 69it [00:43,  1.62it/s]Extractor Estimating: 70it [00:43,  1.63it/s]Extractor Estimating: 71it [00:44,  1.58it/s]Extractor Estimating: 72it [00:45,  1.60it/s]Extractor Estimating: 73it [00:45,  1.62it/s]Extractor Estimating: 74it [00:46,  1.64it/s]Extractor Estimating: 75it [00:46,  1.62it/s]Extractor Estimating: 76it [00:47,  1.63it/s]Extractor Estimating: 77it [00:48,  1.64it/s]Extractor Estimating: 78it [00:48,  1.67it/s]Extractor Estimating: 79it [00:49,  1.74it/s]Extractor Estimating: 80it [00:49,  1.61it/s]Extractor Estimating: 81it [00:50,  1.65it/s]Extractor Estimating: 82it [00:51,  1.69it/s]Extractor Estimating: 83it [00:51,  1.72it/s]Extractor Estimating: 84it [00:52,  1.77it/s]Extractor Estimating: 85it [00:52,  1.76it/s]Extractor Estimating: 86it [00:53,  1.75it/s]Extractor Estimating: 87it [00:53,  1.81it/s]Extractor Estimating: 88it [00:54,  1.74it/s]Extractor Estimating: 89it [00:55,  1.75it/s]Extractor Estimating: 90it [00:55,  1.73it/s]Extractor Estimating: 91it [00:56,  1.72it/s]Extractor Estimating: 92it [00:56,  1.77it/s]Extractor Estimating: 93it [00:57,  1.89it/s]Extractor Estimating: 94it [00:57,  1.97it/s]Extractor Estimating: 95it [00:58,  1.96it/s]Extractor Estimating: 96it [00:58,  1.93it/s]Extractor Estimating: 97it [00:59,  1.87it/s]Extractor Estimating: 98it [00:59,  1.90it/s]Extractor Estimating: 99it [01:00,  1.90it/s]Extractor Estimating: 100it [01:00,  1.88it/s]Extractor Estimating: 101it [01:01,  1.77it/s]Extractor Estimating: 102it [01:02,  1.74it/s]Extractor Estimating: 103it [01:02,  1.64it/s]Extractor Estimating: 104it [01:03,  1.64it/s]Extractor Estimating: 105it [01:03,  1.73it/s]Extractor Estimating: 106it [01:04,  1.77it/s]Extractor Estimating: 107it [01:05,  1.70it/s]Extractor Estimating: 108it [01:05,  1.73it/s]Extractor Estimating: 109it [01:06,  1.70it/s]Extractor Estimating: 110it [01:06,  1.71it/s]Extractor Estimating: 111it [01:07,  1.72it/s]Extractor Estimating: 112it [01:07,  1.72it/s]Extractor Estimating: 113it [01:08,  1.68it/s]Extractor Estimating: 114it [01:09,  1.69it/s]Extractor Estimating: 115it [01:09,  1.68it/s]Extractor Estimating: 116it [01:10,  1.63it/s]Extractor Estimating: 117it [01:10,  1.67it/s]Extractor Estimating: 118it [01:11,  1.66it/s]Extractor Estimating: 119it [01:12,  1.65it/s]Extractor Estimating: 120it [01:12,  1.63it/s]Extractor Estimating: 121it [01:13,  1.67it/s]Extractor Estimating: 122it [01:14,  1.64it/s]Extractor Estimating: 123it [01:14,  1.62it/s]Extractor Estimating: 124it [01:15,  1.67it/s]Extractor Estimating: 125it [01:15,  1.64it/s]Extractor Estimating: 126it [01:16,  1.68it/s]Extractor Estimating: 127it [01:16,  1.72it/s]Extractor Estimating: 128it [01:17,  1.69it/s]Extractor Estimating: 129it [01:18,  1.71it/s]Extractor Estimating: 130it [01:18,  1.69it/s]Extractor Estimating: 131it [01:19,  1.69it/s]Extractor Estimating: 132it [01:19,  1.71it/s]Extractor Estimating: 133it [01:20,  1.71it/s]Extractor Estimating: 134it [01:21,  1.76it/s]Extractor Estimating: 135it [01:21,  1.74it/s]Extractor Estimating: 136it [01:22,  1.70it/s]Extractor Estimating: 137it [01:22,  1.70it/s]Extractor Estimating: 138it [01:23,  1.71it/s]Extractor Estimating: 139it [01:24,  1.70it/s]Extractor Estimating: 140it [01:24,  1.68it/s]Extractor Estimating: 141it [01:25,  1.69it/s]Extractor Estimating: 142it [01:25,  1.71it/s]Extractor Estimating: 143it [01:26,  1.64it/s]Extractor Estimating: 144it [01:27,  1.55it/s]Extractor Estimating: 145it [01:27,  1.58it/s]Extractor Estimating: 146it [01:28,  1.62it/s]Extractor Estimating: 147it [01:28,  1.64it/s]Extractor Estimating: 148it [01:29,  1.69it/s]Extractor Estimating: 149it [01:30,  1.65it/s]Extractor Estimating: 150it [01:30,  1.68it/s]Extractor Estimating: 151it [01:31,  1.69it/s]Extractor Estimating: 152it [01:31,  1.67it/s]Extractor Estimating: 153it [01:32,  1.61it/s]Extractor Estimating: 154it [01:33,  1.54it/s]Extractor Estimating: 155it [01:33,  1.60it/s]Extractor Estimating: 156it [01:34,  1.66it/s]Extractor Estimating: 157it [01:35,  1.64it/s]Extractor Estimating: 158it [01:35,  1.64it/s]Extractor Estimating: 159it [01:36,  1.60it/s]Extractor Estimating: 160it [01:36,  1.63it/s]Extractor Estimating: 161it [01:37,  1.63it/s]Extractor Estimating: 162it [01:38,  1.64it/s]Extractor Estimating: 163it [01:38,  1.52it/s]Extractor Estimating: 164it [01:39,  1.51it/s]Extractor Estimating: 165it [01:40,  1.51it/s]Extractor Estimating: 166it [01:40,  1.51it/s]Extractor Estimating: 167it [01:41,  1.55it/s]Extractor Estimating: 168it [01:42,  1.60it/s]Extractor Estimating: 169it [01:42,  1.60it/s]Extractor Estimating: 170it [01:43,  1.62it/s]Extractor Estimating: 171it [01:43,  1.55it/s]Extractor Estimating: 172it [01:44,  1.59it/s]Extractor Estimating: 173it [01:45,  1.61it/s]Extractor Estimating: 174it [01:45,  1.60it/s]Extractor Estimating: 175it [01:46,  1.61it/s]Extractor Estimating: 176it [01:47,  1.64it/s]Extractor Estimating: 177it [01:47,  1.69it/s]Extractor Estimating: 178it [01:48,  1.68it/s]Extractor Estimating: 179it [01:48,  1.68it/s]Extractor Estimating: 180it [01:49,  1.62it/s]Extractor Estimating: 181it [01:50,  1.61it/s]Extractor Estimating: 182it [01:50,  1.65it/s]Extractor Estimating: 183it [01:51,  1.63it/s]Extractor Estimating: 184it [01:51,  1.69it/s]Extractor Estimating: 185it [01:52,  1.66it/s]Extractor Estimating: 186it [01:53,  1.63it/s]Extractor Estimating: 187it [01:53,  1.66it/s]Extractor Estimating: 188it [01:54,  1.69it/s]Extractor Estimating: 189it [01:54,  1.69it/s]Extractor Estimating: 190it [01:55,  1.66it/s]Extractor Estimating: 191it [01:56,  1.65it/s]Extractor Estimating: 192it [01:56,  1.63it/s]Extractor Estimating: 193it [01:57,  1.66it/s]Extractor Estimating: 194it [01:57,  1.64it/s]Extractor Estimating: 195it [01:58,  1.65it/s]Extractor Estimating: 196it [01:59,  1.63it/s]Extractor Estimating: 197it [01:59,  1.63it/s]Extractor Estimating: 198it [02:00,  1.65it/s]Extractor Estimating: 199it [02:00,  1.64it/s]Extractor Estimating: 200it [02:01,  1.67it/s]Extractor Estimating: 201it [02:02,  1.65it/s]Extractor Estimating: 202it [02:02,  1.64it/s]Extractor Estimating: 203it [02:03,  1.65it/s]Extractor Estimating: 204it [02:03,  1.62it/s]Extractor Estimating: 205it [02:04,  1.60it/s]Extractor Estimating: 206it [02:05,  1.58it/s]Extractor Estimating: 207it [02:05,  1.58it/s]Extractor Estimating: 208it [02:06,  1.60it/s]Extractor Estimating: 209it [02:07,  1.58it/s]Extractor Estimating: 210it [02:07,  1.60it/s]Extractor Estimating: 211it [02:08,  1.61it/s]Extractor Estimating: 212it [02:09,  1.62it/s]Extractor Estimating: 213it [02:09,  1.64it/s]Extractor Estimating: 214it [02:10,  1.60it/s]Extractor Estimating: 215it [02:10,  1.60it/s]Extractor Estimating: 216it [02:11,  1.59it/s]Extractor Estimating: 217it [02:12,  1.58it/s]Extractor Estimating: 218it [02:12,  1.61it/s]Extractor Estimating: 219it [02:13,  1.59it/s]Extractor Estimating: 220it [02:14,  1.61it/s]Extractor Estimating: 221it [02:14,  1.64it/s]Extractor Estimating: 222it [02:15,  1.64it/s]Extractor Estimating: 223it [02:15,  1.66it/s]Extractor Estimating: 224it [02:16,  1.64it/s]Extractor Estimating: 225it [02:17,  1.55it/s]Extractor Estimating: 226it [02:17,  1.59it/s]Extractor Estimating: 227it [02:18,  1.69it/s]Extractor Estimating: 228it [02:18,  1.69it/s]Extractor Estimating: 229it [02:19,  1.67it/s]Extractor Estimating: 230it [02:20,  1.66it/s]Extractor Estimating: 231it [02:20,  1.70it/s]Extractor Estimating: 232it [02:21,  1.70it/s]Extractor Estimating: 233it [02:21,  1.76it/s]Extractor Estimating: 234it [02:22,  1.72it/s]Extractor Estimating: 235it [02:22,  1.69it/s]Extractor Estimating: 236it [02:23,  1.66it/s]Extractor Estimating: 237it [02:24,  1.50it/s]Extractor Estimating: 238it [02:25,  1.54it/s]Extractor Estimating: 239it [02:25,  1.58it/s]Extractor Estimating: 240it [02:26,  1.57it/s]Extractor Estimating: 241it [02:26,  1.60it/s]Extractor Estimating: 242it [02:27,  1.67it/s]Extractor Estimating: 243it [02:28,  1.64it/s]Extractor Estimating: 244it [02:28,  1.60it/s]Extractor Estimating: 245it [02:29,  1.64it/s]Extractor Estimating: 246it [02:29,  1.66it/s]Extractor Estimating: 247it [02:30,  1.68it/s]Extractor Estimating: 248it [02:31,  1.66it/s]Extractor Estimating: 249it [02:31,  1.67it/s]Extractor Estimating: 250it [02:32,  1.74it/s]Extractor Estimating: 250it [02:32,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:05,266 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:05,288 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:05,289 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:05,289 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:05,289 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:36:06,096 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:36:06,097 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:36:06,756 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:36:07,907 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:36:07,907 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:10,996 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:10,998 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:10,998 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:10,998 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:10,998 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:36:11,788 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:36:11,789 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:36:12,402 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:36:12,638 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:36:12,638 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 14:11:29,414 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 14:11:29,477 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4985 mean pseudo reward: 0.9259851177061943
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 23963
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24063, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24063, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.966, loss:733.4159
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.966, loss:686.0024
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 0.972, loss:653.7816
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 0.971, loss:655.0301
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.975, loss:622.4670
>> valid entity prec:0.5032, rec:0.3209, f1:0.3919
>> valid relation prec:0.2650, rec:0.0180, f1:0.0338
>> valid relation with NER prec:0.2650, rec:0.0180, f1:0.0338
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 3.057, loss:638.8769
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.970, loss:603.4411
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.969, loss:632.7025
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.969, loss:630.0041
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.974, loss:643.3461
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5034, rec:0.4123, f1:0.4533
>> valid relation prec:0.3524, rec:0.0295, f1:0.0545
>> valid relation with NER prec:0.3524, rec:0.0295, f1:0.0545
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 60, avg_time 3.092, loss:594.1426
g_step 1200, step 160, avg_time 0.970, loss:613.4661
g_step 1300, step 52, avg_time 0.975, loss:598.7424
g_step 1400, step 152, avg_time 0.980, loss:605.1911
g_step 1500, step 44, avg_time 0.966, loss:580.8120
>> valid entity prec:0.4634, rec:0.2866, f1:0.3542
>> valid relation prec:0.1439, rec:0.0084, f1:0.0159
>> valid relation with NER prec:0.1439, rec:0.0084, f1:0.0159
g_step 1600, step 144, avg_time 3.057, loss:554.2457
g_step 1700, step 36, avg_time 0.966, loss:584.4480
g_step 1800, step 136, avg_time 0.967, loss:539.3365
g_step 1900, step 28, avg_time 0.974, loss:529.8654
g_step 2000, step 128, avg_time 0.974, loss:527.0459
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4708, rec:0.2986, f1:0.3654
>> valid relation prec:0.1366, rec:0.0086, f1:0.0161
>> valid relation with NER prec:0.1366, rec:0.0086, f1:0.0161
g_step 2100, step 20, avg_time 3.057, loss:530.4090
g_step 2200, step 120, avg_time 0.963, loss:507.5919
g_step 2300, step 12, avg_time 0.975, loss:504.4088
g_step 2400, step 112, avg_time 0.967, loss:481.0755
g_step 2500, step 4, avg_time 0.964, loss:493.0671
>> valid entity prec:0.4264, rec:0.4530, f1:0.4393
>> valid relation prec:0.1900, rec:0.0160, f1:0.0295
>> valid relation with NER prec:0.1900, rec:0.0160, f1:0.0295
g_step 2600, step 104, avg_time 3.072, loss:454.8604
g_step 2700, step 204, avg_time 0.973, loss:458.1137
g_step 2800, step 96, avg_time 0.958, loss:434.5390
g_step 2900, step 196, avg_time 0.974, loss:487.0895
g_step 3000, step 88, avg_time 0.979, loss:421.4001
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4866, rec:0.2954, f1:0.3676
>> valid relation prec:0.1515, rec:0.0172, f1:0.0308
>> valid relation with NER prec:0.1515, rec:0.0172, f1:0.0308
g_step 3100, step 188, avg_time 3.052, loss:448.0154
g_step 3200, step 80, avg_time 0.968, loss:411.7001
g_step 3300, step 180, avg_time 0.972, loss:420.0838
g_step 3400, step 72, avg_time 0.969, loss:413.6229
g_step 3500, step 172, avg_time 0.967, loss:398.2693
>> valid entity prec:0.4351, rec:0.3785, f1:0.4048
>> valid relation prec:0.1548, rec:0.0193, f1:0.0344
>> valid relation with NER prec:0.1548, rec:0.0193, f1:0.0344
g_step 3600, step 64, avg_time 3.056, loss:393.6293
g_step 3700, step 164, avg_time 0.979, loss:396.1755
g_step 3800, step 56, avg_time 0.964, loss:379.9445
g_step 3900, step 156, avg_time 0.967, loss:386.9905
g_step 4000, step 48, avg_time 0.964, loss:370.4652
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4577, rec:0.4042, f1:0.4293
>> valid relation prec:0.1732, rec:0.0237, f1:0.0417
>> valid relation with NER prec:0.1732, rec:0.0237, f1:0.0417
g_step 4100, step 148, avg_time 3.059, loss:362.2991
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:11:29 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:11:29 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-11-29_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:11:30 - WARNING - datasets.builder -   Using custom data configuration default-a8613fdbdc1afd55
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a8613fdbdc1afd55/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:11:32,801 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:11:32,838 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:11:32,838 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:11:32,839 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:11:32,936 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:33,019 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:33,019 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:33,019 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:33,019 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:33,019 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:33,019 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:11:33,386 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:11:36,679 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:11:36,754 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a8613fdbdc1afd55/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.44ba/s] 40%|████      | 2/5 [00:00<00:01,  2.73ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.39ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.83ba/s]100%|██████████| 5/5 [00:01<00:00,  4.10ba/s]100%|██████████| 5/5 [00:01<00:00,  3.63ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.10ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.80ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.07ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.24ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.34ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.28ba/s]100%|██████████| 7/7 [00:01<00:00,  4.49ba/s]100%|██████████| 7/7 [00:01<00:00,  4.24ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.92ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.37ba/s]100%|██████████| 5/5 [00:00<00:00,  9.41ba/s]100%|██████████| 5/5 [00:00<00:00,  8.75ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  5.43ba/s] 43%|████▎     | 3/7 [00:00<00:00,  8.64ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.50ba/s]100%|██████████| 7/7 [00:00<00:00, 10.21ba/s]100%|██████████| 7/7 [00:00<00:00,  9.53ba/s]
[INFO|trainer.py:414] 2023-08-28 14:11:42,728 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:11:42,828 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:11:42,857 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 14:11:42,857 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:11:42,857 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:11:42,857 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:11:42,857 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:11:42,857 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:57,  3.31it/s]  1%|          | 2/390 [00:00<01:54,  3.40it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:52,  3.44it/s]  1%|▏         | 5/390 [00:01<01:51,  3.45it/s]  2%|▏         | 6/390 [00:01<01:51,  3.45it/s]  2%|▏         | 7/390 [00:02<01:50,  3.45it/s]  2%|▏         | 8/390 [00:02<01:50,  3.46it/s]  2%|▏         | 9/390 [00:02<01:50,  3.46it/s]  3%|▎         | 10/390 [00:02<01:49,  3.46it/s]  3%|▎         | 11/390 [00:03<01:49,  3.46it/s]  3%|▎         | 12/390 [00:03<01:52,  3.35it/s]  3%|▎         | 13/390 [00:03<01:51,  3.38it/s]  4%|▎         | 14/390 [00:04<01:50,  3.40it/s]  4%|▍         | 15/390 [00:04<01:49,  3.42it/s]  4%|▍         | 16/390 [00:04<01:48,  3.43it/s]  4%|▍         | 17/390 [00:04<01:48,  3.44it/s]  5%|▍         | 18/390 [00:05<01:47,  3.45it/s]  5%|▍         | 19/390 [00:05<01:47,  3.45it/s]  5%|▌         | 20/390 [00:05<01:47,  3.45it/s]  5%|▌         | 21/390 [00:06<01:46,  3.45it/s]  6%|▌         | 22/390 [00:06<01:46,  3.46it/s]  6%|▌         | 23/390 [00:06<01:50,  3.33it/s]  6%|▌         | 24/390 [00:07<01:54,  3.20it/s]  6%|▋         | 25/390 [00:07<01:51,  3.27it/s]  7%|▋         | 26/390 [00:07<01:49,  3.33it/s]  7%|▋         | 27/390 [00:07<01:47,  3.37it/s]  7%|▋         | 28/390 [00:08<01:46,  3.39it/s]  7%|▋         | 29/390 [00:08<01:45,  3.41it/s]  8%|▊         | 30/390 [00:08<01:45,  3.42it/s]  8%|▊         | 31/390 [00:09<01:44,  3.44it/s]  8%|▊         | 32/390 [00:09<01:44,  3.44it/s]  8%|▊         | 33/390 [00:09<01:43,  3.45it/s]  9%|▊         | 34/390 [00:09<01:46,  3.33it/s]  9%|▉         | 35/390 [00:10<01:45,  3.37it/s]  9%|▉         | 36/390 [00:10<01:44,  3.39it/s]  9%|▉         | 37/390 [00:10<01:43,  3.41it/s] 10%|▉         | 38/390 [00:11<01:42,  3.43it/s] 10%|█         | 39/390 [00:11<01:42,  3.44it/s] 10%|█         | 40/390 [00:11<01:41,  3.44it/s] 11%|█         | 41/390 [00:12<01:41,  3.45it/s] 11%|█         | 42/390 [00:12<01:40,  3.45it/s] 11%|█         | 43/390 [00:12<01:40,  3.45it/s] 11%|█▏        | 44/390 [00:12<01:40,  3.46it/s] 12%|█▏        | 45/390 [00:13<01:43,  3.33it/s] 12%|█▏        | 46/390 [00:13<01:42,  3.37it/s] 12%|█▏        | 47/390 [00:13<01:41,  3.40it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.42it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.44it/s] 13%|█▎        | 51/390 [00:14<01:38,  3.44it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.44it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 54/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 55/390 [00:16<01:37,  3.45it/s] 14%|█▍        | 56/390 [00:16<01:40,  3.32it/s] 15%|█▍        | 57/390 [00:16<01:39,  3.35it/s] 15%|█▍        | 58/390 [00:17<01:38,  3.38it/s] 15%|█▌        | 59/390 [00:17<01:37,  3.40it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.42it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.43it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.44it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.43it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.44it/s] 17%|█▋        | 65/390 [00:19<01:34,  3.44it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.45it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.45it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.45it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.45it/s] 18%|█▊        | 70/390 [00:20<01:39,  3.21it/s] 18%|█▊        | 71/390 [00:20<01:37,  3.28it/s] 18%|█▊        | 72/390 [00:21<01:35,  3.32it/s] 19%|█▊        | 73/390 [00:21<01:34,  3.36it/s] 19%|█▉        | 74/390 [00:21<01:33,  3.38it/s] 19%|█▉        | 75/390 [00:22<01:32,  3.40it/s] 19%|█▉        | 76/390 [00:22<01:32,  3.41it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.42it/s] 20%|██        | 78/390 [00:22<01:31,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 14:12:05,787 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:12:05,787 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 14:12:05,787 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.95it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.55it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.90it/s][A
  3%|▎         | 22/861 [00:00<00:21, 38.58it/s][A
  3%|▎         | 27/861 [00:00<00:20, 40.58it/s][A
  4%|▎         | 32/861 [00:00<00:19, 41.90it/s][A
  4%|▍         | 37/861 [00:00<00:19, 42.81it/s][A
  5%|▍         | 42/861 [00:00<00:18, 43.33it/s][A
  5%|▌         | 47/861 [00:01<00:18, 43.79it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.13it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.26it/s][A
  7%|▋         | 62/861 [00:01<00:18, 43.89it/s][A
  8%|▊         | 67/861 [00:01<00:18, 43.82it/s][A
  8%|▊         | 72/861 [00:01<00:17, 43.96it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.22it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.43it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.51it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.59it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.71it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.48it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.28it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.06it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.15it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.34it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.55it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.66it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.67it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.65it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.45it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.24it/s][A
 18%|█▊        | 157/861 [00:03<00:17, 40.39it/s][A
 19%|█▉        | 162/861 [00:03<00:16, 41.69it/s][A
 19%|█▉        | 167/861 [00:03<00:16, 42.68it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.44it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.89it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.15it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.15it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.12it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 43.83it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 43.78it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.05it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.23it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.39it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.54it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.67it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.67it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.40it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.07it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.02it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.07it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.26it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.39it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.56it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.66it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.62it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.47it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.18it/s][A
 34%|███▍      | 292/861 [00:06<00:15, 37.62it/s][A
 34%|███▍      | 297/861 [00:06<00:14, 39.58it/s][A
 35%|███▌      | 302/861 [00:06<00:13, 41.04it/s][A
 36%|███▌      | 307/861 [00:07<00:13, 42.14it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 42.98it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.59it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.96it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.01it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 43.79it/s][A
 39%|███▉      | 337/861 [00:07<00:12, 43.65it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.78it/s][A
 40%|████      | 347/861 [00:07<00:11, 43.99it/s][A
 41%|████      | 352/861 [00:08<00:11, 44.30it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.50it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.64it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.66it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.47it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.25it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.13it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.04it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.13it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 44.36it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.53it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.62it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.74it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.65it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.37it/s][A
 50%|████▉     | 427/861 [00:09<00:10, 41.90it/s][A
 50%|█████     | 432/861 [00:09<00:10, 42.64it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.16it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.59it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.91it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.22it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.39it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.34it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.04it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.96it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.17it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.26it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.40it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.54it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.55it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.67it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.49it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.17it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.12it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.18it/s][A
 61%|██████    | 527/861 [00:12<00:07, 44.27it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.36it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.49it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.62it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.59it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.53it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.37it/s][A
 65%|██████▌   | 562/861 [00:12<00:07, 40.06it/s][A
 66%|██████▌   | 567/861 [00:12<00:07, 41.46it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 42.36it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 43.07it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.64it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.02it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.17it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.04it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 43.77it/s][A
 70%|███████   | 607/861 [00:13<00:05, 43.84it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.05it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 44.28it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.49it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.61it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.72it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.58it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.36it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.12it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.00it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.10it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.27it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.51it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.64it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.68it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.56it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.36it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.20it/s][A
 81%|████████  | 697/861 [00:15<00:03, 41.60it/s][A
 82%|████████▏ | 702/861 [00:16<00:03, 42.55it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.22it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.71it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.09it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.23it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.14it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 43.98it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 43.79it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.97it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 44.13it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.40it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.58it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.67it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.58it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.47it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.24it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.09it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.03it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.32it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.51it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.66it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.52it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.68it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.32it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.23it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.14it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 42.23it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 43.02it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.52it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.94it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.29it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.39it/s][A
                                                 [A                                                
100%|██████████| 861/861 [00:19<00:00, 44.39it/s][A 20%|██        | 78/390 [00:42<01:31,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:12:25,674 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 14:12:25,979 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:12:29,600 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:12:29,878 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:12:29,947 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:55<51:34,  9.95s/it] 21%|██        | 80/390 [00:55<36:29,  7.06s/it] 21%|██        | 81/390 [00:55<25:55,  5.03s/it] 21%|██        | 82/390 [00:56<18:32,  3.61s/it] 21%|██▏       | 83/390 [00:56<13:22,  2.61s/it] 22%|██▏       | 84/390 [00:56<09:46,  1.92s/it] 22%|██▏       | 85/390 [00:57<07:15,  1.43s/it] 22%|██▏       | 86/390 [00:57<05:30,  1.09s/it] 22%|██▏       | 87/390 [00:57<04:16,  1.18it/s] 23%|██▎       | 88/390 [00:58<03:25,  1.47it/s] 23%|██▎       | 89/390 [00:58<02:49,  1.78it/s] 23%|██▎       | 90/390 [00:58<02:24,  2.08it/s] 23%|██▎       | 91/390 [00:58<02:10,  2.30it/s] 24%|██▎       | 92/390 [00:59<01:56,  2.56it/s] 24%|██▍       | 93/390 [00:59<01:47,  2.77it/s] 24%|██▍       | 94/390 [01:00<01:58,  2.51it/s] 24%|██▍       | 95/390 [01:00<01:47,  2.73it/s] 25%|██▍       | 96/390 [01:00<01:40,  2.92it/s] 25%|██▍       | 97/390 [01:00<01:35,  3.06it/s] 25%|██▌       | 98/390 [01:01<01:32,  3.17it/s] 25%|██▌       | 99/390 [01:01<01:29,  3.25it/s] 26%|██▌       | 100/390 [01:01<01:27,  3.31it/s] 26%|██▌       | 101/390 [01:02<01:27,  3.29it/s] 26%|██▌       | 102/390 [01:02<01:26,  3.33it/s] 26%|██▋       | 103/390 [01:02<01:25,  3.37it/s] 27%|██▋       | 104/390 [01:02<01:24,  3.39it/s] 27%|██▋       | 105/390 [01:03<01:23,  3.41it/s] 27%|██▋       | 106/390 [01:03<01:23,  3.42it/s] 27%|██▋       | 107/390 [01:03<01:22,  3.43it/s] 28%|██▊       | 108/390 [01:04<01:22,  3.44it/s] 28%|██▊       | 109/390 [01:04<01:21,  3.44it/s] 28%|██▊       | 110/390 [01:04<01:21,  3.44it/s] 28%|██▊       | 111/390 [01:04<01:20,  3.45it/s] 29%|██▊       | 112/390 [01:05<01:23,  3.34it/s] 29%|██▉       | 113/390 [01:05<01:22,  3.37it/s] 29%|██▉       | 114/390 [01:05<01:21,  3.40it/s] 29%|██▉       | 115/390 [01:06<01:20,  3.41it/s] 30%|██▉       | 116/390 [01:06<01:20,  3.42it/s] 30%|███       | 117/390 [01:06<01:19,  3.43it/s] 30%|███       | 118/390 [01:07<01:19,  3.44it/s] 31%|███       | 119/390 [01:07<01:22,  3.29it/s] 31%|███       | 120/390 [01:07<01:21,  3.33it/s] 31%|███       | 121/390 [01:07<01:19,  3.37it/s] 31%|███▏      | 122/390 [01:08<01:19,  3.39it/s] 32%|███▏      | 123/390 [01:08<01:20,  3.31it/s] 32%|███▏      | 124/390 [01:08<01:19,  3.35it/s] 32%|███▏      | 125/390 [01:09<01:18,  3.38it/s] 32%|███▏      | 126/390 [01:09<01:17,  3.40it/s] 33%|███▎      | 127/390 [01:09<01:17,  3.41it/s] 33%|███▎      | 128/390 [01:09<01:16,  3.42it/s] 33%|███▎      | 129/390 [01:10<01:16,  3.43it/s] 33%|███▎      | 130/390 [01:10<01:15,  3.43it/s] 34%|███▎      | 131/390 [01:10<01:15,  3.44it/s] 34%|███▍      | 132/390 [01:11<01:15,  3.44it/s] 34%|███▍      | 133/390 [01:11<01:14,  3.44it/s] 34%|███▍      | 134/390 [01:11<01:17,  3.32it/s] 35%|███▍      | 135/390 [01:12<01:15,  3.36it/s] 35%|███▍      | 136/390 [01:12<01:15,  3.38it/s] 35%|███▌      | 137/390 [01:12<01:14,  3.40it/s] 35%|███▌      | 138/390 [01:12<01:13,  3.41it/s] 36%|███▌      | 139/390 [01:13<01:13,  3.42it/s] 36%|███▌      | 140/390 [01:13<01:12,  3.43it/s] 36%|███▌      | 141/390 [01:13<01:12,  3.43it/s] 36%|███▋      | 142/390 [01:14<01:12,  3.43it/s] 37%|███▋      | 143/390 [01:14<01:11,  3.44it/s] 37%|███▋      | 144/390 [01:14<01:11,  3.44it/s] 37%|███▋      | 145/390 [01:14<01:13,  3.32it/s] 37%|███▋      | 146/390 [01:15<01:12,  3.35it/s] 38%|███▊      | 147/390 [01:15<01:11,  3.38it/s] 38%|███▊      | 148/390 [01:15<01:11,  3.40it/s] 38%|███▊      | 149/390 [01:16<01:10,  3.41it/s] 38%|███▊      | 150/390 [01:16<01:10,  3.42it/s] 39%|███▊      | 151/390 [01:16<01:09,  3.42it/s] 39%|███▉      | 152/390 [01:17<01:09,  3.43it/s] 39%|███▉      | 153/390 [01:17<01:09,  3.43it/s] 39%|███▉      | 154/390 [01:17<01:08,  3.43it/s] 40%|███▉      | 155/390 [01:17<01:08,  3.44it/s] 40%|████      | 156/390 [01:18<01:09,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 14:13:01,106 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:13:01,107 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 14:13:01,107 >>   Batch size = 8
{'eval_loss': 0.9576054811477661, 'eval_runtime': 19.629, 'eval_samples_per_second': 350.706, 'eval_steps_per_second': 43.864, 'epoch': 0.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.86it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.83it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.19it/s][A
  3%|▎         | 22/861 [00:00<00:18, 44.92it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.77it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.13it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.25it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.16it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.32it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.51it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.64it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.62it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.68it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.43it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.31it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.11it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.08it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.17it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.41it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.62it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.73it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.51it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.38it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.34it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.14it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.09it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.20it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.36it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.57it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.67it/s][A
 18%|█▊        | 157/861 [00:03<00:17, 39.27it/s][A
 19%|█▉        | 162/861 [00:03<00:17, 40.75it/s][A
 19%|█▉        | 167/861 [00:03<00:16, 41.82it/s][A
 20%|█▉        | 172/861 [00:03<00:16, 42.72it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.26it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.72it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.96it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.13it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 43.87it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 43.69it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 43.95it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.13it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.25it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.48it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.60it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.59it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.37it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.13it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.06it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.18it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.26it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.24it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.46it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.60it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.66it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.41it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 44.14it/s][A
 34%|███▍      | 292/861 [00:06<00:14, 38.28it/s][A
 34%|███▍      | 297/861 [00:06<00:14, 40.13it/s][A
 35%|███▌      | 302/861 [00:06<00:13, 41.45it/s][A
 36%|███▌      | 307/861 [00:06<00:13, 42.48it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.19it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.52it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.05it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 43.99it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 43.74it/s][A
 39%|███▉      | 337/861 [00:07<00:12, 43.47it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.72it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.08it/s][A
 41%|████      | 352/861 [00:08<00:11, 44.30it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.41it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.54it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.66it/s][A
 43%|████▎     | 372/861 [00:08<00:10, 44.50it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.03it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 43.77it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 43.85it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.12it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 44.27it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.48it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.58it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.65it/s][A
 48%|████▊     | 417/861 [00:09<00:09, 44.56it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.22it/s][A
 50%|████▉     | 427/861 [00:09<00:11, 38.41it/s][A
 50%|█████     | 432/861 [00:09<00:10, 40.15it/s][A
 51%|█████     | 437/861 [00:09<00:10, 41.52it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 42.50it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.18it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.73it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.08it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.01it/s][A
 54%|█████▍    | 467/861 [00:10<00:09, 43.72it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.56it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.74it/s][A
 56%|█████▌    | 482/861 [00:11<00:08, 44.06it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.31it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.40it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.59it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.71it/s][A
 59%|█████▉    | 507/861 [00:11<00:07, 44.46it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.14it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.87it/s][A
 61%|██████    | 522/861 [00:11<00:07, 43.98it/s][A
 61%|██████    | 527/861 [00:12<00:07, 44.20it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.44it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.61it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.67it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.58it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.36it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.14it/s][A
 65%|██████▌   | 562/861 [00:12<00:07, 39.75it/s][A
 66%|██████▌   | 567/861 [00:12<00:07, 41.24it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 42.26it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 42.87it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.41it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.87it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.08it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.05it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 43.71it/s][A
 70%|███████   | 607/861 [00:13<00:05, 43.72it/s][A
 71%|███████   | 612/861 [00:13<00:05, 43.89it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 44.15it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.40it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.46it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.68it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.51it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.38it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.13it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 43.99it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.07it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.17it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.39it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.59it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.67it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.49it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.23it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.13it/s][A
 81%|████████  | 697/861 [00:15<00:04, 39.85it/s][A
 82%|████████▏ | 702/861 [00:16<00:03, 41.24it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 42.15it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 42.89it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.52it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.87it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.22it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.22it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 43.80it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.65it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 43.84it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.05it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.31it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.40it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.66it/s][A
 90%|████████▉ | 772/861 [00:17<00:01, 44.69it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.47it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.14it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.00it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 43.99it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.11it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.25it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.43it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.64it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.57it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.46it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.18it/s][A
 97%|█████████▋| 832/861 [00:19<00:00, 39.90it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 41.35it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 42.33it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.00it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.56it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.89it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.89it/s][A 40%|████      | 156/390 [01:37<01:09,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:13:21,057 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 14:13:21,391 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:13:25,854 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:13:26,135 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:13:26,281 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:54<42:43, 11.00s/it] 41%|████      | 158/390 [01:54<30:09,  7.80s/it] 41%|████      | 159/390 [01:54<21:24,  5.56s/it] 41%|████      | 160/390 [01:55<15:15,  3.98s/it] 41%|████▏     | 161/390 [01:55<10:58,  2.87s/it] 42%|████▏     | 162/390 [01:55<07:58,  2.10s/it] 42%|████▏     | 163/390 [01:56<05:53,  1.56s/it] 42%|████▏     | 164/390 [01:56<04:26,  1.18s/it] 42%|████▏     | 165/390 [01:56<03:25,  1.10it/s] 43%|████▎     | 166/390 [01:56<02:42,  1.38it/s] 43%|████▎     | 167/390 [01:57<02:13,  1.68it/s] 43%|████▎     | 168/390 [01:57<01:54,  1.93it/s] 43%|████▎     | 169/390 [01:57<01:39,  2.22it/s] 44%|████▎     | 170/390 [01:58<01:28,  2.48it/s] 44%|████▍     | 171/390 [01:58<01:21,  2.70it/s] 44%|████▍     | 172/390 [01:58<01:15,  2.88it/s] 44%|████▍     | 173/390 [01:58<01:11,  3.02it/s] 45%|████▍     | 174/390 [01:59<01:09,  3.13it/s] 45%|████▍     | 175/390 [01:59<01:07,  3.21it/s] 45%|████▌     | 176/390 [02:00<01:17,  2.76it/s] 45%|████▌     | 177/390 [02:00<01:12,  2.92it/s] 46%|████▌     | 178/390 [02:00<01:10,  3.00it/s] 46%|████▌     | 179/390 [02:00<01:07,  3.11it/s] 46%|████▌     | 180/390 [02:01<01:05,  3.19it/s] 46%|████▋     | 181/390 [02:01<01:04,  3.25it/s] 47%|████▋     | 182/390 [02:01<01:03,  3.30it/s] 47%|████▋     | 183/390 [02:02<01:02,  3.33it/s] 47%|████▋     | 184/390 [02:02<01:01,  3.35it/s] 47%|████▋     | 185/390 [02:02<01:00,  3.37it/s] 48%|████▊     | 186/390 [02:03<01:00,  3.38it/s] 48%|████▊     | 187/390 [02:03<00:59,  3.39it/s] 48%|████▊     | 188/390 [02:03<00:59,  3.39it/s] 48%|████▊     | 189/390 [02:03<01:00,  3.33it/s] 49%|████▊     | 190/390 [02:04<00:59,  3.35it/s] 49%|████▉     | 191/390 [02:04<00:59,  3.36it/s] 49%|████▉     | 192/390 [02:04<00:58,  3.37it/s] 49%|████▉     | 193/390 [02:05<00:58,  3.38it/s] 50%|████▉     | 194/390 [02:05<00:57,  3.39it/s] 50%|█████     | 195/390 [02:05<00:57,  3.39it/s] 50%|█████     | 196/390 [02:05<00:57,  3.40it/s] 51%|█████     | 197/390 [02:06<00:56,  3.40it/s] 51%|█████     | 198/390 [02:06<00:56,  3.40it/s] 51%|█████     | 199/390 [02:06<00:56,  3.40it/s] 51%|█████▏    | 200/390 [02:07<00:58,  3.26it/s] 52%|█████▏    | 201/390 [02:07<01:02,  3.01it/s] 52%|█████▏    | 202/390 [02:07<01:00,  3.12it/s] 52%|█████▏    | 203/390 [02:08<00:58,  3.20it/s] 52%|█████▏    | 204/390 [02:08<00:57,  3.25it/s] 53%|█████▎    | 205/390 [02:08<00:56,  3.30it/s] 53%|█████▎    | 206/390 [02:09<00:55,  3.33it/s] 53%|█████▎    | 207/390 [02:09<00:54,  3.35it/s] 53%|█████▎    | 208/390 [02:09<00:54,  3.36it/s] 54%|█████▎    | 209/390 [02:09<00:53,  3.37it/s] 54%|█████▍    | 210/390 [02:10<00:53,  3.38it/s] 54%|█████▍    | 211/390 [02:10<00:52,  3.38it/s] 54%|█████▍    | 212/390 [02:10<00:52,  3.39it/s] 55%|█████▍    | 213/390 [02:11<00:52,  3.39it/s] 55%|█████▍    | 214/390 [02:11<00:51,  3.39it/s] 55%|█████▌    | 215/390 [02:11<00:51,  3.40it/s] 55%|█████▌    | 216/390 [02:12<00:50,  3.41it/s] 56%|█████▌    | 217/390 [02:12<00:50,  3.42it/s] 56%|█████▌    | 218/390 [02:12<00:50,  3.43it/s] 56%|█████▌    | 219/390 [02:12<00:49,  3.43it/s] 56%|█████▋    | 220/390 [02:13<00:51,  3.32it/s] 57%|█████▋    | 221/390 [02:13<00:50,  3.35it/s] 57%|█████▋    | 222/390 [02:13<00:49,  3.38it/s] 57%|█████▋    | 223/390 [02:14<00:49,  3.40it/s] 57%|█████▋    | 224/390 [02:14<00:48,  3.42it/s] 58%|█████▊    | 225/390 [02:14<00:48,  3.43it/s] 58%|█████▊    | 226/390 [02:14<00:47,  3.43it/s] 58%|█████▊    | 227/390 [02:15<00:47,  3.44it/s] 58%|█████▊    | 228/390 [02:15<00:47,  3.44it/s] 59%|█████▊    | 229/390 [02:15<00:46,  3.44it/s] 59%|█████▉    | 230/390 [02:16<00:46,  3.45it/s] 59%|█████▉    | 231/390 [02:16<00:47,  3.33it/s] 59%|█████▉    | 232/390 [02:16<00:47,  3.36it/s] 60%|█████▉    | 233/390 [02:17<00:46,  3.38it/s] 60%|██████    | 234/390 [02:17<00:45,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 14:14:00,191 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:14:00,191 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 14:14:00,191 >>   Batch size = 8
{'eval_loss': 0.9753513932228088, 'eval_runtime': 19.6843, 'eval_samples_per_second': 349.72, 'eval_steps_per_second': 43.74, 'epoch': 1.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.80it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.73it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.86it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.98it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.32it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.80it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.60it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.36it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.44it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.63it/s][A
  7%|▋         | 57/861 [00:01<00:17, 44.68it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.78it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.66it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.34it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.30it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.20it/s][A
 10%|█         | 87/861 [00:01<00:18, 42.22it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.06it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.55it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.96it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.28it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.34it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.30it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.14it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.94it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.02it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.00it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.32it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.53it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.65it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.71it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.55it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.36it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.14it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.12it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.16it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.40it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.58it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.65it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.60it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.53it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.37it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.30it/s][A
 26%|██▌       | 222/861 [00:05<00:15, 40.79it/s][A
 26%|██▋       | 227/861 [00:05<00:15, 42.03it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 42.82it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 43.46it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 43.93it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.10it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.14it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.10it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.82it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.85it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.03it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.31it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.52it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.68it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.68it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.44it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.28it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.15it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.09it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.24it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.48it/s][A
 38%|███▊      | 327/861 [00:07<00:11, 44.56it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.70it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.69it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.56it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.33it/s][A
 41%|████      | 352/861 [00:08<00:11, 44.15it/s][A
 41%|████▏     | 357/861 [00:08<00:13, 38.30it/s][A
 42%|████▏     | 362/861 [00:08<00:12, 40.11it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 41.44it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 42.46it/s][A
 44%|████▍     | 377/861 [00:08<00:11, 43.18it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 43.71it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.12it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.22it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 43.79it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.55it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.77it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.99it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.27it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.50it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.65it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.77it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.61it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.12it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.97it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.04it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.12it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.31it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.43it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.55it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.69it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.57it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.35it/s][A
 57%|█████▋    | 492/861 [00:11<00:09, 39.19it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 40.74it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 41.94it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 42.77it/s][A
 59%|█████▉    | 512/861 [00:11<00:08, 43.40it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.88it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.11it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.15it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.80it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 43.74it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 43.93it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.23it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.45it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.59it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.68it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.72it/s][A
 66%|██████▋   | 572/861 [00:12<00:06, 44.50it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.15it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.96it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.04it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.30it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.41it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.57it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.64it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.66it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 44.41it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.22it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 40.78it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 41.90it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 42.78it/s][A
 75%|███████▍  | 642/861 [00:14<00:05, 43.42it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 43.76it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.15it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.29it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.17it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 43.77it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 43.73it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 43.92it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.22it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.45it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.57it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.66it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.73it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 44.37it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.13it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.98it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.13it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.29it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.49it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.52it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.71it/s][A
 87%|████████▋ | 747/861 [00:16<00:02, 44.63it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.40it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.21it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 39.18it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 40.77it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 41.97it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 42.84it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 43.38it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 43.91it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.09it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.09it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 43.78it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 43.66it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 43.88it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.14it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.36it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.58it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.69it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 44.68it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.43it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.12it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.96it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.02it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 44.02it/s][A 60%|██████    | 234/390 [02:36<00:45,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:14:19,929 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 14:14:20,246 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:14:25,238 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:14:25,426 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:14:25,501 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:51<26:48, 10.38s/it] 61%|██████    | 236/390 [02:51<18:53,  7.36s/it] 61%|██████    | 237/390 [02:51<13:21,  5.24s/it] 61%|██████    | 238/390 [02:52<09:30,  3.76s/it] 61%|██████▏   | 239/390 [02:52<06:50,  2.72s/it] 62%|██████▏   | 240/390 [02:52<04:58,  1.99s/it] 62%|██████▏   | 241/390 [02:52<03:40,  1.48s/it] 62%|██████▏   | 242/390 [02:53<02:46,  1.12s/it] 62%|██████▏   | 243/390 [02:53<02:09,  1.13it/s] 63%|██████▎   | 244/390 [02:53<01:43,  1.42it/s] 63%|██████▎   | 245/390 [02:54<01:24,  1.72it/s] 63%|██████▎   | 246/390 [02:54<01:11,  2.02it/s] 63%|██████▎   | 247/390 [02:54<01:02,  2.27it/s] 64%|██████▎   | 248/390 [02:55<00:57,  2.48it/s] 64%|██████▍   | 249/390 [02:55<00:52,  2.71it/s] 64%|██████▍   | 250/390 [02:55<00:48,  2.88it/s] 64%|██████▍   | 251/390 [02:55<00:45,  3.02it/s] 65%|██████▍   | 252/390 [02:56<00:44,  3.13it/s] 65%|██████▍   | 253/390 [02:56<00:42,  3.21it/s] 65%|██████▌   | 254/390 [02:56<00:41,  3.26it/s] 65%|██████▌   | 255/390 [02:57<00:40,  3.31it/s] 66%|██████▌   | 256/390 [02:57<00:40,  3.33it/s] 66%|██████▌   | 257/390 [02:57<00:39,  3.35it/s] 66%|██████▌   | 258/390 [02:58<00:40,  3.25it/s] 66%|██████▋   | 259/390 [02:58<00:39,  3.29it/s] 67%|██████▋   | 260/390 [02:58<00:39,  3.33it/s] 67%|██████▋   | 261/390 [02:58<00:38,  3.35it/s] 67%|██████▋   | 262/390 [02:59<00:38,  3.37it/s] 67%|██████▋   | 263/390 [02:59<00:37,  3.38it/s] 68%|██████▊   | 264/390 [02:59<00:37,  3.39it/s] 68%|██████▊   | 265/390 [03:00<00:43,  2.87it/s] 68%|██████▊   | 266/390 [03:00<00:41,  3.01it/s] 68%|██████▊   | 267/390 [03:00<00:39,  3.12it/s] 69%|██████▊   | 268/390 [03:01<00:39,  3.08it/s] 69%|██████▉   | 269/390 [03:01<00:38,  3.16it/s] 69%|██████▉   | 270/390 [03:01<00:37,  3.23it/s] 69%|██████▉   | 271/390 [03:02<00:36,  3.28it/s] 70%|██████▉   | 272/390 [03:02<00:35,  3.32it/s] 70%|███████   | 273/390 [03:02<00:35,  3.34it/s] 70%|███████   | 274/390 [03:02<00:34,  3.36it/s] 71%|███████   | 275/390 [03:03<00:34,  3.37it/s] 71%|███████   | 276/390 [03:03<00:33,  3.38it/s] 71%|███████   | 277/390 [03:03<00:33,  3.39it/s] 71%|███████▏  | 278/390 [03:04<00:32,  3.39it/s] 72%|███████▏  | 279/390 [03:04<00:33,  3.29it/s] 72%|███████▏  | 280/390 [03:04<00:33,  3.32it/s] 72%|███████▏  | 281/390 [03:05<00:32,  3.35it/s] 72%|███████▏  | 282/390 [03:05<00:32,  3.36it/s] 73%|███████▎  | 283/390 [03:05<00:31,  3.38it/s] 73%|███████▎  | 284/390 [03:05<00:31,  3.38it/s] 73%|███████▎  | 285/390 [03:06<00:30,  3.39it/s] 73%|███████▎  | 286/390 [03:06<00:30,  3.39it/s] 74%|███████▎  | 287/390 [03:06<00:30,  3.39it/s] 74%|███████▍  | 288/390 [03:07<00:30,  3.40it/s] 74%|███████▍  | 289/390 [03:07<00:29,  3.40it/s] 74%|███████▍  | 290/390 [03:07<00:32,  3.10it/s] 75%|███████▍  | 291/390 [03:08<00:31,  3.18it/s] 75%|███████▍  | 292/390 [03:08<00:30,  3.24it/s] 75%|███████▌  | 293/390 [03:08<00:29,  3.29it/s] 75%|███████▌  | 294/390 [03:08<00:28,  3.32it/s] 76%|███████▌  | 295/390 [03:09<00:28,  3.34it/s] 76%|███████▌  | 296/390 [03:09<00:27,  3.36it/s] 76%|███████▌  | 297/390 [03:09<00:27,  3.37it/s] 76%|███████▋  | 298/390 [03:10<00:27,  3.38it/s] 77%|███████▋  | 299/390 [03:10<00:26,  3.38it/s] 77%|███████▋  | 300/390 [03:10<00:27,  3.29it/s] 77%|███████▋  | 301/390 [03:11<00:26,  3.33it/s] 77%|███████▋  | 302/390 [03:11<00:26,  3.35it/s] 78%|███████▊  | 303/390 [03:11<00:25,  3.36it/s] 78%|███████▊  | 304/390 [03:11<00:25,  3.37it/s] 78%|███████▊  | 305/390 [03:12<00:25,  3.38it/s] 78%|███████▊  | 306/390 [03:12<00:24,  3.38it/s] 79%|███████▊  | 307/390 [03:12<00:24,  3.39it/s] 79%|███████▉  | 308/390 [03:13<00:24,  3.39it/s] 79%|███████▉  | 309/390 [03:13<00:23,  3.39it/s] 79%|███████▉  | 310/390 [03:13<00:23,  3.39it/s] 80%|███████▉  | 311/390 [03:14<00:23,  3.40it/s] 80%|████████  | 312/390 [03:14<00:22,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 14:14:57,245 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:14:57,245 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 14:14:57,245 >>   Batch size = 8
{'eval_loss': 0.985856294631958, 'eval_runtime': 19.614, 'eval_samples_per_second': 350.973, 'eval_steps_per_second': 43.897, 'epoch': 2.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.76it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.55it/s][A
  2%|▏         | 17/861 [00:00<00:17, 46.97it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.39it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.67it/s][A
  4%|▎         | 32/861 [00:00<00:18, 45.13it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.86it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.33it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.30it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.40it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.63it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.76it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.83it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.66it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.53it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.23it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.23it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.16it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.41it/s][A
 12%|█▏        | 102/861 [00:02<00:18, 41.04it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 42.13it/s][A
 13%|█▎        | 112/861 [00:02<00:17, 42.98it/s][A
 14%|█▎        | 117/861 [00:02<00:17, 43.53it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.88it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.99it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.97it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.12it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 43.36it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 43.64it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.01it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.20it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.53it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.52it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.49it/s][A
 21%|██        | 177/861 [00:03<00:15, 44.34it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.21it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.21it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.26it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.24it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.45it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.64it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.67it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.57it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.35it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.19it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.28it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.30it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.42it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.47it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.66it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.69it/s][A
 30%|███       | 262/861 [00:05<00:13, 44.46it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.39it/s][A
 32%|███▏      | 272/861 [00:06<00:14, 41.93it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 42.76it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 43.23it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 43.72it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.04it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.25it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.35it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.33it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.08it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.12it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.17it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.32it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.39it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.62it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.62it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.52it/s][A
 41%|████      | 352/861 [00:07<00:11, 44.29it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.24it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.18it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.24it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.35it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.58it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.56it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.54it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 44.49it/s][A
 46%|████▌     | 397/861 [00:08<00:10, 44.31it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.31it/s][A
 47%|████▋     | 407/861 [00:09<00:12, 37.18it/s][A
 48%|████▊     | 412/861 [00:09<00:11, 39.30it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 40.87it/s][A
 49%|████▉     | 422/861 [00:09<00:10, 42.03it/s][A
 50%|████▉     | 427/861 [00:09<00:10, 42.90it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.53it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.85it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.05it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.80it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.62it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.70it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.03it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.21it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.19it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.58it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.81it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.56it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.32it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.04it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 43.89it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.15it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.37it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.59it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.77it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.71it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.44it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.19it/s][A
 63%|██████▎   | 542/861 [00:12<00:08, 38.24it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 40.10it/s][A
 64%|██████▍   | 552/861 [00:12<00:07, 41.43it/s][A
 65%|██████▍   | 557/861 [00:12<00:07, 42.43it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 43.20it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 43.77it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 44.17it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.16it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.90it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.69it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 43.78it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.06it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.31it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.54it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.68it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 44.72it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.52it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.26it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.02it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.02it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.20it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.39it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.48it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.69it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.75it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.48it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.28it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 37.13it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 39.22it/s][A
 80%|███████▉  | 687/861 [00:15<00:04, 40.79it/s][A
 80%|████████  | 692/861 [00:15<00:04, 41.95it/s][A
 81%|████████  | 697/861 [00:15<00:03, 42.84it/s][A
 82%|████████▏ | 702/861 [00:16<00:03, 43.40it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.85it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.95it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.74it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.49it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.77it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.11it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.37it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.48it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 44.47it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.63it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.48it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.20it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.98it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.15it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.25it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.49it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.60it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.65it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.59it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.52it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.23it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 38.53it/s][A
 95%|█████████▍| 817/861 [00:18<00:01, 40.31it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 41.63it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 42.62it/s][A
 97%|█████████▋| 832/861 [00:19<00:00, 43.28it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 43.67it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.10it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.18it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.85it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.64it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.64it/s][A 80%|████████  | 312/390 [03:34<00:22,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:15:17,474 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 14:15:17,831 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:15:22,988 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:15:23,220 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:15:23,338 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:49<13:46, 10.74s/it] 81%|████████  | 314/390 [03:49<09:38,  7.61s/it] 81%|████████  | 315/390 [03:50<06:46,  5.42s/it] 81%|████████  | 316/390 [03:50<04:47,  3.88s/it] 81%|████████▏ | 317/390 [03:50<03:24,  2.80s/it] 82%|████████▏ | 318/390 [03:50<02:27,  2.05s/it] 82%|████████▏ | 319/390 [03:51<01:48,  1.52s/it] 82%|████████▏ | 320/390 [03:51<01:20,  1.16s/it] 82%|████████▏ | 321/390 [03:51<01:01,  1.12it/s] 83%|████████▎ | 322/390 [03:52<00:48,  1.40it/s] 83%|████████▎ | 323/390 [03:52<00:39,  1.70it/s] 83%|████████▎ | 324/390 [03:52<00:32,  2.00it/s] 83%|████████▎ | 325/390 [03:53<00:29,  2.24it/s] 84%|████████▎ | 326/390 [03:53<00:25,  2.50it/s] 84%|████████▍ | 327/390 [03:53<00:23,  2.65it/s] 84%|████████▍ | 328/390 [03:53<00:21,  2.84it/s] 84%|████████▍ | 329/390 [03:54<00:20,  2.99it/s] 85%|████████▍ | 330/390 [03:54<00:19,  3.11it/s] 85%|████████▍ | 331/390 [03:54<00:18,  3.19it/s] 85%|████████▌ | 332/390 [03:55<00:18,  3.11it/s] 85%|████████▌ | 333/390 [03:55<00:17,  3.20it/s] 86%|████████▌ | 334/390 [03:55<00:17,  3.26it/s] 86%|████████▌ | 335/390 [03:56<00:17,  3.20it/s] 86%|████████▌ | 336/390 [03:56<00:16,  3.26it/s] 86%|████████▋ | 337/390 [03:56<00:16,  3.30it/s] 87%|████████▋ | 338/390 [03:56<00:15,  3.33it/s] 87%|████████▋ | 339/390 [03:57<00:15,  3.35it/s] 87%|████████▋ | 340/390 [03:57<00:14,  3.37it/s] 87%|████████▋ | 341/390 [03:57<00:14,  3.38it/s] 88%|████████▊ | 342/390 [03:58<00:14,  3.39it/s] 88%|████████▊ | 343/390 [03:58<00:13,  3.40it/s] 88%|████████▊ | 344/390 [03:58<00:13,  3.40it/s] 88%|████████▊ | 345/390 [03:58<00:13,  3.40it/s] 89%|████████▊ | 346/390 [03:59<00:13,  3.30it/s] 89%|████████▉ | 347/390 [03:59<00:12,  3.33it/s] 89%|████████▉ | 348/390 [03:59<00:12,  3.35it/s] 89%|████████▉ | 349/390 [04:00<00:14,  2.74it/s] 90%|████████▉ | 350/390 [04:00<00:13,  2.91it/s] 90%|█████████ | 351/390 [04:01<00:12,  3.04it/s] 90%|█████████ | 352/390 [04:01<00:12,  3.14it/s] 91%|█████████ | 353/390 [04:01<00:11,  3.22it/s] 91%|█████████ | 354/390 [04:01<00:11,  3.27it/s] 91%|█████████ | 355/390 [04:02<00:10,  3.31it/s] 91%|█████████▏| 356/390 [04:02<00:10,  3.22it/s] 92%|█████████▏| 357/390 [04:02<00:10,  3.27it/s] 92%|█████████▏| 358/390 [04:03<00:09,  3.31it/s] 92%|█████████▏| 359/390 [04:03<00:09,  3.34it/s] 92%|█████████▏| 360/390 [04:03<00:08,  3.36it/s] 93%|█████████▎| 361/390 [04:03<00:08,  3.37it/s] 93%|█████████▎| 362/390 [04:04<00:08,  3.39it/s] 93%|█████████▎| 363/390 [04:04<00:07,  3.39it/s] 93%|█████████▎| 364/390 [04:04<00:07,  3.40it/s] 94%|█████████▎| 365/390 [04:05<00:07,  3.40it/s] 94%|█████████▍| 366/390 [04:05<00:07,  3.40it/s] 94%|█████████▍| 367/390 [04:05<00:06,  3.35it/s] 94%|█████████▍| 368/390 [04:06<00:06,  3.37it/s] 95%|█████████▍| 369/390 [04:06<00:06,  3.37it/s] 95%|█████████▍| 370/390 [04:06<00:05,  3.38it/s] 95%|█████████▌| 371/390 [04:06<00:05,  3.39it/s] 95%|█████████▌| 372/390 [04:07<00:05,  3.38it/s] 96%|█████████▌| 373/390 [04:07<00:05,  3.39it/s] 96%|█████████▌| 374/390 [04:07<00:04,  3.21it/s] 96%|█████████▌| 375/390 [04:08<00:04,  3.27it/s] 96%|█████████▋| 376/390 [04:08<00:04,  3.30it/s] 97%|█████████▋| 377/390 [04:08<00:04,  3.23it/s] 97%|█████████▋| 378/390 [04:09<00:03,  3.28it/s] 97%|█████████▋| 379/390 [04:09<00:03,  3.31it/s] 97%|█████████▋| 380/390 [04:09<00:02,  3.34it/s] 98%|█████████▊| 381/390 [04:09<00:02,  3.35it/s] 98%|█████████▊| 382/390 [04:10<00:02,  3.37it/s] 98%|█████████▊| 383/390 [04:10<00:02,  3.38it/s] 98%|█████████▊| 384/390 [04:10<00:01,  3.38it/s] 99%|█████████▊| 385/390 [04:11<00:01,  3.38it/s] 99%|█████████▉| 386/390 [04:11<00:01,  3.38it/s] 99%|█████████▉| 387/390 [04:11<00:00,  3.38it/s] 99%|█████████▉| 388/390 [04:12<00:00,  3.30it/s]100%|█████████▉| 389/390 [04:12<00:00,  3.33it/s]100%|██████████| 390/390 [04:12<00:00,  3.35it/s][INFO|trainer.py:2140] 2023-08-28 14:15:55,503 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:15:55,503 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 14:15:55,503 >>   Batch size = 8
{'eval_loss': 0.9952621459960938, 'eval_runtime': 19.6808, 'eval_samples_per_second': 349.783, 'eval_steps_per_second': 43.748, 'epoch': 3.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.82it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.50it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.06it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.11it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.60it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.99it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.58it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.37it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.44it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.63it/s][A
  7%|▋         | 57/861 [00:01<00:17, 44.71it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.74it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.72it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.59it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.30it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.23it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.17it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.32it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.44it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.64it/s][A
 12%|█▏        | 107/861 [00:02<00:16, 44.67it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.61it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.55it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.42it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.21it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 44.07it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.31it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 43.41it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 43.88it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.25it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.28it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.28it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.15it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.06it/s][A
 21%|██        | 177/861 [00:04<00:17, 39.62it/s][A
 21%|██▏       | 183/861 [00:04<00:15, 42.52it/s][A
 22%|██▏       | 188/861 [00:04<00:15, 43.27it/s][A
 22%|██▏       | 193/861 [00:04<00:15, 43.63it/s][A
 23%|██▎       | 198/861 [00:04<00:15, 44.02it/s][A
 24%|██▎       | 203/861 [00:04<00:14, 44.32it/s][A
 24%|██▍       | 208/861 [00:04<00:14, 44.43it/s][A
 25%|██▍       | 213/861 [00:04<00:14, 44.40it/s][A
 25%|██▌       | 218/861 [00:04<00:14, 44.21it/s][A
 26%|██▌       | 223/861 [00:05<00:14, 43.89it/s][A
 26%|██▋       | 228/861 [00:05<00:14, 44.01it/s][A
 27%|██▋       | 233/861 [00:05<00:14, 44.16it/s][A
 28%|██▊       | 238/861 [00:05<00:14, 44.42it/s][A
 28%|██▊       | 243/861 [00:05<00:13, 44.57it/s][A
 29%|██▉       | 248/861 [00:05<00:13, 44.60it/s][A
 29%|██▉       | 253/861 [00:05<00:13, 44.70it/s][A
 30%|██▉       | 258/861 [00:05<00:13, 44.47it/s][A
 31%|███       | 263/861 [00:05<00:13, 44.30it/s][A
 31%|███       | 268/861 [00:06<00:13, 44.14it/s][A
 32%|███▏      | 273/861 [00:06<00:13, 44.12it/s][A
 32%|███▏      | 278/861 [00:06<00:14, 39.01it/s][A
 33%|███▎      | 283/861 [00:06<00:14, 40.66it/s][A
 33%|███▎      | 288/861 [00:06<00:13, 41.95it/s][A
 34%|███▍      | 293/861 [00:06<00:13, 42.78it/s][A
 35%|███▍      | 298/861 [00:06<00:12, 43.39it/s][A
 35%|███▌      | 303/861 [00:06<00:12, 43.83it/s][A
 36%|███▌      | 308/861 [00:06<00:12, 44.01it/s][A
 36%|███▋      | 313/861 [00:07<00:12, 44.14it/s][A
 37%|███▋      | 318/861 [00:07<00:12, 43.82it/s][A
 38%|███▊      | 323/861 [00:07<00:12, 43.66it/s][A
 38%|███▊      | 328/861 [00:07<00:12, 43.92it/s][A
 39%|███▊      | 333/861 [00:07<00:11, 44.19it/s][A
 39%|███▉      | 338/861 [00:07<00:11, 44.39it/s][A
 40%|███▉      | 343/861 [00:07<00:11, 44.52it/s][A
 40%|████      | 348/861 [00:07<00:11, 44.65it/s][A
 41%|████      | 353/861 [00:08<00:11, 44.64it/s][A
 42%|████▏     | 358/861 [00:08<00:11, 44.42it/s][A
 42%|████▏     | 363/861 [00:08<00:11, 44.15it/s][A
 43%|████▎     | 368/861 [00:08<00:11, 44.00it/s][A
 43%|████▎     | 373/861 [00:08<00:11, 44.17it/s][A
 44%|████▍     | 378/861 [00:08<00:10, 44.31it/s][A
 44%|████▍     | 383/861 [00:08<00:10, 44.52it/s][A
 45%|████▌     | 388/861 [00:08<00:10, 44.59it/s][A
 46%|████▌     | 393/861 [00:08<00:10, 44.54it/s][A
 46%|████▌     | 398/861 [00:09<00:10, 44.45it/s][A
 47%|████▋     | 403/861 [00:09<00:10, 44.26it/s][A
 47%|████▋     | 408/861 [00:09<00:10, 44.10it/s][A
 48%|████▊     | 413/861 [00:09<00:10, 43.02it/s][A
 49%|████▊     | 418/861 [00:09<00:10, 43.57it/s][A
 49%|████▉     | 423/861 [00:09<00:09, 43.92it/s][A
 50%|████▉     | 428/861 [00:09<00:09, 44.21it/s][A
 50%|█████     | 433/861 [00:09<00:09, 44.44it/s][A
 51%|█████     | 438/861 [00:09<00:09, 44.46it/s][A
 51%|█████▏    | 443/861 [00:10<00:09, 44.35it/s][A
 52%|█████▏    | 448/861 [00:10<00:09, 44.20it/s][A
 53%|█████▎    | 453/861 [00:10<00:09, 43.88it/s][A
 53%|█████▎    | 458/861 [00:10<00:09, 44.03it/s][A
 54%|█████▍    | 463/861 [00:10<00:08, 44.32it/s][A
 54%|█████▍    | 468/861 [00:10<00:08, 44.48it/s][A
 55%|█████▍    | 473/861 [00:10<00:08, 44.67it/s][A
 56%|█████▌    | 478/861 [00:10<00:08, 44.62it/s][A
 56%|█████▌    | 483/861 [00:10<00:08, 44.57it/s][A
 57%|█████▋    | 488/861 [00:11<00:08, 44.43it/s][A
 57%|█████▋    | 493/861 [00:11<00:08, 44.20it/s][A
 58%|█████▊    | 498/861 [00:11<00:08, 44.08it/s][A
 58%|█████▊    | 503/861 [00:11<00:08, 44.09it/s][A
 59%|█████▉    | 508/861 [00:11<00:07, 44.25it/s][A
 60%|█████▉    | 513/861 [00:11<00:07, 44.48it/s][A
 60%|██████    | 518/861 [00:11<00:07, 44.59it/s][A
 61%|██████    | 523/861 [00:11<00:07, 44.71it/s][A
 61%|██████▏   | 528/861 [00:11<00:07, 44.69it/s][A
 62%|██████▏   | 533/861 [00:12<00:07, 44.51it/s][A
 62%|██████▏   | 538/861 [00:12<00:07, 44.37it/s][A
 63%|██████▎   | 543/861 [00:12<00:07, 44.17it/s][A
 64%|██████▎   | 548/861 [00:12<00:08, 38.24it/s][A
 64%|██████▍   | 553/861 [00:12<00:07, 40.08it/s][A
 65%|██████▍   | 558/861 [00:12<00:07, 41.32it/s][A
 65%|██████▌   | 563/861 [00:12<00:07, 42.38it/s][A
 66%|██████▌   | 568/861 [00:12<00:06, 43.06it/s][A
 67%|██████▋   | 573/861 [00:13<00:06, 43.61it/s][A
 67%|██████▋   | 578/861 [00:13<00:06, 44.02it/s][A
 68%|██████▊   | 583/861 [00:13<00:06, 44.09it/s][A
 68%|██████▊   | 588/861 [00:13<00:06, 43.85it/s][A
 69%|██████▉   | 593/861 [00:13<00:06, 43.68it/s][A
 69%|██████▉   | 598/861 [00:13<00:06, 43.75it/s][A
 70%|███████   | 603/861 [00:13<00:05, 44.01it/s][A
 71%|███████   | 608/861 [00:13<00:05, 44.28it/s][A
 71%|███████   | 613/861 [00:13<00:05, 44.43it/s][A
 72%|███████▏  | 618/861 [00:14<00:05, 44.69it/s][A
 72%|███████▏  | 623/861 [00:14<00:05, 44.67it/s][A
 73%|███████▎  | 628/861 [00:14<00:05, 44.38it/s][A
 74%|███████▎  | 633/861 [00:14<00:05, 44.17it/s][A
 74%|███████▍  | 638/861 [00:14<00:05, 44.01it/s][A
 75%|███████▍  | 643/861 [00:14<00:04, 44.07it/s][A
 75%|███████▌  | 648/861 [00:14<00:04, 44.22it/s][A
 76%|███████▌  | 653/861 [00:14<00:04, 44.42it/s][A
 76%|███████▋  | 658/861 [00:14<00:04, 44.50it/s][A
 77%|███████▋  | 663/861 [00:15<00:04, 44.68it/s][A
 78%|███████▊  | 668/861 [00:15<00:04, 44.66it/s][A
 78%|███████▊  | 673/861 [00:15<00:04, 44.47it/s][A
 79%|███████▊  | 678/861 [00:15<00:04, 44.22it/s][A
 79%|███████▉  | 683/861 [00:15<00:04, 38.16it/s][A
 80%|███████▉  | 688/861 [00:15<00:04, 40.05it/s][A
 80%|████████  | 693/861 [00:15<00:04, 41.40it/s][A
 81%|████████  | 698/861 [00:15<00:03, 42.43it/s][A
 82%|████████▏ | 703/861 [00:16<00:03, 43.22it/s][A
 82%|████████▏ | 708/861 [00:16<00:03, 43.63it/s][A
 83%|████████▎ | 713/861 [00:16<00:03, 44.09it/s][A
 83%|████████▎ | 718/861 [00:16<00:03, 44.16it/s][A
 84%|████████▍ | 723/861 [00:16<00:03, 43.78it/s][A
 85%|████████▍ | 728/861 [00:16<00:03, 43.58it/s][A
 85%|████████▌ | 733/861 [00:16<00:02, 43.75it/s][A
 86%|████████▌ | 738/861 [00:16<00:02, 44.04it/s][A
 86%|████████▋ | 743/861 [00:16<00:02, 44.33it/s][A
 87%|████████▋ | 748/861 [00:17<00:02, 44.46it/s][A
 87%|████████▋ | 753/861 [00:17<00:02, 44.59it/s][A
 88%|████████▊ | 758/861 [00:17<00:02, 44.62it/s][A
 89%|████████▊ | 763/861 [00:17<00:02, 44.48it/s][A
 89%|████████▉ | 768/861 [00:17<00:02, 44.19it/s][A
 90%|████████▉ | 773/861 [00:17<00:02, 43.94it/s][A
 90%|█████████ | 778/861 [00:17<00:01, 44.08it/s][A
 91%|█████████ | 783/861 [00:17<00:01, 44.29it/s][A
 92%|█████████▏| 788/861 [00:17<00:01, 44.44it/s][A
 92%|█████████▏| 793/861 [00:18<00:01, 44.53it/s][A
 93%|█████████▎| 798/861 [00:18<00:01, 44.55it/s][A
 93%|█████████▎| 803/861 [00:18<00:01, 44.56it/s][A
 94%|█████████▍| 808/861 [00:18<00:01, 44.45it/s][A
 94%|█████████▍| 813/861 [00:18<00:01, 44.07it/s][A
 95%|█████████▌| 818/861 [00:18<00:01, 40.49it/s][A
 96%|█████████▌| 823/861 [00:18<00:00, 41.66it/s][A
 96%|█████████▌| 828/861 [00:18<00:00, 42.61it/s][A
 97%|█████████▋| 833/861 [00:18<00:00, 43.36it/s][A
 97%|█████████▋| 838/861 [00:19<00:00, 43.76it/s][A
 98%|█████████▊| 843/861 [00:19<00:00, 44.14it/s][A
 98%|█████████▊| 848/861 [00:19<00:00, 44.28it/s][A
 99%|█████████▉| 853/861 [00:19<00:00, 44.00it/s][A
100%|█████████▉| 858/861 [00:19<00:00, 43.81it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.81it/s][A100%|██████████| 390/390 [04:32<00:00,  3.35it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:16:15,275 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 14:16:15,627 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:16:19,615 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:16:19,921 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:16:20,015 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:16:28,127 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:16:28,165 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78 (score: 0.9576054811477661).
                                                 100%|██████████| 390/390 [04:55<00:00,  3.35it/s]100%|██████████| 390/390 [04:55<00:00,  1.32it/s]
[INFO|trainer.py:1894] 2023-08-28 14:16:38,169 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 14:16:38,286 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:16:41,396 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:16:41,559 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:16:41,655 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:16:42,192 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:16:42,192 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:16:42,192 >>   train_loss               =     0.5773
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:16:42,192 >>   train_runtime            = 0:04:55.15
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:16:42,192 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:16:42,192 >>   train_samples_per_second =       84.7
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:16:42,193 >>   train_steps_per_second   =      1.321
{'eval_loss': 0.9981625080108643, 'eval_runtime': 19.6247, 'eval_samples_per_second': 350.783, 'eval_steps_per_second': 43.873, 'epoch': 4.99}
{'train_runtime': 295.1585, 'train_samples_per_second': 84.7, 'train_steps_per_second': 1.321, 'train_loss': 0.57731691018129, 'epoch': 4.99}
08/28/2023 14:16:42 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:16:42,447 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:16:42,447 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 14:16:42,447 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 55.48it/s]  1%|▏         | 12/861 [00:00<00:17, 48.71it/s]  2%|▏         | 17/861 [00:00<00:17, 47.11it/s]  3%|▎         | 22/861 [00:00<00:18, 46.27it/s]  3%|▎         | 27/861 [00:00<00:18, 44.18it/s]  4%|▎         | 32/861 [00:00<00:25, 32.69it/s]  4%|▍         | 37/861 [00:00<00:22, 36.09it/s]  5%|▍         | 42/861 [00:01<00:21, 38.48it/s]  5%|▌         | 47/861 [00:01<00:20, 40.29it/s]  6%|▌         | 52/861 [00:01<00:19, 41.62it/s]  7%|▋         | 57/861 [00:01<00:18, 42.61it/s]  7%|▋         | 62/861 [00:01<00:18, 43.31it/s]  8%|▊         | 67/861 [00:01<00:18, 43.81it/s]  8%|▊         | 72/861 [00:01<00:18, 43.71it/s]  9%|▉         | 77/861 [00:01<00:17, 43.60it/s] 10%|▉         | 82/861 [00:01<00:17, 43.64it/s] 10%|█         | 87/861 [00:02<00:18, 42.76it/s] 11%|█         | 92/861 [00:02<00:17, 43.43it/s] 11%|█▏        | 97/861 [00:02<00:17, 43.81it/s] 12%|█▏        | 102/861 [00:02<00:17, 44.24it/s] 12%|█▏        | 107/861 [00:02<00:16, 44.42it/s] 13%|█▎        | 112/861 [00:02<00:16, 44.54it/s] 14%|█▎        | 117/861 [00:02<00:16, 44.39it/s] 14%|█▍        | 122/861 [00:02<00:16, 44.21it/s] 15%|█▍        | 127/861 [00:02<00:16, 43.98it/s] 15%|█▌        | 132/861 [00:03<00:16, 44.16it/s] 16%|█▌        | 137/861 [00:03<00:16, 44.33it/s] 16%|█▋        | 142/861 [00:03<00:16, 44.51it/s] 17%|█▋        | 147/861 [00:03<00:15, 44.68it/s] 18%|█▊        | 152/861 [00:03<00:15, 44.70it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.66it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.48it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.24it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.07it/s] 21%|██        | 177/861 [00:04<00:15, 44.12it/s] 21%|██        | 182/861 [00:04<00:15, 44.32it/s] 22%|██▏       | 187/861 [00:04<00:15, 44.51it/s] 22%|██▏       | 192/861 [00:04<00:15, 41.90it/s] 23%|██▎       | 197/861 [00:04<00:15, 42.82it/s] 23%|██▎       | 202/861 [00:04<00:15, 43.41it/s] 24%|██▍       | 207/861 [00:04<00:14, 43.82it/s] 25%|██▍       | 212/861 [00:04<00:14, 43.96it/s] 25%|██▌       | 217/861 [00:05<00:14, 43.99it/s] 26%|██▌       | 222/861 [00:05<00:14, 44.15it/s] 26%|██▋       | 227/861 [00:05<00:14, 44.16it/s] 27%|██▋       | 232/861 [00:05<00:14, 43.98it/s] 28%|██▊       | 237/861 [00:05<00:14, 44.13it/s] 28%|██▊       | 242/861 [00:05<00:13, 44.33it/s] 29%|██▊       | 247/861 [00:05<00:13, 44.54it/s] 29%|██▉       | 252/861 [00:05<00:13, 44.50it/s] 30%|██▉       | 257/861 [00:05<00:13, 44.46it/s] 30%|███       | 262/861 [00:06<00:13, 44.48it/s] 31%|███       | 267/861 [00:06<00:13, 44.40it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.33it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.22it/s] 33%|███▎      | 282/861 [00:06<00:13, 44.31it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.43it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.58it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.52it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.53it/s] 36%|███▌      | 307/861 [00:07<00:12, 44.38it/s] 36%|███▌      | 312/861 [00:07<00:12, 44.34it/s] 37%|███▋      | 317/861 [00:07<00:12, 44.25it/s] 37%|███▋      | 322/861 [00:07<00:12, 43.62it/s] 38%|███▊      | 327/861 [00:07<00:12, 43.95it/s] 39%|███▊      | 332/861 [00:07<00:12, 44.04it/s] 39%|███▉      | 337/861 [00:07<00:11, 44.31it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.31it/s] 40%|████      | 347/861 [00:07<00:11, 44.39it/s] 41%|████      | 352/861 [00:08<00:11, 44.34it/s] 41%|████▏     | 357/861 [00:08<00:11, 44.28it/s] 42%|████▏     | 362/861 [00:08<00:12, 39.48it/s] 43%|████▎     | 367/861 [00:08<00:12, 41.07it/s] 43%|████▎     | 372/861 [00:08<00:11, 42.16it/s] 44%|████▍     | 377/861 [00:08<00:11, 43.00it/s] 44%|████▍     | 382/861 [00:08<00:10, 43.62it/s] 45%|████▍     | 387/861 [00:08<00:10, 43.96it/s] 46%|████▌     | 392/861 [00:08<00:10, 44.25it/s] 46%|████▌     | 397/861 [00:09<00:10, 44.25it/s] 47%|████▋     | 402/861 [00:09<00:10, 43.94it/s] 47%|████▋     | 407/861 [00:09<00:10, 43.71it/s] 48%|████▊     | 412/861 [00:09<00:10, 43.78it/s] 48%|████▊     | 417/861 [00:09<00:10, 43.81it/s] 49%|████▉     | 422/861 [00:09<00:09, 44.21it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.43it/s] 50%|█████     | 432/861 [00:09<00:09, 44.56it/s] 51%|█████     | 437/861 [00:10<00:09, 44.62it/s] 51%|█████▏    | 442/861 [00:10<00:09, 44.41it/s] 52%|█████▏    | 447/861 [00:10<00:09, 44.17it/s] 52%|█████▏    | 452/861 [00:10<00:09, 44.05it/s] 53%|█████▎    | 457/861 [00:10<00:09, 42.12it/s] 54%|█████▎    | 462/861 [00:10<00:09, 42.98it/s] 54%|█████▍    | 467/861 [00:10<00:09, 43.44it/s] 55%|█████▍    | 472/861 [00:10<00:08, 43.88it/s] 55%|█████▌    | 477/861 [00:10<00:08, 44.27it/s] 56%|█████▌    | 482/861 [00:11<00:08, 44.34it/s] 57%|█████▋    | 487/861 [00:11<00:08, 44.23it/s] 57%|█████▋    | 492/861 [00:11<00:08, 43.97it/s] 58%|█████▊    | 497/861 [00:11<00:08, 43.79it/s] 58%|█████▊    | 502/861 [00:11<00:08, 43.97it/s] 59%|█████▉    | 507/861 [00:11<00:08, 44.11it/s] 59%|█████▉    | 512/861 [00:11<00:07, 44.30it/s] 60%|██████    | 517/861 [00:11<00:07, 44.52it/s] 61%|██████    | 522/861 [00:11<00:07, 44.57it/s] 61%|██████    | 527/861 [00:12<00:07, 44.64it/s] 62%|██████▏   | 532/861 [00:12<00:07, 44.49it/s] 62%|██████▏   | 537/861 [00:12<00:07, 44.22it/s] 63%|██████▎   | 542/861 [00:12<00:07, 44.00it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.04it/s] 64%|██████▍   | 552/861 [00:12<00:06, 44.19it/s] 65%|██████▍   | 557/861 [00:12<00:06, 44.45it/s] 65%|██████▌   | 562/861 [00:12<00:06, 44.60it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.68it/s] 66%|██████▋   | 572/861 [00:13<00:06, 44.61it/s] 67%|██████▋   | 577/861 [00:13<00:06, 44.38it/s] 68%|██████▊   | 582/861 [00:13<00:06, 44.13it/s] 68%|██████▊   | 587/861 [00:13<00:06, 44.09it/s] 69%|██████▉   | 592/861 [00:13<00:06, 41.84it/s] 69%|██████▉   | 597/861 [00:13<00:06, 42.77it/s] 70%|██████▉   | 602/861 [00:13<00:05, 43.37it/s] 70%|███████   | 607/861 [00:13<00:05, 43.90it/s] 71%|███████   | 612/861 [00:13<00:05, 44.20it/s] 72%|███████▏  | 617/861 [00:14<00:05, 44.24it/s] 72%|███████▏  | 622/861 [00:14<00:05, 44.20it/s] 73%|███████▎  | 627/861 [00:14<00:05, 43.95it/s] 73%|███████▎  | 632/861 [00:14<00:05, 43.67it/s] 74%|███████▍  | 637/861 [00:14<00:05, 43.88it/s] 75%|███████▍  | 642/861 [00:14<00:04, 44.18it/s] 75%|███████▌  | 647/861 [00:14<00:04, 44.44it/s] 76%|███████▌  | 652/861 [00:14<00:04, 44.53it/s] 76%|███████▋  | 657/861 [00:15<00:04, 44.65it/s] 77%|███████▋  | 662/861 [00:15<00:04, 44.65it/s] 77%|███████▋  | 667/861 [00:15<00:04, 44.37it/s] 78%|███████▊  | 672/861 [00:15<00:04, 44.06it/s] 79%|███████▊  | 677/861 [00:15<00:04, 43.83it/s] 79%|███████▉  | 682/861 [00:15<00:04, 43.96it/s] 80%|███████▉  | 687/861 [00:15<00:03, 44.16it/s] 80%|████████  | 692/861 [00:15<00:03, 44.26it/s] 81%|████████  | 697/861 [00:15<00:03, 44.50it/s] 82%|████████▏ | 702/861 [00:16<00:03, 44.65it/s] 82%|████████▏ | 707/861 [00:16<00:03, 44.60it/s] 83%|████████▎ | 712/861 [00:16<00:03, 44.47it/s] 83%|████████▎ | 717/861 [00:16<00:03, 44.08it/s] 84%|████████▍ | 722/861 [00:16<00:03, 44.00it/s] 84%|████████▍ | 727/861 [00:16<00:03, 41.90it/s] 85%|████████▌ | 732/861 [00:16<00:03, 42.74it/s] 86%|████████▌ | 737/861 [00:16<00:02, 43.38it/s] 86%|████████▌ | 742/861 [00:16<00:02, 43.80it/s] 87%|████████▋ | 747/861 [00:17<00:02, 41.54it/s] 87%|████████▋ | 752/861 [00:17<00:02, 43.74it/s] 88%|████████▊ | 757/861 [00:17<00:02, 43.89it/s] 89%|████████▊ | 762/861 [00:17<00:02, 43.75it/s] 89%|████████▉ | 767/861 [00:17<00:02, 43.53it/s] 90%|████████▉ | 772/861 [00:17<00:02, 43.83it/s] 90%|█████████ | 777/861 [00:17<00:01, 44.13it/s] 91%|█████████ | 782/861 [00:17<00:01, 44.40it/s] 91%|█████████▏| 787/861 [00:17<00:01, 44.52it/s] 92%|█████████▏| 792/861 [00:18<00:01, 44.39it/s] 93%|█████████▎| 797/861 [00:18<00:01, 44.43it/s] 93%|█████████▎| 802/861 [00:18<00:01, 44.31it/s] 94%|█████████▎| 807/861 [00:18<00:01, 44.09it/s] 94%|█████████▍| 812/861 [00:18<00:01, 43.92it/s] 95%|█████████▍| 817/861 [00:18<00:01, 43.98it/s] 95%|█████████▌| 822/861 [00:18<00:00, 44.22it/s] 96%|█████████▌| 827/861 [00:18<00:00, 44.33it/s] 97%|█████████▋| 832/861 [00:18<00:00, 44.44it/s] 97%|█████████▋| 837/861 [00:19<00:00, 44.44it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.40it/s] 98%|█████████▊| 847/861 [00:19<00:00, 44.33it/s] 99%|█████████▉| 852/861 [00:19<00:00, 44.14it/s]100%|█████████▉| 857/861 [00:19<00:00, 44.02it/s]100%|██████████| 861/861 [00:19<00:00, 43.81it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:17:02,118 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:17:02,118 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:17:02,118 >>   eval_loss               =     0.9576
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:17:02,118 >>   eval_runtime            = 0:00:19.67
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:17:02,118 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:17:02,118 >>   eval_samples_per_second =    349.959
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:17:02,118 >>   eval_steps_per_second   =      43.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:17:02,118 >>   perplexity              =     2.6055
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:12,090 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:12,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:12,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:12,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:12,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:17:12,854 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:17:12,855 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:17:13,492 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:17:14,566 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:17:14,566 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:17,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:17,702 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:17,702 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:17,702 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:17,702 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:17:18,362 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:17:18,364 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:17:18,986 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:17:19,147 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:17:19,147 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.61it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:11,  1.60it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:13,  1.58it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:16,  1.59it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.57it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.57it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:25,  1.57it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.57it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.60it/s]Extractor Predicting: 46it [00:29,  1.62it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:30,  1.57it/s]Extractor Predicting: 49it [00:31,  1.56it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:32,  1.55it/s]Extractor Predicting: 52it [00:33,  1.54it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:34,  1.50it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:36,  1.52it/s]Extractor Predicting: 57it [00:36,  1.55it/s]Extractor Predicting: 58it [00:37,  1.54it/s]Extractor Predicting: 59it [00:38,  1.40it/s]Extractor Predicting: 60it [00:39,  1.43it/s]Extractor Predicting: 61it [00:39,  1.48it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.46it/s]Extractor Predicting: 64it [00:41,  1.48it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.52it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:44,  1.53it/s]Extractor Predicting: 69it [00:44,  1.53it/s]Extractor Predicting: 70it [00:45,  1.51it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:46,  1.51it/s]Extractor Predicting: 73it [00:47,  1.47it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:49,  1.52it/s]Extractor Predicting: 77it [00:50,  1.51it/s]Extractor Predicting: 78it [00:51,  1.46it/s]Extractor Predicting: 79it [00:51,  1.52it/s]Extractor Predicting: 80it [00:52,  1.56it/s]Extractor Predicting: 81it [00:52,  1.59it/s]Extractor Predicting: 82it [00:53,  1.66it/s]Extractor Predicting: 83it [00:54,  1.56it/s]Extractor Predicting: 84it [00:54,  1.59it/s]Extractor Predicting: 85it [00:55,  1.62it/s]Extractor Predicting: 86it [00:55,  1.64it/s]Extractor Predicting: 87it [00:56,  1.63it/s]Extractor Predicting: 88it [00:57,  1.64it/s]Extractor Predicting: 89it [00:57,  1.63it/s]Extractor Predicting: 90it [00:58,  1.67it/s]Extractor Predicting: 91it [00:59,  1.50it/s]Extractor Predicting: 92it [00:59,  1.58it/s]Extractor Predicting: 93it [01:00,  1.58it/s]Extractor Predicting: 94it [01:00,  1.62it/s]Extractor Predicting: 95it [01:01,  1.68it/s]Extractor Predicting: 96it [01:02,  1.58it/s]Extractor Predicting: 97it [01:02,  1.65it/s]Extractor Predicting: 98it [01:03,  1.66it/s]Extractor Predicting: 99it [01:03,  1.68it/s]Extractor Predicting: 100it [01:04,  1.68it/s]Extractor Predicting: 101it [01:05,  1.59it/s]Extractor Predicting: 102it [01:05,  1.60it/s]Extractor Predicting: 103it [01:06,  1.63it/s]Extractor Predicting: 104it [01:06,  1.64it/s]Extractor Predicting: 105it [01:07,  1.65it/s]Extractor Predicting: 106it [01:08,  1.59it/s]Extractor Predicting: 107it [01:08,  1.59it/s]Extractor Predicting: 108it [01:09,  1.66it/s]Extractor Predicting: 109it [01:09,  1.66it/s]Extractor Predicting: 110it [01:10,  1.69it/s]Extractor Predicting: 111it [01:11,  1.68it/s]Extractor Predicting: 112it [01:11,  1.65it/s]Extractor Predicting: 113it [01:12,  1.65it/s]Extractor Predicting: 114it [01:13,  1.63it/s]Extractor Predicting: 115it [01:13,  1.68it/s]Extractor Predicting: 116it [01:14,  1.64it/s]Extractor Predicting: 117it [01:14,  1.63it/s]Extractor Predicting: 118it [01:15,  1.63it/s]Extractor Predicting: 119it [01:16,  1.67it/s]Extractor Predicting: 120it [01:16,  1.70it/s]Extractor Predicting: 121it [01:17,  1.67it/s]Extractor Predicting: 122it [01:17,  1.62it/s]Extractor Predicting: 123it [01:18,  1.68it/s]Extractor Predicting: 124it [01:19,  1.65it/s]Extractor Predicting: 125it [01:19,  1.69it/s]Extractor Predicting: 126it [01:20,  1.68it/s]Extractor Predicting: 127it [01:20,  1.70it/s]Extractor Predicting: 128it [01:21,  1.64it/s]Extractor Predicting: 129it [01:21,  1.68it/s]Extractor Predicting: 130it [01:22,  1.69it/s]Extractor Predicting: 131it [01:23,  1.70it/s]Extractor Predicting: 132it [01:23,  1.71it/s]Extractor Predicting: 133it [01:24,  1.71it/s]Extractor Predicting: 134it [01:24,  1.70it/s]Extractor Predicting: 135it [01:25,  1.71it/s]Extractor Predicting: 136it [01:26,  1.71it/s]Extractor Predicting: 137it [01:26,  1.68it/s]Extractor Predicting: 138it [01:27,  1.68it/s]Extractor Predicting: 139it [01:27,  1.65it/s]Extractor Predicting: 140it [01:28,  1.64it/s]Extractor Predicting: 141it [01:29,  1.66it/s]Extractor Predicting: 142it [01:29,  1.64it/s]Extractor Predicting: 143it [01:30,  1.68it/s]Extractor Predicting: 144it [01:30,  1.64it/s]Extractor Predicting: 145it [01:31,  1.66it/s]Extractor Predicting: 146it [01:32,  1.67it/s]Extractor Predicting: 147it [01:32,  1.65it/s]Extractor Predicting: 148it [01:33,  1.66it/s]Extractor Predicting: 149it [01:33,  1.71it/s]Extractor Predicting: 150it [01:34,  1.63it/s]Extractor Predicting: 151it [01:35,  1.58it/s]Extractor Predicting: 152it [01:35,  1.53it/s]Extractor Predicting: 153it [01:36,  1.50it/s]Extractor Predicting: 154it [01:37,  1.43it/s]Extractor Predicting: 155it [01:38,  1.46it/s]Extractor Predicting: 156it [01:38,  1.43it/s]Extractor Predicting: 157it [01:39,  1.44it/s]Extractor Predicting: 158it [01:40,  1.44it/s]Extractor Predicting: 159it [01:41,  1.31it/s]Extractor Predicting: 160it [01:41,  1.33it/s]Extractor Predicting: 161it [01:42,  1.36it/s]Extractor Predicting: 162it [01:43,  1.35it/s]Extractor Predicting: 163it [01:44,  1.36it/s]Extractor Predicting: 164it [01:44,  1.37it/s]Extractor Predicting: 165it [01:45,  1.35it/s]Extractor Predicting: 166it [01:46,  1.34it/s]Extractor Predicting: 167it [01:46,  1.36it/s]Extractor Predicting: 168it [01:47,  1.37it/s]Extractor Predicting: 169it [01:48,  1.36it/s]Extractor Predicting: 170it [01:49,  1.37it/s]Extractor Predicting: 171it [01:49,  1.35it/s]Extractor Predicting: 172it [01:50,  1.34it/s]Extractor Predicting: 173it [01:51,  1.37it/s]Extractor Predicting: 174it [01:52,  1.42it/s]Extractor Predicting: 175it [01:52,  1.41it/s]Extractor Predicting: 176it [01:54,  1.10it/s]Extractor Predicting: 177it [01:54,  1.19it/s]Extractor Predicting: 178it [01:55,  1.24it/s]Extractor Predicting: 179it [01:56,  1.28it/s]Extractor Predicting: 180it [01:57,  1.27it/s]Extractor Predicting: 181it [01:57,  1.30it/s]Extractor Predicting: 182it [01:58,  1.33it/s]Extractor Predicting: 183it [01:59,  1.37it/s]Extractor Predicting: 184it [01:59,  1.40it/s]Extractor Predicting: 185it [02:00,  1.41it/s]Extractor Predicting: 186it [02:01,  1.43it/s]Extractor Predicting: 187it [02:01,  1.41it/s]Extractor Predicting: 188it [02:02,  1.45it/s]Extractor Predicting: 189it [02:03,  1.40it/s]Extractor Predicting: 190it [02:04,  1.38it/s]Extractor Predicting: 191it [02:04,  1.36it/s]Extractor Predicting: 192it [02:05,  1.33it/s]Extractor Predicting: 193it [02:06,  1.37it/s]Extractor Predicting: 194it [02:07,  1.40it/s]Extractor Predicting: 195it [02:07,  1.40it/s]Extractor Predicting: 196it [02:08,  1.43it/s]Extractor Predicting: 197it [02:09,  1.45it/s]Extractor Predicting: 198it [02:09,  1.47it/s]Extractor Predicting: 199it [02:10,  1.48it/s]Extractor Predicting: 200it [02:11,  1.48it/s]Extractor Predicting: 201it [02:11,  1.43it/s]Extractor Predicting: 202it [02:12,  1.44it/s]Extractor Predicting: 203it [02:13,  1.47it/s]Extractor Predicting: 204it [02:13,  1.47it/s]Extractor Predicting: 205it [02:14,  1.50it/s]Extractor Predicting: 206it [02:15,  1.47it/s]Extractor Predicting: 207it [02:15,  1.49it/s]Extractor Predicting: 208it [02:16,  1.49it/s]Extractor Predicting: 209it [02:17,  1.54it/s]Extractor Predicting: 210it [02:17,  1.51it/s]Extractor Predicting: 211it [02:18,  1.46it/s]Extractor Predicting: 212it [02:19,  1.48it/s]Extractor Predicting: 213it [02:19,  1.49it/s]Extractor Predicting: 214it [02:20,  1.51it/s]Extractor Predicting: 215it [02:21,  1.51it/s]Extractor Predicting: 216it [02:21,  1.52it/s]Extractor Predicting: 217it [02:22,  1.51it/s]Extractor Predicting: 218it [02:23,  1.55it/s]Extractor Predicting: 219it [02:23,  1.50it/s]Extractor Predicting: 220it [02:24,  1.47it/s]Extractor Predicting: 221it [02:25,  1.50it/s]Extractor Predicting: 222it [02:25,  1.51it/s]Extractor Predicting: 223it [02:26,  1.53it/s]Extractor Predicting: 224it [02:27,  1.52it/s]Extractor Predicting: 225it [02:27,  1.50it/s]Extractor Predicting: 226it [02:28,  1.51it/s]Extractor Predicting: 227it [02:29,  1.50it/s]Extractor Predicting: 228it [02:29,  1.51it/s]Extractor Predicting: 229it [02:30,  1.51it/s]Extractor Predicting: 230it [02:31,  1.50it/s]Extractor Predicting: 231it [02:31,  1.54it/s]Extractor Predicting: 232it [02:32,  1.56it/s]Extractor Predicting: 233it [02:33,  1.53it/s]Extractor Predicting: 234it [02:33,  1.52it/s]Extractor Predicting: 235it [02:34,  1.50it/s]Extractor Predicting: 236it [02:35,  1.52it/s]Extractor Predicting: 237it [02:35,  1.46it/s]Extractor Predicting: 237it [02:35,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:08,752 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:08,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:08,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:08,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:08,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:20:09,469 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:20:09,470 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:20:10,098 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:20:11,171 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:20:11,171 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:14,305 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:14,381 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:14,381 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:14,381 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:14,381 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:20:15,124 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:20:15,125 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:20:15,770 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:20:15,945 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:20:15,945 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.460446247464503,
  "recall": 0.03297501452643812,
  "score": 0.06154263250643893,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.73it/s]Extractor Predicting: 8it [00:04,  1.75it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:08,  1.64it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:11,  1.68it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:13,  1.73it/s]Extractor Predicting: 23it [00:13,  1.76it/s]Extractor Predicting: 24it [00:14,  1.75it/s]Extractor Predicting: 25it [00:14,  1.75it/s]Extractor Predicting: 26it [00:15,  1.76it/s]Extractor Predicting: 27it [00:15,  1.74it/s]Extractor Predicting: 28it [00:16,  1.73it/s]Extractor Predicting: 29it [00:17,  1.71it/s]Extractor Predicting: 30it [00:17,  1.69it/s]Extractor Predicting: 31it [00:18,  1.70it/s]Extractor Predicting: 32it [00:18,  1.72it/s]Extractor Predicting: 33it [00:19,  1.67it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:20,  1.66it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:21,  1.73it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.62it/s]Extractor Predicting: 40it [00:23,  1.61it/s]Extractor Predicting: 41it [00:24,  1.54it/s]Extractor Predicting: 42it [00:25,  1.55it/s]Extractor Predicting: 43it [00:25,  1.58it/s]Extractor Predicting: 44it [00:26,  1.58it/s]Extractor Predicting: 45it [00:27,  1.58it/s]Extractor Predicting: 46it [00:27,  1.54it/s]Extractor Predicting: 47it [00:28,  1.55it/s]Extractor Predicting: 48it [00:28,  1.57it/s]Extractor Predicting: 49it [00:29,  1.56it/s]Extractor Predicting: 50it [00:30,  1.56it/s]Extractor Predicting: 51it [00:30,  1.58it/s]Extractor Predicting: 52it [00:31,  1.58it/s]Extractor Predicting: 53it [00:32,  1.59it/s]Extractor Predicting: 54it [00:32,  1.58it/s]Extractor Predicting: 55it [00:33,  1.58it/s]Extractor Predicting: 56it [00:33,  1.61it/s]Extractor Predicting: 57it [00:34,  1.57it/s]Extractor Predicting: 58it [00:35,  1.59it/s]Extractor Predicting: 59it [00:35,  1.58it/s]Extractor Predicting: 60it [00:36,  1.56it/s]Extractor Predicting: 61it [00:37,  1.54it/s]Extractor Predicting: 62it [00:37,  1.58it/s]Extractor Predicting: 63it [00:38,  1.60it/s]Extractor Predicting: 64it [00:39,  1.63it/s]Extractor Predicting: 65it [00:39,  1.61it/s]Extractor Predicting: 66it [00:40,  1.62it/s]Extractor Predicting: 67it [00:40,  1.62it/s]Extractor Predicting: 68it [00:41,  1.61it/s]Extractor Predicting: 69it [00:42,  1.61it/s]Extractor Predicting: 70it [00:42,  1.60it/s]Extractor Predicting: 71it [00:43,  1.61it/s]Extractor Predicting: 72it [00:44,  1.61it/s]Extractor Predicting: 73it [00:44,  1.57it/s]Extractor Predicting: 74it [00:45,  1.60it/s]Extractor Predicting: 75it [00:45,  1.60it/s]Extractor Predicting: 76it [00:46,  1.58it/s]Extractor Predicting: 77it [00:47,  1.59it/s]Extractor Predicting: 78it [00:47,  1.59it/s]Extractor Predicting: 79it [00:48,  1.60it/s]Extractor Predicting: 80it [00:49,  1.62it/s]Extractor Predicting: 81it [00:49,  1.60it/s]Extractor Predicting: 82it [00:50,  1.55it/s]Extractor Predicting: 83it [00:50,  1.60it/s]Extractor Predicting: 84it [00:51,  1.59it/s]Extractor Predicting: 85it [00:52,  1.57it/s]Extractor Predicting: 86it [00:52,  1.60it/s]Extractor Predicting: 87it [00:53,  1.62it/s]Extractor Predicting: 88it [00:54,  1.58it/s]Extractor Predicting: 89it [00:54,  1.57it/s]Extractor Predicting: 90it [00:55,  1.52it/s]Extractor Predicting: 91it [00:56,  1.53it/s]Extractor Predicting: 92it [00:56,  1.57it/s]Extractor Predicting: 93it [00:57,  1.62it/s]Extractor Predicting: 94it [00:57,  1.57it/s]Extractor Predicting: 95it [00:58,  1.57it/s]Extractor Predicting: 96it [00:59,  1.61it/s]Extractor Predicting: 97it [00:59,  1.60it/s]Extractor Predicting: 98it [01:00,  1.58it/s]Extractor Predicting: 99it [01:01,  1.60it/s]Extractor Predicting: 100it [01:01,  1.51it/s]Extractor Predicting: 101it [01:02,  1.58it/s]Extractor Predicting: 102it [01:03,  1.57it/s]Extractor Predicting: 103it [01:03,  1.45it/s]Extractor Predicting: 104it [01:04,  1.52it/s]Extractor Predicting: 105it [01:05,  1.54it/s]Extractor Predicting: 106it [01:05,  1.56it/s]Extractor Predicting: 107it [01:06,  1.60it/s]Extractor Predicting: 108it [01:06,  1.64it/s]Extractor Predicting: 109it [01:07,  1.60it/s]Extractor Predicting: 110it [01:08,  1.60it/s]Extractor Predicting: 111it [01:08,  1.59it/s]Extractor Predicting: 112it [01:09,  1.59it/s]Extractor Predicting: 113it [01:09,  1.64it/s]Extractor Predicting: 114it [01:10,  1.58it/s]Extractor Predicting: 115it [01:11,  1.57it/s]Extractor Predicting: 116it [01:11,  1.61it/s]Extractor Predicting: 117it [01:12,  1.61it/s]Extractor Predicting: 118it [01:13,  1.64it/s]Extractor Predicting: 119it [01:13,  1.63it/s]Extractor Predicting: 120it [01:14,  1.62it/s]Extractor Predicting: 121it [01:15,  1.57it/s]Extractor Predicting: 122it [01:15,  1.53it/s]Extractor Predicting: 123it [01:16,  1.56it/s]Extractor Predicting: 124it [01:16,  1.59it/s]Extractor Predicting: 125it [01:17,  1.62it/s]Extractor Predicting: 126it [01:18,  1.63it/s]Extractor Predicting: 127it [01:18,  1.60it/s]Extractor Predicting: 128it [01:19,  1.64it/s]Extractor Predicting: 129it [01:19,  1.64it/s]Extractor Predicting: 130it [01:20,  1.66it/s]Extractor Predicting: 131it [01:21,  1.65it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.58it/s]Extractor Predicting: 134it [01:23,  1.62it/s]Extractor Predicting: 135it [01:23,  1.62it/s]Extractor Predicting: 136it [01:24,  1.63it/s]Extractor Predicting: 137it [01:24,  1.59it/s]Extractor Predicting: 138it [01:25,  1.58it/s]Extractor Predicting: 139it [01:26,  1.57it/s]Extractor Predicting: 140it [01:26,  1.63it/s]Extractor Predicting: 141it [01:27,  1.62it/s]Extractor Predicting: 142it [01:28,  1.57it/s]Extractor Predicting: 143it [01:28,  1.56it/s]Extractor Predicting: 144it [01:29,  1.56it/s]Extractor Predicting: 145it [01:29,  1.58it/s]Extractor Predicting: 146it [01:30,  1.57it/s]Extractor Predicting: 147it [01:31,  1.55it/s]Extractor Predicting: 148it [01:32,  1.50it/s]Extractor Predicting: 149it [01:32,  1.49it/s]Extractor Predicting: 150it [01:33,  1.52it/s]Extractor Predicting: 151it [01:33,  1.55it/s]Extractor Predicting: 152it [01:34,  1.52it/s]Extractor Predicting: 153it [01:35,  1.54it/s]Extractor Predicting: 154it [01:35,  1.51it/s]Extractor Predicting: 155it [01:36,  1.53it/s]Extractor Predicting: 156it [01:37,  1.55it/s]Extractor Predicting: 157it [01:37,  1.50it/s]Extractor Predicting: 158it [01:38,  1.50it/s]Extractor Predicting: 159it [01:39,  1.50it/s]Extractor Predicting: 160it [01:39,  1.54it/s]Extractor Predicting: 161it [01:40,  1.56it/s]Extractor Predicting: 162it [01:41,  1.52it/s]Extractor Predicting: 163it [01:41,  1.49it/s]Extractor Predicting: 164it [01:42,  1.49it/s]Extractor Predicting: 165it [01:43,  1.52it/s]Extractor Predicting: 166it [01:43,  1.50it/s]Extractor Predicting: 167it [01:44,  1.52it/s]Extractor Predicting: 168it [01:45,  1.64it/s]Extractor Predicting: 168it [01:45,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:14,180 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:14,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:14,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:14,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:14,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:22:15,031 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:22:15,032 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:22:15,748 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:22:16,927 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:22:16,928 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:20,424 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:20,427 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:20,427 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:20,427 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:20,427 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:22:21,501 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:22:21,502 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:22:22,237 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:22:22,594 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:22:22,676 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4090909090909091,
  "recall": 0.04249502982107356,
  "score": 0.0769923457901846,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:08,  1.53it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:10,  1.47it/s]Extractor Predicting: 18it [00:11,  1.47it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 19it [00:12,  1.56it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:22:38,910 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:22:38,963 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:22:39,072 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:22:39,074 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:22:39,124 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:22:53,064 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:22:53,104 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:22:53,295 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:22:53,296 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:22:53,483 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:22:53,554 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:22:53,554 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:22:53,554 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:22:53,554 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:22:53,554 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:22:53,554 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5833333333333334,
  "recall": 0.006756756756756757,
  "score": 0.013358778625954198,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:22:53,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:54,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:55,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:55,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:56,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:57,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:57,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:58,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:58,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:59,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:00,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:00,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:01,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:01,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:02,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:03,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:03,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:04,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:04,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:05,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:11<01:47, 11.90s/it][WARNING|generation_utils.py:914] 2023-08-28 14:23:05,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:06,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:07,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:07,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:08,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:09,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:09,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:10,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:11,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:11,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:12,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:12,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:13,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:14,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:14,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:15,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:16,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:16,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:17,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:18,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:18,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:19,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:20,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:20,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:21,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:21,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:28<01:56, 14.62s/it][WARNING|generation_utils.py:914] 2023-08-28 14:23:22,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:23,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:23,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:24,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:25,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:25,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:26,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:27,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:27,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:28,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:29,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:29,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:30,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:31,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:31,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:32,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:32,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:33,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:34,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:34,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:35,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:41<01:38, 14.11s/it][WARNING|generation_utils.py:914] 2023-08-28 14:23:35,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:36,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:37,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:37,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:38,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:38,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:39,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:40,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:40,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:41,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:41,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:42,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:43,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:43,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:44,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:44,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:45,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:46,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:46,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:47,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:48,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:55<01:22, 13.75s/it][WARNING|generation_utils.py:914] 2023-08-28 14:23:49,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:49,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:50,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:50,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:51,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:52,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:52,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:53,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:53,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:54,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:55,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:55,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:56,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:56,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:57,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:57,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:58,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:58,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:59,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:00,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:06<01:04, 12.95s/it][WARNING|generation_utils.py:914] 2023-08-28 14:24:00,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:01,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:01,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:02,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:02,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:03,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:04,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:04,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:05,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:05,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:06,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:07,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:07,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:08,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:09,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:09,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:10,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:10,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:11,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:12,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:12,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:19<00:51, 12.80s/it][WARNING|generation_utils.py:914] 2023-08-28 14:24:13,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:13,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:14,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:14,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:15,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:16,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:16,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:17,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:18,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:18,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:19,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:19,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:20,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:21,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:21,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:22,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:23,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:23,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:24,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:24,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:25,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:26,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:32<00:39, 13.09s/it][WARNING|generation_utils.py:914] 2023-08-28 14:24:26,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:27,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:27,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:28,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:29,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:29,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:30,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:31,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:31,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:32,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:32,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:33,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:34,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:35,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:35,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:36,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:36,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:37,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:37,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:38,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:39,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:45<00:26, 13.02s/it][WARNING|generation_utils.py:914] 2023-08-28 14:24:39,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:40,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:40,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:41,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:42,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:42,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:43,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:44,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:44,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:45,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:45,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:46,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:46,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:47,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:48,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:48,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:49,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:50,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:50,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:57<00:12, 12.58s/it][WARNING|generation_utils.py:914] 2023-08-28 14:24:51,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:52,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:52,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:53,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:54,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:54,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:55,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:56,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:57,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:57,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:58,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:58,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:59,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:25:00,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:25:01,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:25:01,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:25:02,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:25:03,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:25:03,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:25:04,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:25:05,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:11<00:00, 13.21s/it]Generating: 100%|██████████| 10/10 [02:11<00:00, 13.20s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:13,731 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:13,751 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:13,751 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:13,751 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:13,751 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:25:14,575 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:25:14,576 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:25:15,210 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:25:16,379 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:25:16,379 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:19,490 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:19,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:19,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:19,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:19,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:25:20,560 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:25:20,561 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:25:21,237 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:25:21,530 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:25:21,530 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 450, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 494, 'raw': 672}
{'target': 600, 'success': 521, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 593, 'raw': 800}
{'target': 600, 'success': 618, 'raw': 832}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.7427884615384616, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 630, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 630, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8948863636363636, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9002976190476191, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 411, 'raw': 416}
{'target': 600, 'success': 443, 'raw': 448}
{'target': 600, 'success': 475, 'raw': 480}
{'target': 600, 'success': 505, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 569, 'raw': 576}
{'target': 600, 'success': 601, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9884868421052632, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 9142
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9242, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.56it/s]Extractor Estimating: 2it [00:01,  1.58it/s]Extractor Estimating: 3it [00:01,  1.63it/s]Extractor Estimating: 4it [00:02,  1.67it/s]Extractor Estimating: 5it [00:02,  1.72it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.69it/s]Extractor Estimating: 8it [00:04,  1.68it/s]Extractor Estimating: 9it [00:05,  1.70it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.64it/s]Extractor Estimating: 12it [00:07,  1.68it/s]Extractor Estimating: 13it [00:07,  1.65it/s]Extractor Estimating: 14it [00:08,  1.71it/s]Extractor Estimating: 15it [00:08,  1.71it/s]Extractor Estimating: 16it [00:09,  1.70it/s]Extractor Estimating: 17it [00:10,  1.69it/s]Extractor Estimating: 18it [00:10,  1.68it/s]Extractor Estimating: 19it [00:11,  1.70it/s]Extractor Estimating: 20it [00:11,  1.67it/s]Extractor Estimating: 21it [00:12,  1.71it/s]Extractor Estimating: 22it [00:13,  1.71it/s]Extractor Estimating: 23it [00:13,  1.70it/s]Extractor Estimating: 24it [00:14,  1.68it/s]Extractor Estimating: 25it [00:14,  1.67it/s]Extractor Estimating: 26it [00:15,  1.61it/s]Extractor Estimating: 27it [00:16,  1.57it/s]Extractor Estimating: 28it [00:16,  1.55it/s]Extractor Estimating: 29it [00:17,  1.56it/s]Extractor Estimating: 30it [00:18,  1.58it/s]Extractor Estimating: 31it [00:18,  1.61it/s]Extractor Estimating: 32it [00:19,  1.61it/s]Extractor Estimating: 33it [00:19,  1.63it/s]Extractor Estimating: 34it [00:20,  1.59it/s]Extractor Estimating: 35it [00:21,  1.60it/s]Extractor Estimating: 36it [00:21,  1.58it/s]Extractor Estimating: 37it [00:22,  1.54it/s]Extractor Estimating: 38it [00:23,  1.58it/s]Extractor Estimating: 39it [00:23,  1.60it/s]Extractor Estimating: 40it [00:24,  1.57it/s]Extractor Estimating: 41it [00:25,  1.58it/s]Extractor Estimating: 42it [00:25,  1.62it/s]Extractor Estimating: 43it [00:26,  1.59it/s]Extractor Estimating: 44it [00:26,  1.61it/s]Extractor Estimating: 45it [00:27,  1.60it/s]Extractor Estimating: 46it [00:28,  1.54it/s]Extractor Estimating: 47it [00:28,  1.59it/s]Extractor Estimating: 48it [00:29,  1.59it/s]Extractor Estimating: 49it [00:30,  1.61it/s]Extractor Estimating: 50it [00:30,  1.59it/s]Extractor Estimating: 51it [00:31,  1.55it/s]Extractor Estimating: 52it [00:31,  1.63it/s]Extractor Estimating: 53it [00:32,  1.60it/s]Extractor Estimating: 54it [00:33,  1.68it/s]Extractor Estimating: 55it [00:33,  1.68it/s]Extractor Estimating: 56it [00:34,  1.66it/s]Extractor Estimating: 57it [00:34,  1.69it/s]Extractor Estimating: 58it [00:35,  1.71it/s]Extractor Estimating: 59it [00:36,  1.65it/s]Extractor Estimating: 60it [00:36,  1.63it/s]Extractor Estimating: 61it [00:37,  1.61it/s]Extractor Estimating: 62it [00:38,  1.58it/s]Extractor Estimating: 63it [00:38,  1.60it/s]Extractor Estimating: 64it [00:39,  1.64it/s]Extractor Estimating: 65it [00:39,  1.64it/s]Extractor Estimating: 66it [00:40,  1.63it/s]Extractor Estimating: 67it [00:41,  1.68it/s]Extractor Estimating: 68it [00:41,  1.65it/s]Extractor Estimating: 69it [00:42,  1.68it/s]Extractor Estimating: 70it [00:42,  1.67it/s]Extractor Estimating: 71it [00:43,  1.70it/s]Extractor Estimating: 72it [00:43,  1.70it/s]Extractor Estimating: 73it [00:44,  1.73it/s]Extractor Estimating: 74it [00:45,  1.60it/s]Extractor Estimating: 75it [00:45,  1.67it/s]Extractor Estimating: 76it [00:46,  1.76it/s]Extractor Estimating: 77it [00:46,  1.88it/s]Extractor Estimating: 78it [00:47,  1.90it/s]Extractor Estimating: 79it [00:47,  1.89it/s]Extractor Estimating: 80it [00:48,  1.91it/s]Extractor Estimating: 81it [00:48,  1.90it/s]Extractor Estimating: 82it [00:49,  1.95it/s]Extractor Estimating: 83it [00:49,  1.94it/s]Extractor Estimating: 84it [00:50,  1.94it/s]Extractor Estimating: 85it [00:50,  2.02it/s]Extractor Estimating: 86it [00:51,  1.98it/s]Extractor Estimating: 87it [00:52,  1.77it/s]Extractor Estimating: 88it [00:52,  1.80it/s]Extractor Estimating: 89it [00:53,  1.85it/s]Extractor Estimating: 90it [00:53,  1.88it/s]Extractor Estimating: 91it [00:54,  1.90it/s]Extractor Estimating: 92it [00:54,  1.79it/s]Extractor Estimating: 93it [00:55,  1.83it/s]Extractor Estimating: 94it [00:55,  1.78it/s]Extractor Estimating: 95it [00:56,  1.82it/s]Extractor Estimating: 96it [00:56,  1.82it/s]Extractor Estimating: 97it [00:57,  1.95it/s]Extractor Estimating: 98it [00:57,  2.00it/s]Extractor Estimating: 99it [00:58,  1.98it/s]Extractor Estimating: 100it [00:58,  1.84it/s]Extractor Estimating: 101it [00:59,  1.83it/s]Extractor Estimating: 102it [01:00,  1.74it/s]Extractor Estimating: 103it [01:00,  1.75it/s]Extractor Estimating: 104it [01:01,  1.80it/s]Extractor Estimating: 105it [01:01,  1.73it/s]Extractor Estimating: 106it [01:02,  1.58it/s]Extractor Estimating: 107it [01:03,  1.65it/s]Extractor Estimating: 108it [01:03,  1.65it/s]Extractor Estimating: 109it [01:04,  1.68it/s]Extractor Estimating: 110it [01:04,  1.69it/s]Extractor Estimating: 111it [01:05,  1.62it/s]Extractor Estimating: 112it [01:06,  1.64it/s]Extractor Estimating: 113it [01:06,  1.68it/s]Extractor Estimating: 114it [01:07,  1.68it/s]Extractor Estimating: 115it [01:07,  1.68it/s]Extractor Estimating: 116it [01:08,  1.69it/s]Extractor Estimating: 117it [01:09,  1.71it/s]Extractor Estimating: 118it [01:09,  1.73it/s]Extractor Estimating: 119it [01:10,  1.73it/s]Extractor Estimating: 120it [01:10,  1.75it/s]Extractor Estimating: 121it [01:11,  1.69it/s]Extractor Estimating: 122it [01:12,  1.71it/s]Extractor Estimating: 123it [01:12,  1.73it/s]Extractor Estimating: 124it [01:13,  1.74it/s]Extractor Estimating: 125it [01:13,  1.77it/s]Extractor Estimating: 126it [01:14,  1.71it/s]Extractor Estimating: 127it [01:14,  1.65it/s]Extractor Estimating: 128it [01:15,  1.71it/s]Extractor Estimating: 129it [01:16,  1.71it/s]Extractor Estimating: 130it [01:16,  1.71it/s]Extractor Estimating: 131it [01:17,  1.69it/s]Extractor Estimating: 132it [01:17,  1.69it/s]Extractor Estimating: 133it [01:18,  1.69it/s]Extractor Estimating: 134it [01:19,  1.70it/s]Extractor Estimating: 135it [01:19,  1.66it/s]Extractor Estimating: 136it [01:20,  1.67it/s]Extractor Estimating: 137it [01:20,  1.70it/s]Extractor Estimating: 138it [01:21,  1.71it/s]Extractor Estimating: 139it [01:22,  1.70it/s]Extractor Estimating: 140it [01:22,  1.71it/s]Extractor Estimating: 141it [01:23,  1.68it/s]Extractor Estimating: 142it [01:23,  1.72it/s]Extractor Estimating: 143it [01:24,  1.76it/s]Extractor Estimating: 144it [01:24,  1.74it/s]Extractor Estimating: 145it [01:25,  1.70it/s]Extractor Estimating: 146it [01:26,  1.68it/s]Extractor Estimating: 147it [01:26,  1.68it/s]Extractor Estimating: 148it [01:27,  1.68it/s]Extractor Estimating: 149it [01:27,  1.70it/s]Extractor Estimating: 150it [01:28,  1.70it/s]Extractor Estimating: 151it [01:29,  1.56it/s]Extractor Estimating: 152it [01:29,  1.60it/s]Extractor Estimating: 153it [01:30,  1.58it/s]Extractor Estimating: 154it [01:31,  1.57it/s]Extractor Estimating: 155it [01:31,  1.58it/s]Extractor Estimating: 156it [01:32,  1.62it/s]Extractor Estimating: 157it [01:32,  1.66it/s]Extractor Estimating: 158it [01:33,  1.69it/s]Extractor Estimating: 159it [01:34,  1.61it/s]Extractor Estimating: 160it [01:34,  1.64it/s]Extractor Estimating: 161it [01:35,  1.65it/s]Extractor Estimating: 162it [01:35,  1.68it/s]Extractor Estimating: 163it [01:36,  1.64it/s]Extractor Estimating: 164it [01:37,  1.61it/s]Extractor Estimating: 165it [01:37,  1.59it/s]Extractor Estimating: 166it [01:38,  1.63it/s]Extractor Estimating: 167it [01:39,  1.65it/s]Extractor Estimating: 168it [01:39,  1.66it/s]Extractor Estimating: 169it [01:40,  1.66it/s]Extractor Estimating: 170it [01:40,  1.69it/s]Extractor Estimating: 171it [01:41,  1.68it/s]Extractor Estimating: 172it [01:41,  1.67it/s]Extractor Estimating: 173it [01:42,  1.67it/s]Extractor Estimating: 174it [01:43,  1.64it/s]Extractor Estimating: 175it [01:43,  1.62it/s]Extractor Estimating: 176it [01:44,  1.67it/s]Extractor Estimating: 177it [01:45,  1.68it/s]Extractor Estimating: 178it [01:45,  1.66it/s]Extractor Estimating: 179it [01:46,  1.63it/s]Extractor Estimating: 180it [01:46,  1.66it/s]Extractor Estimating: 181it [01:47,  1.68it/s]Extractor Estimating: 182it [01:48,  1.67it/s]Extractor Estimating: 183it [01:48,  1.69it/s]Extractor Estimating: 184it [01:49,  1.72it/s]Extractor Estimating: 185it [01:49,  1.68it/s]Extractor Estimating: 186it [01:50,  1.72it/s]Extractor Estimating: 187it [01:51,  1.63it/s]Extractor Estimating: 188it [01:51,  1.63it/s]Extractor Estimating: 189it [01:52,  1.61it/s]Extractor Estimating: 190it [01:52,  1.68it/s]Extractor Estimating: 191it [01:53,  1.64it/s]Extractor Estimating: 192it [01:54,  1.62it/s]Extractor Estimating: 193it [01:54,  1.68it/s]Extractor Estimating: 194it [01:55,  1.68it/s]Extractor Estimating: 195it [01:55,  1.73it/s]Extractor Estimating: 196it [01:56,  1.75it/s]Extractor Estimating: 197it [01:56,  1.75it/s]Extractor Estimating: 198it [01:57,  1.70it/s]Extractor Estimating: 199it [01:58,  1.69it/s]Extractor Estimating: 200it [01:58,  1.74it/s]Extractor Estimating: 201it [01:59,  1.68it/s]Extractor Estimating: 202it [01:59,  1.66it/s]Extractor Estimating: 203it [02:00,  1.61it/s]Extractor Estimating: 204it [02:01,  1.59it/s]Extractor Estimating: 205it [02:01,  1.61it/s]Extractor Estimating: 206it [02:02,  1.61it/s]Extractor Estimating: 207it [02:03,  1.59it/s]Extractor Estimating: 208it [02:03,  1.53it/s]Extractor Estimating: 209it [02:04,  1.50it/s]Extractor Estimating: 210it [02:05,  1.51it/s]Extractor Estimating: 211it [02:05,  1.54it/s]Extractor Estimating: 212it [02:06,  1.57it/s]Extractor Estimating: 213it [02:07,  1.54it/s]Extractor Estimating: 214it [02:07,  1.56it/s]Extractor Estimating: 215it [02:08,  1.55it/s]Extractor Estimating: 216it [02:08,  1.57it/s]Extractor Estimating: 217it [02:09,  1.58it/s]Extractor Estimating: 218it [02:10,  1.56it/s]Extractor Estimating: 219it [02:10,  1.55it/s]Extractor Estimating: 220it [02:11,  1.54it/s]Extractor Estimating: 221it [02:12,  1.55it/s]Extractor Estimating: 222it [02:12,  1.56it/s]Extractor Estimating: 223it [02:13,  1.53it/s]Extractor Estimating: 224it [02:14,  1.56it/s]Extractor Estimating: 225it [02:14,  1.58it/s]Extractor Estimating: 226it [02:15,  1.59it/s]Extractor Estimating: 227it [02:15,  1.66it/s]Extractor Estimating: 228it [02:16,  1.67it/s]Extractor Estimating: 229it [02:17,  1.71it/s]Extractor Estimating: 230it [02:17,  1.72it/s]Extractor Estimating: 231it [02:18,  1.74it/s]Extractor Estimating: 232it [02:18,  1.70it/s]Extractor Estimating: 233it [02:19,  1.75it/s]Extractor Estimating: 234it [02:19,  1.72it/s]Extractor Estimating: 235it [02:20,  1.77it/s]Extractor Estimating: 236it [02:21,  1.72it/s]Extractor Estimating: 237it [02:21,  1.77it/s]Extractor Estimating: 238it [02:22,  1.63it/s]Extractor Estimating: 239it [02:22,  1.69it/s]Extractor Estimating: 240it [02:23,  1.74it/s]Extractor Estimating: 241it [02:23,  1.80it/s]Extractor Estimating: 242it [02:24,  1.79it/s]Extractor Estimating: 243it [02:25,  1.71it/s]Extractor Estimating: 244it [02:25,  1.71it/s]Extractor Estimating: 245it [02:26,  1.74it/s]Extractor Estimating: 246it [02:26,  1.74it/s]Extractor Estimating: 247it [02:27,  1.74it/s]Extractor Estimating: 248it [02:28,  1.72it/s]Extractor Estimating: 249it [02:28,  1.66it/s]Extractor Estimating: 250it [02:29,  1.76it/s]Extractor Estimating: 250it [02:29,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:13,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:13,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:13,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:13,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:13,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:28:13,544 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:28:13,545 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:28:13,904 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:28:15,060 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:28:15,060 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:17,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:17,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:17,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:17,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:28:17,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:28:17,926 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:28:17,927 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:28:18,242 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:28:18,481 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:28:18,481 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:04:12,282 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:04:12,660 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4991 mean pseudo reward: 0.9029483671387103
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 22297
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22397, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22397, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.978, loss:573.6507
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.979, loss:559.7624
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 0.986, loss:543.9006
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 0.968, loss:512.8905
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.982, loss:509.7594
>> valid entity prec:0.4704, rec:0.4869, f1:0.4785
>> valid relation prec:0.1166, rec:0.0089, f1:0.0165
>> valid relation with NER prec:0.1166, rec:0.0089, f1:0.0165
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 3.091, loss:513.5720
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.972, loss:487.8144
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.980, loss:525.4550
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.983, loss:510.5394
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.976, loss:499.1901
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4677, rec:0.4470, f1:0.4571
>> valid relation prec:0.1976, rec:0.0119, f1:0.0225
>> valid relation with NER prec:0.1976, rec:0.0119, f1:0.0225
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 60, avg_time 3.063, loss:500.0833
g_step 1200, step 160, avg_time 0.979, loss:501.2107
g_step 1300, step 52, avg_time 0.982, loss:467.3289
g_step 1400, step 152, avg_time 0.975, loss:468.7558
g_step 1500, step 44, avg_time 0.975, loss:480.2584
>> valid entity prec:0.5007, rec:0.3317, f1:0.3990
>> valid relation prec:0.2441, rec:0.0257, f1:0.0465
>> valid relation with NER prec:0.2441, rec:0.0257, f1:0.0465
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 144, avg_time 3.061, loss:470.3006
g_step 1700, step 36, avg_time 0.977, loss:440.3581
g_step 1800, step 136, avg_time 0.979, loss:428.5039
g_step 1900, step 28, avg_time 0.969, loss:436.6500
g_step 2000, step 128, avg_time 0.989, loss:414.8812
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4733, rec:0.4270, f1:0.4490
>> valid relation prec:0.2207, rec:0.0288, f1:0.0509
>> valid relation with NER prec:0.2207, rec:0.0288, f1:0.0509
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 3.088, loss:429.3580
g_step 2200, step 120, avg_time 0.974, loss:393.6870
g_step 2300, step 12, avg_time 0.976, loss:400.0230
g_step 2400, step 112, avg_time 0.979, loss:372.7994
g_step 2500, step 4, avg_time 0.967, loss:410.8745
>> valid entity prec:0.4950, rec:0.3982, f1:0.4414
>> valid relation prec:0.1738, rec:0.0141, f1:0.0261
>> valid relation with NER prec:0.1738, rec:0.0141, f1:0.0261
g_step 2600, step 104, avg_time 3.083, loss:370.0113
g_step 2700, step 204, avg_time 0.971, loss:371.9755
g_step 2800, step 96, avg_time 0.970, loss:348.8879
g_step 2900, step 196, avg_time 0.980, loss:372.1502
g_step 3000, step 88, avg_time 0.978, loss:336.6239
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4705, rec:0.4766, f1:0.4736
>> valid relation prec:0.1938, rec:0.0307, f1:0.0530
>> valid relation with NER prec:0.1938, rec:0.0307, f1:0.0530
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 188, avg_time 3.103, loss:349.0840
g_step 3200, step 80, avg_time 0.975, loss:317.7176
g_step 3300, step 180, avg_time 0.982, loss:340.7451
g_step 3400, step 72, avg_time 0.984, loss:316.5191
g_step 3500, step 172, avg_time 0.977, loss:332.6285
>> valid entity prec:0.4756, rec:0.3951, f1:0.4316
>> valid relation prec:0.1390, rec:0.0263, f1:0.0442
>> valid relation with NER prec:0.1390, rec:0.0263, f1:0.0442
g_step 3600, step 64, avg_time 3.072, loss:309.7044
g_step 3700, step 164, avg_time 0.974, loss:310.3203
g_step 3800, step 56, avg_time 0.977, loss:301.2194
g_step 3900, step 156, avg_time 0.977, loss:291.0725
g_step 4000, step 48, avg_time 0.975, loss:294.8125
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4882, rec:0.3283, f1:0.3926
>> valid relation prec:0.2365, rec:0.0254, f1:0.0459
>> valid relation with NER prec:0.2365, rec:0.0254, f1:0.0459
g_step 4100, step 148, avg_time 3.079, loss:291.2759
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:04:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:04:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-04-12_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:04:13 - WARNING - datasets.builder -   Using custom data configuration default-894f2f239e63e38b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-894f2f239e63e38b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:04:16,413 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:04:16,414 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:04:16,414 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:04:16,415 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:04:16,515 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:04:16,565 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:04:16,566 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:04:16,566 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:04:16,566 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:04:16,566 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:04:16,566 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:04:17,085 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:04:20,185 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:04:20,215 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-894f2f239e63e38b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.89ba/s] 40%|████      | 2/5 [00:00<00:01,  2.96ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.61ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.02ba/s]100%|██████████| 5/5 [00:01<00:00,  4.27ba/s]100%|██████████| 5/5 [00:01<00:00,  3.70ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.76ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.59ba/s] 43%|████▎     | 3/7 [00:00<00:01,  3.94ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  4.15ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.29ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.36ba/s]100%|██████████| 7/7 [00:01<00:00,  4.46ba/s]100%|██████████| 7/7 [00:01<00:00,  4.16ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  5.85ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.64ba/s]100%|██████████| 5/5 [00:00<00:00,  9.58ba/s]100%|██████████| 5/5 [00:00<00:00,  9.07ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.43ba/s] 43%|████▎     | 3/7 [00:00<00:00,  7.83ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.11ba/s]100%|██████████| 7/7 [00:00<00:00,  9.99ba/s]100%|██████████| 7/7 [00:00<00:00,  9.07ba/s]
[INFO|trainer.py:414] 2023-08-28 16:04:25,921 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:04:26,031 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:04:26,031 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-28 16:04:26,031 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:04:26,031 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:04:26,031 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:04:26,031 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:04:26,031 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:56,  3.34it/s]  1%|          | 2/390 [00:00<02:01,  3.21it/s]  1%|          | 3/390 [00:00<01:56,  3.31it/s]  1%|          | 4/390 [00:01<01:54,  3.37it/s]  1%|▏         | 5/390 [00:01<01:53,  3.39it/s]  2%|▏         | 6/390 [00:01<01:52,  3.41it/s]  2%|▏         | 7/390 [00:02<01:52,  3.41it/s]  2%|▏         | 8/390 [00:02<01:51,  3.42it/s]  2%|▏         | 9/390 [00:02<01:51,  3.43it/s]  3%|▎         | 10/390 [00:02<01:50,  3.44it/s]  3%|▎         | 11/390 [00:03<01:50,  3.44it/s]  3%|▎         | 12/390 [00:03<01:49,  3.44it/s]  3%|▎         | 13/390 [00:03<01:49,  3.45it/s]  4%|▎         | 14/390 [00:04<01:49,  3.45it/s]  4%|▍         | 15/390 [00:04<01:53,  3.31it/s]  4%|▍         | 16/390 [00:04<01:51,  3.35it/s]  4%|▍         | 17/390 [00:05<01:50,  3.38it/s]  5%|▍         | 18/390 [00:05<01:49,  3.40it/s]  5%|▍         | 19/390 [00:05<01:48,  3.41it/s]  5%|▌         | 20/390 [00:05<01:48,  3.42it/s]  5%|▌         | 21/390 [00:06<01:47,  3.43it/s]  6%|▌         | 22/390 [00:06<01:47,  3.44it/s]  6%|▌         | 23/390 [00:06<01:46,  3.44it/s]  6%|▌         | 24/390 [00:07<01:46,  3.44it/s]  6%|▋         | 25/390 [00:07<01:45,  3.44it/s]  7%|▋         | 26/390 [00:07<01:45,  3.44it/s]  7%|▋         | 27/390 [00:07<01:45,  3.44it/s]  7%|▋         | 28/390 [00:08<01:45,  3.45it/s]  7%|▋         | 29/390 [00:08<01:44,  3.45it/s]  8%|▊         | 30/390 [00:08<01:44,  3.45it/s]  8%|▊         | 31/390 [00:09<01:44,  3.45it/s]  8%|▊         | 32/390 [00:09<01:43,  3.44it/s]  8%|▊         | 33/390 [00:09<01:48,  3.28it/s]  9%|▊         | 34/390 [00:09<01:47,  3.33it/s]  9%|▉         | 35/390 [00:10<01:45,  3.36it/s]  9%|▉         | 36/390 [00:10<01:44,  3.39it/s]  9%|▉         | 37/390 [00:10<01:43,  3.40it/s] 10%|▉         | 38/390 [00:11<01:43,  3.41it/s] 10%|█         | 39/390 [00:11<01:42,  3.42it/s] 10%|█         | 40/390 [00:11<01:47,  3.26it/s] 11%|█         | 41/390 [00:12<01:45,  3.31it/s] 11%|█         | 42/390 [00:12<01:43,  3.35it/s] 11%|█         | 43/390 [00:12<01:42,  3.38it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.40it/s] 12%|█▏        | 45/390 [00:13<01:41,  3.41it/s] 12%|█▏        | 46/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 48/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 50/390 [00:15<02:21,  2.40it/s] 13%|█▎        | 51/390 [00:15<02:08,  2.63it/s] 13%|█▎        | 52/390 [00:15<01:59,  2.83it/s] 14%|█▎        | 53/390 [00:15<01:52,  2.99it/s] 14%|█▍        | 54/390 [00:16<01:47,  3.11it/s] 14%|█▍        | 55/390 [00:16<01:44,  3.20it/s] 14%|█▍        | 56/390 [00:16<01:45,  3.16it/s] 15%|█▍        | 57/390 [00:17<01:42,  3.23it/s] 15%|█▍        | 58/390 [00:17<01:40,  3.29it/s] 15%|█▌        | 59/390 [00:17<01:39,  3.34it/s] 15%|█▌        | 60/390 [00:18<01:38,  3.36it/s] 16%|█▌        | 61/390 [00:18<01:37,  3.39it/s] 16%|█▌        | 62/390 [00:18<01:36,  3.40it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.41it/s] 16%|█▋        | 64/390 [00:19<01:35,  3.42it/s] 17%|█▋        | 65/390 [00:19<01:34,  3.42it/s] 17%|█▋        | 66/390 [00:19<01:44,  3.10it/s] 17%|█▋        | 67/390 [00:20<01:41,  3.20it/s] 17%|█▋        | 68/390 [00:20<01:38,  3.27it/s] 18%|█▊        | 69/390 [00:20<01:36,  3.32it/s] 18%|█▊        | 70/390 [00:21<01:35,  3.35it/s] 18%|█▊        | 71/390 [00:21<01:34,  3.38it/s] 18%|█▊        | 72/390 [00:21<01:33,  3.40it/s] 19%|█▊        | 73/390 [00:22<01:41,  3.11it/s] 19%|█▉        | 74/390 [00:22<01:38,  3.20it/s] 19%|█▉        | 75/390 [00:22<01:36,  3.27it/s] 19%|█▉        | 76/390 [00:22<01:34,  3.32it/s] 20%|█▉        | 77/390 [00:23<01:33,  3.36it/s] 20%|██        | 78/390 [00:23<01:32,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 16:04:49,563 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:04:49,563 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 16:04:49,563 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.54it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.28it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.31it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.36it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.73it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.44it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.26it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.10it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.26it/s][A
  6%|▌         | 52/861 [00:01<00:20, 40.19it/s][A
  7%|▋         | 57/861 [00:01<00:19, 41.42it/s][A
  7%|▋         | 62/861 [00:01<00:18, 42.28it/s][A
  8%|▊         | 67/861 [00:01<00:18, 43.00it/s][A
  8%|▊         | 72/861 [00:01<00:18, 43.49it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.69it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.82it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.88it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.54it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.54it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.83it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.01it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.15it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.29it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.31it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.33it/s][A
 15%|█▌        | 132/861 [00:03<00:16, 43.99it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.82it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 43.74it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 43.96it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.14it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.20it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.26it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.33it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.17it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.93it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.83it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.86it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.04it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.14it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.01it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.27it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 44.34it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 44.00it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.82it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.87it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 43.92it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.13it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.12it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.20it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.30it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.16it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.95it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.90it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.86it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.08it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 43.49it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 43.79it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 43.95it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.05it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.00it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 43.85it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.79it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.74it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.88it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.10it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.16it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.24it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.16it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.07it/s][A
 41%|████      | 352/861 [00:08<00:11, 43.89it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.91it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.91it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.98it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.10it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.27it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.14it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.03it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 43.77it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 43.69it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.82it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.80it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.89it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.03it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.24it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.33it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.13it/s][A
 51%|█████     | 437/861 [00:09<00:09, 44.02it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.97it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.88it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.88it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.90it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.02it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.20it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.15it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.03it/s][A
 56%|█████▌    | 482/861 [00:10<00:08, 44.06it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.98it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.95it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 43.90it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 43.26it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 43.58it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 43.80it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.07it/s][A
 61%|██████    | 522/861 [00:11<00:07, 44.04it/s][A
 61%|██████    | 527/861 [00:11<00:07, 44.05it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.04it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.03it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.01it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.07it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.19it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.31it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.42it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.31it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 44.35it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.04it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.03it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.11it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.21it/s][A
 69%|██████▉   | 597/861 [00:13<00:06, 43.45it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 43.85it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.02it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.17it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 44.15it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 40.23it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 41.68it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 42.59it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 43.16it/s][A
 75%|███████▍  | 642/861 [00:14<00:05, 43.69it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 43.86it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.00it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 44.19it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.01it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 43.72it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 43.82it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.05it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.28it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.45it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.33it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.38it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 44.33it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 44.09it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.91it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.98it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.10it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 41.35it/s][A
 85%|████████▌ | 732/861 [00:16<00:03, 42.35it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 43.06it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 43.54it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 43.79it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.79it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.79it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.85it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.70it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.88it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.13it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.25it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.33it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.28it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.26it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.13it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.01it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 43.84it/s][A
 95%|█████████▍| 817/861 [00:18<00:01, 43.98it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.08it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.22it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.37it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 44.38it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 44.38it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.23it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.98it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.99it/s][A
                                                 [A                                                
100%|██████████| 861/861 [00:19<00:00, 43.99it/s][A 20%|██        | 78/390 [00:43<01:32,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:05:09,423 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 16:05:09,736 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:05:13,500 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:05:13,936 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:05:14,007 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:57<53:24, 10.30s/it] 21%|██        | 80/390 [00:57<37:47,  7.31s/it] 21%|██        | 81/390 [00:57<26:49,  5.21s/it] 21%|██        | 82/390 [00:58<19:10,  3.73s/it] 21%|██▏       | 83/390 [00:58<13:49,  2.70s/it] 22%|██▏       | 84/390 [00:58<10:05,  1.98s/it] 22%|██▏       | 85/390 [00:58<07:29,  1.47s/it] 22%|██▏       | 86/390 [00:59<05:40,  1.12s/it] 22%|██▏       | 87/390 [00:59<04:24,  1.15it/s] 23%|██▎       | 88/390 [00:59<03:31,  1.43it/s] 23%|██▎       | 89/390 [01:00<02:53,  1.73it/s] 23%|██▎       | 90/390 [01:00<02:27,  2.03it/s] 23%|██▎       | 91/390 [01:00<02:12,  2.26it/s] 24%|██▎       | 92/390 [01:01<01:58,  2.52it/s] 24%|██▍       | 93/390 [01:01<01:48,  2.73it/s] 24%|██▍       | 94/390 [01:01<01:42,  2.90it/s] 24%|██▍       | 95/390 [01:01<01:37,  3.03it/s] 25%|██▍       | 96/390 [01:02<01:33,  3.13it/s] 25%|██▍       | 97/390 [01:02<01:31,  3.21it/s] 25%|██▌       | 98/390 [01:02<01:29,  3.26it/s] 25%|██▌       | 99/390 [01:03<01:28,  3.30it/s] 26%|██▌       | 100/390 [01:03<01:27,  3.33it/s] 26%|██▌       | 101/390 [01:03<01:26,  3.35it/s] 26%|██▌       | 102/390 [01:04<01:27,  3.30it/s] 26%|██▋       | 103/390 [01:04<01:26,  3.33it/s] 27%|██▋       | 104/390 [01:04<01:25,  3.35it/s] 27%|██▋       | 105/390 [01:04<01:24,  3.36it/s] 27%|██▋       | 106/390 [01:05<01:24,  3.37it/s] 27%|██▋       | 107/390 [01:05<01:23,  3.38it/s] 28%|██▊       | 108/390 [01:05<01:23,  3.38it/s] 28%|██▊       | 109/390 [01:06<01:22,  3.39it/s] 28%|██▊       | 110/390 [01:06<01:22,  3.39it/s] 28%|██▊       | 111/390 [01:06<01:22,  3.39it/s] 29%|██▊       | 112/390 [01:06<01:21,  3.39it/s] 29%|██▉       | 113/390 [01:07<01:25,  3.22it/s] 29%|██▉       | 114/390 [01:07<01:24,  3.27it/s] 29%|██▉       | 115/390 [01:07<01:23,  3.31it/s] 30%|██▉       | 116/390 [01:08<01:22,  3.33it/s] 30%|███       | 117/390 [01:08<01:21,  3.35it/s] 30%|███       | 118/390 [01:08<01:20,  3.37it/s] 31%|███       | 119/390 [01:09<01:20,  3.38it/s] 31%|███       | 120/390 [01:09<01:19,  3.38it/s] 31%|███       | 121/390 [01:09<01:19,  3.39it/s] 31%|███▏      | 122/390 [01:09<01:19,  3.39it/s] 32%|███▏      | 123/390 [01:10<01:20,  3.33it/s] 32%|███▏      | 124/390 [01:10<01:19,  3.35it/s] 32%|███▏      | 125/390 [01:10<01:18,  3.36it/s] 32%|███▏      | 126/390 [01:11<01:18,  3.37it/s] 33%|███▎      | 127/390 [01:11<01:17,  3.38it/s] 33%|███▎      | 128/390 [01:11<01:17,  3.38it/s] 33%|███▎      | 129/390 [01:12<01:17,  3.39it/s] 33%|███▎      | 130/390 [01:12<01:16,  3.39it/s] 34%|███▎      | 131/390 [01:12<01:16,  3.39it/s] 34%|███▍      | 132/390 [01:12<01:16,  3.39it/s] 34%|███▍      | 133/390 [01:13<01:15,  3.39it/s] 34%|███▍      | 134/390 [01:13<01:15,  3.39it/s] 35%|███▍      | 135/390 [01:13<01:15,  3.39it/s] 35%|███▍      | 136/390 [01:14<01:14,  3.39it/s] 35%|███▌      | 137/390 [01:14<01:14,  3.39it/s] 35%|███▌      | 138/390 [01:14<01:14,  3.39it/s] 36%|███▌      | 139/390 [01:14<01:14,  3.39it/s] 36%|███▌      | 140/390 [01:15<01:13,  3.40it/s] 36%|███▌      | 141/390 [01:15<01:13,  3.39it/s] 36%|███▋      | 142/390 [01:15<01:13,  3.39it/s] 37%|███▋      | 143/390 [01:16<01:19,  3.11it/s] 37%|███▋      | 144/390 [01:16<01:17,  3.19it/s] 37%|███▋      | 145/390 [01:16<01:15,  3.25it/s] 37%|███▋      | 146/390 [01:17<01:14,  3.29it/s] 38%|███▊      | 147/390 [01:17<01:13,  3.32it/s] 38%|███▊      | 148/390 [01:17<01:12,  3.34it/s] 38%|███▊      | 149/390 [01:18<01:11,  3.36it/s] 38%|███▊      | 150/390 [01:18<01:11,  3.37it/s] 39%|███▊      | 151/390 [01:18<01:10,  3.37it/s] 39%|███▉      | 152/390 [01:18<01:10,  3.38it/s] 39%|███▉      | 153/390 [01:19<01:15,  3.15it/s] 39%|███▉      | 154/390 [01:19<01:13,  3.22it/s] 40%|███▉      | 155/390 [01:19<01:11,  3.27it/s] 40%|████      | 156/390 [01:20<01:10,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 16:05:46,231 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:05:46,231 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 16:05:46,231 >>   Batch size = 8
{'eval_loss': 0.9791051745414734, 'eval_runtime': 19.6205, 'eval_samples_per_second': 350.858, 'eval_steps_per_second': 43.883, 'epoch': 0.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.16it/s][A
  1%|▏         | 12/861 [00:00<00:17, 47.92it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.12it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.34it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.79it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.61it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.60it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.15it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.24it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.42it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.52it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.33it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.31it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.23it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.26it/s][A
 10%|▉         | 82/861 [00:01<00:20, 37.28it/s][A
 10%|█         | 87/861 [00:02<00:19, 39.42it/s][A
 11%|█         | 92/861 [00:02<00:18, 40.88it/s][A
 11%|█▏        | 97/861 [00:02<00:18, 41.92it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 42.75it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 43.36it/s][A
 13%|█▎        | 112/861 [00:02<00:17, 43.77it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 43.88it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.59it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.43it/s][A
 15%|█▌        | 132/861 [00:03<00:16, 43.53it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.83it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.11it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.32it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.46it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.54it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.36it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.08it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.74it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.89it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.02it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.29it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.43it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.55it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.47it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.39it/s][A
 25%|██▍       | 212/861 [00:04<00:15, 42.63it/s][A
 25%|██▌       | 217/861 [00:04<00:14, 43.08it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.37it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.65it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.06it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.24it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.42it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.37it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 43.96it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.82it/s][A
 30%|███       | 262/861 [00:05<00:13, 43.97it/s][A
 31%|███       | 267/861 [00:06<00:13, 44.08it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.15it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.37it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.43it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.50it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.32it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.14it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.04it/s][A
 36%|███▌      | 307/861 [00:06<00:12, 44.08it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.18it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.30it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.43it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.38it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.38it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.28it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.13it/s][A
 40%|████      | 347/861 [00:07<00:12, 42.05it/s][A
 41%|████      | 352/861 [00:08<00:11, 42.79it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.31it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.68it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.04it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.17it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.21it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.08it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 43.78it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 43.81it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 44.02it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.30it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.43it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.49it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.39it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.35it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.06it/s][A
 50%|█████     | 432/861 [00:09<00:09, 43.85it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.94it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.00it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.29it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.48it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.50it/s][A
 54%|█████▎    | 462/861 [00:10<00:08, 44.44it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.18it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.05it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 43.92it/s][A
 56%|█████▌    | 482/861 [00:10<00:09, 41.61it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 42.47it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.11it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 43.50it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 43.92it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.14it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.19it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.08it/s][A
 61%|██████    | 522/861 [00:11<00:07, 43.84it/s][A
 61%|██████    | 527/861 [00:12<00:07, 43.73it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.84it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.14it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.35it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.36it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.52it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.32it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.17it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 43.96it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 43.94it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.06it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.18it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.26it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.27it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.35it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.34it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.20it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.08it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 41.33it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 42.33it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.07it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 43.41it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 43.82it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.00it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 43.95it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 43.93it/s][A
 76%|███████▋  | 657/861 [00:14<00:04, 43.75it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.78it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.06it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.16it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.39it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.53it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.43it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.27it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.05it/s][A
 82%|████████▏ | 702/861 [00:15<00:03, 43.95it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.93it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 44.13it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.21it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.41it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.52it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.47it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.23it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.11it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 44.00it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 42.46it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.05it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.63it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 41.16it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.47it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 43.74it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 43.79it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 43.78it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 43.66it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.78it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.15it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.31it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.31it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.32it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.40it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.22it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 43.99it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 43.86it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.98it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 44.08it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 44.25it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 44.36it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 44.36it/s][A 40%|████      | 156/390 [01:39<01:10,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:06:06,011 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 16:06:06,281 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:06:10,245 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:06:10,406 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:06:10,497 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:55<41:57, 10.80s/it] 41%|████      | 158/390 [01:55<29:40,  7.67s/it] 41%|████      | 159/390 [01:56<21:01,  5.46s/it] 41%|████      | 160/390 [01:56<14:59,  3.91s/it] 41%|████▏     | 161/390 [01:56<10:47,  2.83s/it] 42%|████▏     | 162/390 [01:57<07:51,  2.07s/it] 42%|████▏     | 163/390 [01:57<05:48,  1.53s/it] 42%|████▏     | 164/390 [01:57<04:22,  1.16s/it] 42%|████▏     | 165/390 [01:57<03:22,  1.11it/s] 43%|████▎     | 166/390 [01:58<02:41,  1.39it/s] 43%|████▎     | 167/390 [01:58<02:12,  1.69it/s] 43%|████▎     | 168/390 [01:58<01:54,  1.93it/s] 43%|████▎     | 169/390 [01:59<01:39,  2.22it/s] 44%|████▎     | 170/390 [01:59<01:28,  2.48it/s] 44%|████▍     | 171/390 [01:59<01:21,  2.69it/s] 44%|████▍     | 172/390 [02:00<01:15,  2.87it/s] 44%|████▍     | 173/390 [02:00<01:12,  3.01it/s] 45%|████▍     | 174/390 [02:00<01:09,  3.11it/s] 45%|████▍     | 175/390 [02:00<01:07,  3.19it/s] 45%|████▌     | 176/390 [02:01<01:05,  3.25it/s] 45%|████▌     | 177/390 [02:01<01:04,  3.29it/s] 46%|████▌     | 178/390 [02:01<01:03,  3.32it/s] 46%|████▌     | 179/390 [02:02<01:05,  3.23it/s] 46%|████▌     | 180/390 [02:02<01:04,  3.28it/s] 46%|████▋     | 181/390 [02:02<01:03,  3.31it/s] 47%|████▋     | 182/390 [02:02<01:02,  3.34it/s] 47%|████▋     | 183/390 [02:03<01:01,  3.36it/s] 47%|████▋     | 184/390 [02:03<01:01,  3.37it/s] 47%|████▋     | 185/390 [02:03<01:00,  3.37it/s] 48%|████▊     | 186/390 [02:04<01:00,  3.38it/s] 48%|████▊     | 187/390 [02:04<00:59,  3.39it/s] 48%|████▊     | 188/390 [02:04<00:59,  3.39it/s] 48%|████▊     | 189/390 [02:05<00:59,  3.39it/s] 49%|████▊     | 190/390 [02:05<01:01,  3.26it/s] 49%|████▉     | 191/390 [02:05<01:00,  3.30it/s] 49%|████▉     | 192/390 [02:05<00:59,  3.33it/s] 49%|████▉     | 193/390 [02:06<00:58,  3.35it/s] 50%|████▉     | 194/390 [02:06<00:58,  3.36it/s] 50%|█████     | 195/390 [02:06<00:57,  3.37it/s] 50%|█████     | 196/390 [02:07<00:57,  3.38it/s] 51%|█████     | 197/390 [02:07<00:57,  3.38it/s] 51%|█████     | 198/390 [02:07<00:56,  3.39it/s] 51%|█████     | 199/390 [02:08<00:56,  3.38it/s] 51%|█████▏    | 200/390 [02:08<00:56,  3.39it/s] 52%|█████▏    | 201/390 [02:08<00:57,  3.28it/s] 52%|█████▏    | 202/390 [02:08<00:56,  3.31it/s] 52%|█████▏    | 203/390 [02:09<00:56,  3.34it/s] 52%|█████▏    | 204/390 [02:09<00:55,  3.35it/s] 53%|█████▎    | 205/390 [02:09<00:54,  3.36it/s] 53%|█████▎    | 206/390 [02:10<00:54,  3.37it/s] 53%|█████▎    | 207/390 [02:10<00:54,  3.38it/s] 53%|█████▎    | 208/390 [02:10<00:53,  3.38it/s] 54%|█████▎    | 209/390 [02:11<00:53,  3.39it/s] 54%|█████▍    | 210/390 [02:11<00:53,  3.39it/s] 54%|█████▍    | 211/390 [02:11<00:52,  3.39it/s] 54%|█████▍    | 212/390 [02:11<00:54,  3.25it/s] 55%|█████▍    | 213/390 [02:12<00:53,  3.30it/s] 55%|█████▍    | 214/390 [02:12<00:52,  3.32it/s] 55%|█████▌    | 215/390 [02:12<00:52,  3.34it/s] 55%|█████▌    | 216/390 [02:13<00:51,  3.36it/s] 56%|█████▌    | 217/390 [02:13<00:51,  3.37it/s] 56%|█████▌    | 218/390 [02:13<00:51,  3.37it/s] 56%|█████▌    | 219/390 [02:14<00:50,  3.38it/s] 56%|█████▋    | 220/390 [02:14<00:50,  3.38it/s] 57%|█████▋    | 221/390 [02:14<00:54,  3.09it/s] 57%|█████▋    | 222/390 [02:14<00:52,  3.17it/s] 57%|█████▋    | 223/390 [02:15<00:51,  3.24it/s] 57%|█████▋    | 224/390 [02:15<00:50,  3.30it/s] 58%|█████▊    | 225/390 [02:15<00:49,  3.34it/s] 58%|█████▊    | 226/390 [02:16<00:48,  3.37it/s] 58%|█████▊    | 227/390 [02:16<00:48,  3.39it/s] 58%|█████▊    | 228/390 [02:16<00:47,  3.40it/s] 59%|█████▊    | 229/390 [02:17<00:47,  3.41it/s] 59%|█████▉    | 230/390 [02:17<00:46,  3.42it/s] 59%|█████▉    | 231/390 [02:17<00:48,  3.25it/s] 59%|█████▉    | 232/390 [02:17<00:47,  3.30it/s] 60%|█████▉    | 233/390 [02:18<00:46,  3.34it/s] 60%|██████    | 234/390 [02:18<00:46,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 16:06:44,606 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:06:44,607 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 16:06:44,607 >>   Batch size = 8
{'eval_loss': 0.9939128160476685, 'eval_runtime': 19.6249, 'eval_samples_per_second': 350.778, 'eval_steps_per_second': 43.873, 'epoch': 1.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.76it/s][A
  1%|▏         | 12/861 [00:00<00:17, 47.99it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.14it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.12it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.74it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.59it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.36it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.08it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.14it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.33it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.28it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.17it/s][A
  8%|▊         | 67/861 [00:01<00:18, 44.09it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.08it/s][A
  9%|▉         | 77/861 [00:01<00:17, 43.98it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.98it/s][A
 10%|█         | 87/861 [00:01<00:18, 41.00it/s][A
 11%|█         | 92/861 [00:02<00:18, 42.02it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 42.76it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.24it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 43.54it/s][A
 13%|█▎        | 112/861 [00:02<00:17, 43.70it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 43.79it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.79it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.50it/s][A
 15%|█▌        | 132/861 [00:02<00:16, 43.63it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.83it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.08it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.24it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 44.18it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.16it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.11it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 43.87it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 43.72it/s][A
 21%|██        | 177/861 [00:04<00:15, 43.83it/s][A
 21%|██        | 182/861 [00:04<00:15, 44.00it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 44.18it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.27it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 44.21it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.18it/s][A
 24%|██▍       | 207/861 [00:04<00:14, 44.07it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.83it/s][A
 25%|██▌       | 217/861 [00:05<00:14, 43.73it/s][A
 26%|██▌       | 222/861 [00:05<00:16, 37.94it/s][A
 26%|██▋       | 227/861 [00:05<00:15, 39.73it/s][A
 27%|██▋       | 232/861 [00:05<00:15, 41.03it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 42.07it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 42.79it/s][A
 29%|██▊       | 247/861 [00:05<00:14, 43.33it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 43.69it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.68it/s][A
 30%|███       | 262/861 [00:06<00:13, 43.35it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.26it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.21it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.70it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 43.95it/s][A
 33%|███▎      | 287/861 [00:06<00:12, 44.18it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.38it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.33it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.01it/s][A
 36%|███▌      | 307/861 [00:07<00:12, 43.72it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 43.51it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.71it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.92it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.14it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.32it/s][A
 39%|███▉      | 337/861 [00:07<00:11, 44.30it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 44.27it/s][A
 40%|████      | 347/861 [00:07<00:11, 44.00it/s][A
 41%|████      | 352/861 [00:08<00:11, 42.44it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 42.76it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.13it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.46it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 43.80it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.00it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.19it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.07it/s][A
 46%|████▌     | 392/861 [00:08<00:10, 43.74it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 43.69it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.63it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.82it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.96it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.04it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.21it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.28it/s][A
 50%|█████     | 432/861 [00:09<00:09, 44.06it/s][A
 51%|█████     | 437/861 [00:09<00:09, 43.92it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.68it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.71it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.91it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 44.01it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 44.12it/s][A
 54%|█████▍    | 467/861 [00:10<00:08, 44.20it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 44.26it/s][A
 55%|█████▌    | 477/861 [00:10<00:08, 44.06it/s][A
 56%|█████▌    | 482/861 [00:11<00:08, 43.95it/s][A
 57%|█████▋    | 487/861 [00:11<00:09, 40.95it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 42.00it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 42.70it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 43.30it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 43.53it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 43.88it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.94it/s][A
 61%|██████    | 522/861 [00:11<00:07, 43.91it/s][A
 61%|██████    | 527/861 [00:12<00:07, 43.58it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.73it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 43.84it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.18it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.37it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.38it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.48it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 44.31it/s][A
 66%|██████▌   | 567/861 [00:12<00:06, 44.10it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 43.94it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 43.88it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.05it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.22it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 44.33it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.36it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.50it/s][A
 70%|███████   | 607/861 [00:13<00:05, 44.43it/s][A
 71%|███████   | 612/861 [00:13<00:05, 44.21it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 44.02it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 41.85it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 42.69it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 43.29it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 43.63it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 43.85it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.02it/s][A
 76%|███████▌  | 652/861 [00:14<00:04, 44.01it/s][A
 76%|███████▋  | 657/861 [00:15<00:04, 44.02it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.78it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 43.83it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.10it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.34it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.42it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.40it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.27it/s][A
 81%|████████  | 697/861 [00:15<00:03, 44.30it/s][A
 82%|████████▏ | 702/861 [00:16<00:03, 44.13it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.97it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.97it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 44.12it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 44.30it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 44.48it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 44.45it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 44.31it/s][A
 86%|████████▌ | 742/861 [00:16<00:02, 44.15it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 44.07it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.04it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 42.24it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 42.93it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.40it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.88it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.06it/s][A
 91%|█████████ | 782/861 [00:17<00:01, 44.15it/s][A
 91%|█████████▏| 787/861 [00:17<00:01, 44.09it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.00it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.81it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 43.91it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 43.97it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.32it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.43it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.48it/s][A
 96%|█████████▌| 827/861 [00:18<00:00, 44.40it/s][A
 97%|█████████▋| 832/861 [00:18<00:00, 44.26it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 40.49it/s][A
 98%|█████████▊| 843/861 [00:19<00:00, 43.06it/s][A
 98%|█████████▊| 848/861 [00:19<00:00, 43.48it/s][A
 99%|█████████▉| 853/861 [00:19<00:00, 43.72it/s][A
100%|█████████▉| 858/861 [00:19<00:00, 44.10it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 44.10it/s][A 60%|██████    | 234/390 [02:38<00:46,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:07:04,382 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 16:07:04,589 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:07:08,270 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:07:08,710 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:07:08,818 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:54<28:45, 11.13s/it] 61%|██████    | 236/390 [02:55<20:15,  7.89s/it] 61%|██████    | 237/390 [02:55<14:18,  5.61s/it] 61%|██████    | 238/390 [02:55<10:10,  4.02s/it] 61%|██████▏   | 239/390 [02:56<07:17,  2.90s/it] 62%|██████▏   | 240/390 [02:56<05:17,  2.12s/it] 62%|██████▏   | 241/390 [02:56<03:53,  1.57s/it] 62%|██████▏   | 242/390 [02:57<02:55,  1.18s/it] 62%|██████▏   | 243/390 [02:57<02:14,  1.09it/s] 63%|██████▎   | 244/390 [02:57<01:46,  1.37it/s] 63%|██████▎   | 245/390 [02:57<01:26,  1.67it/s] 63%|██████▎   | 246/390 [02:58<01:12,  1.98it/s] 63%|██████▎   | 247/390 [02:58<01:05,  2.19it/s] 64%|██████▎   | 248/390 [02:58<00:57,  2.46it/s] 64%|██████▍   | 249/390 [02:59<00:52,  2.69it/s] 64%|██████▍   | 250/390 [02:59<00:48,  2.88it/s] 64%|██████▍   | 251/390 [02:59<00:45,  3.03it/s] 65%|██████▍   | 252/390 [02:59<00:43,  3.14it/s] 65%|██████▍   | 253/390 [03:00<00:42,  3.23it/s] 65%|██████▌   | 254/390 [03:00<00:41,  3.29it/s] 65%|██████▌   | 255/390 [03:00<00:40,  3.33it/s] 66%|██████▌   | 256/390 [03:01<00:39,  3.37it/s] 66%|██████▌   | 257/390 [03:01<00:39,  3.39it/s] 66%|██████▌   | 258/390 [03:01<00:40,  3.27it/s] 66%|██████▋   | 259/390 [03:02<00:39,  3.32it/s] 67%|██████▋   | 260/390 [03:02<00:38,  3.36it/s] 67%|██████▋   | 261/390 [03:02<00:38,  3.38it/s] 67%|██████▋   | 262/390 [03:02<00:37,  3.40it/s] 67%|██████▋   | 263/390 [03:03<00:37,  3.41it/s] 68%|██████▊   | 264/390 [03:03<00:36,  3.42it/s] 68%|██████▊   | 265/390 [03:03<00:36,  3.43it/s] 68%|██████▊   | 266/390 [03:04<00:36,  3.42it/s] 68%|██████▊   | 267/390 [03:04<00:35,  3.43it/s] 69%|██████▊   | 268/390 [03:04<00:35,  3.44it/s] 69%|██████▉   | 269/390 [03:05<00:37,  3.26it/s] 69%|██████▉   | 270/390 [03:05<00:36,  3.31it/s] 69%|██████▉   | 271/390 [03:05<00:35,  3.35it/s] 70%|██████▉   | 272/390 [03:05<00:34,  3.38it/s] 70%|███████   | 273/390 [03:06<00:34,  3.40it/s] 70%|███████   | 274/390 [03:06<00:33,  3.41it/s] 71%|███████   | 275/390 [03:06<00:33,  3.42it/s] 71%|███████   | 276/390 [03:07<00:33,  3.43it/s] 71%|███████   | 277/390 [03:07<00:32,  3.43it/s] 71%|███████▏  | 278/390 [03:07<00:32,  3.44it/s] 72%|███████▏  | 279/390 [03:07<00:32,  3.43it/s] 72%|███████▏  | 280/390 [03:08<00:34,  3.22it/s] 72%|███████▏  | 281/390 [03:08<00:33,  3.29it/s] 72%|███████▏  | 282/390 [03:08<00:32,  3.33it/s] 73%|███████▎  | 283/390 [03:09<00:31,  3.37it/s] 73%|███████▎  | 284/390 [03:09<00:31,  3.39it/s] 73%|███████▎  | 285/390 [03:09<00:30,  3.41it/s] 73%|███████▎  | 286/390 [03:10<00:30,  3.42it/s] 74%|███████▎  | 287/390 [03:10<00:30,  3.43it/s] 74%|███████▍  | 288/390 [03:10<00:29,  3.43it/s] 74%|███████▍  | 289/390 [03:10<00:29,  3.44it/s] 74%|███████▍  | 290/390 [03:11<00:29,  3.43it/s] 75%|███████▍  | 291/390 [03:11<00:30,  3.25it/s] 75%|███████▍  | 292/390 [03:11<00:29,  3.30it/s] 75%|███████▌  | 293/390 [03:12<00:29,  3.34it/s] 75%|███████▌  | 294/390 [03:12<00:28,  3.37it/s] 76%|███████▌  | 295/390 [03:12<00:27,  3.39it/s] 76%|███████▌  | 296/390 [03:12<00:27,  3.41it/s] 76%|███████▌  | 297/390 [03:13<00:27,  3.42it/s] 76%|███████▋  | 298/390 [03:13<00:26,  3.42it/s] 77%|███████▋  | 299/390 [03:13<00:26,  3.43it/s] 77%|███████▋  | 300/390 [03:14<00:26,  3.43it/s] 77%|███████▋  | 301/390 [03:14<00:25,  3.44it/s] 77%|███████▋  | 302/390 [03:14<00:26,  3.34it/s] 78%|███████▊  | 303/390 [03:15<00:25,  3.37it/s] 78%|███████▊  | 304/390 [03:15<00:25,  3.39it/s] 78%|███████▊  | 305/390 [03:15<00:24,  3.40it/s] 78%|███████▊  | 306/390 [03:15<00:24,  3.41it/s] 79%|███████▊  | 307/390 [03:16<00:24,  3.42it/s] 79%|███████▉  | 308/390 [03:16<00:23,  3.43it/s] 79%|███████▉  | 309/390 [03:16<00:23,  3.43it/s] 79%|███████▉  | 310/390 [03:17<00:23,  3.43it/s] 80%|███████▉  | 311/390 [03:17<00:23,  3.43it/s] 80%|████████  | 312/390 [03:17<00:22,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 16:07:43,724 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:07:43,724 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 16:07:43,724 >>   Batch size = 8
{'eval_loss': 1.0105862617492676, 'eval_runtime': 19.6858, 'eval_samples_per_second': 349.694, 'eval_steps_per_second': 43.737, 'epoch': 2.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.24it/s][A
  1%|▏         | 12/861 [00:00<00:17, 47.75it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.11it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.25it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.72it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.63it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.34it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.20it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.30it/s][A
  6%|▌         | 52/861 [00:01<00:22, 35.45it/s][A
  7%|▋         | 57/861 [00:01<00:21, 37.86it/s][A
  7%|▋         | 62/861 [00:01<00:20, 39.72it/s][A
  8%|▊         | 67/861 [00:01<00:19, 41.10it/s][A
  8%|▊         | 72/861 [00:01<00:18, 41.99it/s][A
  9%|▉         | 77/861 [00:01<00:18, 42.83it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.39it/s][A
 10%|█         | 87/861 [00:02<00:17, 43.49it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.30it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.10it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.26it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 43.69it/s][A
 13%|█▎        | 112/861 [00:02<00:17, 43.94it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.04it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 44.23it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.36it/s][A
 15%|█▌        | 132/861 [00:03<00:16, 44.15it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.88it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 43.74it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 43.69it/s][A
 18%|█▊        | 152/861 [00:03<00:16, 43.79it/s][A
 18%|█▊        | 157/861 [00:03<00:16, 43.96it/s][A
 19%|█▉        | 162/861 [00:03<00:15, 44.20it/s][A
 19%|█▉        | 167/861 [00:03<00:15, 44.37it/s][A
 20%|█▉        | 172/861 [00:03<00:15, 44.35it/s][A
 21%|██        | 177/861 [00:04<00:15, 44.17it/s][A
 21%|██        | 182/861 [00:04<00:18, 37.04it/s][A
 22%|██▏       | 187/861 [00:04<00:17, 39.03it/s][A
 22%|██▏       | 192/861 [00:04<00:16, 40.58it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 41.59it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 42.48it/s][A
 24%|██▍       | 207/861 [00:04<00:15, 43.06it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.46it/s][A
 25%|██▌       | 217/861 [00:05<00:14, 43.59it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.31it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 43.25it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 43.50it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 43.81it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.05it/s][A
 29%|██▊       | 247/861 [00:05<00:13, 44.20it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 44.22it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 44.24it/s][A
 30%|███       | 262/861 [00:06<00:13, 43.98it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.81it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 43.65it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.77it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 43.82it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 44.14it/s][A
 34%|███▍      | 292/861 [00:06<00:12, 44.28it/s][A
 34%|███▍      | 297/861 [00:06<00:12, 44.28it/s][A
 35%|███▌      | 302/861 [00:06<00:12, 44.24it/s][A
 36%|███▌      | 307/861 [00:07<00:12, 44.04it/s][A
 36%|███▌      | 312/861 [00:07<00:15, 36.56it/s][A
 37%|███▋      | 317/861 [00:07<00:14, 38.72it/s][A
 37%|███▋      | 322/861 [00:07<00:13, 40.21it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 41.53it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 42.41it/s][A
 39%|███▉      | 337/861 [00:07<00:12, 43.08it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.57it/s][A
 40%|████      | 347/861 [00:08<00:11, 43.61it/s][A
 41%|████      | 352/861 [00:08<00:11, 43.40it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 43.12it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 43.46it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 43.58it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 43.95it/s][A
 44%|████▍     | 377/861 [00:08<00:10, 44.17it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 44.26it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 44.36it/s][A
 46%|████▌     | 392/861 [00:09<00:10, 44.13it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 43.74it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 43.61it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 43.69it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 43.91it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.07it/s][A
 49%|████▉     | 422/861 [00:09<00:09, 44.27it/s][A
 50%|████▉     | 427/861 [00:09<00:09, 44.40it/s][A
 50%|█████     | 432/861 [00:10<00:09, 44.29it/s][A
 51%|█████     | 437/861 [00:10<00:09, 44.05it/s][A
 51%|█████▏    | 442/861 [00:10<00:11, 37.70it/s][A
 52%|█████▏    | 447/861 [00:10<00:10, 39.54it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 40.95it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 41.93it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 42.75it/s][A
 54%|█████▍    | 467/861 [00:10<00:09, 43.24it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.64it/s][A
 55%|█████▌    | 477/861 [00:11<00:08, 43.76it/s][A
 56%|█████▌    | 482/861 [00:11<00:08, 43.41it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 43.05it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 43.35it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 43.75it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 43.91it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 44.17it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 44.30it/s][A
 60%|██████    | 517/861 [00:11<00:07, 44.33it/s][A
 61%|██████    | 522/861 [00:12<00:07, 44.01it/s][A
 61%|██████    | 527/861 [00:12<00:07, 43.77it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 43.64it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 43.76it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 43.88it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.04it/s][A
 64%|██████▍   | 552/861 [00:12<00:06, 44.22it/s][A
 65%|██████▍   | 557/861 [00:12<00:06, 44.32it/s][A
 65%|██████▌   | 562/861 [00:13<00:06, 44.30it/s][A
 66%|██████▌   | 567/861 [00:13<00:06, 44.06it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 42.26it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 42.71it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 43.09it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 43.55it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 43.77it/s][A
 69%|██████▉   | 597/861 [00:13<00:05, 44.02it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 44.13it/s][A
 70%|███████   | 607/861 [00:14<00:05, 44.05it/s][A
 71%|███████   | 612/861 [00:14<00:05, 43.70it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 43.67it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 43.76it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 43.86it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.07it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 44.12it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 44.28it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 44.21it/s][A
 76%|███████▌  | 652/861 [00:15<00:04, 44.09it/s][A
 76%|███████▋  | 657/861 [00:15<00:04, 43.93it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 43.94it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 43.91it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 43.94it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.07it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 44.18it/s][A
 80%|███████▉  | 687/861 [00:15<00:03, 44.24it/s][A
 80%|████████  | 692/861 [00:15<00:03, 44.21it/s][A
 81%|████████  | 697/861 [00:16<00:03, 44.04it/s][A
 82%|████████▏ | 702/861 [00:16<00:03, 44.02it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 39.92it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 41.24it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 42.08it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 42.88it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.35it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 43.69it/s][A
 86%|████████▌ | 737/861 [00:17<00:02, 43.82it/s][A
 86%|████████▌ | 742/861 [00:17<00:02, 43.77it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 43.32it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 43.38it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 43.65it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 43.89it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 44.10it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 44.30it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 44.20it/s][A
 91%|█████████ | 782/861 [00:18<00:01, 44.20it/s][A
 91%|█████████▏| 787/861 [00:18<00:01, 43.83it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 43.70it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 43.61it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 43.90it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.09it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 44.28it/s][A
 95%|█████████▍| 817/861 [00:18<00:00, 44.34it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 44.32it/s][A
 96%|█████████▌| 827/861 [00:19<00:00, 44.17it/s][A
 97%|█████████▋| 832/861 [00:19<00:00, 44.05it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 43.89it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 40.83it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 41.89it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 42.79it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.42it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.42it/s][A 80%|████████  | 312/390 [03:37<00:22,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:08:03,744 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 16:08:04,023 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:08:08,577 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:08:08,992 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:08:09,135 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:53<14:00, 10.91s/it] 81%|████████  | 314/390 [03:53<09:48,  7.75s/it] 81%|████████  | 315/390 [03:54<06:53,  5.51s/it] 81%|████████  | 316/390 [03:54<05:00,  4.06s/it] 81%|████████▏ | 317/390 [03:54<03:34,  2.94s/it] 82%|████████▏ | 318/390 [03:55<02:34,  2.14s/it] 82%|████████▏ | 319/390 [03:55<01:52,  1.59s/it] 82%|████████▏ | 320/390 [03:55<01:23,  1.20s/it] 82%|████████▏ | 321/390 [03:56<01:03,  1.08it/s] 83%|████████▎ | 322/390 [03:56<00:49,  1.36it/s] 83%|████████▎ | 323/390 [03:56<00:40,  1.63it/s] 83%|████████▎ | 324/390 [03:57<00:34,  1.94it/s] 83%|████████▎ | 325/390 [03:57<00:29,  2.23it/s] 84%|████████▎ | 326/390 [03:57<00:25,  2.50it/s] 84%|████████▍ | 327/390 [03:57<00:23,  2.72it/s] 84%|████████▍ | 328/390 [03:58<00:21,  2.90it/s] 84%|████████▍ | 329/390 [03:58<00:20,  3.05it/s] 85%|████████▍ | 330/390 [03:58<00:19,  3.15it/s] 85%|████████▍ | 331/390 [03:59<00:18,  3.23it/s] 85%|████████▌ | 332/390 [03:59<00:17,  3.29it/s] 85%|████████▌ | 333/390 [03:59<00:17,  3.34it/s] 86%|████████▌ | 334/390 [03:59<00:17,  3.25it/s] 86%|████████▌ | 335/390 [04:00<00:16,  3.30it/s] 86%|████████▌ | 336/390 [04:00<00:16,  3.34it/s] 86%|████████▋ | 337/390 [04:00<00:15,  3.37it/s] 87%|████████▋ | 338/390 [04:01<00:15,  3.39it/s] 87%|████████▋ | 339/390 [04:01<00:14,  3.41it/s] 87%|████████▋ | 340/390 [04:01<00:14,  3.42it/s] 87%|████████▋ | 341/390 [04:02<00:14,  3.43it/s] 88%|████████▊ | 342/390 [04:02<00:13,  3.43it/s] 88%|████████▊ | 343/390 [04:02<00:13,  3.43it/s] 88%|████████▊ | 344/390 [04:02<00:13,  3.44it/s] 88%|████████▊ | 345/390 [04:03<00:13,  3.29it/s] 89%|████████▊ | 346/390 [04:03<00:13,  3.33it/s] 89%|████████▉ | 347/390 [04:03<00:12,  3.37it/s] 89%|████████▉ | 348/390 [04:04<00:12,  3.39it/s] 89%|████████▉ | 349/390 [04:04<00:12,  3.40it/s] 90%|████████▉ | 350/390 [04:04<00:11,  3.41it/s] 90%|█████████ | 351/390 [04:04<00:11,  3.42it/s] 90%|█████████ | 352/390 [04:05<00:11,  3.43it/s] 91%|█████████ | 353/390 [04:05<00:10,  3.43it/s] 91%|█████████ | 354/390 [04:05<00:10,  3.43it/s] 91%|█████████ | 355/390 [04:06<00:10,  3.43it/s] 91%|█████████▏| 356/390 [04:06<00:10,  3.27it/s] 92%|█████████▏| 357/390 [04:06<00:09,  3.32it/s] 92%|█████████▏| 358/390 [04:07<00:09,  3.35it/s] 92%|█████████▏| 359/390 [04:07<00:09,  3.38it/s] 92%|█████████▏| 360/390 [04:07<00:08,  3.40it/s] 93%|█████████▎| 361/390 [04:07<00:08,  3.41it/s] 93%|█████████▎| 362/390 [04:08<00:08,  3.42it/s] 93%|█████████▎| 363/390 [04:08<00:07,  3.42it/s] 93%|█████████▎| 364/390 [04:08<00:07,  3.43it/s] 94%|█████████▎| 365/390 [04:09<00:07,  3.43it/s] 94%|█████████▍| 366/390 [04:09<00:06,  3.43it/s] 94%|█████████▍| 367/390 [04:09<00:06,  3.35it/s] 94%|█████████▍| 368/390 [04:10<00:06,  3.37it/s] 95%|█████████▍| 369/390 [04:10<00:06,  3.39it/s] 95%|█████████▍| 370/390 [04:10<00:05,  3.40it/s] 95%|█████████▌| 371/390 [04:10<00:05,  3.41it/s] 95%|█████████▌| 372/390 [04:11<00:05,  3.42it/s] 96%|█████████▌| 373/390 [04:11<00:04,  3.43it/s] 96%|█████████▌| 374/390 [04:11<00:04,  3.43it/s] 96%|█████████▌| 375/390 [04:12<00:04,  3.43it/s] 96%|█████████▋| 376/390 [04:12<00:04,  3.43it/s] 97%|█████████▋| 377/390 [04:12<00:03,  3.44it/s] 97%|█████████▋| 378/390 [04:12<00:03,  3.28it/s] 97%|█████████▋| 379/390 [04:13<00:03,  3.32it/s] 97%|█████████▋| 380/390 [04:13<00:02,  3.36it/s] 98%|█████████▊| 381/390 [04:13<00:02,  3.38it/s] 98%|█████████▊| 382/390 [04:14<00:02,  3.40it/s] 98%|█████████▊| 383/390 [04:14<00:02,  3.41it/s] 98%|█████████▊| 384/390 [04:14<00:01,  3.42it/s] 99%|█████████▊| 385/390 [04:14<00:01,  3.42it/s] 99%|█████████▉| 386/390 [04:15<00:01,  3.43it/s] 99%|█████████▉| 387/390 [04:15<00:00,  3.43it/s] 99%|█████████▉| 388/390 [04:15<00:00,  3.43it/s]100%|█████████▉| 389/390 [04:16<00:00,  3.29it/s]100%|██████████| 390/390 [04:16<00:00,  3.33it/s][INFO|trainer.py:2140] 2023-08-28 16:08:42,530 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:08:42,530 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 16:08:42,530 >>   Batch size = 8
{'eval_loss': 1.0201404094696045, 'eval_runtime': 19.8999, 'eval_samples_per_second': 345.931, 'eval_steps_per_second': 43.266, 'epoch': 3.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.72it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.39it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.32it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.27it/s][A
  3%|▎         | 27/861 [00:00<00:18, 44.84it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.38it/s][A
  4%|▍         | 37/861 [00:00<00:22, 37.31it/s][A
  5%|▍         | 42/861 [00:00<00:20, 39.35it/s][A
  5%|▌         | 47/861 [00:01<00:19, 40.78it/s][A
  6%|▌         | 52/861 [00:01<00:19, 41.78it/s][A
  7%|▋         | 57/861 [00:01<00:18, 42.55it/s][A
  7%|▋         | 62/861 [00:01<00:18, 43.17it/s][A
  8%|▊         | 67/861 [00:01<00:18, 43.58it/s][A
  8%|▊         | 72/861 [00:01<00:18, 43.71it/s][A
  9%|▉         | 77/861 [00:01<00:18, 43.41it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.42it/s][A
 10%|█         | 87/861 [00:02<00:17, 43.68it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.77it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 44.00it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.22it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.27it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.32it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.03it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.89it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 43.78it/s][A
 15%|█▌        | 132/861 [00:03<00:16, 43.85it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 43.95it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.10it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.22it/s][A
 18%|█▊        | 152/861 [00:03<00:15, 44.32it/s][A
 18%|█▊        | 157/861 [00:03<00:15, 44.24it/s][A
 19%|█▉        | 162/861 [00:03<00:19, 36.33it/s][A
 19%|█▉        | 167/861 [00:03<00:18, 38.45it/s][A
 20%|█▉        | 172/861 [00:04<00:17, 40.14it/s][A
 21%|██        | 177/861 [00:04<00:16, 41.35it/s][A
 21%|██        | 182/861 [00:04<00:16, 42.26it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 42.91it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 43.37it/s][A
 23%|██▎       | 197/861 [00:04<00:15, 43.60it/s][A
 23%|██▎       | 202/861 [00:04<00:15, 43.27it/s][A
 24%|██▍       | 207/861 [00:04<00:15, 43.18it/s][A
 25%|██▍       | 212/861 [00:04<00:14, 43.39it/s][A
 25%|██▌       | 217/861 [00:05<00:14, 43.68it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.88it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.18it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.35it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.36it/s][A
 28%|██▊       | 242/861 [00:05<00:14, 44.08it/s][A
 29%|██▊       | 247/861 [00:05<00:14, 43.84it/s][A
 29%|██▉       | 252/861 [00:05<00:13, 43.70it/s][A
 30%|██▉       | 257/861 [00:05<00:13, 43.66it/s][A
 30%|███       | 262/861 [00:06<00:13, 43.87it/s][A
 31%|███       | 267/861 [00:06<00:13, 43.98it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 44.23it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 44.30it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 44.35it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 44.05it/s][A
 34%|███▍      | 292/861 [00:06<00:15, 37.11it/s][A
 34%|███▍      | 297/861 [00:06<00:14, 39.13it/s][A
 35%|███▌      | 302/861 [00:07<00:13, 40.61it/s][A
 36%|███▌      | 307/861 [00:07<00:13, 41.73it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 42.55it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 43.15it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 43.66it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 43.68it/s][A
 39%|███▊      | 332/861 [00:07<00:12, 43.40it/s][A
 39%|███▉      | 337/861 [00:07<00:12, 43.14it/s][A
 40%|███▉      | 342/861 [00:07<00:11, 43.42it/s][A
 40%|████      | 347/861 [00:08<00:11, 43.66it/s][A
 41%|████      | 352/861 [00:08<00:11, 43.89it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.14it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.29it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.37it/s][A
 43%|████▎     | 372/861 [00:08<00:11, 44.15it/s][A
 44%|████▍     | 377/861 [00:08<00:11, 43.75it/s][A
 44%|████▍     | 382/861 [00:08<00:10, 43.56it/s][A
 45%|████▍     | 387/861 [00:08<00:10, 43.73it/s][A
 46%|████▌     | 392/861 [00:09<00:10, 43.97it/s][A
 46%|████▌     | 397/861 [00:09<00:10, 44.09it/s][A
 47%|████▋     | 402/861 [00:09<00:10, 44.27it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 44.38it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 44.38it/s][A
 48%|████▊     | 417/861 [00:09<00:10, 44.10it/s][A
 49%|████▉     | 422/861 [00:09<00:11, 38.52it/s][A
 50%|████▉     | 427/861 [00:09<00:10, 40.18it/s][A
 50%|█████     | 432/861 [00:10<00:10, 41.43it/s][A
 51%|█████     | 437/861 [00:10<00:10, 42.15it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 43.05it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 43.52it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 43.80it/s][A
 53%|█████▎    | 457/861 [00:10<00:09, 43.75it/s][A
 54%|█████▎    | 462/861 [00:10<00:09, 43.46it/s][A
 54%|█████▍    | 467/861 [00:10<00:09, 43.30it/s][A
 55%|█████▍    | 472/861 [00:10<00:08, 43.44it/s][A
 55%|█████▌    | 477/861 [00:11<00:08, 43.64it/s][A
 56%|█████▌    | 482/861 [00:11<00:08, 43.95it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.16it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.24it/s][A
 58%|█████▊    | 497/861 [00:11<00:08, 44.27it/s][A
 58%|█████▊    | 502/861 [00:11<00:08, 44.01it/s][A
 59%|█████▉    | 507/861 [00:11<00:08, 43.74it/s][A
 59%|█████▉    | 512/861 [00:11<00:07, 43.73it/s][A
 60%|██████    | 517/861 [00:11<00:07, 43.68it/s][A
 61%|██████    | 522/861 [00:12<00:07, 43.83it/s][A
 61%|██████    | 527/861 [00:12<00:07, 44.01it/s][A
 62%|██████▏   | 532/861 [00:12<00:07, 44.13it/s][A
 62%|██████▏   | 537/861 [00:12<00:07, 44.35it/s][A
 63%|██████▎   | 542/861 [00:12<00:07, 44.29it/s][A
 64%|██████▎   | 547/861 [00:12<00:07, 44.12it/s][A
 64%|██████▍   | 552/861 [00:12<00:07, 42.81it/s][A
 65%|██████▍   | 557/861 [00:12<00:07, 43.15it/s][A
 65%|██████▌   | 562/861 [00:12<00:06, 43.49it/s][A
 66%|██████▌   | 567/861 [00:13<00:06, 43.40it/s][A
 66%|██████▋   | 572/861 [00:13<00:06, 43.76it/s][A
 67%|██████▋   | 577/861 [00:13<00:06, 44.03it/s][A
 68%|██████▊   | 582/861 [00:13<00:06, 44.11it/s][A
 68%|██████▊   | 587/861 [00:13<00:06, 44.07it/s][A
 69%|██████▉   | 592/861 [00:13<00:06, 43.87it/s][A
 69%|██████▉   | 597/861 [00:13<00:06, 43.73it/s][A
 70%|██████▉   | 602/861 [00:13<00:05, 43.76it/s][A
 70%|███████   | 607/861 [00:14<00:05, 43.76it/s][A
 71%|███████   | 612/861 [00:14<00:05, 43.95it/s][A
 72%|███████▏  | 617/861 [00:14<00:05, 44.08it/s][A
 72%|███████▏  | 622/861 [00:14<00:05, 44.33it/s][A
 73%|███████▎  | 627/861 [00:14<00:05, 44.32it/s][A
 73%|███████▎  | 632/861 [00:14<00:05, 44.17it/s][A
 74%|███████▍  | 637/861 [00:14<00:05, 43.93it/s][A
 75%|███████▍  | 642/861 [00:14<00:04, 43.80it/s][A
 75%|███████▌  | 647/861 [00:14<00:04, 43.77it/s][A
 76%|███████▌  | 652/861 [00:15<00:04, 43.91it/s][A
 76%|███████▋  | 657/861 [00:15<00:04, 44.10it/s][A
 77%|███████▋  | 662/861 [00:15<00:04, 44.16it/s][A
 77%|███████▋  | 667/861 [00:15<00:04, 44.28it/s][A
 78%|███████▊  | 672/861 [00:15<00:04, 44.24it/s][A
 79%|███████▊  | 677/861 [00:15<00:04, 44.06it/s][A
 79%|███████▉  | 682/861 [00:15<00:04, 43.94it/s][A
 80%|███████▉  | 687/861 [00:15<00:04, 42.38it/s][A
 80%|████████  | 692/861 [00:15<00:03, 42.95it/s][A
 81%|████████  | 697/861 [00:16<00:03, 43.33it/s][A
 82%|████████▏ | 702/861 [00:16<00:03, 43.58it/s][A
 82%|████████▏ | 707/861 [00:16<00:03, 43.85it/s][A
 83%|████████▎ | 712/861 [00:16<00:03, 43.95it/s][A
 83%|████████▎ | 717/861 [00:16<00:03, 43.95it/s][A
 84%|████████▍ | 722/861 [00:16<00:03, 43.82it/s][A
 84%|████████▍ | 727/861 [00:16<00:03, 43.62it/s][A
 85%|████████▌ | 732/861 [00:16<00:02, 43.70it/s][A
 86%|████████▌ | 737/861 [00:16<00:02, 43.88it/s][A
 86%|████████▌ | 742/861 [00:17<00:02, 43.99it/s][A
 87%|████████▋ | 747/861 [00:17<00:02, 44.09it/s][A
 87%|████████▋ | 752/861 [00:17<00:02, 44.15it/s][A
 88%|████████▊ | 757/861 [00:17<00:02, 44.23it/s][A
 89%|████████▊ | 762/861 [00:17<00:02, 44.07it/s][A
 89%|████████▉ | 767/861 [00:17<00:02, 43.89it/s][A
 90%|████████▉ | 772/861 [00:17<00:02, 43.77it/s][A
 90%|█████████ | 777/861 [00:17<00:01, 43.85it/s][A
 91%|█████████ | 782/861 [00:18<00:01, 43.94it/s][A
 91%|█████████▏| 787/861 [00:18<00:01, 44.06it/s][A
 92%|█████████▏| 792/861 [00:18<00:01, 44.16it/s][A
 93%|█████████▎| 797/861 [00:18<00:01, 44.24it/s][A
 93%|█████████▎| 802/861 [00:18<00:01, 44.20it/s][A
 94%|█████████▎| 807/861 [00:18<00:01, 44.07it/s][A
 94%|█████████▍| 812/861 [00:18<00:01, 43.90it/s][A
 95%|█████████▍| 817/861 [00:18<00:01, 43.74it/s][A
 95%|█████████▌| 822/861 [00:18<00:00, 39.58it/s][A
 96%|█████████▌| 827/861 [00:19<00:00, 40.88it/s][A
 97%|█████████▋| 832/861 [00:19<00:00, 41.99it/s][A
 97%|█████████▋| 837/861 [00:19<00:00, 42.71it/s][A
 98%|█████████▊| 842/861 [00:19<00:00, 43.24it/s][A
 98%|█████████▊| 847/861 [00:19<00:00, 43.64it/s][A
 99%|█████████▉| 852/861 [00:19<00:00, 43.77it/s][A
100%|█████████▉| 857/861 [00:19<00:00, 43.52it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:19<00:00, 43.52it/s][A100%|██████████| 390/390 [04:36<00:00,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:09:02,704 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 16:09:02,986 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:09:13,789 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:09:14,073 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:09:14,171 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 16:09:23,305 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 16:09:23,341 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78 (score: 0.9791051745414734).
                                                 100%|██████████| 390/390 [05:10<00:00,  3.33it/s]100%|██████████| 390/390 [05:10<00:00,  1.25it/s]
[INFO|trainer.py:1894] 2023-08-28 16:09:36,975 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 16:09:37,239 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:09:40,892 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:09:41,121 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:09:41,231 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:09:41,880 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:09:41,881 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:09:41,881 >>   train_loss               =     0.5047
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:09:41,881 >>   train_runtime            = 0:05:10.90
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:09:41,881 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:09:41,881 >>   train_samples_per_second =     80.394
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:09:41,881 >>   train_steps_per_second   =      1.254
{'eval_loss': 1.0239449739456177, 'eval_runtime': 19.8611, 'eval_samples_per_second': 346.608, 'eval_steps_per_second': 43.351, 'epoch': 4.99}
{'train_runtime': 310.907, 'train_samples_per_second': 80.394, 'train_steps_per_second': 1.254, 'train_loss': 0.5046783056014623, 'epoch': 4.99}
08/28/2023 16:09:42 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 16:09:42,178 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:09:42,178 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 16:09:42,178 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 55.55it/s]  1%|▏         | 12/861 [00:00<00:17, 48.97it/s]  2%|▏         | 17/861 [00:00<00:17, 47.23it/s]  3%|▎         | 22/861 [00:00<00:18, 46.32it/s]  3%|▎         | 27/861 [00:00<00:18, 45.98it/s]  4%|▎         | 32/861 [00:00<00:18, 45.55it/s]  4%|▍         | 37/861 [00:00<00:18, 45.45it/s]  5%|▍         | 42/861 [00:00<00:18, 44.88it/s]  5%|▌         | 47/861 [00:01<00:18, 44.36it/s]  6%|▌         | 52/861 [00:01<00:18, 44.10it/s]  7%|▋         | 57/861 [00:01<00:18, 44.21it/s]  7%|▋         | 62/861 [00:01<00:17, 44.40it/s]  8%|▊         | 67/861 [00:01<00:17, 44.56it/s]  8%|▊         | 72/861 [00:01<00:17, 44.73it/s]  9%|▉         | 77/861 [00:01<00:17, 44.78it/s] 10%|▉         | 82/861 [00:01<00:17, 43.50it/s] 10%|█         | 87/861 [00:01<00:17, 43.63it/s] 11%|█         | 92/861 [00:02<00:17, 43.64it/s] 11%|█▏        | 97/861 [00:02<00:17, 43.65it/s] 12%|█▏        | 102/861 [00:02<00:17, 43.87it/s] 12%|█▏        | 107/861 [00:02<00:17, 44.15it/s] 13%|█▎        | 112/861 [00:02<00:16, 44.32it/s] 14%|█▎        | 117/861 [00:02<00:16, 44.51it/s] 14%|█▍        | 122/861 [00:02<00:16, 44.51it/s] 15%|█▍        | 127/861 [00:02<00:16, 44.52it/s] 15%|█▌        | 132/861 [00:02<00:16, 44.39it/s] 16%|█▌        | 137/861 [00:03<00:16, 44.14it/s] 16%|█▋        | 142/861 [00:03<00:16, 44.08it/s] 17%|█▋        | 147/861 [00:03<00:16, 44.21it/s] 18%|█▊        | 152/861 [00:03<00:15, 44.33it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.46it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.61it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.62it/s] 20%|█▉        | 172/861 [00:03<00:15, 44.54it/s] 21%|██        | 177/861 [00:03<00:15, 44.32it/s] 21%|██        | 182/861 [00:04<00:15, 44.20it/s] 22%|██▏       | 187/861 [00:04<00:15, 44.10it/s] 22%|██▏       | 192/861 [00:04<00:15, 44.21it/s] 23%|██▎       | 197/861 [00:04<00:14, 44.39it/s] 23%|██▎       | 202/861 [00:04<00:14, 44.51it/s] 24%|██▍       | 207/861 [00:04<00:14, 44.56it/s] 25%|██▍       | 212/861 [00:04<00:14, 44.53it/s] 25%|██▌       | 217/861 [00:04<00:16, 37.93it/s] 26%|██▌       | 222/861 [00:05<00:16, 39.82it/s] 26%|██▋       | 227/861 [00:05<00:15, 41.18it/s] 27%|██▋       | 232/861 [00:05<00:14, 42.24it/s] 28%|██▊       | 237/861 [00:05<00:14, 42.95it/s] 28%|██▊       | 242/861 [00:05<00:14, 43.55it/s] 29%|██▊       | 247/861 [00:05<00:13, 43.94it/s] 29%|██▉       | 252/861 [00:05<00:13, 43.99it/s] 30%|██▉       | 257/861 [00:05<00:13, 43.69it/s] 30%|███       | 262/861 [00:05<00:13, 43.59it/s] 31%|███       | 267/861 [00:06<00:13, 43.76it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.05it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.25it/s] 33%|███▎      | 282/861 [00:06<00:13, 44.44it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.53it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.66it/s] 34%|███▍      | 297/861 [00:06<00:12, 44.50it/s] 35%|███▌      | 302/861 [00:06<00:12, 44.05it/s] 36%|███▌      | 307/861 [00:06<00:12, 43.92it/s] 36%|███▌      | 312/861 [00:07<00:12, 44.01it/s] 37%|███▋      | 317/861 [00:07<00:12, 44.16it/s] 37%|███▋      | 322/861 [00:07<00:12, 44.38it/s] 38%|███▊      | 327/861 [00:07<00:12, 44.48it/s] 39%|███▊      | 332/861 [00:07<00:11, 44.58it/s] 39%|███▉      | 337/861 [00:07<00:11, 44.65it/s] 40%|███▉      | 342/861 [00:07<00:11, 44.42it/s] 40%|████      | 347/861 [00:07<00:11, 44.17it/s] 41%|████      | 352/861 [00:08<00:13, 38.79it/s] 41%|████▏     | 357/861 [00:08<00:12, 40.50it/s] 42%|████▏     | 362/861 [00:08<00:11, 41.64it/s] 43%|████▎     | 367/861 [00:08<00:11, 42.61it/s] 43%|████▎     | 372/861 [00:08<00:11, 43.27it/s] 44%|████▍     | 377/861 [00:08<00:11, 43.69it/s] 44%|████▍     | 382/861 [00:08<00:10, 44.08it/s] 45%|████▍     | 387/861 [00:08<00:10, 44.10it/s] 46%|████▌     | 392/861 [00:08<00:10, 43.77it/s] 46%|████▌     | 397/861 [00:09<00:10, 43.52it/s] 47%|████▋     | 402/861 [00:09<00:10, 43.66it/s] 47%|████▋     | 407/861 [00:09<00:10, 43.98it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.25it/s] 48%|████▊     | 417/861 [00:09<00:09, 44.43it/s] 49%|████▉     | 422/861 [00:09<00:09, 44.59it/s] 50%|████▉     | 427/861 [00:09<00:09, 44.62it/s] 50%|█████     | 432/861 [00:09<00:09, 44.40it/s] 51%|█████     | 437/861 [00:09<00:09, 44.12it/s] 51%|█████▏    | 442/861 [00:10<00:09, 43.93it/s] 52%|█████▏    | 447/861 [00:10<00:09, 43.88it/s] 52%|█████▏    | 452/861 [00:10<00:09, 44.06it/s] 53%|█████▎    | 457/861 [00:10<00:09, 44.31it/s] 54%|█████▎    | 462/861 [00:10<00:08, 44.50it/s] 54%|█████▍    | 467/861 [00:10<00:08, 44.65it/s] 55%|█████▍    | 472/861 [00:10<00:08, 44.59it/s] 55%|█████▌    | 477/861 [00:10<00:08, 44.45it/s] 56%|█████▌    | 482/861 [00:10<00:08, 44.07it/s] 57%|█████▋    | 487/861 [00:11<00:09, 38.69it/s] 57%|█████▋    | 492/861 [00:11<00:09, 40.36it/s] 58%|█████▊    | 497/861 [00:11<00:08, 41.68it/s] 58%|█████▊    | 502/861 [00:11<00:08, 42.58it/s] 59%|█████▉    | 507/861 [00:11<00:08, 43.27it/s] 59%|█████▉    | 512/861 [00:11<00:07, 43.72it/s] 60%|██████    | 517/861 [00:11<00:07, 44.09it/s] 61%|██████    | 522/861 [00:11<00:07, 44.09it/s] 61%|██████    | 527/861 [00:12<00:07, 43.69it/s] 62%|██████▏   | 532/861 [00:12<00:07, 43.45it/s] 62%|██████▏   | 537/861 [00:12<00:07, 43.62it/s] 63%|██████▎   | 542/861 [00:12<00:07, 43.78it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.16it/s] 64%|██████▍   | 552/861 [00:12<00:06, 44.40it/s] 65%|██████▍   | 557/861 [00:12<00:06, 44.56it/s] 65%|██████▌   | 562/861 [00:12<00:06, 44.64it/s] 66%|██████▌   | 567/861 [00:12<00:06, 44.40it/s] 66%|██████▋   | 572/861 [00:13<00:06, 44.11it/s] 67%|██████▋   | 577/861 [00:13<00:06, 43.82it/s] 68%|██████▊   | 582/861 [00:13<00:06, 43.90it/s] 68%|██████▊   | 587/861 [00:13<00:06, 43.94it/s] 69%|██████▉   | 592/861 [00:13<00:06, 44.26it/s] 69%|██████▉   | 597/861 [00:13<00:05, 44.44it/s] 70%|██████▉   | 602/861 [00:13<00:05, 44.58it/s] 70%|███████   | 607/861 [00:13<00:05, 44.59it/s] 71%|███████   | 612/861 [00:13<00:05, 44.33it/s] 72%|███████▏  | 617/861 [00:14<00:05, 44.09it/s] 72%|███████▏  | 622/861 [00:14<00:05, 42.37it/s] 73%|███████▎  | 627/861 [00:14<00:05, 42.85it/s] 73%|███████▎  | 632/861 [00:14<00:05, 43.27it/s] 74%|███████▍  | 637/861 [00:14<00:05, 43.60it/s] 75%|███████▍  | 642/861 [00:14<00:04, 43.98it/s] 75%|███████▌  | 647/861 [00:14<00:04, 44.32it/s] 76%|███████▌  | 652/861 [00:14<00:04, 44.28it/s] 76%|███████▋  | 657/861 [00:14<00:04, 44.16it/s] 77%|███████▋  | 662/861 [00:15<00:04, 43.82it/s] 77%|███████▋  | 667/861 [00:15<00:04, 43.93it/s] 78%|███████▊  | 672/861 [00:15<00:04, 44.02it/s] 79%|███████▊  | 677/861 [00:15<00:04, 44.16it/s] 79%|███████▉  | 682/861 [00:15<00:04, 44.28it/s] 80%|███████▉  | 687/861 [00:15<00:03, 44.49it/s] 80%|████████  | 692/861 [00:15<00:03, 44.49it/s] 81%|████████  | 697/861 [00:15<00:03, 44.33it/s] 82%|████████▏ | 702/861 [00:15<00:03, 44.04it/s] 82%|████████▏ | 707/861 [00:16<00:03, 43.94it/s] 83%|████████▎ | 712/861 [00:16<00:03, 43.89it/s] 83%|████████▎ | 717/861 [00:16<00:03, 44.02it/s] 84%|████████▍ | 722/861 [00:16<00:03, 44.27it/s] 84%|████████▍ | 727/861 [00:16<00:03, 44.31it/s] 85%|████████▌ | 732/861 [00:16<00:02, 44.55it/s] 86%|████████▌ | 737/861 [00:16<00:02, 44.61it/s] 86%|████████▌ | 742/861 [00:16<00:02, 44.36it/s] 87%|████████▋ | 747/861 [00:17<00:02, 44.16it/s] 87%|████████▋ | 752/861 [00:17<00:02, 43.90it/s] 88%|████████▊ | 757/861 [00:17<00:02, 39.16it/s] 89%|████████▊ | 762/861 [00:17<00:02, 40.68it/s] 89%|████████▉ | 767/861 [00:17<00:02, 41.86it/s] 90%|████████▉ | 772/861 [00:17<00:02, 42.73it/s] 90%|█████████ | 777/861 [00:17<00:01, 43.21it/s] 91%|█████████ | 782/861 [00:17<00:01, 43.63it/s] 91%|█████████▏| 787/861 [00:17<00:01, 43.99it/s] 92%|█████████▏| 792/861 [00:18<00:01, 43.94it/s] 93%|█████████▎| 797/861 [00:18<00:01, 43.64it/s] 93%|█████████▎| 802/861 [00:18<00:01, 43.47it/s] 94%|█████████▎| 807/861 [00:18<00:01, 43.70it/s] 94%|█████████▍| 812/861 [00:18<00:01, 43.98it/s] 95%|█████████▍| 817/861 [00:18<00:00, 44.14it/s] 95%|█████████▌| 822/861 [00:18<00:00, 44.42it/s] 96%|█████████▌| 827/861 [00:18<00:00, 44.47it/s] 97%|█████████▋| 832/861 [00:18<00:00, 44.49it/s] 97%|█████████▋| 837/861 [00:19<00:00, 44.24it/s] 98%|█████████▊| 842/861 [00:19<00:00, 44.01it/s] 98%|█████████▊| 847/861 [00:19<00:00, 43.84it/s] 99%|█████████▉| 852/861 [00:19<00:00, 43.90it/s]100%|█████████▉| 857/861 [00:19<00:00, 44.12it/s]100%|██████████| 861/861 [00:19<00:00, 43.85it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:10:01,831 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:10:01,831 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:10:01,831 >>   eval_loss               =     0.9791
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:10:01,831 >>   eval_runtime            = 0:00:19.65
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:10:01,831 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:10:01,831 >>   eval_samples_per_second =    350.277
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:10:01,831 >>   eval_steps_per_second   =      43.81
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:10:01,831 >>   perplexity              =     2.6621
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:15,457 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:15,496 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:15,496 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:15,496 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:15,496 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:10:16,441 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:10:16,442 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:10:17,065 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:10:18,210 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:10:18,210 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:21,310 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:21,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:21,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:21,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:10:21,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:10:22,243 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:10:22,244 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:10:22,894 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:10:23,111 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:10:23,111 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.41it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.45it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.60it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:11,  1.59it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:13,  1.56it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.58it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.58it/s]Extractor Predicting: 26it [00:17,  1.58it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.56it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.42it/s]Extractor Predicting: 34it [00:22,  1.45it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:23,  1.48it/s]Extractor Predicting: 37it [00:24,  1.49it/s]Extractor Predicting: 38it [00:25,  1.48it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.53it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:30,  1.60it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.54it/s]Extractor Predicting: 49it [00:32,  1.54it/s]Extractor Predicting: 50it [00:32,  1.55it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:34,  1.52it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:36,  1.50it/s]Extractor Predicting: 56it [00:36,  1.52it/s]Extractor Predicting: 57it [00:37,  1.55it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.53it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:42,  1.50it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.55it/s]Extractor Predicting: 68it [00:44,  1.55it/s]Extractor Predicting: 69it [00:45,  1.54it/s]Extractor Predicting: 70it [00:46,  1.49it/s]Extractor Predicting: 71it [00:46,  1.49it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:48,  1.51it/s]Extractor Predicting: 74it [00:48,  1.53it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:49,  1.51it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.52it/s]Extractor Predicting: 79it [00:51,  1.56it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:53,  1.60it/s]Extractor Predicting: 82it [00:53,  1.67it/s]Extractor Predicting: 83it [00:54,  1.63it/s]Extractor Predicting: 84it [00:54,  1.63it/s]Extractor Predicting: 85it [00:55,  1.62it/s]Extractor Predicting: 86it [00:56,  1.64it/s]Extractor Predicting: 87it [00:56,  1.62it/s]Extractor Predicting: 88it [00:57,  1.63it/s]Extractor Predicting: 89it [00:58,  1.62it/s]Extractor Predicting: 90it [00:58,  1.64it/s]Extractor Predicting: 91it [00:59,  1.65it/s]Extractor Predicting: 92it [00:59,  1.68it/s]Extractor Predicting: 93it [01:00,  1.65it/s]Extractor Predicting: 94it [01:00,  1.67it/s]Extractor Predicting: 95it [01:01,  1.70it/s]Extractor Predicting: 96it [01:02,  1.64it/s]Extractor Predicting: 97it [01:02,  1.69it/s]Extractor Predicting: 98it [01:03,  1.69it/s]Extractor Predicting: 99it [01:03,  1.69it/s]Extractor Predicting: 100it [01:04,  1.69it/s]Extractor Predicting: 101it [01:05,  1.66it/s]Extractor Predicting: 102it [01:05,  1.63it/s]Extractor Predicting: 103it [01:06,  1.65it/s]Extractor Predicting: 104it [01:06,  1.65it/s]Extractor Predicting: 105it [01:07,  1.65it/s]Extractor Predicting: 106it [01:08,  1.67it/s]Extractor Predicting: 107it [01:08,  1.61it/s]Extractor Predicting: 108it [01:09,  1.68it/s]Extractor Predicting: 109it [01:10,  1.67it/s]Extractor Predicting: 110it [01:10,  1.70it/s]Extractor Predicting: 111it [01:11,  1.71it/s]Extractor Predicting: 112it [01:11,  1.66it/s]Extractor Predicting: 113it [01:12,  1.66it/s]Extractor Predicting: 114it [01:13,  1.64it/s]Extractor Predicting: 115it [01:13,  1.66it/s]Extractor Predicting: 116it [01:14,  1.65it/s]Extractor Predicting: 117it [01:14,  1.63it/s]Extractor Predicting: 118it [01:15,  1.64it/s]Extractor Predicting: 119it [01:16,  1.67it/s]Extractor Predicting: 120it [01:16,  1.66it/s]Extractor Predicting: 121it [01:17,  1.67it/s]Extractor Predicting: 122it [01:17,  1.62it/s]Extractor Predicting: 123it [01:18,  1.68it/s]Extractor Predicting: 124it [01:19,  1.64it/s]Extractor Predicting: 125it [01:19,  1.66it/s]Extractor Predicting: 126it [01:20,  1.68it/s]Extractor Predicting: 127it [01:21,  1.53it/s]Extractor Predicting: 128it [01:21,  1.51it/s]Extractor Predicting: 129it [01:22,  1.58it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.61it/s]Extractor Predicting: 132it [01:24,  1.66it/s]Extractor Predicting: 133it [01:24,  1.66it/s]Extractor Predicting: 134it [01:25,  1.66it/s]Extractor Predicting: 135it [01:25,  1.67it/s]Extractor Predicting: 136it [01:26,  1.64it/s]Extractor Predicting: 137it [01:27,  1.63it/s]Extractor Predicting: 138it [01:27,  1.64it/s]Extractor Predicting: 139it [01:28,  1.61it/s]Extractor Predicting: 140it [01:28,  1.60it/s]Extractor Predicting: 141it [01:29,  1.59it/s]Extractor Predicting: 142it [01:30,  1.59it/s]Extractor Predicting: 143it [01:30,  1.64it/s]Extractor Predicting: 144it [01:31,  1.61it/s]Extractor Predicting: 145it [01:32,  1.62it/s]Extractor Predicting: 146it [01:32,  1.62it/s]Extractor Predicting: 147it [01:33,  1.62it/s]Extractor Predicting: 148it [01:33,  1.63it/s]Extractor Predicting: 149it [01:34,  1.68it/s]Extractor Predicting: 150it [01:35,  1.61it/s]Extractor Predicting: 151it [01:35,  1.55it/s]Extractor Predicting: 152it [01:36,  1.51it/s]Extractor Predicting: 153it [01:37,  1.49it/s]Extractor Predicting: 154it [01:38,  1.41it/s]Extractor Predicting: 155it [01:38,  1.44it/s]Extractor Predicting: 156it [01:39,  1.40it/s]Extractor Predicting: 157it [01:40,  1.44it/s]Extractor Predicting: 158it [01:40,  1.43it/s]Extractor Predicting: 159it [01:41,  1.42it/s]Extractor Predicting: 160it [01:42,  1.41it/s]Extractor Predicting: 161it [01:42,  1.44it/s]Extractor Predicting: 162it [01:43,  1.40it/s]Extractor Predicting: 163it [01:44,  1.38it/s]Extractor Predicting: 164it [01:45,  1.38it/s]Extractor Predicting: 165it [01:45,  1.36it/s]Extractor Predicting: 166it [01:46,  1.36it/s]Extractor Predicting: 167it [01:47,  1.37it/s]Extractor Predicting: 168it [01:48,  1.36it/s]Extractor Predicting: 169it [01:48,  1.35it/s]Extractor Predicting: 170it [01:49,  1.36it/s]Extractor Predicting: 171it [01:50,  1.38it/s]Extractor Predicting: 172it [01:51,  1.36it/s]Extractor Predicting: 173it [01:51,  1.36it/s]Extractor Predicting: 174it [01:52,  1.40it/s]Extractor Predicting: 175it [01:53,  1.40it/s]Extractor Predicting: 176it [01:53,  1.38it/s]Extractor Predicting: 177it [01:54,  1.41it/s]Extractor Predicting: 178it [01:55,  1.39it/s]Extractor Predicting: 179it [01:56,  1.38it/s]Extractor Predicting: 180it [01:56,  1.37it/s]Extractor Predicting: 181it [01:57,  1.38it/s]Extractor Predicting: 182it [01:58,  1.38it/s]Extractor Predicting: 183it [01:58,  1.39it/s]Extractor Predicting: 184it [01:59,  1.42it/s]Extractor Predicting: 185it [02:00,  1.44it/s]Extractor Predicting: 186it [02:00,  1.44it/s]Extractor Predicting: 187it [02:01,  1.42it/s]Extractor Predicting: 188it [02:02,  1.44it/s]Extractor Predicting: 189it [02:03,  1.39it/s]Extractor Predicting: 190it [02:03,  1.38it/s]Extractor Predicting: 191it [02:04,  1.36it/s]Extractor Predicting: 192it [02:05,  1.33it/s]Extractor Predicting: 193it [02:06,  1.36it/s]Extractor Predicting: 194it [02:06,  1.39it/s]Extractor Predicting: 195it [02:07,  1.40it/s]Extractor Predicting: 196it [02:08,  1.43it/s]Extractor Predicting: 197it [02:08,  1.45it/s]Extractor Predicting: 198it [02:09,  1.45it/s]Extractor Predicting: 199it [02:10,  1.47it/s]Extractor Predicting: 200it [02:10,  1.48it/s]Extractor Predicting: 201it [02:11,  1.42it/s]Extractor Predicting: 202it [02:12,  1.44it/s]Extractor Predicting: 203it [02:12,  1.46it/s]Extractor Predicting: 204it [02:13,  1.46it/s]Extractor Predicting: 205it [02:14,  1.51it/s]Extractor Predicting: 206it [02:14,  1.47it/s]Extractor Predicting: 207it [02:15,  1.48it/s]Extractor Predicting: 208it [02:16,  1.49it/s]Extractor Predicting: 209it [02:16,  1.54it/s]Extractor Predicting: 210it [02:17,  1.53it/s]Extractor Predicting: 211it [02:18,  1.43it/s]Extractor Predicting: 212it [02:19,  1.45it/s]Extractor Predicting: 213it [02:19,  1.48it/s]Extractor Predicting: 214it [02:20,  1.49it/s]Extractor Predicting: 215it [02:20,  1.53it/s]Extractor Predicting: 216it [02:21,  1.49it/s]Extractor Predicting: 217it [02:22,  1.49it/s]Extractor Predicting: 218it [02:22,  1.53it/s]Extractor Predicting: 219it [02:23,  1.49it/s]Extractor Predicting: 220it [02:24,  1.48it/s]Extractor Predicting: 221it [02:25,  1.33it/s]Extractor Predicting: 222it [02:25,  1.38it/s]Extractor Predicting: 223it [02:26,  1.43it/s]Extractor Predicting: 224it [02:27,  1.44it/s]Extractor Predicting: 225it [02:27,  1.47it/s]Extractor Predicting: 226it [02:28,  1.43it/s]Extractor Predicting: 227it [02:29,  1.44it/s]Extractor Predicting: 228it [02:30,  1.46it/s]Extractor Predicting: 229it [02:30,  1.47it/s]Extractor Predicting: 230it [02:31,  1.48it/s]Extractor Predicting: 231it [02:31,  1.51it/s]Extractor Predicting: 232it [02:32,  1.53it/s]Extractor Predicting: 233it [02:33,  1.50it/s]Extractor Predicting: 234it [02:33,  1.50it/s]Extractor Predicting: 235it [02:34,  1.48it/s]Extractor Predicting: 236it [02:35,  1.48it/s]Extractor Predicting: 237it [02:36,  1.43it/s]Extractor Predicting: 237it [02:36,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:14,065 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:14,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:14,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:14,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:14,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:13:14,969 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:13:14,970 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:13:15,616 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:13:16,762 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:13:16,762 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:19,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:19,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:19,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:19,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:13:19,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:13:20,720 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:13:20,721 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:13:21,374 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:13:21,620 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:13:21,620 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.41576506955177744,
  "recall": 0.03907611853573504,
  "score": 0.07143805603505511,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.70it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.68it/s]Extractor Predicting: 23it [00:13,  1.71it/s]Extractor Predicting: 24it [00:14,  1.74it/s]Extractor Predicting: 25it [00:15,  1.73it/s]Extractor Predicting: 26it [00:15,  1.74it/s]Extractor Predicting: 27it [00:16,  1.72it/s]Extractor Predicting: 28it [00:16,  1.72it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.68it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.70it/s]Extractor Predicting: 33it [00:19,  1.66it/s]Extractor Predicting: 34it [00:20,  1.64it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:21,  1.63it/s]Extractor Predicting: 37it [00:22,  1.69it/s]Extractor Predicting: 38it [00:22,  1.74it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:24,  1.58it/s]Extractor Predicting: 42it [00:25,  1.53it/s]Extractor Predicting: 43it [00:26,  1.56it/s]Extractor Predicting: 44it [00:26,  1.55it/s]Extractor Predicting: 45it [00:27,  1.56it/s]Extractor Predicting: 46it [00:28,  1.52it/s]Extractor Predicting: 47it [00:28,  1.50it/s]Extractor Predicting: 48it [00:29,  1.54it/s]Extractor Predicting: 49it [00:30,  1.54it/s]Extractor Predicting: 50it [00:30,  1.55it/s]Extractor Predicting: 51it [00:31,  1.57it/s]Extractor Predicting: 52it [00:31,  1.53it/s]Extractor Predicting: 53it [00:32,  1.55it/s]Extractor Predicting: 54it [00:33,  1.55it/s]Extractor Predicting: 55it [00:33,  1.56it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.55it/s]Extractor Predicting: 58it [00:35,  1.58it/s]Extractor Predicting: 59it [00:36,  1.56it/s]Extractor Predicting: 60it [00:37,  1.59it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:38,  1.58it/s]Extractor Predicting: 63it [00:38,  1.59it/s]Extractor Predicting: 64it [00:39,  1.62it/s]Extractor Predicting: 65it [00:40,  1.62it/s]Extractor Predicting: 66it [00:40,  1.63it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:42,  1.61it/s]Extractor Predicting: 69it [00:42,  1.61it/s]Extractor Predicting: 70it [00:43,  1.60it/s]Extractor Predicting: 71it [00:44,  1.46it/s]Extractor Predicting: 72it [00:44,  1.48it/s]Extractor Predicting: 73it [00:45,  1.48it/s]Extractor Predicting: 74it [00:46,  1.54it/s]Extractor Predicting: 75it [00:46,  1.55it/s]Extractor Predicting: 76it [00:47,  1.54it/s]Extractor Predicting: 77it [00:47,  1.55it/s]Extractor Predicting: 78it [00:48,  1.56it/s]Extractor Predicting: 79it [00:49,  1.58it/s]Extractor Predicting: 80it [00:49,  1.59it/s]Extractor Predicting: 81it [00:50,  1.57it/s]Extractor Predicting: 82it [00:51,  1.53it/s]Extractor Predicting: 83it [00:51,  1.58it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:53,  1.54it/s]Extractor Predicting: 86it [00:53,  1.57it/s]Extractor Predicting: 87it [00:54,  1.59it/s]Extractor Predicting: 88it [00:54,  1.56it/s]Extractor Predicting: 89it [00:55,  1.55it/s]Extractor Predicting: 90it [00:56,  1.55it/s]Extractor Predicting: 91it [00:56,  1.54it/s]Extractor Predicting: 92it [00:57,  1.58it/s]Extractor Predicting: 93it [00:58,  1.61it/s]Extractor Predicting: 94it [00:58,  1.57it/s]Extractor Predicting: 95it [00:59,  1.54it/s]Extractor Predicting: 96it [01:00,  1.58it/s]Extractor Predicting: 97it [01:00,  1.58it/s]Extractor Predicting: 98it [01:01,  1.59it/s]Extractor Predicting: 99it [01:01,  1.61it/s]Extractor Predicting: 100it [01:02,  1.49it/s]Extractor Predicting: 101it [01:03,  1.57it/s]Extractor Predicting: 102it [01:03,  1.54it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:05,  1.61it/s]Extractor Predicting: 105it [01:05,  1.59it/s]Extractor Predicting: 106it [01:06,  1.63it/s]Extractor Predicting: 107it [01:06,  1.65it/s]Extractor Predicting: 108it [01:07,  1.68it/s]Extractor Predicting: 109it [01:08,  1.62it/s]Extractor Predicting: 110it [01:08,  1.61it/s]Extractor Predicting: 111it [01:09,  1.59it/s]Extractor Predicting: 112it [01:10,  1.59it/s]Extractor Predicting: 113it [01:10,  1.64it/s]Extractor Predicting: 114it [01:11,  1.60it/s]Extractor Predicting: 115it [01:11,  1.57it/s]Extractor Predicting: 116it [01:12,  1.61it/s]Extractor Predicting: 117it [01:13,  1.60it/s]Extractor Predicting: 118it [01:13,  1.64it/s]Extractor Predicting: 119it [01:14,  1.62it/s]Extractor Predicting: 120it [01:15,  1.59it/s]Extractor Predicting: 121it [01:15,  1.54it/s]Extractor Predicting: 122it [01:16,  1.53it/s]Extractor Predicting: 123it [01:17,  1.56it/s]Extractor Predicting: 124it [01:17,  1.58it/s]Extractor Predicting: 125it [01:18,  1.62it/s]Extractor Predicting: 126it [01:18,  1.62it/s]Extractor Predicting: 127it [01:19,  1.61it/s]Extractor Predicting: 128it [01:20,  1.63it/s]Extractor Predicting: 129it [01:20,  1.63it/s]Extractor Predicting: 130it [01:21,  1.64it/s]Extractor Predicting: 131it [01:21,  1.64it/s]Extractor Predicting: 132it [01:22,  1.60it/s]Extractor Predicting: 133it [01:23,  1.58it/s]Extractor Predicting: 134it [01:23,  1.61it/s]Extractor Predicting: 135it [01:24,  1.62it/s]Extractor Predicting: 136it [01:24,  1.63it/s]Extractor Predicting: 137it [01:25,  1.61it/s]Extractor Predicting: 138it [01:26,  1.58it/s]Extractor Predicting: 139it [01:26,  1.57it/s]Extractor Predicting: 140it [01:27,  1.63it/s]Extractor Predicting: 141it [01:28,  1.61it/s]Extractor Predicting: 142it [01:28,  1.58it/s]Extractor Predicting: 143it [01:29,  1.52it/s]Extractor Predicting: 144it [01:30,  1.53it/s]Extractor Predicting: 145it [01:30,  1.56it/s]Extractor Predicting: 146it [01:31,  1.55it/s]Extractor Predicting: 147it [01:32,  1.57it/s]Extractor Predicting: 148it [01:32,  1.49it/s]Extractor Predicting: 149it [01:33,  1.49it/s]Extractor Predicting: 150it [01:34,  1.38it/s]Extractor Predicting: 151it [01:34,  1.44it/s]Extractor Predicting: 152it [01:35,  1.47it/s]Extractor Predicting: 153it [01:36,  1.48it/s]Extractor Predicting: 154it [01:36,  1.46it/s]Extractor Predicting: 155it [01:37,  1.49it/s]Extractor Predicting: 156it [01:38,  1.52it/s]Extractor Predicting: 157it [01:38,  1.49it/s]Extractor Predicting: 158it [01:39,  1.47it/s]Extractor Predicting: 159it [01:40,  1.47it/s]Extractor Predicting: 160it [01:40,  1.51it/s]Extractor Predicting: 161it [01:41,  1.53it/s]Extractor Predicting: 162it [01:42,  1.51it/s]Extractor Predicting: 163it [01:42,  1.46it/s]Extractor Predicting: 164it [01:43,  1.46it/s]Extractor Predicting: 165it [01:44,  1.49it/s]Extractor Predicting: 166it [01:44,  1.48it/s]Extractor Predicting: 167it [01:45,  1.51it/s]Extractor Predicting: 168it [01:46,  1.59it/s]Extractor Predicting: 168it [01:46,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:19,366 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:19,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:19,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:19,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:19,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:15:20,303 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:15:20,304 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:15:20,936 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:15:22,083 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:15:22,083 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:25,192 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:25,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:25,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:25,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:15:25,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:15:26,059 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:15:26,060 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:15:26,692 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:15:26,923 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:15:26,923 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2830188679245283,
  "recall": 0.06709741550695825,
  "score": 0.10847730012053032,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:11,  1.47it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 19it [00:12,  1.54it/s]
[INFO|configuration_utils.py:515] 2023-08-28 16:15:41,492 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:15:41,532 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:15:41,611 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:15:41,612 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 16:15:41,661 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:15:58,969 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 16:15:59,043 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 16:15:59,395 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:15:59,396 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:15:59,616 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:15:59,722 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:15:59,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:15:59,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:15:59,723 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:15:59,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:15:59,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4166666666666667,
  "recall": 0.019305019305019305,
  "score": 0.03690036900369004,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 16:16:00,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:00,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:01,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:02,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:02,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:03,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:03,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:04,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:04,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:05,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:06,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:06,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:07,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:07,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:08,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:08,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:09,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:10,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:10,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:11,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:11<01:44, 11.62s/it][WARNING|generation_utils.py:914] 2023-08-28 16:16:12,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:12,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:13,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:14,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:14,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:15,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:15,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:16,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:17,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:17,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:18,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:19,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:19,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:20,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:21,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:21,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:22,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:22,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:23,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:23,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:24,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:25,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:25,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:25<01:45, 13.18s/it][WARNING|generation_utils.py:914] 2023-08-28 16:16:26,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:27,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:27,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:28,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:28,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:29,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:30,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:31,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:31,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:32,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:33,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:34,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:34,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:35,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:36,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:36,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:37,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:38,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:38,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:39,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:39,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:40<01:35, 13.61s/it][WARNING|generation_utils.py:914] 2023-08-28 16:16:40,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:40,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:41,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:42,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:42,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:43,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:43,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:44,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:44,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:45,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:46,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:46,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:47,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:47,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:48,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:48,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:49,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:49,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:50,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:50,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:51<01:16, 12.67s/it][WARNING|generation_utils.py:914] 2023-08-28 16:16:51,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:52,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:52,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:53,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:53,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:54,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:55,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:55,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:56,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:56,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:57,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:57,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:58,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:59,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:59,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:00,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:00,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:01,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:02,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:02,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:03,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:03<01:02, 12.47s/it][WARNING|generation_utils.py:914] 2023-08-28 16:17:03,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:04,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:04,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:05,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:06,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:06,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:07,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:07,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:08,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:08,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:09,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:10,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:10,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:11,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:11,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:12,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:12,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:13,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:14,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:14,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:14<00:48, 12.10s/it][WARNING|generation_utils.py:914] 2023-08-28 16:17:15,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:15,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:16,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:17,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:17,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:18,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:18,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:19,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:20,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:20,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:21,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:22,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:22,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:23,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:24,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:24,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:25,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:25,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:26,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:27,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:27,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:27<00:37, 12.46s/it][WARNING|generation_utils.py:914] 2023-08-28 16:17:28,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:28,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:29,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:30,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:30,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:31,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:32,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:32,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:33,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:33,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:34,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:35,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:35,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:36,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:37,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:37,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:38,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:38,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:39,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:39,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:40,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:40<00:25, 12.55s/it][WARNING|generation_utils.py:914] 2023-08-28 16:17:41,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:41,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:42,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:42,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:43,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:44,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:44,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:45,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:45,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:46,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:46,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:47,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:48,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:48,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:49,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:49,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:50,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:51,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:51,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:52,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:52<00:12, 12.32s/it][WARNING|generation_utils.py:914] 2023-08-28 16:17:52,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:53,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:54,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:54,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:55,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:56,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:56,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:57,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:58,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:58,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:17:59,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:00,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:00,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:01,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:02,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:03,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:03,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:04,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:04,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:18:05,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:06<00:00, 12.69s/it]Generating: 100%|██████████| 10/10 [02:06<00:00, 12.60s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:14,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:14,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:14,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:14,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:14,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:18:15,698 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:18:15,699 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:18:16,324 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:18:17,470 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:18:17,470 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:21,601 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:21,640 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:21,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:21,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:18:21,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:18:22,510 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:18:22,511 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:18:23,168 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:18:23,412 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:18:23,412 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : composer . Context : Later in 2008 , he composed covers of the songs The Last Waltz , The Last Waltz II , The Last Waltz III , The Last Waltz IV and The Last Waltz V . Head Entity : The Last Waltz , Tail Entity : John H. Gopnik .\n']
['Relation : composer . Context : Later in 2008 , he composed covers of the songs The Last Waltz , The Last Waltz II , The Last Waltz III , The Last Waltz IV and The Last Waltz V . Head Entity : The Last Waltz , Tail Entity : John H. Gopnik .\n', 'Relation : composer . Context : Another track titled The Last Waltz features his backing vocals . Head Entity : The Last Waltz , Tail Entity : Billie Holiday .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.965625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : date of birth .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.9166666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : creator .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 407, 'raw': 416}
{'target': 600, 'success': 439, 'raw': 448}
{'target': 600, 'success': 471, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 535, 'raw': 544}
{'target': 600, 'success': 567, 'raw': 576}
{'target': 600, 'success': 599, 'raw': 608}
{'target': 600, 'success': 630, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.984375, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.940625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 7888
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7988, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.70it/s]Extractor Estimating: 2it [00:01,  1.62it/s]Extractor Estimating: 3it [00:01,  1.53it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.63it/s]Extractor Estimating: 6it [00:03,  1.72it/s]Extractor Estimating: 7it [00:04,  1.73it/s]Extractor Estimating: 8it [00:04,  1.73it/s]Extractor Estimating: 9it [00:05,  1.70it/s]Extractor Estimating: 10it [00:06,  1.68it/s]Extractor Estimating: 11it [00:06,  1.72it/s]Extractor Estimating: 12it [00:07,  1.70it/s]Extractor Estimating: 13it [00:07,  1.72it/s]Extractor Estimating: 14it [00:08,  1.76it/s]Extractor Estimating: 15it [00:08,  1.72it/s]Extractor Estimating: 16it [00:09,  1.72it/s]Extractor Estimating: 17it [00:10,  1.70it/s]Extractor Estimating: 18it [00:10,  1.72it/s]Extractor Estimating: 19it [00:11,  1.75it/s]Extractor Estimating: 20it [00:11,  1.66it/s]Extractor Estimating: 21it [00:12,  1.73it/s]Extractor Estimating: 22it [00:12,  1.72it/s]Extractor Estimating: 23it [00:13,  1.72it/s]Extractor Estimating: 24it [00:14,  1.71it/s]Extractor Estimating: 25it [00:14,  1.67it/s]Extractor Estimating: 26it [00:15,  1.66it/s]Extractor Estimating: 27it [00:15,  1.67it/s]Extractor Estimating: 28it [00:16,  1.65it/s]Extractor Estimating: 29it [00:17,  1.66it/s]Extractor Estimating: 30it [00:17,  1.64it/s]Extractor Estimating: 31it [00:18,  1.63it/s]Extractor Estimating: 32it [00:19,  1.64it/s]Extractor Estimating: 33it [00:19,  1.68it/s]Extractor Estimating: 34it [00:20,  1.64it/s]Extractor Estimating: 35it [00:20,  1.72it/s]Extractor Estimating: 36it [00:21,  1.65it/s]Extractor Estimating: 37it [00:22,  1.65it/s]Extractor Estimating: 38it [00:22,  1.62it/s]Extractor Estimating: 39it [00:23,  1.66it/s]Extractor Estimating: 40it [00:23,  1.66it/s]Extractor Estimating: 41it [00:24,  1.68it/s]Extractor Estimating: 42it [00:25,  1.69it/s]Extractor Estimating: 43it [00:25,  1.67it/s]Extractor Estimating: 44it [00:26,  1.64it/s]Extractor Estimating: 45it [00:26,  1.62it/s]Extractor Estimating: 46it [00:27,  1.61it/s]Extractor Estimating: 47it [00:28,  1.66it/s]Extractor Estimating: 48it [00:28,  1.71it/s]Extractor Estimating: 49it [00:29,  1.73it/s]Extractor Estimating: 50it [00:29,  1.68it/s]Extractor Estimating: 51it [00:30,  1.65it/s]Extractor Estimating: 52it [00:31,  1.65it/s]Extractor Estimating: 53it [00:31,  1.70it/s]Extractor Estimating: 54it [00:32,  1.63it/s]Extractor Estimating: 55it [00:32,  1.64it/s]Extractor Estimating: 56it [00:33,  1.64it/s]Extractor Estimating: 57it [00:34,  1.64it/s]Extractor Estimating: 58it [00:34,  1.63it/s]Extractor Estimating: 59it [00:35,  1.60it/s]Extractor Estimating: 60it [00:35,  1.63it/s]Extractor Estimating: 61it [00:36,  1.62it/s]Extractor Estimating: 62it [00:37,  1.68it/s]Extractor Estimating: 63it [00:37,  1.72it/s]Extractor Estimating: 64it [00:38,  1.76it/s]Extractor Estimating: 65it [00:38,  1.80it/s]Extractor Estimating: 66it [00:39,  1.87it/s]Extractor Estimating: 67it [00:39,  1.80it/s]Extractor Estimating: 68it [00:40,  1.69it/s]Extractor Estimating: 69it [00:41,  1.75it/s]Extractor Estimating: 70it [00:41,  1.76it/s]Extractor Estimating: 71it [00:42,  1.77it/s]Extractor Estimating: 72it [00:42,  1.76it/s]Extractor Estimating: 73it [00:43,  1.64it/s]Extractor Estimating: 74it [00:44,  1.64it/s]Extractor Estimating: 75it [00:44,  1.68it/s]Extractor Estimating: 76it [00:45,  1.80it/s]Extractor Estimating: 77it [00:45,  1.86it/s]Extractor Estimating: 78it [00:46,  1.96it/s]Extractor Estimating: 79it [00:46,  2.00it/s]Extractor Estimating: 80it [00:46,  2.05it/s]Extractor Estimating: 81it [00:47,  2.10it/s]Extractor Estimating: 82it [00:47,  2.03it/s]Extractor Estimating: 83it [00:48,  2.05it/s]Extractor Estimating: 84it [00:48,  2.05it/s]Extractor Estimating: 85it [00:49,  2.09it/s]Extractor Estimating: 86it [00:49,  2.07it/s]Extractor Estimating: 87it [00:50,  2.14it/s]Extractor Estimating: 88it [00:50,  2.11it/s]Extractor Estimating: 89it [00:51,  2.17it/s]Extractor Estimating: 90it [00:51,  2.21it/s]Extractor Estimating: 91it [00:52,  2.25it/s]Extractor Estimating: 92it [00:52,  2.15it/s]Extractor Estimating: 93it [00:53,  2.08it/s]Extractor Estimating: 94it [00:53,  2.12it/s]Extractor Estimating: 95it [00:54,  2.07it/s]Extractor Estimating: 96it [00:54,  2.18it/s]Extractor Estimating: 97it [00:54,  2.15it/s]Extractor Estimating: 98it [00:55,  2.12it/s]Extractor Estimating: 99it [00:55,  1.98it/s]Extractor Estimating: 100it [00:56,  2.02it/s]Extractor Estimating: 101it [00:57,  1.93it/s]Extractor Estimating: 102it [00:57,  1.80it/s]Extractor Estimating: 103it [00:58,  1.70it/s]Extractor Estimating: 104it [00:58,  1.78it/s]Extractor Estimating: 105it [00:59,  1.73it/s]Extractor Estimating: 106it [01:00,  1.75it/s]Extractor Estimating: 107it [01:00,  1.72it/s]Extractor Estimating: 108it [01:01,  1.68it/s]Extractor Estimating: 109it [01:01,  1.56it/s]Extractor Estimating: 110it [01:02,  1.60it/s]Extractor Estimating: 111it [01:03,  1.63it/s]Extractor Estimating: 112it [01:03,  1.68it/s]Extractor Estimating: 113it [01:04,  1.68it/s]Extractor Estimating: 114it [01:04,  1.67it/s]Extractor Estimating: 115it [01:05,  1.71it/s]Extractor Estimating: 116it [01:06,  1.64it/s]Extractor Estimating: 117it [01:06,  1.69it/s]Extractor Estimating: 118it [01:07,  1.67it/s]Extractor Estimating: 119it [01:07,  1.66it/s]Extractor Estimating: 120it [01:08,  1.73it/s]Extractor Estimating: 121it [01:09,  1.65it/s]Extractor Estimating: 122it [01:09,  1.70it/s]Extractor Estimating: 123it [01:10,  1.72it/s]Extractor Estimating: 124it [01:10,  1.72it/s]Extractor Estimating: 125it [01:11,  1.70it/s]Extractor Estimating: 126it [01:12,  1.67it/s]Extractor Estimating: 127it [01:12,  1.73it/s]Extractor Estimating: 128it [01:13,  1.74it/s]Extractor Estimating: 129it [01:13,  1.70it/s]Extractor Estimating: 130it [01:14,  1.71it/s]Extractor Estimating: 131it [01:14,  1.75it/s]Extractor Estimating: 132it [01:15,  1.68it/s]Extractor Estimating: 133it [01:16,  1.69it/s]Extractor Estimating: 134it [01:16,  1.72it/s]Extractor Estimating: 135it [01:17,  1.74it/s]Extractor Estimating: 136it [01:17,  1.72it/s]Extractor Estimating: 137it [01:18,  1.74it/s]Extractor Estimating: 138it [01:18,  1.77it/s]Extractor Estimating: 139it [01:19,  1.79it/s]Extractor Estimating: 140it [01:19,  1.80it/s]Extractor Estimating: 141it [01:20,  1.81it/s]Extractor Estimating: 142it [01:21,  1.83it/s]Extractor Estimating: 143it [01:21,  1.82it/s]Extractor Estimating: 144it [01:22,  1.76it/s]Extractor Estimating: 145it [01:22,  1.78it/s]Extractor Estimating: 146it [01:23,  1.80it/s]Extractor Estimating: 147it [01:23,  1.76it/s]Extractor Estimating: 148it [01:24,  1.79it/s]Extractor Estimating: 149it [01:25,  1.79it/s]Extractor Estimating: 150it [01:25,  1.80it/s]Extractor Estimating: 151it [01:26,  1.81it/s]Extractor Estimating: 152it [01:26,  1.74it/s]Extractor Estimating: 153it [01:27,  1.71it/s]Extractor Estimating: 154it [01:28,  1.65it/s]Extractor Estimating: 155it [01:28,  1.66it/s]Extractor Estimating: 156it [01:29,  1.64it/s]Extractor Estimating: 157it [01:29,  1.66it/s]Extractor Estimating: 158it [01:30,  1.68it/s]Extractor Estimating: 159it [01:30,  1.73it/s]Extractor Estimating: 160it [01:31,  1.70it/s]Extractor Estimating: 161it [01:32,  1.72it/s]Extractor Estimating: 162it [01:32,  1.63it/s]Extractor Estimating: 163it [01:33,  1.52it/s]Extractor Estimating: 164it [01:34,  1.53it/s]Extractor Estimating: 165it [01:34,  1.60it/s]Extractor Estimating: 166it [01:35,  1.60it/s]Extractor Estimating: 167it [01:35,  1.62it/s]Extractor Estimating: 168it [01:36,  1.59it/s]Extractor Estimating: 169it [01:37,  1.57it/s]Extractor Estimating: 170it [01:37,  1.63it/s]Extractor Estimating: 171it [01:38,  1.68it/s]Extractor Estimating: 172it [01:38,  1.70it/s]Extractor Estimating: 173it [01:39,  1.65it/s]Extractor Estimating: 174it [01:40,  1.68it/s]Extractor Estimating: 175it [01:40,  1.68it/s]Extractor Estimating: 176it [01:41,  1.71it/s]Extractor Estimating: 177it [01:42,  1.62it/s]Extractor Estimating: 178it [01:42,  1.68it/s]Extractor Estimating: 179it [01:43,  1.67it/s]Extractor Estimating: 180it [01:43,  1.67it/s]Extractor Estimating: 181it [01:44,  1.69it/s]Extractor Estimating: 182it [01:44,  1.67it/s]Extractor Estimating: 183it [01:45,  1.67it/s]Extractor Estimating: 184it [01:46,  1.68it/s]Extractor Estimating: 185it [01:46,  1.71it/s]Extractor Estimating: 186it [01:47,  1.67it/s]Extractor Estimating: 187it [01:47,  1.70it/s]Extractor Estimating: 188it [01:48,  1.71it/s]Extractor Estimating: 189it [01:49,  1.74it/s]Extractor Estimating: 190it [01:49,  1.75it/s]Extractor Estimating: 191it [01:50,  1.78it/s]Extractor Estimating: 192it [01:50,  1.70it/s]Extractor Estimating: 193it [01:51,  1.72it/s]Extractor Estimating: 194it [01:51,  1.75it/s]Extractor Estimating: 195it [01:52,  1.73it/s]Extractor Estimating: 196it [01:53,  1.74it/s]Extractor Estimating: 197it [01:53,  1.71it/s]Extractor Estimating: 198it [01:54,  1.75it/s]Extractor Estimating: 199it [01:54,  1.79it/s]Extractor Estimating: 200it [01:55,  1.78it/s]Extractor Estimating: 201it [01:55,  1.70it/s]Extractor Estimating: 202it [01:56,  1.63it/s]Extractor Estimating: 203it [01:57,  1.57it/s]Extractor Estimating: 204it [01:57,  1.58it/s]Extractor Estimating: 205it [01:58,  1.53it/s]Extractor Estimating: 206it [01:59,  1.51it/s]Extractor Estimating: 207it [02:00,  1.51it/s]Extractor Estimating: 208it [02:00,  1.54it/s]Extractor Estimating: 209it [02:01,  1.55it/s]Extractor Estimating: 210it [02:01,  1.55it/s]Extractor Estimating: 211it [02:02,  1.54it/s]Extractor Estimating: 212it [02:03,  1.54it/s]Extractor Estimating: 213it [02:03,  1.55it/s]Extractor Estimating: 214it [02:04,  1.56it/s]Extractor Estimating: 215it [02:05,  1.55it/s]Extractor Estimating: 216it [02:05,  1.55it/s]Extractor Estimating: 217it [02:06,  1.54it/s]Extractor Estimating: 218it [02:07,  1.55it/s]Extractor Estimating: 219it [02:07,  1.51it/s]Extractor Estimating: 220it [02:08,  1.51it/s]Extractor Estimating: 221it [02:09,  1.53it/s]Extractor Estimating: 222it [02:09,  1.51it/s]Extractor Estimating: 223it [02:10,  1.52it/s]Extractor Estimating: 224it [02:11,  1.47it/s]Extractor Estimating: 225it [02:11,  1.52it/s]Extractor Estimating: 226it [02:12,  1.60it/s]Extractor Estimating: 227it [02:12,  1.66it/s]Extractor Estimating: 228it [02:13,  1.66it/s]Extractor Estimating: 229it [02:14,  1.68it/s]Extractor Estimating: 230it [02:14,  1.72it/s]Extractor Estimating: 231it [02:15,  1.72it/s]Extractor Estimating: 232it [02:15,  1.73it/s]Extractor Estimating: 233it [02:16,  1.79it/s]Extractor Estimating: 234it [02:16,  1.79it/s]Extractor Estimating: 235it [02:17,  1.60it/s]Extractor Estimating: 236it [02:18,  1.65it/s]Extractor Estimating: 237it [02:18,  1.74it/s]Extractor Estimating: 238it [02:19,  1.81it/s]Extractor Estimating: 239it [02:19,  1.78it/s]Extractor Estimating: 240it [02:20,  1.74it/s]Extractor Estimating: 241it [02:20,  1.75it/s]Extractor Estimating: 242it [02:21,  1.75it/s]Extractor Estimating: 243it [02:22,  1.78it/s]Extractor Estimating: 244it [02:22,  1.79it/s]Extractor Estimating: 245it [02:23,  1.76it/s]Extractor Estimating: 246it [02:23,  1.78it/s]Extractor Estimating: 247it [02:24,  1.77it/s]Extractor Estimating: 248it [02:24,  1.80it/s]Extractor Estimating: 249it [02:25,  1.79it/s]Extractor Estimating: 250it [02:25,  1.76it/s]Extractor Estimating: 250it [02:25,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:16,020 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:16,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:16,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:16,060 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:16,060 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:21:16,737 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:21:16,738 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:21:17,099 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:21:18,310 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:21:18,310 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:19,973 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:20,035 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:20,035 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:20,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:21:20,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:21:20,633 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:21:20,635 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:21:21,010 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:21:21,258 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:21:21,258 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:56:18,775 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:56:19,625 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4999 mean pseudo reward: 0.9192134739047171
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl'}
train vocab size: 20487
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20587, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20587, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.945, loss:489.1267
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.021, loss:447.1582
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.946, loss:415.3874
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.967, loss:418.4906
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.947, loss:380.7538
>> valid entity prec:0.5210, rec:0.3239, f1:0.3995
>> valid relation prec:0.2658, rec:0.0250, f1:0.0457
>> valid relation with NER prec:0.2658, rec:0.0250, f1:0.0457
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.092, loss:391.1191
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 0.966, loss:371.0406
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.958, loss:380.6240
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.954, loss:364.2985
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.957, loss:383.9195
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4594, rec:0.4616, f1:0.4605
>> valid relation prec:0.3050, rec:0.0391, f1:0.0693
>> valid relation with NER prec:0.3050, rec:0.0391, f1:0.0693
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 55, avg_time 3.134, loss:357.9682
g_step 1200, step 155, avg_time 0.975, loss:362.3901
g_step 1300, step 46, avg_time 0.956, loss:345.4228
g_step 1400, step 146, avg_time 0.958, loss:352.7062
g_step 1500, step 37, avg_time 0.943, loss:343.0205
>> valid entity prec:0.4742, rec:0.4084, f1:0.4389
>> valid relation prec:0.1740, rec:0.0249, f1:0.0435
>> valid relation with NER prec:0.1740, rec:0.0249, f1:0.0435
g_step 1600, step 137, avg_time 3.057, loss:331.9417
g_step 1700, step 28, avg_time 0.960, loss:327.5308
g_step 1800, step 128, avg_time 0.960, loss:318.5206
g_step 1900, step 19, avg_time 0.961, loss:315.2263
g_step 2000, step 119, avg_time 0.960, loss:306.6260
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4734, rec:0.4033, f1:0.4356
>> valid relation prec:0.1879, rec:0.0379, f1:0.0631
>> valid relation with NER prec:0.1879, rec:0.0379, f1:0.0631
g_step 2100, step 10, avg_time 3.063, loss:307.8506
g_step 2200, step 110, avg_time 0.956, loss:271.7524
g_step 2300, step 1, avg_time 0.967, loss:298.4277
g_step 2400, step 101, avg_time 0.958, loss:261.8632
g_step 2500, step 201, avg_time 0.966, loss:300.8593
>> valid entity prec:0.4664, rec:0.4343, f1:0.4498
>> valid relation prec:0.1687, rec:0.0366, f1:0.0602
>> valid relation with NER prec:0.1687, rec:0.0366, f1:0.0602
g_step 2600, step 92, avg_time 3.070, loss:258.4189
g_step 2700, step 192, avg_time 0.948, loss:287.2727
g_step 2800, step 83, avg_time 0.950, loss:253.5791
g_step 2900, step 183, avg_time 0.960, loss:279.9427
g_step 3000, step 74, avg_time 0.953, loss:241.4011
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4443, rec:0.4715, f1:0.4575
>> valid relation prec:0.1480, rec:0.0326, f1:0.0534
>> valid relation with NER prec:0.1480, rec:0.0326, f1:0.0534
g_step 3100, step 174, avg_time 3.069, loss:262.5176
g_step 3200, step 65, avg_time 0.961, loss:241.9849
g_step 3300, step 165, avg_time 0.942, loss:249.8578
g_step 3400, step 56, avg_time 0.951, loss:239.5784
g_step 3500, step 156, avg_time 0.946, loss:241.3991
>> valid entity prec:0.4896, rec:0.3035, f1:0.3747
>> valid relation prec:0.1717, rec:0.0246, f1:0.0430
>> valid relation with NER prec:0.1717, rec:0.0246, f1:0.0430
g_step 3600, step 47, avg_time 3.000, loss:237.7738
g_step 3700, step 147, avg_time 0.948, loss:241.7461
g_step 3800, step 38, avg_time 0.950, loss:231.7117
g_step 3900, step 138, avg_time 0.963, loss:225.8639
g_step 4000, step 29, avg_time 0.949, loss:219.0959
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4582, rec:0.3634, f1:0.4053
>> valid relation prec:0.1687, rec:0.0275, f1:0.0473
>> valid relation with NER prec:0.1687, rec:0.0275, f1:0.0473
g_step 4100, step 129, avg_time 3.009, loss:210.1251
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:56:19 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:56:19 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-56-18_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:56:21 - WARNING - datasets.builder -   Using custom data configuration default-16ea94e0c608da9b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-16ea94e0c608da9b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:56:31,003 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:56:31,112 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:56:31,113 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:56:31,114 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:56:32,059 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:56:32,248 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:56:32,248 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:56:32,248 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:56:32,248 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:56:32,248 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:56:32,248 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:56:34,044 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:56:37,559 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:56:37,559 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-16ea94e0c608da9b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.45ba/s] 40%|████      | 2/5 [00:00<00:01,  2.50ba/s] 60%|██████    | 3/5 [00:01<00:00,  3.27ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.81ba/s]100%|██████████| 5/5 [00:01<00:00,  4.20ba/s]100%|██████████| 5/5 [00:01<00:00,  3.21ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.96ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.76ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.12ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.32ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.45ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.53ba/s]100%|██████████| 7/7 [00:01<00:00,  4.75ba/s]100%|██████████| 7/7 [00:01<00:00,  4.22ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.13ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.70ba/s]100%|██████████| 5/5 [00:00<00:00,  8.45ba/s]100%|██████████| 5/5 [00:00<00:00,  7.38ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.64ba/s] 43%|████▎     | 3/7 [00:00<00:00,  8.25ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  7.23ba/s]100%|██████████| 7/7 [00:00<00:00,  8.73ba/s]100%|██████████| 7/7 [00:00<00:00,  8.09ba/s]
[INFO|trainer.py:414] 2023-08-28 17:56:44,633 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:56:44,822 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:56:44,822 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 17:56:44,822 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:56:44,823 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:56:44,823 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:56:44,823 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:56:44,823 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:57,  3.32it/s]  1%|          | 2/390 [00:00<01:54,  3.40it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:52,  3.44it/s]  1%|▏         | 5/390 [00:01<01:51,  3.45it/s]  2%|▏         | 6/390 [00:01<01:51,  3.44it/s]  2%|▏         | 7/390 [00:02<01:51,  3.43it/s]  2%|▏         | 8/390 [00:02<01:51,  3.43it/s]  2%|▏         | 9/390 [00:02<01:51,  3.41it/s]  3%|▎         | 10/390 [00:02<01:51,  3.41it/s]  3%|▎         | 11/390 [00:03<01:51,  3.41it/s]  3%|▎         | 12/390 [00:03<01:50,  3.41it/s]  3%|▎         | 13/390 [00:03<01:50,  3.41it/s]  4%|▎         | 14/390 [00:04<01:50,  3.41it/s]  4%|▍         | 15/390 [00:04<01:49,  3.41it/s]  4%|▍         | 16/390 [00:04<01:49,  3.41it/s]  4%|▍         | 17/390 [00:05<01:55,  3.23it/s]  5%|▍         | 18/390 [00:05<01:53,  3.28it/s]  5%|▍         | 19/390 [00:05<01:51,  3.32it/s]  5%|▌         | 20/390 [00:05<01:50,  3.35it/s]  5%|▌         | 21/390 [00:06<01:49,  3.36it/s]  6%|▌         | 22/390 [00:06<01:49,  3.38it/s]  6%|▌         | 23/390 [00:06<01:48,  3.39it/s]  6%|▌         | 24/390 [00:07<01:47,  3.39it/s]  6%|▋         | 25/390 [00:07<01:47,  3.40it/s]  7%|▋         | 26/390 [00:07<01:46,  3.40it/s]  7%|▋         | 27/390 [00:07<01:46,  3.40it/s]  7%|▋         | 28/390 [00:08<01:46,  3.41it/s]  7%|▋         | 29/390 [00:08<01:45,  3.41it/s]  8%|▊         | 30/390 [00:08<01:45,  3.41it/s]  8%|▊         | 31/390 [00:09<01:45,  3.41it/s]  8%|▊         | 32/390 [00:09<01:44,  3.41it/s]  8%|▊         | 33/390 [00:09<01:44,  3.41it/s]  9%|▊         | 34/390 [00:10<02:33,  2.31it/s]  9%|▉         | 35/390 [00:10<02:18,  2.56it/s]  9%|▉         | 36/390 [00:11<02:07,  2.77it/s]  9%|▉         | 37/390 [00:11<02:00,  2.93it/s] 10%|▉         | 38/390 [00:11<01:54,  3.06it/s] 10%|█         | 39/390 [00:11<01:51,  3.16it/s] 10%|█         | 40/390 [00:12<01:48,  3.23it/s] 11%|█         | 41/390 [00:12<01:46,  3.28it/s] 11%|█         | 42/390 [00:12<01:44,  3.32it/s] 11%|█         | 43/390 [00:13<01:43,  3.34it/s] 11%|█▏        | 44/390 [00:13<01:42,  3.36it/s] 12%|█▏        | 45/390 [00:13<01:42,  3.38it/s] 12%|█▏        | 46/390 [00:13<01:41,  3.38it/s] 12%|█▏        | 47/390 [00:14<01:41,  3.39it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.39it/s] 13%|█▎        | 49/390 [00:14<01:40,  3.40it/s] 13%|█▎        | 50/390 [00:15<01:45,  3.21it/s] 13%|█▎        | 51/390 [00:15<01:43,  3.27it/s] 13%|█▎        | 52/390 [00:15<01:42,  3.31it/s] 14%|█▎        | 53/390 [00:16<01:41,  3.33it/s] 14%|█▍        | 54/390 [00:16<01:40,  3.36it/s] 14%|█▍        | 55/390 [00:16<01:39,  3.37it/s] 14%|█▍        | 56/390 [00:16<01:38,  3.37it/s] 15%|█▍        | 57/390 [00:17<01:51,  2.99it/s] 15%|█▍        | 58/390 [00:17<01:46,  3.10it/s] 15%|█▌        | 59/390 [00:18<01:43,  3.19it/s] 15%|█▌        | 60/390 [00:18<01:41,  3.25it/s] 16%|█▌        | 61/390 [00:18<01:39,  3.29it/s] 16%|█▌        | 62/390 [00:18<01:38,  3.33it/s] 16%|█▌        | 63/390 [00:19<01:37,  3.35it/s] 16%|█▋        | 64/390 [00:19<01:36,  3.36it/s] 17%|█▋        | 65/390 [00:19<01:36,  3.38it/s] 17%|█▋        | 66/390 [00:20<01:35,  3.38it/s] 17%|█▋        | 67/390 [00:20<01:41,  3.18it/s] 17%|█▋        | 68/390 [00:20<01:39,  3.24it/s] 18%|█▊        | 69/390 [00:21<01:37,  3.29it/s] 18%|█▊        | 70/390 [00:21<01:36,  3.32it/s] 18%|█▊        | 71/390 [00:21<01:35,  3.35it/s] 18%|█▊        | 72/390 [00:21<01:34,  3.36it/s] 19%|█▊        | 73/390 [00:22<01:33,  3.38it/s] 19%|█▉        | 74/390 [00:22<01:33,  3.38it/s] 19%|█▉        | 75/390 [00:22<01:33,  3.39it/s] 19%|█▉        | 76/390 [00:23<01:32,  3.39it/s] 20%|█▉        | 77/390 [00:23<01:32,  3.40it/s] 20%|██        | 78/390 [00:23<01:31,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 17:57:08,521 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:57:08,521 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 17:57:08,521 >>   Batch size = 8

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.61it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.16it/s][A
  2%|▏         | 17/861 [00:00<00:18, 46.64it/s][A
  3%|▎         | 22/861 [00:00<00:18, 45.53it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.22it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.83it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.54it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.25it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.46it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.62it/s][A
  7%|▋         | 57/861 [00:01<00:17, 44.72it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.55it/s][A
  8%|▊         | 67/861 [00:01<00:25, 30.98it/s][A
  8%|▊         | 72/861 [00:01<00:23, 34.24it/s][A
  9%|▉         | 77/861 [00:01<00:21, 36.89it/s][A
 10%|▉         | 82/861 [00:01<00:19, 38.97it/s][A
 10%|█         | 87/861 [00:02<00:19, 40.47it/s][A
 11%|█         | 92/861 [00:02<00:18, 41.80it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 42.67it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.19it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 43.07it/s][A
 13%|█▎        | 112/861 [00:02<00:17, 43.22it/s][A
 14%|█▎        | 117/861 [00:02<00:17, 43.39it/s][A
 14%|█▍        | 122/861 [00:02<00:16, 43.91it/s][A
 15%|█▍        | 127/861 [00:02<00:16, 44.19it/s][A
 15%|█▌        | 132/861 [00:03<00:16, 44.36it/s][A
 16%|█▌        | 137/861 [00:03<00:16, 44.57it/s][A
 16%|█▋        | 142/861 [00:03<00:16, 44.67it/s][A
 17%|█▋        | 147/861 [00:03<00:16, 44.28it/s][A
 18%|█▊        | 152/861 [00:04<00:38, 18.18it/s][A
 18%|█▊        | 157/861 [00:04<00:31, 22.14it/s][A
 19%|█▉        | 162/861 [00:04<00:26, 26.11it/s][A
 19%|█▉        | 167/861 [00:04<00:23, 29.85it/s][A
 20%|█▉        | 172/861 [00:04<00:20, 33.15it/s][A
 21%|██        | 177/861 [00:04<00:18, 36.02it/s][A
 21%|██        | 182/861 [00:04<00:17, 38.29it/s][A
 22%|██▏       | 187/861 [00:04<00:16, 39.93it/s][A
 22%|██▏       | 192/861 [00:04<00:16, 40.72it/s][A
 23%|██▎       | 197/861 [00:05<00:15, 41.55it/s][A
 23%|██▎       | 202/861 [00:05<00:15, 42.40it/s][A
 24%|██▍       | 207/861 [00:05<00:15, 43.06it/s][A
 25%|██▍       | 212/861 [00:05<00:14, 43.57it/s][A
 25%|██▌       | 217/861 [00:05<00:14, 43.95it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 44.25it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.30it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.07it/s][A
 28%|██▊       | 237/861 [00:06<00:14, 43.98it/s][A
 28%|██▊       | 242/861 [00:06<00:14, 43.90it/s][A
 29%|██▊       | 247/861 [00:06<00:13, 44.02it/s][A
 29%|██▉       | 252/861 [00:06<00:13, 44.15it/s][A
 30%|██▉       | 257/861 [00:07<00:13, 44.20it/s][A
 30%|███       | 262/861 [00:07<00:43, 13.87it/s][A
 31%|███       | 267/861 [00:07<00:33, 17.50it/s][A
 32%|███▏      | 272/861 [00:07<00:27, 21.40it/s][A
 32%|███▏      | 277/861 [00:07<00:22, 25.39it/s][A
 33%|███▎      | 282/861 [00:07<00:19, 29.13it/s][A
 33%|███▎      | 287/861 [00:07<00:17, 32.63it/s][A
 34%|███▍      | 292/861 [00:08<00:16, 35.55it/s][A
 34%|███▍      | 297/861 [00:08<00:14, 37.78it/s][A
 35%|███▌      | 302/861 [00:08<00:14, 39.22it/s][A
 36%|███▌      | 307/861 [00:08<00:13, 40.34it/s][A
 36%|███▌      | 312/861 [00:08<00:13, 41.34it/s][A
 37%|███▋      | 317/861 [00:08<00:12, 42.35it/s][A
 37%|███▋      | 322/861 [00:08<00:12, 43.07it/s][A
 38%|███▊      | 327/861 [00:08<00:12, 43.53it/s][A
 39%|███▊      | 332/861 [00:08<00:12, 43.97it/s][A
 39%|███▉      | 337/861 [00:09<00:11, 44.24it/s][A
 40%|███▉      | 342/861 [00:09<00:14, 36.52it/s][A
 40%|████      | 347/861 [00:09<00:13, 38.71it/s][A
 41%|████      | 352/861 [00:09<00:14, 34.99it/s][A
 41%|████▏     | 357/861 [00:09<00:15, 32.58it/s][A
 42%|████▏     | 363/861 [00:09<00:13, 36.99it/s][A
 43%|████▎     | 368/861 [00:09<00:12, 38.98it/s][A
 43%|████▎     | 373/861 [00:10<00:12, 40.56it/s][A
 44%|████▍     | 378/861 [00:10<00:11, 41.79it/s][A
 44%|████▍     | 383/861 [00:10<00:11, 42.68it/s][A
 45%|████▌     | 388/861 [00:10<00:10, 43.07it/s][A
 46%|████▌     | 393/861 [00:10<00:10, 43.49it/s][A
 46%|████▌     | 398/861 [00:10<00:10, 43.62it/s][A
 47%|████▋     | 403/861 [00:10<00:10, 43.37it/s][A
 47%|████▋     | 408/861 [00:10<00:10, 43.72it/s][A
 48%|████▊     | 413/861 [00:10<00:10, 43.97it/s][A
 49%|████▊     | 418/861 [00:11<00:10, 44.27it/s][A
 49%|████▉     | 423/861 [00:11<00:09, 44.50it/s][A
 50%|████▉     | 428/861 [00:11<00:09, 44.56it/s][A
 50%|█████     | 433/861 [00:11<00:09, 44.50it/s][A
 51%|█████     | 438/861 [00:11<00:09, 44.39it/s][A
 51%|█████▏    | 443/861 [00:11<00:09, 44.14it/s][A
 52%|█████▏    | 448/861 [00:11<00:09, 43.98it/s][A
 53%|█████▎    | 453/861 [00:11<00:09, 44.02it/s][A
 53%|█████▎    | 458/861 [00:11<00:09, 44.35it/s][A
 54%|█████▍    | 463/861 [00:12<00:08, 44.50it/s][A
 54%|█████▍    | 468/861 [00:12<00:17, 21.83it/s][A
 55%|█████▍    | 473/861 [00:12<00:14, 25.94it/s][A
 56%|█████▌    | 478/861 [00:12<00:12, 29.73it/s][A
 56%|█████▌    | 483/861 [00:12<00:11, 33.11it/s][A
 57%|█████▋    | 488/861 [00:13<00:10, 35.92it/s][A
 57%|█████▋    | 493/861 [00:13<00:09, 38.16it/s][A
 58%|█████▊    | 498/861 [00:13<00:09, 40.04it/s][A
 58%|█████▊    | 503/861 [00:13<00:08, 41.37it/s][A
 59%|█████▉    | 508/861 [00:13<00:08, 42.02it/s][A
 60%|█████▉    | 513/861 [00:13<00:08, 42.27it/s][A
 60%|██████    | 518/861 [00:13<00:08, 42.70it/s][A
 61%|██████    | 523/861 [00:13<00:07, 43.05it/s][A
 61%|██████▏   | 528/861 [00:13<00:07, 43.66it/s][A
 62%|██████▏   | 533/861 [00:14<00:07, 44.05it/s][A
 62%|██████▏   | 538/861 [00:14<00:07, 44.29it/s][A
 63%|██████▎   | 543/861 [00:14<00:18, 17.16it/s][A
 64%|██████▎   | 548/861 [00:14<00:14, 21.08it/s][A
 64%|██████▍   | 553/861 [00:15<00:12, 25.05it/s][A
 65%|██████▍   | 558/861 [00:15<00:10, 28.91it/s][A
 65%|██████▌   | 563/861 [00:15<00:09, 32.34it/s][A
 66%|██████▌   | 568/861 [00:15<00:08, 35.31it/s][A
 67%|██████▋   | 573/861 [00:15<00:07, 37.75it/s][A
 67%|██████▋   | 578/861 [00:15<00:07, 39.39it/s][A
 68%|██████▊   | 583/861 [00:15<00:06, 40.52it/s][A
 68%|██████▊   | 588/861 [00:15<00:06, 41.30it/s][A
 69%|██████▉   | 593/861 [00:16<00:06, 42.11it/s][A
 69%|██████▉   | 598/861 [00:16<00:06, 42.78it/s][A
 70%|███████   | 603/861 [00:16<00:05, 43.44it/s][A
 71%|███████   | 608/861 [00:16<00:05, 43.78it/s][A
 71%|███████   | 613/861 [00:16<00:05, 44.15it/s][A
 72%|███████▏  | 618/861 [00:16<00:05, 44.37it/s][A
 72%|███████▏  | 623/861 [00:16<00:05, 44.23it/s][A
 73%|███████▎  | 628/861 [00:16<00:05, 44.00it/s][A
 74%|███████▎  | 633/861 [00:16<00:05, 43.78it/s][A
 74%|███████▍  | 638/861 [00:17<00:05, 43.92it/s][A
 75%|███████▍  | 643/861 [00:17<00:04, 44.04it/s][A
 75%|███████▌  | 648/861 [00:17<00:04, 44.07it/s][A
 76%|███████▌  | 653/861 [00:17<00:05, 39.60it/s][A
 76%|███████▋  | 658/861 [00:17<00:04, 41.12it/s][A
 77%|███████▋  | 663/861 [00:17<00:04, 42.22it/s][A
 78%|███████▊  | 668/861 [00:17<00:04, 42.95it/s][A
 78%|███████▊  | 673/861 [00:17<00:04, 43.45it/s][A
 79%|███████▊  | 678/861 [00:17<00:04, 43.67it/s][A
 79%|███████▉  | 683/861 [00:18<00:04, 43.89it/s][A
 80%|███████▉  | 688/861 [00:18<00:03, 43.88it/s][A
 80%|████████  | 693/861 [00:18<00:03, 43.64it/s][A
 81%|████████  | 698/861 [00:18<00:03, 43.77it/s][A
 82%|████████▏ | 703/861 [00:18<00:03, 44.04it/s][A
 82%|████████▏ | 708/861 [00:18<00:03, 44.19it/s][A
 83%|████████▎ | 713/861 [00:18<00:03, 44.43it/s][A
 83%|████████▎ | 718/861 [00:18<00:03, 44.53it/s][A
 84%|████████▍ | 723/861 [00:18<00:03, 44.47it/s][A
 85%|████████▍ | 728/861 [00:19<00:02, 44.41it/s][A
 85%|████████▌ | 733/861 [00:19<00:02, 44.05it/s][A
 86%|████████▌ | 738/861 [00:19<00:02, 43.97it/s][A
 86%|████████▋ | 743/861 [00:19<00:02, 44.03it/s][A
 87%|████████▋ | 748/861 [00:19<00:02, 44.20it/s][A
 87%|████████▋ | 753/861 [00:19<00:02, 44.35it/s][A
 88%|████████▊ | 758/861 [00:19<00:02, 44.57it/s][A
 89%|████████▊ | 763/861 [00:19<00:02, 44.58it/s][A
 89%|████████▉ | 768/861 [00:20<00:02, 40.48it/s][A
 90%|████████▉ | 773/861 [00:20<00:02, 41.66it/s][A
 90%|█████████ | 778/861 [00:20<00:01, 42.35it/s][A
 91%|█████████ | 783/861 [00:20<00:01, 42.82it/s][A
 92%|█████████▏| 788/861 [00:20<00:01, 43.10it/s][A
 92%|█████████▏| 793/861 [00:20<00:01, 43.57it/s][A
 93%|█████████▎| 798/861 [00:20<00:01, 43.88it/s][A
 93%|█████████▎| 803/861 [00:20<00:01, 44.11it/s][A
 94%|█████████▍| 808/861 [00:20<00:01, 43.89it/s][A
 94%|█████████▍| 813/861 [00:21<00:01, 44.11it/s][A
 95%|█████████▌| 818/861 [00:21<00:00, 44.23it/s][A
 96%|█████████▌| 823/861 [00:21<00:00, 44.28it/s][A
 96%|█████████▌| 828/861 [00:21<00:00, 44.24it/s][A
 97%|█████████▋| 833/861 [00:21<00:00, 44.27it/s][A
 97%|█████████▋| 838/861 [00:21<00:00, 44.31it/s][A
 98%|█████████▊| 843/861 [00:21<00:00, 44.31it/s][A
 98%|█████████▊| 848/861 [00:21<00:00, 44.25it/s][A
 99%|█████████▉| 853/861 [00:21<00:00, 44.25it/s][A
100%|█████████▉| 858/861 [00:22<00:00, 44.16it/s][A
                                                 [A                                                
100%|██████████| 861/861 [00:22<00:00, 44.16it/s][A 20%|██        | 78/390 [00:45<01:31,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:57:32,238 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 17:57:32,556 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:57:44,332 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:57:44,819 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:57:44,992 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:25<1:37:46, 18.86s/it] 21%|██        | 80/390 [01:27<1:10:06, 13.57s/it] 21%|██        | 81/390 [01:27<49:22,  9.59s/it]   21%|██        | 82/390 [01:27<34:53,  6.80s/it] 21%|██▏       | 83/390 [01:27<24:47,  4.85s/it] 22%|██▏       | 84/390 [01:28<17:45,  3.48s/it] 22%|██▏       | 85/390 [01:28<12:50,  2.52s/it] 22%|██▏       | 86/390 [01:28<09:23,  1.85s/it] 22%|██▏       | 87/390 [01:29<07:00,  1.39s/it] 23%|██▎       | 88/390 [01:30<07:06,  1.41s/it] 23%|██▎       | 89/390 [01:30<05:24,  1.08s/it] 23%|██▎       | 90/390 [01:31<04:12,  1.19it/s] 23%|██▎       | 91/390 [01:31<03:22,  1.48it/s] 24%|██▎       | 92/390 [01:31<02:47,  1.78it/s] 24%|██▍       | 93/390 [01:32<02:22,  2.08it/s] 24%|██▍       | 94/390 [01:32<02:05,  2.35it/s] 24%|██▍       | 95/390 [01:33<03:38,  1.35it/s] 25%|██▍       | 96/390 [01:34<02:58,  1.65it/s] 25%|██▍       | 97/390 [01:34<02:30,  1.95it/s] 25%|██▌       | 98/390 [01:34<02:10,  2.24it/s] 25%|██▌       | 99/390 [01:34<01:56,  2.50it/s] 26%|██▌       | 100/390 [01:35<01:46,  2.72it/s] 26%|██▌       | 101/390 [01:35<01:39,  2.89it/s] 26%|██▌       | 102/390 [01:36<03:11,  1.50it/s] 26%|██▋       | 103/390 [01:37<02:38,  1.81it/s] 27%|██▋       | 104/390 [01:37<02:15,  2.11it/s] 27%|██▋       | 105/390 [01:37<01:59,  2.39it/s] 27%|██▋       | 106/390 [01:38<01:47,  2.64it/s] 27%|██▋       | 107/390 [01:38<01:39,  2.84it/s] 28%|██▊       | 108/390 [01:38<01:34,  3.00it/s] 28%|██▊       | 109/390 [01:39<01:40,  2.79it/s] 28%|██▊       | 110/390 [01:39<01:34,  2.97it/s] 28%|██▊       | 111/390 [01:39<01:29,  3.10it/s] 29%|██▊       | 112/390 [01:40<01:26,  3.20it/s] 29%|██▉       | 113/390 [01:40<01:24,  3.28it/s] 29%|██▉       | 114/390 [01:40<01:22,  3.33it/s] 29%|██▉       | 115/390 [01:40<01:21,  3.37it/s] 30%|██▉       | 116/390 [01:41<01:20,  3.40it/s] 30%|███       | 117/390 [01:41<01:19,  3.42it/s] 30%|███       | 118/390 [01:41<01:19,  3.43it/s] 31%|███       | 119/390 [01:42<01:24,  3.20it/s] 31%|███       | 120/390 [01:42<01:22,  3.28it/s] 31%|███       | 121/390 [01:42<01:20,  3.33it/s] 31%|███▏      | 122/390 [01:42<01:19,  3.37it/s] 32%|███▏      | 123/390 [01:43<01:18,  3.40it/s] 32%|███▏      | 124/390 [01:43<01:17,  3.42it/s] 32%|███▏      | 125/390 [01:43<01:17,  3.43it/s] 32%|███▏      | 126/390 [01:44<01:16,  3.43it/s] 33%|███▎      | 127/390 [01:44<01:16,  3.44it/s] 33%|███▎      | 128/390 [01:44<01:16,  3.44it/s] 33%|███▎      | 129/390 [01:44<01:15,  3.45it/s] 33%|███▎      | 130/390 [01:45<01:19,  3.25it/s] 34%|███▎      | 131/390 [01:45<01:18,  3.31it/s] 34%|███▍      | 132/390 [01:45<01:17,  3.35it/s] 34%|███▍      | 133/390 [01:46<01:15,  3.39it/s] 34%|███▍      | 134/390 [01:46<01:15,  3.41it/s] 35%|███▍      | 135/390 [01:46<01:18,  3.23it/s] 35%|███▍      | 136/390 [01:47<01:17,  3.29it/s] 35%|███▌      | 137/390 [01:47<01:15,  3.34it/s] 35%|███▌      | 138/390 [01:47<01:14,  3.37it/s] 36%|███▌      | 139/390 [01:48<01:13,  3.40it/s] 36%|███▌      | 140/390 [01:48<01:17,  3.23it/s] 36%|███▌      | 141/390 [01:48<01:15,  3.29it/s] 36%|███▋      | 142/390 [01:48<01:14,  3.33it/s] 37%|███▋      | 143/390 [01:49<01:13,  3.37it/s] 37%|███▋      | 144/390 [01:49<01:12,  3.40it/s] 37%|███▋      | 145/390 [01:49<01:11,  3.42it/s] 37%|███▋      | 146/390 [01:50<01:11,  3.43it/s] 38%|███▊      | 147/390 [01:50<01:32,  2.63it/s] 38%|███▊      | 148/390 [01:50<01:25,  2.84it/s] 38%|███▊      | 149/390 [01:51<01:20,  3.00it/s] 38%|███▊      | 150/390 [01:51<01:16,  3.12it/s] 39%|███▊      | 151/390 [01:51<01:14,  3.21it/s] 39%|███▉      | 152/390 [01:52<01:12,  3.28it/s] 39%|███▉      | 153/390 [01:52<01:11,  3.33it/s] 39%|███▉      | 154/390 [01:52<01:10,  3.37it/s] 40%|███▉      | 155/390 [01:52<01:09,  3.39it/s] 40%|████      | 156/390 [01:53<01:08,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 17:58:38,202 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:58:38,202 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 17:58:38,202 >>   Batch size = 8
{'eval_loss': 1.022870659828186, 'eval_runtime': 22.1615, 'eval_samples_per_second': 310.629, 'eval_steps_per_second': 38.851, 'epoch': 0.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 55.97it/s][A
  1%|▏         | 12/861 [00:00<00:35, 23.97it/s][A
  2%|▏         | 18/861 [00:00<00:26, 31.36it/s][A
  3%|▎         | 23/861 [00:00<00:23, 35.10it/s][A
  3%|▎         | 28/861 [00:00<00:21, 37.90it/s][A
  4%|▍         | 33/861 [00:00<00:20, 40.00it/s][A
  4%|▍         | 38/861 [00:01<00:19, 41.39it/s][A
  5%|▍         | 43/861 [00:01<00:19, 42.55it/s][A
  6%|▌         | 48/861 [00:01<00:18, 43.26it/s][A
  6%|▌         | 53/861 [00:01<00:18, 43.43it/s][A
  7%|▋         | 58/861 [00:01<00:18, 43.46it/s][A
  7%|▋         | 63/861 [00:01<00:18, 43.19it/s][A
  8%|▊         | 68/861 [00:01<00:18, 43.55it/s][A
  8%|▊         | 73/861 [00:02<00:17, 43.88it/s][A
  9%|▉         | 78/861 [00:02<00:27, 28.74it/s][A
 10%|▉         | 83/861 [00:02<00:24, 32.28it/s][A
 10%|█         | 88/861 [00:02<00:21, 35.25it/s][A
 11%|█         | 93/861 [00:02<00:20, 37.70it/s][A
 11%|█▏        | 98/861 [00:02<00:19, 39.63it/s][A
 12%|█▏        | 103/861 [00:02<00:18, 41.05it/s][A
 13%|█▎        | 108/861 [00:02<00:17, 42.16it/s][A
 13%|█▎        | 113/861 [00:02<00:17, 42.90it/s][A
 14%|█▎        | 118/861 [00:03<00:17, 42.99it/s][A
 14%|█▍        | 123/861 [00:03<00:17, 43.09it/s][A
 15%|█▍        | 128/861 [00:03<00:16, 43.28it/s][A
 15%|█▌        | 133/861 [00:03<00:16, 43.65it/s][A
 16%|█▌        | 138/861 [00:03<00:16, 44.07it/s][A
 17%|█▋        | 143/861 [00:03<00:16, 44.31it/s][A
 17%|█▋        | 148/861 [00:03<00:16, 44.51it/s][A
 18%|█▊        | 153/861 [00:03<00:15, 44.50it/s][A
 18%|█▊        | 158/861 [00:03<00:15, 44.43it/s][A
 19%|█▉        | 163/861 [00:04<00:15, 44.26it/s][A
 20%|█▉        | 168/861 [00:04<00:15, 44.13it/s][A
 20%|██        | 173/861 [00:04<00:15, 44.13it/s][A
 21%|██        | 178/861 [00:04<00:15, 44.11it/s][A
 21%|██▏       | 183/861 [00:04<00:15, 44.33it/s][A
 22%|██▏       | 188/861 [00:04<00:15, 44.44it/s][A
 22%|██▏       | 193/861 [00:04<00:14, 44.63it/s][A
 23%|██▎       | 198/861 [00:05<00:14, 44.62it/s][A
 24%|██▎       | 203/861 [00:05<00:24, 26.34it/s][A
 24%|██▍       | 208/861 [00:05<00:21, 30.06it/s][A
 25%|██▍       | 213/861 [00:05<00:19, 33.40it/s][A
 25%|██▌       | 218/861 [00:05<00:17, 36.20it/s][A
 26%|██▌       | 223/861 [00:05<00:16, 38.43it/s][A
 26%|██▋       | 228/861 [00:05<00:15, 40.19it/s][A
 27%|██▋       | 233/861 [00:05<00:15, 41.45it/s][A
 28%|██▊       | 238/861 [00:05<00:14, 42.30it/s][A
 28%|██▊       | 243/861 [00:06<00:14, 42.52it/s][A
 29%|██▉       | 248/861 [00:06<00:14, 42.69it/s][A
 29%|██▉       | 253/861 [00:06<00:14, 43.18it/s][A
 30%|██▉       | 258/861 [00:06<00:13, 43.52it/s][A
 31%|███       | 263/861 [00:06<00:13, 43.94it/s][A
 31%|███       | 268/861 [00:06<00:13, 44.24it/s][A
 32%|███▏      | 273/861 [00:06<00:13, 44.46it/s][A
 32%|███▏      | 278/861 [00:06<00:13, 44.60it/s][A
 33%|███▎      | 283/861 [00:07<00:13, 44.35it/s][A
 33%|███▎      | 288/861 [00:07<00:15, 36.40it/s][A
 34%|███▍      | 293/861 [00:07<00:14, 38.65it/s][A
 35%|███▍      | 298/861 [00:07<00:13, 40.35it/s][A
 35%|███▌      | 303/861 [00:07<00:13, 41.60it/s][A
 36%|███▌      | 308/861 [00:07<00:13, 42.53it/s][A
 36%|███▋      | 313/861 [00:07<00:12, 43.22it/s][A
 37%|███▋      | 318/861 [00:07<00:12, 43.75it/s][A
 38%|███▊      | 323/861 [00:07<00:12, 43.83it/s][A
 38%|███▊      | 328/861 [00:08<00:12, 43.63it/s][A
 39%|███▊      | 333/861 [00:08<00:12, 43.56it/s][A
 39%|███▉      | 338/861 [00:08<00:11, 43.62it/s][A
 40%|███▉      | 343/861 [00:08<00:11, 44.00it/s][A
 40%|████      | 348/861 [00:08<00:11, 44.26it/s][A
 41%|████      | 353/861 [00:08<00:11, 44.52it/s][A
 42%|████▏     | 358/861 [00:08<00:11, 44.65it/s][A
 42%|████▏     | 363/861 [00:08<00:11, 44.63it/s][A
 43%|████▎     | 368/861 [00:08<00:11, 44.30it/s][A
 43%|████▎     | 373/861 [00:09<00:11, 44.10it/s][A
 44%|████▍     | 378/861 [00:09<00:10, 43.91it/s][A
 44%|████▍     | 383/861 [00:09<00:10, 43.90it/s][A
 45%|████▌     | 388/861 [00:09<00:10, 44.06it/s][A
 46%|████▌     | 393/861 [00:09<00:10, 44.34it/s][A
 46%|████▌     | 398/861 [00:09<00:10, 44.52it/s][A
 47%|████▋     | 403/861 [00:09<00:10, 44.65it/s][A
 47%|████▋     | 408/861 [00:09<00:10, 44.59it/s][A
 48%|████▊     | 413/861 [00:10<00:10, 44.34it/s][A
 49%|████▊     | 418/861 [00:10<00:11, 40.25it/s][A
 49%|████▉     | 423/861 [00:10<00:10, 41.53it/s][A
 50%|████▉     | 428/861 [00:10<00:10, 42.35it/s][A
 50%|█████     | 433/861 [00:10<00:09, 43.09it/s][A
 51%|█████     | 438/861 [00:10<00:09, 43.52it/s][A
 51%|█████▏    | 443/861 [00:10<00:09, 43.96it/s][A
 52%|█████▏    | 448/861 [00:10<00:09, 44.21it/s][A
 53%|█████▎    | 453/861 [00:10<00:09, 44.05it/s][A
 53%|█████▎    | 458/861 [00:11<00:09, 43.84it/s][A
 54%|█████▍    | 463/861 [00:11<00:10, 36.31it/s][A
 54%|█████▍    | 468/861 [00:11<00:10, 38.76it/s][A
 55%|█████▍    | 473/861 [00:11<00:09, 40.43it/s][A
 56%|█████▌    | 478/861 [00:11<00:09, 41.66it/s][A
 56%|█████▌    | 483/861 [00:11<00:08, 42.56it/s][A
 57%|█████▋    | 488/861 [00:11<00:08, 43.21it/s][A
 57%|█████▋    | 493/861 [00:12<00:15, 23.63it/s][A
 58%|█████▊    | 498/861 [00:13<00:29, 12.30it/s][A
 58%|█████▊    | 503/861 [00:13<00:22, 15.89it/s][A
 59%|█████▉    | 508/861 [00:13<00:17, 19.73it/s][A
 60%|█████▉    | 513/861 [00:13<00:14, 23.69it/s][A
 60%|██████    | 518/861 [00:13<00:12, 27.58it/s][A
 61%|██████    | 523/861 [00:13<00:10, 31.19it/s][A
 61%|██████▏   | 528/861 [00:13<00:09, 34.31it/s][A
 62%|██████▏   | 533/861 [00:13<00:08, 36.95it/s][A
 62%|██████▏   | 538/861 [00:13<00:08, 38.92it/s][A
 63%|██████▎   | 543/861 [00:14<00:07, 39.99it/s][A
 64%|██████▎   | 548/861 [00:14<00:07, 40.98it/s][A
 64%|██████▍   | 553/861 [00:14<00:07, 41.83it/s][A
 65%|██████▍   | 558/861 [00:14<00:07, 42.61it/s][A
 65%|██████▌   | 563/861 [00:14<00:06, 43.31it/s][A
 66%|██████▌   | 568/861 [00:14<00:06, 43.63it/s][A
 67%|██████▋   | 573/861 [00:14<00:06, 43.99it/s][A
 67%|██████▋   | 578/861 [00:14<00:06, 44.30it/s][A
 68%|██████▊   | 583/861 [00:14<00:06, 44.28it/s][A
 68%|██████▊   | 588/861 [00:15<00:06, 44.00it/s][A
 69%|██████▉   | 593/861 [00:15<00:06, 43.92it/s][A
 69%|██████▉   | 598/861 [00:15<00:05, 43.89it/s][A
 70%|███████   | 603/861 [00:16<00:14, 17.23it/s][A
 71%|███████   | 608/861 [00:16<00:11, 21.13it/s][A
 71%|███████   | 613/861 [00:16<00:09, 25.14it/s][A
 72%|███████▏  | 618/861 [00:16<00:08, 28.96it/s][A
 72%|███████▏  | 623/861 [00:16<00:07, 32.36it/s][A
 73%|███████▎  | 628/861 [00:16<00:06, 35.37it/s][A
 74%|███████▎  | 633/861 [00:16<00:06, 37.73it/s][A
 74%|███████▍  | 638/861 [00:16<00:05, 39.53it/s][A
 75%|███████▍  | 643/861 [00:16<00:05, 40.45it/s][A
 75%|███████▌  | 648/861 [00:17<00:05, 41.28it/s][A
 76%|███████▌  | 653/861 [00:17<00:04, 42.03it/s][A
 76%|███████▋  | 658/861 [00:17<00:04, 42.81it/s][A
 77%|███████▋  | 663/861 [00:17<00:04, 43.38it/s][A
 78%|███████▊  | 668/861 [00:17<00:04, 43.89it/s][A
 78%|███████▊  | 673/861 [00:17<00:04, 44.20it/s][A
 79%|███████▊  | 678/861 [00:17<00:04, 44.26it/s][A
 79%|███████▉  | 683/861 [00:17<00:04, 44.21it/s][A
 80%|███████▉  | 688/861 [00:18<00:03, 43.90it/s][A
 80%|████████  | 693/861 [00:18<00:10, 16.54it/s][A
 81%|████████  | 698/861 [00:18<00:07, 20.40it/s][A
 82%|████████▏ | 703/861 [00:18<00:06, 24.40it/s][A
 82%|████████▏ | 708/861 [00:19<00:05, 28.27it/s][A
 83%|████████▎ | 713/861 [00:19<00:04, 31.76it/s][A
 83%|████████▎ | 718/861 [00:19<00:04, 34.83it/s][A
 84%|████████▍ | 723/861 [00:19<00:03, 37.37it/s][A
 85%|████████▍ | 728/861 [00:19<00:03, 39.12it/s][A
 85%|████████▌ | 733/861 [00:19<00:03, 40.26it/s][A
 86%|████████▌ | 738/861 [00:19<00:02, 41.04it/s][A
 86%|████████▋ | 743/861 [00:19<00:02, 41.99it/s][A
 87%|████████▋ | 748/861 [00:19<00:02, 42.74it/s][A
 87%|████████▋ | 753/861 [00:20<00:02, 43.34it/s][A
 88%|████████▊ | 758/861 [00:20<00:02, 43.79it/s][A
 89%|████████▊ | 763/861 [00:20<00:02, 44.09it/s][A
 89%|████████▉ | 768/861 [00:20<00:02, 44.17it/s][A
 90%|████████▉ | 773/861 [00:20<00:01, 44.16it/s][A
 90%|█████████ | 778/861 [00:20<00:01, 43.78it/s][A
 91%|█████████ | 783/861 [00:20<00:01, 43.81it/s][A
 92%|█████████▏| 788/861 [00:20<00:01, 43.87it/s][A
 92%|█████████▏| 793/861 [00:21<00:01, 43.90it/s][A
 93%|█████████▎| 798/861 [00:21<00:01, 37.91it/s][A
 93%|█████████▎| 803/861 [00:21<00:01, 39.82it/s][A
 94%|█████████▍| 808/861 [00:21<00:01, 41.18it/s][A
 94%|█████████▍| 813/861 [00:21<00:01, 42.17it/s][A
 95%|█████████▌| 818/861 [00:21<00:01, 42.89it/s][A
 96%|█████████▌| 823/861 [00:21<00:00, 43.53it/s][A
 96%|█████████▌| 828/861 [00:21<00:00, 43.95it/s][A
 97%|█████████▋| 833/861 [00:21<00:00, 44.04it/s][A
 97%|█████████▋| 838/861 [00:22<00:00, 43.72it/s][A
 98%|█████████▊| 843/861 [00:22<00:00, 43.49it/s][A
 98%|█████████▊| 848/861 [00:22<00:00, 43.69it/s][A
 99%|█████████▉| 853/861 [00:22<00:00, 44.01it/s][A
100%|█████████▉| 858/861 [00:22<00:00, 44.21it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:22<00:00, 44.21it/s][A 40%|████      | 156/390 [02:15<01:08,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:59:01,674 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 17:59:02,143 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:59:18,463 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:59:21,993 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:59:23,068 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [03:26<1:49:43, 28.26s/it] 41%|████      | 158/390 [03:27<1:16:52, 19.88s/it] 41%|████      | 159/390 [03:27<53:55, 14.01s/it]   41%|████      | 160/390 [03:27<37:55,  9.89s/it] 41%|████▏     | 161/390 [03:28<26:45,  7.01s/it] 42%|████▏     | 162/390 [03:28<18:59,  5.00s/it] 42%|████▏     | 163/390 [03:28<13:34,  3.59s/it] 42%|████▏     | 164/390 [03:28<09:47,  2.60s/it] 42%|████▏     | 165/390 [03:29<07:09,  1.91s/it] 43%|████▎     | 166/390 [03:29<05:18,  1.42s/it] 43%|████▎     | 167/390 [03:29<04:01,  1.08s/it] 43%|████▎     | 168/390 [03:30<03:08,  1.18it/s] 43%|████▎     | 169/390 [03:31<03:17,  1.12it/s] 44%|████▎     | 170/390 [03:31<02:37,  1.40it/s] 44%|████▍     | 171/390 [03:31<02:08,  1.70it/s] 44%|████▍     | 172/390 [03:31<01:49,  2.00it/s] 44%|████▍     | 173/390 [03:32<01:35,  2.28it/s] 45%|████▍     | 174/390 [03:32<01:25,  2.53it/s] 45%|████▍     | 175/390 [03:32<01:18,  2.74it/s] 45%|████▌     | 176/390 [03:33<01:13,  2.91it/s] 45%|████▌     | 177/390 [03:33<01:18,  2.71it/s] 46%|████▌     | 178/390 [03:33<01:13,  2.89it/s] 46%|████▌     | 179/390 [03:34<01:09,  3.03it/s] 46%|████▌     | 180/390 [03:34<01:07,  3.13it/s] 46%|████▋     | 181/390 [03:34<01:05,  3.21it/s] 47%|████▋     | 182/390 [03:35<01:03,  3.27it/s] 47%|████▋     | 183/390 [03:35<01:02,  3.31it/s] 47%|████▋     | 184/390 [03:35<01:01,  3.33it/s] 47%|████▋     | 185/390 [03:36<01:34,  2.18it/s] 48%|████▊     | 186/390 [03:36<01:27,  2.33it/s] 48%|████▊     | 187/390 [03:37<01:18,  2.57it/s] 48%|████▊     | 188/390 [03:37<01:12,  2.78it/s] 48%|████▊     | 189/390 [03:37<01:26,  2.31it/s] 49%|████▊     | 190/390 [03:38<01:18,  2.56it/s] 49%|████▉     | 191/390 [03:38<01:11,  2.77it/s] 49%|████▉     | 192/390 [03:38<01:07,  2.94it/s] 49%|████▉     | 193/390 [03:39<01:04,  3.07it/s] 50%|████▉     | 194/390 [03:39<01:01,  3.16it/s] 50%|█████     | 195/390 [03:40<01:17,  2.51it/s] 50%|█████     | 196/390 [03:40<01:11,  2.73it/s] 51%|█████     | 197/390 [03:40<01:06,  2.90it/s] 51%|█████     | 198/390 [03:40<01:03,  3.04it/s] 51%|█████     | 199/390 [03:41<01:00,  3.14it/s] 51%|█████▏    | 200/390 [03:41<00:58,  3.22it/s] 52%|█████▏    | 201/390 [03:41<00:57,  3.28it/s] 52%|█████▏    | 202/390 [03:42<00:56,  3.31it/s] 52%|█████▏    | 203/390 [03:42<00:55,  3.34it/s] 52%|█████▏    | 204/390 [03:42<00:55,  3.36it/s] 53%|█████▎    | 205/390 [03:43<00:57,  3.20it/s] 53%|█████▎    | 206/390 [03:43<00:56,  3.26it/s] 53%|█████▎    | 207/390 [03:43<00:55,  3.30it/s] 53%|█████▎    | 208/390 [03:43<00:54,  3.33it/s] 54%|█████▎    | 209/390 [03:44<00:53,  3.36it/s] 54%|█████▍    | 210/390 [03:44<00:53,  3.37it/s] 54%|█████▍    | 211/390 [03:44<00:52,  3.38it/s] 54%|█████▍    | 212/390 [03:45<00:52,  3.40it/s] 55%|█████▍    | 213/390 [03:45<00:52,  3.40it/s] 55%|█████▍    | 214/390 [03:45<00:51,  3.40it/s] 55%|█████▌    | 215/390 [03:45<00:51,  3.41it/s] 55%|█████▌    | 216/390 [03:46<01:12,  2.41it/s] 56%|█████▌    | 217/390 [03:47<01:16,  2.25it/s] 56%|█████▌    | 218/390 [03:47<01:08,  2.51it/s] 56%|█████▌    | 219/390 [03:47<01:02,  2.72it/s] 56%|█████▋    | 220/390 [03:48<00:58,  2.90it/s] 57%|█████▋    | 221/390 [03:48<00:55,  3.03it/s] 57%|█████▋    | 222/390 [03:48<00:53,  3.14it/s] 57%|█████▋    | 223/390 [03:48<00:51,  3.21it/s] 57%|█████▋    | 224/390 [03:49<00:50,  3.26it/s] 58%|█████▊    | 225/390 [03:49<00:53,  3.09it/s] 58%|█████▊    | 226/390 [03:49<00:51,  3.17it/s] 58%|█████▊    | 227/390 [03:50<00:50,  3.24it/s] 58%|█████▊    | 228/390 [03:50<00:49,  3.29it/s] 59%|█████▊    | 229/390 [03:50<00:48,  3.33it/s] 59%|█████▉    | 230/390 [03:51<00:47,  3.35it/s] 59%|█████▉    | 231/390 [03:51<00:47,  3.37it/s] 59%|█████▉    | 232/390 [03:51<00:57,  2.76it/s] 60%|█████▉    | 233/390 [03:52<00:53,  2.92it/s] 60%|██████    | 234/390 [03:52<00:51,  3.06it/s][INFO|trainer.py:2140] 2023-08-28 18:00:37,415 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:00:37,415 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 18:00:37,415 >>   Batch size = 8
{'eval_loss': 1.0389851331710815, 'eval_runtime': 22.5701, 'eval_samples_per_second': 305.006, 'eval_steps_per_second': 38.148, 'epoch': 1.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.30it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.98it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.05it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.34it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.82it/s][A
  4%|▎         | 32/861 [00:00<00:18, 45.70it/s][A
  4%|▍         | 37/861 [00:00<00:18, 45.40it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.62it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.23it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.29it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.41it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.45it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.58it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.72it/s][A
  9%|▉         | 77/861 [00:01<00:19, 39.37it/s][A
 10%|▉         | 82/861 [00:01<00:18, 41.55it/s][A
 10%|█         | 87/861 [00:01<00:18, 42.50it/s][A
 11%|█         | 92/861 [00:02<00:17, 43.14it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.59it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 43.91it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.17it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.40it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.28it/s][A
 14%|█▍        | 122/861 [00:03<00:54, 13.68it/s][A
 15%|█▍        | 126/861 [00:03<00:49, 14.87it/s][A
 15%|█▌        | 131/861 [00:03<00:38, 18.83it/s][A
 16%|█▌        | 136/861 [00:04<00:31, 22.95it/s][A
 16%|█▋        | 141/861 [00:04<00:26, 26.95it/s][A
 17%|█▋        | 146/861 [00:04<00:23, 30.71it/s][A
 18%|█▊        | 151/861 [00:04<00:20, 34.01it/s][A
 18%|█▊        | 156/861 [00:04<00:19, 36.65it/s][A
 19%|█▊        | 161/861 [00:04<00:18, 38.79it/s][A
 19%|█▉        | 166/861 [00:04<00:17, 40.05it/s][A
 20%|█▉        | 171/861 [00:04<00:16, 40.93it/s][A
 20%|██        | 176/861 [00:04<00:16, 41.88it/s][A
 21%|██        | 181/861 [00:05<00:15, 42.68it/s][A
 22%|██▏       | 186/861 [00:05<00:15, 43.33it/s][A
 22%|██▏       | 191/861 [00:05<00:15, 43.75it/s][A
 23%|██▎       | 196/861 [00:05<00:15, 44.06it/s][A
 23%|██▎       | 201/861 [00:05<00:14, 44.31it/s][A
 24%|██▍       | 206/861 [00:05<00:14, 44.34it/s][A
 25%|██▍       | 211/861 [00:05<00:14, 44.17it/s][A
 25%|██▌       | 216/861 [00:05<00:14, 43.96it/s][A
 26%|██▌       | 221/861 [00:05<00:14, 44.04it/s][A
 26%|██▌       | 226/861 [00:06<00:14, 44.20it/s][A
 27%|██▋       | 231/861 [00:06<00:14, 44.30it/s][A
 27%|██▋       | 236/861 [00:06<00:14, 44.54it/s][A
 28%|██▊       | 241/861 [00:06<00:13, 44.67it/s][A
 29%|██▊       | 246/861 [00:06<00:13, 44.71it/s][A
 29%|██▉       | 251/861 [00:06<00:13, 44.50it/s][A
 30%|██▉       | 256/861 [00:06<00:22, 27.39it/s][A
 30%|███       | 261/861 [00:07<00:19, 31.03it/s][A
 31%|███       | 265/861 [00:07<00:21, 28.11it/s][A
 31%|███▏      | 270/861 [00:07<00:18, 31.88it/s][A
 32%|███▏      | 275/861 [00:07<00:16, 35.02it/s][A
 33%|███▎      | 280/861 [00:07<00:15, 37.49it/s][A
 33%|███▎      | 285/861 [00:07<00:14, 39.54it/s][A
 34%|███▎      | 290/861 [00:07<00:13, 41.04it/s][A
 34%|███▍      | 295/861 [00:07<00:13, 42.19it/s][A
 35%|███▍      | 300/861 [00:08<00:13, 42.89it/s][A
 35%|███▌      | 305/861 [00:08<00:12, 43.09it/s][A
 36%|███▌      | 310/861 [00:08<00:12, 43.25it/s][A
 37%|███▋      | 315/861 [00:08<00:12, 43.51it/s][A
 37%|███▋      | 320/861 [00:08<00:12, 43.90it/s][A
 38%|███▊      | 325/861 [00:08<00:12, 44.28it/s][A
 38%|███▊      | 330/861 [00:08<00:11, 44.60it/s][A
 39%|███▉      | 335/861 [00:08<00:11, 44.68it/s][A
 39%|███▉      | 340/861 [00:08<00:11, 44.83it/s][A
 40%|████      | 345/861 [00:09<00:11, 44.61it/s][A
 41%|████      | 350/861 [00:09<00:11, 44.41it/s][A
 41%|████      | 355/861 [00:09<00:11, 44.28it/s][A
 42%|████▏     | 360/861 [00:09<00:11, 44.27it/s][A
 42%|████▏     | 365/861 [00:09<00:11, 44.41it/s][A
 43%|████▎     | 370/861 [00:09<00:11, 44.57it/s][A
 44%|████▎     | 375/861 [00:09<00:10, 44.66it/s][A
 44%|████▍     | 380/861 [00:09<00:10, 44.88it/s][A
 45%|████▍     | 385/861 [00:09<00:10, 44.94it/s][A
 45%|████▌     | 390/861 [00:10<00:10, 44.76it/s][A
 46%|████▌     | 395/861 [00:10<00:15, 30.15it/s][A
 46%|████▋     | 400/861 [00:10<00:13, 33.52it/s][A
 47%|████▋     | 405/861 [00:10<00:12, 36.36it/s][A
 48%|████▊     | 410/861 [00:10<00:11, 38.62it/s][A
 48%|████▊     | 415/861 [00:10<00:11, 40.30it/s][A
 49%|████▉     | 420/861 [00:10<00:10, 41.65it/s][A
 49%|████▉     | 425/861 [00:10<00:10, 42.69it/s][A
 50%|████▉     | 430/861 [00:11<00:09, 43.22it/s][A
 51%|█████     | 435/861 [00:11<00:09, 43.27it/s][A
 51%|█████     | 440/861 [00:11<00:09, 43.28it/s][A
 52%|█████▏    | 445/861 [00:11<00:09, 43.51it/s][A
 52%|█████▏    | 450/861 [00:11<00:09, 44.10it/s][A
 53%|█████▎    | 455/861 [00:11<00:09, 44.38it/s][A
 53%|█████▎    | 460/861 [00:11<00:09, 44.55it/s][A
 54%|█████▍    | 465/861 [00:11<00:08, 44.70it/s][A
 55%|█████▍    | 470/861 [00:12<00:08, 44.80it/s][A
 55%|█████▌    | 475/861 [00:12<00:08, 44.63it/s][A
 56%|█████▌    | 480/861 [00:12<00:08, 44.25it/s][A
 56%|█████▋    | 485/861 [00:12<00:08, 44.04it/s][A
 57%|█████▋    | 490/861 [00:12<00:08, 44.16it/s][A
 57%|█████▋    | 495/861 [00:12<00:08, 44.38it/s][A
 58%|█████▊    | 500/861 [00:12<00:08, 44.55it/s][A
 59%|█████▊    | 505/861 [00:12<00:07, 44.79it/s][A
 59%|█████▉    | 510/861 [00:12<00:07, 44.94it/s][A
 60%|█████▉    | 515/861 [00:13<00:07, 44.93it/s][A
 60%|██████    | 520/861 [00:13<00:07, 44.60it/s][A
 61%|██████    | 525/861 [00:13<00:11, 29.80it/s][A
 62%|██████▏   | 530/861 [00:13<00:09, 33.19it/s][A
 62%|██████▏   | 535/861 [00:13<00:09, 35.92it/s][A
 63%|██████▎   | 540/861 [00:13<00:08, 38.31it/s][A
 63%|██████▎   | 545/861 [00:13<00:07, 40.18it/s][A
 64%|██████▍   | 550/861 [00:13<00:07, 41.62it/s][A
 64%|██████▍   | 555/861 [00:14<00:07, 42.64it/s][A
 65%|██████▌   | 560/861 [00:14<00:06, 43.20it/s][A
 66%|██████▌   | 565/861 [00:14<00:06, 43.09it/s][A
 66%|██████▌   | 570/861 [00:14<00:06, 43.26it/s][A
 67%|██████▋   | 575/861 [00:14<00:06, 43.50it/s][A
 67%|██████▋   | 580/861 [00:14<00:06, 43.92it/s][A
 68%|██████▊   | 585/861 [00:14<00:06, 44.24it/s][A
 69%|██████▊   | 590/861 [00:14<00:06, 44.48it/s][A
 69%|██████▉   | 595/861 [00:14<00:05, 44.71it/s][A
 70%|██████▉   | 600/861 [00:15<00:05, 44.87it/s][A
 70%|███████   | 605/861 [00:15<00:05, 44.60it/s][A
 71%|███████   | 610/861 [00:15<00:05, 44.36it/s][A
 71%|███████▏  | 615/861 [00:15<00:05, 44.24it/s][A
 72%|███████▏  | 620/861 [00:15<00:05, 44.17it/s][A
 73%|███████▎  | 625/861 [00:15<00:05, 44.28it/s][A
 73%|███████▎  | 630/861 [00:15<00:05, 44.51it/s][A
 74%|███████▍  | 635/861 [00:15<00:05, 44.70it/s][A
 74%|███████▍  | 640/861 [00:16<00:04, 44.92it/s][A
 75%|███████▍  | 645/861 [00:16<00:04, 44.91it/s][A
 75%|███████▌  | 650/861 [00:17<00:04, 44.61it/s][A
 76%|███████▌  | 655/861 [00:17<00:21,  9.59it/s][A
 77%|███████▋  | 660/861 [00:17<00:16, 12.55it/s][A
 77%|███████▋  | 665/861 [00:17<00:12, 16.03it/s][A
 78%|███████▊  | 670/861 [00:18<00:09, 19.88it/s][A
 78%|███████▊  | 675/861 [00:18<00:07, 23.88it/s][A
 79%|███████▉  | 680/861 [00:18<00:06, 27.82it/s][A
 80%|███████▉  | 685/861 [00:18<00:05, 31.46it/s][A
 80%|████████  | 690/861 [00:18<00:04, 34.52it/s][A
 81%|████████  | 695/861 [00:18<00:04, 36.75it/s][A
 81%|████████▏ | 700/861 [00:18<00:04, 38.48it/s][A
 82%|████████▏ | 705/861 [00:18<00:03, 40.05it/s][A
 82%|████████▏ | 710/861 [00:18<00:03, 41.39it/s][A
 83%|████████▎ | 715/861 [00:19<00:03, 42.34it/s][A
 84%|████████▎ | 720/861 [00:19<00:03, 43.17it/s][A
 84%|████████▍ | 725/861 [00:19<00:03, 43.80it/s][A
 85%|████████▍ | 730/861 [00:19<00:02, 44.20it/s][A
 85%|████████▌ | 735/861 [00:19<00:02, 44.22it/s][A
 86%|████████▌ | 740/861 [00:19<00:02, 44.02it/s][A
 87%|████████▋ | 745/861 [00:19<00:02, 43.92it/s][A
 87%|████████▋ | 750/861 [00:19<00:02, 43.96it/s][A
 88%|████████▊ | 755/861 [00:19<00:02, 44.07it/s][A
 88%|████████▊ | 760/861 [00:20<00:02, 44.44it/s][A
 89%|████████▉ | 765/861 [00:20<00:02, 44.69it/s][A
 89%|████████▉ | 770/861 [00:20<00:02, 44.88it/s][A
 90%|█████████ | 775/861 [00:20<00:05, 17.19it/s][A
 91%|█████████ | 780/861 [00:21<00:03, 21.12it/s][A
 91%|█████████ | 785/861 [00:21<00:03, 25.12it/s][A
 92%|█████████▏| 790/861 [00:21<00:02, 28.96it/s][A
 92%|█████████▏| 795/861 [00:21<00:02, 32.48it/s][A
 93%|█████████▎| 800/861 [00:21<00:01, 35.47it/s][A
 93%|█████████▎| 805/861 [00:21<00:01, 37.88it/s][A
 94%|█████████▍| 810/861 [00:21<00:01, 39.63it/s][A
 95%|█████████▍| 815/861 [00:21<00:01, 40.75it/s][A
 95%|█████████▌| 820/861 [00:21<00:00, 41.51it/s][A
 96%|█████████▌| 825/861 [00:22<00:00, 42.24it/s][A
 96%|█████████▋| 830/861 [00:22<00:00, 42.94it/s][A
 97%|█████████▋| 835/861 [00:22<00:00, 43.41it/s][A
 98%|█████████▊| 840/861 [00:22<00:00, 43.99it/s][A
 98%|█████████▊| 845/861 [00:22<00:00, 44.37it/s][A
 99%|█████████▊| 850/861 [00:22<00:00, 44.62it/s][A
 99%|█████████▉| 855/861 [00:22<00:00, 44.48it/s][A
100%|█████████▉| 860/861 [00:22<00:00, 44.23it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:22<00:00, 44.23it/s][A 60%|██████    | 234/390 [04:15<00:51,  3.06it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:01:00,608 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 18:01:01,234 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:01:12,655 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:01:14,371 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:01:14,723 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [04:56<50:34, 19.58s/it] 61%|██████    | 236/390 [04:57<35:27, 13.81s/it] 61%|██████    | 237/390 [04:57<24:52,  9.76s/it] 61%|██████    | 238/390 [04:57<17:31,  6.92s/it] 61%|██████▏   | 239/390 [04:58<12:24,  4.93s/it] 62%|██████▏   | 240/390 [04:58<08:50,  3.54s/it] 62%|██████▏   | 241/390 [04:58<06:22,  2.56s/it] 62%|██████▏   | 242/390 [04:59<04:38,  1.88s/it] 62%|██████▏   | 243/390 [04:59<03:26,  1.41s/it] 63%|██████▎   | 244/390 [04:59<02:36,  1.07s/it] 63%|██████▎   | 245/390 [04:59<02:01,  1.19it/s] 63%|██████▎   | 246/390 [05:00<01:37,  1.48it/s] 63%|██████▎   | 247/390 [05:00<01:25,  1.67it/s] 64%|██████▎   | 248/390 [05:00<01:11,  1.97it/s] 64%|██████▍   | 249/390 [05:01<01:02,  2.26it/s] 64%|██████▍   | 250/390 [05:01<00:55,  2.51it/s] 64%|██████▍   | 251/390 [05:01<00:50,  2.73it/s] 65%|██████▍   | 252/390 [05:02<00:47,  2.90it/s] 65%|██████▍   | 253/390 [05:02<00:45,  3.04it/s] 65%|██████▌   | 254/390 [05:02<00:43,  3.14it/s] 65%|██████▌   | 255/390 [05:03<00:42,  3.21it/s] 66%|██████▌   | 256/390 [05:03<00:40,  3.27it/s] 66%|██████▌   | 257/390 [05:03<00:44,  3.01it/s] 66%|██████▌   | 258/390 [05:03<00:42,  3.12it/s] 66%|██████▋   | 259/390 [05:04<00:40,  3.20it/s] 67%|██████▋   | 260/390 [05:04<00:39,  3.26it/s] 67%|██████▋   | 261/390 [05:04<00:39,  3.29it/s] 67%|██████▋   | 262/390 [05:05<00:38,  3.32it/s] 67%|██████▋   | 263/390 [05:05<00:37,  3.35it/s] 68%|██████▊   | 264/390 [05:05<00:37,  3.37it/s] 68%|██████▊   | 265/390 [05:06<00:36,  3.38it/s] 68%|██████▊   | 266/390 [05:06<00:36,  3.39it/s] 68%|██████▊   | 267/390 [05:06<00:47,  2.58it/s] 69%|██████▊   | 268/390 [05:07<00:43,  2.79it/s] 69%|██████▉   | 269/390 [05:07<00:41,  2.94it/s] 69%|██████▉   | 270/390 [05:07<00:39,  3.07it/s] 69%|██████▉   | 271/390 [05:08<00:37,  3.16it/s] 70%|██████▉   | 272/390 [05:08<00:36,  3.23it/s] 70%|███████   | 273/390 [05:08<00:35,  3.28it/s] 70%|███████   | 274/390 [05:08<00:34,  3.32it/s] 71%|███████   | 275/390 [05:09<00:34,  3.34it/s] 71%|███████   | 276/390 [05:09<00:33,  3.36it/s] 71%|███████   | 277/390 [05:09<00:35,  3.18it/s] 71%|███████▏  | 278/390 [05:10<00:34,  3.24it/s] 72%|███████▏  | 279/390 [05:10<00:33,  3.29it/s] 72%|███████▏  | 280/390 [05:10<00:33,  3.32it/s] 72%|███████▏  | 281/390 [05:11<00:32,  3.35it/s] 72%|███████▏  | 282/390 [05:11<00:32,  3.36it/s] 73%|███████▎  | 283/390 [05:11<00:32,  3.27it/s] 73%|███████▎  | 284/390 [05:12<00:32,  3.31it/s] 73%|███████▎  | 285/390 [05:12<00:31,  3.33it/s] 73%|███████▎  | 286/390 [05:12<00:30,  3.36it/s] 74%|███████▎  | 287/390 [05:12<00:30,  3.37it/s] 74%|███████▍  | 288/390 [05:13<00:30,  3.38it/s] 74%|███████▍  | 289/390 [05:13<00:29,  3.39it/s] 74%|███████▍  | 290/390 [05:13<00:29,  3.39it/s] 75%|███████▍  | 291/390 [05:14<00:29,  3.39it/s] 75%|███████▍  | 292/390 [05:14<00:28,  3.40it/s] 75%|███████▌  | 293/390 [05:14<00:30,  3.23it/s] 75%|███████▌  | 294/390 [05:15<00:29,  3.28it/s] 76%|███████▌  | 295/390 [05:15<00:28,  3.32it/s] 76%|███████▌  | 296/390 [05:15<00:28,  3.34it/s] 76%|███████▌  | 297/390 [05:15<00:27,  3.35it/s] 76%|███████▋  | 298/390 [05:16<00:27,  3.37it/s] 77%|███████▋  | 299/390 [05:16<00:26,  3.38it/s] 77%|███████▋  | 300/390 [05:16<00:26,  3.39it/s] 77%|███████▋  | 301/390 [05:17<00:26,  3.39it/s] 77%|███████▋  | 302/390 [05:17<00:25,  3.39it/s] 78%|███████▊  | 303/390 [05:17<00:25,  3.39it/s] 78%|███████▊  | 304/390 [05:18<00:35,  2.45it/s] 78%|███████▊  | 305/390 [05:18<00:31,  2.68it/s] 78%|███████▊  | 306/390 [05:18<00:29,  2.87it/s] 79%|███████▊  | 307/390 [05:19<00:27,  3.03it/s] 79%|███████▉  | 308/390 [05:19<00:26,  3.14it/s] 79%|███████▉  | 309/390 [05:19<00:25,  3.23it/s] 79%|███████▉  | 310/390 [05:20<00:24,  3.29it/s] 80%|███████▉  | 311/390 [05:20<00:23,  3.34it/s] 80%|████████  | 312/390 [05:20<00:23,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 18:02:05,514 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:02:05,858 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 18:02:05,858 >>   Batch size = 8
{'eval_loss': 1.0571030378341675, 'eval_runtime': 22.9331, 'eval_samples_per_second': 300.177, 'eval_steps_per_second': 37.544, 'epoch': 2.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:14, 57.38it/s][A
  1%|▏         | 12/861 [00:00<00:17, 49.15it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.28it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.41it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.95it/s][A
  4%|▎         | 32/861 [00:00<00:18, 45.62it/s][A
  4%|▍         | 37/861 [00:00<00:18, 45.41it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.79it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.24it/s][A
  6%|▌         | 52/861 [00:01<00:18, 43.93it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.08it/s][A
  7%|▋         | 62/861 [00:01<00:18, 44.35it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.54it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.66it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.69it/s][A
 10%|▉         | 82/861 [00:01<00:17, 44.70it/s][A
 10%|█         | 87/861 [00:01<00:17, 44.38it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.03it/s][A
 11%|█▏        | 97/861 [00:02<00:17, 43.82it/s][A
 12%|█▏        | 102/861 [00:02<00:17, 44.00it/s][A
 12%|█▏        | 107/861 [00:02<00:17, 44.24it/s][A
 13%|█▎        | 112/861 [00:02<00:16, 44.47it/s][A
 14%|█▎        | 117/861 [00:02<00:16, 44.61it/s][A
 14%|█▍        | 122/861 [00:03<00:16, 44.70it/s][A
 15%|█▍        | 127/861 [00:03<00:33, 21.88it/s][A
 15%|█▌        | 132/861 [00:03<00:28, 25.83it/s][A
 16%|█▌        | 137/861 [00:03<00:24, 29.56it/s][A
 16%|█▋        | 142/861 [00:03<00:21, 32.96it/s][A
 17%|█▋        | 147/861 [00:03<00:19, 35.78it/s][A
 18%|█▊        | 152/861 [00:03<00:18, 38.14it/s][A
 18%|█▊        | 157/861 [00:03<00:17, 39.96it/s][A
 19%|█▉        | 162/861 [00:03<00:16, 41.18it/s][A
 19%|█▉        | 167/861 [00:04<00:16, 41.58it/s][A
 20%|█▉        | 172/861 [00:04<00:16, 42.13it/s][A
 21%|██        | 177/861 [00:04<00:16, 42.64it/s][A
 21%|██        | 182/861 [00:04<00:15, 43.27it/s][A
 22%|██▏       | 187/861 [00:04<00:15, 43.77it/s][A
 22%|██▏       | 192/861 [00:04<00:15, 44.09it/s][A
 23%|██▎       | 197/861 [00:04<00:14, 44.34it/s][A
 23%|██▎       | 202/861 [00:04<00:14, 44.51it/s][A
 24%|██▍       | 207/861 [00:05<00:14, 44.34it/s][A
 25%|██▍       | 212/861 [00:05<00:14, 43.86it/s][A
 25%|██▌       | 217/861 [00:05<00:14, 43.80it/s][A
 26%|██▌       | 222/861 [00:05<00:14, 43.87it/s][A
 26%|██▋       | 227/861 [00:05<00:14, 44.14it/s][A
 27%|██▋       | 232/861 [00:05<00:14, 44.19it/s][A
 28%|██▊       | 237/861 [00:05<00:14, 44.47it/s][A
 28%|██▊       | 242/861 [00:05<00:13, 44.64it/s][A
 29%|██▊       | 247/861 [00:06<00:18, 33.61it/s][A
 29%|██▉       | 252/861 [00:06<00:16, 36.30it/s][A
 30%|██▉       | 257/861 [00:06<00:15, 38.50it/s][A
 30%|███       | 262/861 [00:06<00:14, 40.24it/s][A
 31%|███       | 267/861 [00:06<00:14, 41.48it/s][A
 32%|███▏      | 272/861 [00:06<00:13, 42.46it/s][A
 32%|███▏      | 277/861 [00:06<00:13, 43.18it/s][A
 33%|███▎      | 282/861 [00:06<00:13, 43.52it/s][A
 33%|███▎      | 287/861 [00:06<00:13, 43.30it/s][A
 34%|███▍      | 292/861 [00:07<00:13, 43.29it/s][A
 34%|███▍      | 297/861 [00:07<00:13, 43.36it/s][A
 35%|███▌      | 302/861 [00:07<00:12, 43.74it/s][A
 36%|███▌      | 307/861 [00:07<00:12, 44.01it/s][A
 36%|███▌      | 312/861 [00:07<00:12, 44.29it/s][A
 37%|███▋      | 317/861 [00:07<00:12, 44.48it/s][A
 37%|███▋      | 322/861 [00:07<00:12, 44.63it/s][A
 38%|███▊      | 327/861 [00:07<00:12, 44.41it/s][A
 39%|███▊      | 332/861 [00:07<00:11, 44.18it/s][A
 39%|███▉      | 337/861 [00:08<00:11, 43.81it/s][A
 40%|███▉      | 342/861 [00:08<00:11, 43.73it/s][A
 40%|████      | 347/861 [00:08<00:11, 43.96it/s][A
 41%|████      | 352/861 [00:08<00:11, 44.26it/s][A
 41%|████▏     | 357/861 [00:08<00:11, 44.40it/s][A
 42%|████▏     | 362/861 [00:08<00:11, 44.63it/s][A
 43%|████▎     | 367/861 [00:08<00:11, 44.61it/s][A
 43%|████▎     | 372/861 [00:09<00:11, 44.42it/s][A
 44%|████▍     | 377/861 [00:09<00:17, 26.92it/s][A
 44%|████▍     | 382/861 [00:09<00:15, 30.58it/s][A
 45%|████▍     | 387/861 [00:09<00:14, 33.80it/s][A
 46%|████▌     | 392/861 [00:09<00:12, 36.48it/s][A
 46%|████▌     | 397/861 [00:09<00:12, 38.62it/s][A
 47%|████▋     | 402/861 [00:09<00:11, 40.33it/s][A
 47%|████▋     | 407/861 [00:09<00:10, 41.61it/s][A
 48%|████▊     | 412/861 [00:09<00:10, 42.38it/s][A
 48%|████▊     | 417/861 [00:10<00:10, 42.54it/s][A
 49%|████▉     | 422/861 [00:10<00:10, 42.66it/s][A
 50%|████▉     | 427/861 [00:10<00:10, 43.02it/s][A
 50%|█████     | 432/861 [00:10<00:09, 43.58it/s][A
 51%|█████     | 437/861 [00:10<00:09, 43.93it/s][A
 51%|█████▏    | 442/861 [00:10<00:09, 44.25it/s][A
 52%|█████▏    | 447/861 [00:10<00:09, 44.45it/s][A
 52%|█████▏    | 452/861 [00:10<00:09, 44.53it/s][A
 53%|█████▎    | 457/861 [00:11<00:09, 44.32it/s][A
 54%|█████▎    | 462/861 [00:11<00:09, 44.05it/s][A
 54%|█████▍    | 467/861 [00:11<00:09, 43.71it/s][A
 55%|█████▍    | 472/861 [00:11<00:08, 43.80it/s][A
 55%|█████▌    | 477/861 [00:11<00:08, 43.97it/s][A
 56%|█████▌    | 482/861 [00:11<00:08, 44.22it/s][A
 57%|█████▋    | 487/861 [00:11<00:08, 44.42it/s][A
 57%|█████▋    | 492/861 [00:11<00:08, 44.58it/s][A
 58%|█████▊    | 497/861 [00:12<00:08, 44.38it/s][A
 58%|█████▊    | 502/861 [00:12<00:25, 14.12it/s][A
 59%|█████▉    | 507/861 [00:12<00:19, 17.78it/s][A
 59%|█████▉    | 512/861 [00:13<00:16, 21.70it/s][A
 60%|██████    | 517/861 [00:13<00:13, 25.70it/s][A
 61%|██████    | 522/861 [00:13<00:11, 29.43it/s][A
 61%|██████    | 527/861 [00:13<00:10, 32.83it/s][A
 62%|██████▏   | 532/861 [00:13<00:09, 35.73it/s][A
 62%|██████▏   | 537/861 [00:13<00:08, 37.91it/s][A
 63%|██████▎   | 542/861 [00:13<00:08, 39.24it/s][A
 64%|██████▎   | 547/861 [00:13<00:07, 40.32it/s][A
 64%|██████▍   | 552/861 [00:13<00:07, 41.42it/s][A
 65%|██████▍   | 557/861 [00:14<00:07, 42.23it/s][A
 65%|██████▌   | 562/861 [00:14<00:06, 42.98it/s][A
 66%|██████▌   | 567/861 [00:14<00:06, 43.41it/s][A
 66%|██████▋   | 572/861 [00:14<00:06, 43.89it/s][A
 67%|██████▋   | 577/861 [00:14<00:06, 44.19it/s][A
 68%|██████▊   | 582/861 [00:14<00:06, 44.20it/s][A
 68%|██████▊   | 587/861 [00:14<00:06, 43.87it/s][A
 69%|██████▉   | 592/861 [00:14<00:06, 43.80it/s][A
 69%|██████▉   | 597/861 [00:15<00:06, 43.70it/s][A
 70%|██████▉   | 602/861 [00:15<00:07, 36.71it/s][A
 70%|███████   | 607/861 [00:15<00:06, 38.83it/s][A
 71%|███████   | 612/861 [00:15<00:06, 40.40it/s][A
 72%|███████▏  | 617/861 [00:15<00:05, 41.64it/s][A
 72%|███████▏  | 622/861 [00:15<00:05, 42.59it/s][A
 73%|███████▎  | 627/861 [00:15<00:05, 43.22it/s][A
 73%|███████▎  | 632/861 [00:15<00:05, 43.75it/s][A
 74%|███████▍  | 637/861 [00:15<00:05, 43.78it/s][A
 75%|███████▍  | 642/861 [00:16<00:05, 43.67it/s][A
 75%|███████▌  | 647/861 [00:16<00:04, 43.43it/s][A
 76%|███████▌  | 652/861 [00:17<00:18, 11.11it/s][A
 76%|███████▋  | 657/861 [00:17<00:14, 14.37it/s][A
 77%|███████▋  | 662/861 [00:17<00:11, 18.02it/s][A
 77%|███████▋  | 667/861 [00:17<00:08, 21.98it/s][A
 78%|███████▊  | 672/861 [00:17<00:07, 25.92it/s][A
 79%|███████▊  | 677/861 [00:17<00:06, 29.73it/s][A
 79%|███████▉  | 682/861 [00:18<00:05, 33.11it/s][A
 80%|███████▉  | 687/861 [00:18<00:05, 30.90it/s][A
 80%|████████  | 692/861 [00:18<00:04, 34.04it/s][A
 81%|████████  | 697/861 [00:18<00:04, 36.72it/s][A
 82%|████████▏ | 702/861 [00:18<00:04, 38.83it/s][A
 82%|████████▏ | 707/861 [00:18<00:03, 40.50it/s][A
 83%|████████▎ | 712/861 [00:18<00:03, 41.56it/s][A
 83%|████████▎ | 717/861 [00:18<00:03, 42.57it/s][A
 84%|████████▍ | 722/861 [00:19<00:03, 43.03it/s][A
 84%|████████▍ | 727/861 [00:19<00:03, 43.03it/s][A
 85%|████████▌ | 732/861 [00:19<00:03, 42.98it/s][A
 86%|████████▌ | 737/861 [00:19<00:02, 43.35it/s][A
 86%|████████▌ | 742/861 [00:19<00:02, 43.65it/s][A
 87%|████████▋ | 747/861 [00:19<00:02, 44.07it/s][A
 87%|████████▋ | 752/861 [00:19<00:02, 44.30it/s][A
 88%|████████▊ | 757/861 [00:19<00:02, 44.47it/s][A
 89%|████████▊ | 762/861 [00:19<00:02, 44.51it/s][A
 89%|████████▉ | 767/861 [00:20<00:02, 44.27it/s][A
 90%|████████▉ | 772/861 [00:20<00:02, 43.99it/s][A
 90%|█████████ | 777/861 [00:20<00:01, 43.68it/s][A
 91%|█████████ | 782/861 [00:20<00:01, 43.81it/s][A
 91%|█████████▏| 787/861 [00:20<00:01, 44.00it/s][A
 92%|█████████▏| 792/861 [00:20<00:01, 44.18it/s][A
 93%|█████████▎| 797/861 [00:20<00:01, 44.40it/s][A
 93%|█████████▎| 802/861 [00:20<00:01, 44.58it/s][A
 94%|█████████▎| 807/861 [00:21<00:01, 44.53it/s][A
 94%|█████████▍| 812/861 [00:21<00:01, 29.28it/s][A
 95%|█████████▍| 817/861 [00:21<00:01, 32.66it/s][A
 95%|█████████▌| 822/861 [00:21<00:01, 35.51it/s][A
 96%|█████████▌| 827/861 [00:21<00:00, 37.81it/s][A
 97%|█████████▋| 832/861 [00:21<00:00, 39.71it/s][A
 97%|█████████▋| 837/861 [00:21<00:00, 41.16it/s][A
 98%|█████████▊| 842/861 [00:21<00:00, 42.22it/s][A
 98%|█████████▊| 847/861 [00:22<00:00, 42.81it/s][A
 99%|█████████▉| 852/861 [00:22<00:00, 42.86it/s][A
100%|█████████▉| 857/861 [00:22<00:00, 42.84it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:22<00:00, 42.84it/s][A 80%|████████  | 312/390 [05:43<00:23,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:02:29,942 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 18:02:30,471 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:02:42,800 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:02:43,174 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:02:43,315 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [06:18<22:30, 17.54s/it] 81%|████████  | 314/390 [06:18<15:40, 12.38s/it] 81%|████████  | 315/390 [06:19<10:56,  8.75s/it] 81%|████████  | 316/390 [06:19<07:39,  6.22s/it] 81%|████████▏ | 317/390 [06:19<05:24,  4.44s/it] 82%|████████▏ | 318/390 [06:19<03:50,  3.20s/it] 82%|████████▏ | 319/390 [06:20<02:45,  2.32s/it] 82%|████████▏ | 320/390 [06:20<02:00,  1.72s/it] 82%|████████▏ | 321/390 [06:20<01:28,  1.29s/it] 83%|████████▎ | 322/390 [06:21<01:07,  1.01it/s] 83%|████████▎ | 323/390 [06:21<00:52,  1.28it/s] 83%|████████▎ | 324/390 [06:21<00:41,  1.57it/s] 83%|████████▎ | 325/390 [06:22<00:36,  1.78it/s] 84%|████████▎ | 326/390 [06:22<00:30,  2.08it/s] 84%|████████▍ | 327/390 [06:22<00:26,  2.35it/s] 84%|████████▍ | 328/390 [06:22<00:23,  2.60it/s] 84%|████████▍ | 329/390 [06:23<00:21,  2.79it/s] 85%|████████▍ | 330/390 [06:23<00:20,  2.95it/s] 85%|████████▍ | 331/390 [06:23<00:19,  3.08it/s] 85%|████████▌ | 332/390 [06:24<00:18,  3.17it/s] 85%|████████▌ | 333/390 [06:24<00:17,  3.24it/s] 86%|████████▌ | 334/390 [06:24<00:17,  3.29it/s] 86%|████████▌ | 335/390 [06:25<00:20,  2.64it/s] 86%|████████▌ | 336/390 [06:25<00:19,  2.83it/s] 86%|████████▋ | 337/390 [06:25<00:17,  2.98it/s] 87%|████████▋ | 338/390 [06:26<00:16,  3.10it/s] 87%|████████▋ | 339/390 [06:26<00:16,  3.19it/s] 87%|████████▋ | 340/390 [06:26<00:15,  3.25it/s] 87%|████████▋ | 341/390 [06:27<00:14,  3.30it/s] 88%|████████▊ | 342/390 [06:27<00:14,  3.33it/s] 88%|████████▊ | 343/390 [06:27<00:14,  3.35it/s] 88%|████████▊ | 344/390 [06:27<00:13,  3.37it/s] 88%|████████▊ | 345/390 [06:28<00:20,  2.17it/s] 89%|████████▊ | 346/390 [06:29<00:18,  2.44it/s] 89%|████████▉ | 347/390 [06:29<00:16,  2.66it/s] 89%|████████▉ | 348/390 [06:29<00:14,  2.85it/s] 89%|████████▉ | 349/390 [06:29<00:13,  2.99it/s] 90%|████████▉ | 350/390 [06:30<00:12,  3.10it/s] 90%|█████████ | 351/390 [06:30<00:12,  3.19it/s] 90%|█████████ | 352/390 [06:30<00:11,  3.25it/s] 91%|█████████ | 353/390 [06:31<00:11,  3.29it/s] 91%|█████████ | 354/390 [06:31<00:13,  2.64it/s] 91%|█████████ | 355/390 [06:31<00:12,  2.83it/s] 91%|█████████▏| 356/390 [06:32<00:11,  2.98it/s] 92%|█████████▏| 357/390 [06:32<00:10,  3.09it/s] 92%|█████████▏| 358/390 [06:32<00:10,  3.18it/s] 92%|█████████▏| 359/390 [06:33<00:09,  3.24it/s] 92%|█████████▏| 360/390 [06:33<00:09,  3.29it/s] 93%|█████████▎| 361/390 [06:33<00:08,  3.32it/s] 93%|█████████▎| 362/390 [06:34<00:08,  3.35it/s] 93%|█████████▎| 363/390 [06:34<00:08,  3.36it/s] 93%|█████████▎| 364/390 [06:35<00:12,  2.06it/s] 94%|█████████▎| 365/390 [06:35<00:10,  2.34it/s] 94%|█████████▍| 366/390 [06:35<00:09,  2.58it/s] 94%|█████████▍| 367/390 [06:36<00:08,  2.78it/s] 94%|█████████▍| 368/390 [06:36<00:07,  2.94it/s] 95%|█████████▍| 369/390 [06:36<00:06,  3.07it/s] 95%|█████████▍| 370/390 [06:37<00:06,  3.16it/s] 95%|█████████▌| 371/390 [06:37<00:09,  2.04it/s] 95%|█████████▌| 372/390 [06:38<00:08,  2.13it/s] 96%|█████████▌| 373/390 [06:38<00:07,  2.40it/s] 96%|█████████▌| 374/390 [06:38<00:06,  2.63it/s] 96%|█████████▌| 375/390 [06:39<00:05,  2.82it/s] 96%|█████████▋| 376/390 [06:39<00:04,  2.97it/s] 97%|█████████▋| 377/390 [06:39<00:04,  3.10it/s] 97%|█████████▋| 378/390 [06:40<00:03,  3.20it/s] 97%|█████████▋| 379/390 [06:40<00:03,  3.27it/s] 97%|█████████▋| 380/390 [06:40<00:03,  3.32it/s] 98%|█████████▊| 381/390 [06:40<00:02,  3.36it/s] 98%|█████████▊| 382/390 [06:41<00:02,  2.83it/s] 98%|█████████▊| 383/390 [06:41<00:02,  2.99it/s] 98%|█████████▊| 384/390 [06:42<00:01,  3.11it/s] 99%|█████████▊| 385/390 [06:42<00:01,  3.21it/s] 99%|█████████▉| 386/390 [06:42<00:01,  3.28it/s] 99%|█████████▉| 387/390 [06:42<00:00,  3.33it/s] 99%|█████████▉| 388/390 [06:43<00:00,  3.36it/s]100%|█████████▉| 389/390 [06:43<00:00,  3.39it/s]100%|██████████| 390/390 [06:44<00:00,  1.97it/s][INFO|trainer.py:2140] 2023-08-28 18:03:29,290 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:03:29,290 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 18:03:29,290 >>   Batch size = 8
{'eval_loss': 1.0679160356521606, 'eval_runtime': 22.7571, 'eval_samples_per_second': 302.498, 'eval_steps_per_second': 37.834, 'epoch': 3.99}

  0%|          | 0/861 [00:00<?, ?it/s][A
  1%|          | 6/861 [00:00<00:15, 56.00it/s][A
  1%|▏         | 12/861 [00:00<00:17, 48.80it/s][A
  2%|▏         | 17/861 [00:00<00:17, 47.08it/s][A
  3%|▎         | 22/861 [00:00<00:18, 46.24it/s][A
  3%|▎         | 27/861 [00:00<00:18, 45.60it/s][A
  4%|▎         | 32/861 [00:00<00:18, 44.80it/s][A
  4%|▍         | 37/861 [00:00<00:18, 44.08it/s][A
  5%|▍         | 42/861 [00:00<00:18, 44.06it/s][A
  5%|▌         | 47/861 [00:01<00:18, 44.15it/s][A
  6%|▌         | 52/861 [00:01<00:18, 44.36it/s][A
  7%|▋         | 57/861 [00:01<00:18, 44.56it/s][A
  7%|▋         | 62/861 [00:01<00:17, 44.68it/s][A
  8%|▊         | 67/861 [00:01<00:17, 44.76it/s][A
  8%|▊         | 72/861 [00:01<00:17, 44.62it/s][A
  9%|▉         | 77/861 [00:01<00:17, 44.30it/s][A
 10%|▉         | 82/861 [00:01<00:17, 43.92it/s][A
 10%|█         | 87/861 [00:01<00:17, 43.97it/s][A
 11%|█         | 92/861 [00:02<00:17, 44.08it/s][A
 11%|█▏        | 97/861 [00:02<00:33, 22.92it/s][A
 12%|█▏        | 102/861 [00:02<00:28, 26.84it/s][A
 12%|█▏        | 107/861 [00:02<00:24, 30.48it/s][A
 13%|█▎        | 111/861 [00:03<00:37, 19.93it/s][A
 13%|█▎        | 116/861 [00:03<00:30, 24.17it/s][A
 14%|█▍        | 121/861 [00:03<00:26, 28.17it/s][A
 15%|█▍        | 126/861 [00:03<00:23, 31.76it/s][A
 15%|█▌        | 131/861 [00:03<00:20, 34.89it/s][A
 16%|█▌        | 136/861 [00:03<00:19, 37.41it/s][A
 16%|█▋        | 141/861 [00:03<00:18, 39.30it/s][A
 17%|█▋        | 146/861 [00:03<00:17, 40.84it/s][A
 18%|█▊        | 151/861 [00:04<00:17, 41.59it/s][A
 18%|█▊        | 156/861 [00:04<00:16, 42.01it/s][A
 19%|█▊        | 161/861 [00:04<00:16, 42.66it/s][A
 19%|█▉        | 166/861 [00:04<00:16, 43.17it/s][A
 20%|█▉        | 171/861 [00:04<00:15, 43.66it/s][A
 20%|██        | 176/861 [00:04<00:15, 43.95it/s][A
 21%|██        | 181/861 [00:04<00:15, 44.07it/s][A
 22%|██▏       | 186/861 [00:04<00:15, 44.28it/s][A
 22%|██▏       | 191/861 [00:04<00:15, 44.26it/s][A
 23%|██▎       | 196/861 [00:05<00:15, 44.10it/s][A
 23%|██▎       | 201/861 [00:05<00:16, 39.12it/s][A
 24%|██▍       | 206/861 [00:05<00:16, 40.67it/s][A
 25%|██▍       | 211/861 [00:05<00:15, 41.88it/s][A
 25%|██▌       | 216/861 [00:05<00:15, 42.67it/s][A
 26%|██▌       | 221/861 [00:05<00:14, 43.32it/s][A
 26%|██▌       | 226/861 [00:05<00:14, 43.70it/s][A
 27%|██▋       | 231/861 [00:05<00:14, 44.05it/s][A
 27%|██▋       | 236/861 [00:05<00:14, 44.19it/s][A
 28%|██▊       | 241/861 [00:06<00:14, 43.80it/s][A
 29%|██▊       | 246/861 [00:06<00:14, 43.59it/s][A
 29%|██▉       | 251/861 [00:06<00:13, 43.71it/s][A
 30%|██▉       | 256/861 [00:06<00:13, 43.94it/s][A
 30%|███       | 261/861 [00:06<00:13, 44.17it/s][A
 31%|███       | 266/861 [00:06<00:13, 44.39it/s][A
 31%|███▏      | 271/861 [00:06<00:13, 44.49it/s][A
 32%|███▏      | 276/861 [00:06<00:13, 44.62it/s][A
 33%|███▎      | 281/861 [00:07<00:13, 44.33it/s][A
 33%|███▎      | 286/861 [00:07<00:13, 44.02it/s][A
 34%|███▍      | 291/861 [00:07<00:12, 43.90it/s][A
 34%|███▍      | 296/861 [00:07<00:12, 43.96it/s][A
 35%|███▍      | 301/861 [00:07<00:12, 44.16it/s][A
 36%|███▌      | 306/861 [00:07<00:12, 44.27it/s][A
 36%|███▌      | 311/861 [00:07<00:12, 44.51it/s][A
 37%|███▋      | 316/861 [00:07<00:12, 44.66it/s][A
 37%|███▋      | 321/861 [00:07<00:12, 44.67it/s][A
 38%|███▊      | 326/861 [00:08<00:16, 31.98it/s][A
 38%|███▊      | 330/861 [00:08<00:26, 20.07it/s][A
 39%|███▉      | 335/861 [00:08<00:21, 24.31it/s][A
 39%|███▉      | 340/861 [00:08<00:18, 28.30it/s][A
 40%|████      | 345/861 [00:08<00:16, 31.85it/s][A
 41%|████      | 350/861 [00:09<00:14, 34.98it/s][A
 41%|████      | 355/861 [00:09<00:13, 37.49it/s][A
 42%|████▏     | 360/861 [00:09<00:14, 34.45it/s][A
 42%|████▏     | 365/861 [00:09<00:13, 37.07it/s][A
 43%|████▎     | 370/861 [00:09<00:12, 38.91it/s][A
 44%|████▎     | 375/861 [00:09<00:11, 40.61it/s][A
 44%|████▍     | 380/861 [00:09<00:11, 41.80it/s][A
 45%|████▍     | 385/861 [00:09<00:11, 42.55it/s][A
 45%|████▌     | 390/861 [00:09<00:10, 43.34it/s][A
 46%|████▌     | 395/861 [00:10<00:24, 19.20it/s][A
 46%|████▋     | 400/861 [00:10<00:19, 23.45it/s][A
 47%|████▋     | 405/861 [00:10<00:16, 27.37it/s][A
 48%|████▊     | 410/861 [00:10<00:14, 31.02it/s][A
 48%|████▊     | 415/861 [00:11<00:13, 34.13it/s][A
 49%|████▉     | 420/861 [00:11<00:11, 36.80it/s][A
 49%|████▉     | 425/861 [00:11<00:11, 38.90it/s][A
 50%|████▉     | 430/861 [00:11<00:15, 28.07it/s][A
 51%|█████     | 435/861 [00:11<00:13, 31.66it/s][A
 51%|█████     | 440/861 [00:11<00:12, 34.75it/s][A
 52%|█████▏    | 445/861 [00:11<00:11, 37.23it/s][A
 52%|█████▏    | 450/861 [00:11<00:10, 39.21it/s][A
 53%|█████▎    | 455/861 [00:12<00:09, 40.73it/s][A
 53%|█████▎    | 460/861 [00:12<00:09, 41.94it/s][A
 54%|█████▍    | 465/861 [00:12<00:09, 42.63it/s][A
 55%|█████▍    | 470/861 [00:12<00:09, 42.74it/s][A
 55%|█████▌    | 475/861 [00:12<00:09, 42.85it/s][A
 56%|█████▌    | 480/861 [00:12<00:08, 43.26it/s][A
 56%|█████▋    | 485/861 [00:12<00:08, 43.64it/s][A
 57%|█████▋    | 490/861 [00:12<00:08, 43.96it/s][A
 57%|█████▋    | 495/861 [00:13<00:08, 44.17it/s][A
 58%|█████▊    | 500/861 [00:13<00:08, 44.43it/s][A
 59%|█████▊    | 505/861 [00:13<00:08, 44.49it/s][A
 59%|█████▉    | 510/861 [00:13<00:07, 44.28it/s][A
 60%|█████▉    | 515/861 [00:13<00:07, 43.99it/s][A
 60%|██████    | 520/861 [00:13<00:07, 43.85it/s][A
 61%|██████    | 525/861 [00:13<00:07, 43.87it/s][A
 62%|██████▏   | 530/861 [00:13<00:07, 44.04it/s][A
 62%|██████▏   | 535/861 [00:13<00:07, 44.23it/s][A
 63%|██████▎   | 540/861 [00:14<00:07, 44.29it/s][A
 63%|██████▎   | 545/861 [00:14<00:07, 44.43it/s][A
 64%|██████▍   | 550/861 [00:14<00:06, 44.57it/s][A
 64%|██████▍   | 555/861 [00:14<00:06, 44.33it/s][A
 65%|██████▌   | 560/861 [00:14<00:10, 30.01it/s][A
 66%|██████▌   | 565/861 [00:14<00:08, 33.33it/s][A
 66%|██████▌   | 570/861 [00:14<00:08, 36.07it/s][A
 67%|██████▋   | 575/861 [00:14<00:07, 38.22it/s][A
 67%|██████▋   | 580/861 [00:15<00:07, 40.01it/s][A
 68%|██████▊   | 585/861 [00:15<00:06, 41.39it/s][A
 69%|██████▊   | 590/861 [00:15<00:06, 42.40it/s][A
 69%|██████▉   | 595/861 [00:15<00:06, 42.90it/s][A
 70%|██████▉   | 600/861 [00:15<00:06, 42.92it/s][A
 70%|███████   | 605/861 [00:15<00:05, 43.01it/s][A
 71%|███████   | 610/861 [00:15<00:05, 43.25it/s][A
 71%|███████▏  | 615/861 [00:15<00:05, 43.52it/s][A
 72%|███████▏  | 620/861 [00:16<00:05, 43.97it/s][A
 73%|███████▎  | 625/861 [00:16<00:05, 44.26it/s][A
 73%|███████▎  | 630/861 [00:16<00:05, 44.47it/s][A
 74%|███████▍  | 635/861 [00:16<00:05, 44.56it/s][A
 74%|███████▍  | 640/861 [00:16<00:04, 44.39it/s][A
 75%|███████▍  | 645/861 [00:16<00:04, 44.02it/s][A
 75%|███████▌  | 650/861 [00:16<00:04, 43.84it/s][A
 76%|███████▌  | 655/861 [00:16<00:04, 43.80it/s][A
 77%|███████▋  | 660/861 [00:16<00:04, 43.96it/s][A
 77%|███████▋  | 665/861 [00:17<00:04, 44.19it/s][A
 78%|███████▊  | 670/861 [00:17<00:04, 44.37it/s][A
 78%|███████▊  | 675/861 [00:17<00:04, 44.46it/s][A
 79%|███████▉  | 680/861 [00:17<00:04, 44.58it/s][A
 80%|███████▉  | 685/861 [00:17<00:03, 44.42it/s][A
 80%|████████  | 690/861 [00:17<00:05, 31.70it/s][A
 81%|████████  | 695/861 [00:17<00:04, 34.65it/s][A
 81%|████████▏ | 700/861 [00:17<00:04, 37.18it/s][A
 82%|████████▏ | 705/861 [00:18<00:03, 39.13it/s][A
 82%|████████▏ | 710/861 [00:18<00:03, 40.62it/s][A
 83%|████████▎ | 715/861 [00:18<00:03, 41.82it/s][A
 84%|████████▎ | 720/861 [00:18<00:03, 42.71it/s][A
 84%|████████▍ | 725/861 [00:18<00:03, 43.15it/s][A
 85%|████████▍ | 730/861 [00:18<00:03, 43.08it/s][A
 85%|████████▌ | 735/861 [00:18<00:02, 43.12it/s][A
 86%|████████▌ | 740/861 [00:18<00:02, 43.36it/s][A
 87%|████████▋ | 745/861 [00:18<00:02, 43.77it/s][A
 87%|████████▋ | 750/861 [00:19<00:02, 44.00it/s][A
 88%|████████▊ | 755/861 [00:19<00:02, 44.15it/s][A
 88%|████████▊ | 760/861 [00:19<00:02, 44.19it/s][A
 89%|████████▉ | 765/861 [00:19<00:02, 44.41it/s][A
 89%|████████▉ | 770/861 [00:19<00:02, 44.37it/s][A
 90%|█████████ | 775/861 [00:19<00:01, 44.09it/s][A
 91%|█████████ | 780/861 [00:19<00:01, 43.86it/s][A
 91%|█████████ | 785/861 [00:19<00:01, 43.94it/s][A
 92%|█████████▏| 790/861 [00:19<00:01, 44.11it/s][A
 92%|█████████▏| 795/861 [00:20<00:01, 44.23it/s][A
 93%|█████████▎| 800/861 [00:20<00:01, 44.32it/s][A
 93%|█████████▎| 805/861 [00:20<00:01, 44.35it/s][A
 94%|█████████▍| 810/861 [00:20<00:01, 44.47it/s][A
 95%|█████████▍| 815/861 [00:20<00:01, 44.36it/s][A
 95%|█████████▌| 820/861 [00:20<00:01, 37.78it/s][A
 96%|█████████▌| 825/861 [00:20<00:00, 39.64it/s][A
 96%|█████████▋| 830/861 [00:20<00:00, 41.03it/s][A
 97%|█████████▋| 835/861 [00:21<00:00, 42.11it/s][A
 98%|█████████▊| 840/861 [00:21<00:00, 42.81it/s][A
 98%|█████████▊| 845/861 [00:21<00:00, 43.42it/s][A
 99%|█████████▊| 850/861 [00:21<00:00, 43.84it/s][A
 99%|█████████▉| 855/861 [00:21<00:00, 43.94it/s][A
100%|█████████▉| 860/861 [00:21<00:00, 43.70it/s][A
                                                 [A                                                 
100%|██████████| 861/861 [00:21<00:00, 43.70it/s][A100%|██████████| 390/390 [07:06<00:00,  1.97it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:03:51,609 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 18:03:52,115 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:04:01,026 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:04:01,311 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:04:01,473 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:04:44,642 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:04:44,895 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78 (score: 1.022870659828186).
                                                 100%|██████████| 390/390 [08:57<00:00,  1.97it/s]100%|██████████| 390/390 [08:57<00:00,  1.38s/it]
[INFO|trainer.py:1894] 2023-08-28 18:05:42,832 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 18:05:43,098 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:05:50,492 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:05:50,873 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:05:51,749 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:05:54,318 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:54,497 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:54,497 >>   train_loss               =     0.4142
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:54,497 >>   train_runtime            = 0:08:57.35
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:54,497 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:54,497 >>   train_samples_per_second =     46.524
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:05:54,497 >>   train_steps_per_second   =      0.726
{'eval_loss': 1.074559211730957, 'eval_runtime': 21.6767, 'eval_samples_per_second': 317.576, 'eval_steps_per_second': 39.72, 'epoch': 4.99}
{'train_runtime': 537.3585, 'train_samples_per_second': 46.524, 'train_steps_per_second': 0.726, 'train_loss': 0.41420957125150243, 'epoch': 4.99}
08/28/2023 18:05:56 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:05:56,100 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:05:56,100 >>   Num examples = 6884
[INFO|trainer.py:2145] 2023-08-28 18:05:56,100 >>   Batch size = 8
  0%|          | 0/861 [00:00<?, ?it/s]  1%|          | 6/861 [00:00<00:15, 55.96it/s]  1%|▏         | 12/861 [00:00<00:17, 49.34it/s]  2%|▏         | 17/861 [00:00<00:17, 47.53it/s]  3%|▎         | 22/861 [00:00<00:17, 46.62it/s]  3%|▎         | 27/861 [00:00<00:18, 46.06it/s]  4%|▎         | 32/861 [00:00<00:18, 45.78it/s]  4%|▍         | 37/861 [00:00<00:18, 45.66it/s]  5%|▍         | 42/861 [00:00<00:18, 45.23it/s]  5%|▌         | 47/861 [00:01<00:18, 44.71it/s]  6%|▌         | 52/861 [00:01<00:18, 44.49it/s]  7%|▋         | 57/861 [00:01<00:18, 44.52it/s]  7%|▋         | 62/861 [00:01<00:31, 25.71it/s]  8%|▊         | 67/861 [00:01<00:26, 29.58it/s]  8%|▊         | 72/861 [00:01<00:23, 33.03it/s]  9%|▉         | 77/861 [00:01<00:21, 35.91it/s] 10%|▉         | 82/861 [00:02<00:20, 38.23it/s] 10%|█         | 87/861 [00:02<00:19, 40.20it/s] 11%|█         | 92/861 [00:02<00:18, 41.71it/s] 11%|█▏        | 97/861 [00:02<00:17, 42.74it/s] 12%|█▏        | 102/861 [00:02<00:17, 43.08it/s] 12%|█▏        | 107/861 [00:02<00:17, 43.36it/s] 13%|█▎        | 112/861 [00:02<00:17, 43.71it/s] 14%|█▎        | 117/861 [00:02<00:16, 44.23it/s] 14%|█▍        | 122/861 [00:02<00:16, 44.63it/s] 15%|█▍        | 127/861 [00:03<00:16, 44.95it/s] 15%|█▌        | 132/861 [00:03<00:16, 45.05it/s] 16%|█▌        | 137/861 [00:03<00:15, 45.29it/s] 16%|█▋        | 142/861 [00:03<00:15, 45.16it/s] 17%|█▋        | 147/861 [00:03<00:15, 44.89it/s] 18%|█▊        | 152/861 [00:03<00:15, 44.63it/s] 18%|█▊        | 157/861 [00:03<00:15, 44.52it/s] 19%|█▉        | 162/861 [00:03<00:15, 44.74it/s] 19%|█▉        | 167/861 [00:03<00:15, 44.88it/s] 20%|█▉        | 172/861 [00:04<00:15, 45.17it/s] 21%|██        | 177/861 [00:04<00:15, 45.35it/s] 21%|██        | 182/861 [00:04<00:14, 45.38it/s] 22%|██▏       | 187/861 [00:04<00:18, 37.21it/s] 22%|██▏       | 192/861 [00:04<00:16, 39.43it/s] 23%|██▎       | 197/861 [00:04<00:16, 41.02it/s] 23%|██▎       | 202/861 [00:04<00:15, 42.35it/s] 24%|██▍       | 207/861 [00:04<00:15, 43.23it/s] 25%|██▍       | 212/861 [00:05<00:14, 43.96it/s] 25%|██▌       | 217/861 [00:05<00:14, 44.42it/s] 26%|██▌       | 222/861 [00:05<00:14, 44.63it/s] 26%|██▋       | 227/861 [00:05<00:14, 44.29it/s] 27%|██▋       | 232/861 [00:05<00:14, 44.06it/s] 28%|██▊       | 237/861 [00:05<00:14, 44.28it/s] 28%|██▊       | 242/861 [00:05<00:13, 44.55it/s] 29%|██▊       | 247/861 [00:05<00:13, 44.75it/s] 29%|██▉       | 252/861 [00:05<00:13, 45.01it/s] 30%|██▉       | 257/861 [00:06<00:13, 45.17it/s] 30%|███       | 262/861 [00:06<00:13, 45.21it/s] 31%|███       | 267/861 [00:06<00:13, 45.28it/s] 32%|███▏      | 272/861 [00:06<00:13, 44.90it/s] 32%|███▏      | 277/861 [00:06<00:13, 44.57it/s] 33%|███▎      | 282/861 [00:06<00:12, 44.65it/s] 33%|███▎      | 287/861 [00:06<00:12, 44.71it/s] 34%|███▍      | 292/861 [00:06<00:12, 44.96it/s] 34%|███▍      | 297/861 [00:06<00:12, 45.08it/s] 35%|███▌      | 302/861 [00:07<00:12, 45.20it/s] 36%|███▌      | 307/861 [00:07<00:12, 45.31it/s] 36%|███▌      | 312/861 [00:07<00:12, 45.18it/s] 37%|███▋      | 317/861 [00:07<00:12, 45.01it/s] 37%|███▋      | 322/861 [00:07<00:24, 22.17it/s] 38%|███▊      | 327/861 [00:07<00:20, 26.20it/s] 39%|███▊      | 332/861 [00:08<00:17, 29.99it/s] 39%|███▉      | 337/861 [00:08<00:15, 33.42it/s] 40%|███▉      | 342/861 [00:08<00:14, 36.32it/s] 40%|████      | 347/861 [00:08<00:13, 38.64it/s] 41%|████      | 352/861 [00:08<00:12, 40.45it/s] 41%|████▏     | 357/861 [00:08<00:12, 41.73it/s] 42%|████▏     | 362/861 [00:08<00:11, 42.30it/s] 43%|████▎     | 367/861 [00:08<00:11, 42.74it/s] 43%|████▎     | 372/861 [00:08<00:11, 43.33it/s] 44%|████▍     | 377/861 [00:09<00:11, 43.79it/s] 44%|████▍     | 382/861 [00:09<00:10, 44.23it/s] 45%|████▍     | 387/861 [00:09<00:10, 44.57it/s] 46%|████▌     | 392/861 [00:09<00:10, 44.91it/s] 46%|████▌     | 397/861 [00:09<00:10, 45.03it/s] 47%|████▋     | 402/861 [00:09<00:10, 45.06it/s] 47%|████▋     | 407/861 [00:09<00:10, 44.74it/s] 48%|████▊     | 412/861 [00:09<00:10, 44.47it/s] 48%|████▊     | 417/861 [00:09<00:09, 44.52it/s] 49%|████▉     | 422/861 [00:10<00:09, 44.61it/s] 50%|████▉     | 427/861 [00:10<00:09, 44.78it/s] 50%|█████     | 432/861 [00:10<00:09, 44.94it/s] 51%|█████     | 437/861 [00:10<00:09, 45.11it/s] 51%|█████▏    | 442/861 [00:10<00:11, 35.10it/s] 52%|█████▏    | 447/861 [00:10<00:10, 37.68it/s] 52%|█████▏    | 452/861 [00:10<00:10, 39.72it/s] 53%|█████▎    | 457/861 [00:10<00:09, 41.32it/s] 54%|█████▎    | 462/861 [00:11<00:09, 42.49it/s] 54%|█████▍    | 467/861 [00:11<00:09, 43.32it/s] 55%|█████▍    | 472/861 [00:11<00:08, 44.01it/s] 55%|█████▌    | 477/861 [00:11<00:08, 44.30it/s] 56%|█████▌    | 482/861 [00:11<00:08, 44.05it/s] 57%|█████▋    | 487/861 [00:11<00:08, 43.91it/s] 57%|█████▋    | 492/861 [00:11<00:08, 44.08it/s] 58%|█████▊    | 497/861 [00:11<00:08, 44.44it/s] 58%|█████▊    | 502/861 [00:11<00:08, 44.69it/s] 59%|█████▉    | 507/861 [00:12<00:07, 44.94it/s] 59%|█████▉    | 512/861 [00:12<00:07, 45.12it/s] 60%|██████    | 517/861 [00:12<00:07, 45.23it/s] 61%|██████    | 522/861 [00:12<00:07, 45.16it/s] 61%|██████    | 527/861 [00:12<00:07, 44.67it/s] 62%|██████▏   | 532/861 [00:12<00:07, 44.48it/s] 62%|██████▏   | 537/861 [00:12<00:07, 44.52it/s] 63%|██████▎   | 542/861 [00:12<00:07, 44.60it/s] 64%|██████▎   | 547/861 [00:12<00:07, 44.80it/s] 64%|██████▍   | 552/861 [00:13<00:06, 44.78it/s] 65%|██████▍   | 557/861 [00:13<00:06, 44.90it/s] 65%|██████▌   | 562/861 [00:13<00:06, 45.24it/s] 66%|██████▌   | 567/861 [00:13<00:06, 45.04it/s] 66%|██████▋   | 572/861 [00:13<00:06, 44.73it/s] 67%|██████▋   | 577/861 [00:13<00:07, 39.81it/s] 68%|██████▊   | 582/861 [00:13<00:06, 41.39it/s] 68%|██████▊   | 587/861 [00:13<00:06, 42.55it/s] 69%|██████▉   | 592/861 [00:14<00:06, 43.40it/s] 69%|██████▉   | 597/861 [00:14<00:06, 43.99it/s] 70%|██████▉   | 602/861 [00:14<00:05, 44.44it/s] 70%|███████   | 607/861 [00:14<00:05, 44.71it/s] 71%|███████   | 612/861 [00:14<00:05, 44.66it/s] 72%|███████▏  | 617/861 [00:14<00:05, 44.27it/s] 72%|███████▏  | 622/861 [00:14<00:05, 44.13it/s] 73%|███████▎  | 627/861 [00:14<00:05, 44.25it/s] 73%|███████▎  | 632/861 [00:14<00:05, 44.51it/s] 74%|███████▍  | 637/861 [00:15<00:05, 44.68it/s] 75%|███████▍  | 642/861 [00:15<00:04, 44.88it/s] 75%|███████▌  | 647/861 [00:15<00:04, 44.96it/s] 76%|███████▌  | 652/861 [00:15<00:04, 45.07it/s] 76%|███████▋  | 657/861 [00:15<00:04, 44.83it/s] 77%|███████▋  | 662/861 [00:15<00:04, 44.49it/s] 77%|███████▋  | 667/861 [00:15<00:04, 44.46it/s] 78%|███████▊  | 672/861 [00:15<00:04, 44.47it/s] 79%|███████▊  | 677/861 [00:15<00:04, 44.48it/s] 79%|███████▉  | 682/861 [00:16<00:04, 44.45it/s] 80%|███████▉  | 687/861 [00:16<00:03, 44.80it/s] 80%|████████  | 692/861 [00:16<00:03, 44.97it/s] 81%|████████  | 697/861 [00:16<00:03, 44.92it/s] 82%|████████▏ | 702/861 [00:16<00:03, 44.83it/s] 82%|████████▏ | 707/861 [00:16<00:03, 44.67it/s] 83%|████████▎ | 712/861 [00:16<00:05, 26.24it/s] 83%|████████▎ | 717/861 [00:17<00:04, 30.05it/s] 84%|████████▎ | 721/861 [00:17<00:06, 21.30it/s] 84%|████████▍ | 726/861 [00:17<00:05, 25.53it/s] 85%|████████▍ | 731/861 [00:17<00:04, 29.49it/s] 85%|████████▌ | 736/861 [00:17<00:03, 33.02it/s] 86%|████████▌ | 741/861 [00:17<00:03, 35.91it/s] 87%|████████▋ | 746/861 [00:17<00:02, 38.35it/s] 87%|████████▋ | 751/861 [00:18<00:02, 40.19it/s] 88%|████████▊ | 756/861 [00:18<00:02, 41.54it/s] 88%|████████▊ | 761/861 [00:18<00:02, 42.09it/s] 89%|████████▉ | 766/861 [00:18<00:02, 42.47it/s] 90%|████████▉ | 771/861 [00:18<00:02, 42.93it/s] 90%|█████████ | 776/861 [00:18<00:01, 43.53it/s] 91%|█████████ | 781/861 [00:18<00:01, 44.02it/s] 91%|█████████▏| 786/861 [00:18<00:01, 44.40it/s] 92%|█████████▏| 791/861 [00:18<00:01, 44.69it/s] 92%|█████████▏| 796/861 [00:19<00:01, 44.83it/s] 93%|█████████▎| 801/861 [00:19<00:01, 44.77it/s] 94%|█████████▎| 806/861 [00:19<00:01, 44.42it/s] 94%|█████████▍| 811/861 [00:19<00:01, 44.24it/s] 95%|█████████▍| 816/861 [00:19<00:01, 44.16it/s] 95%|█████████▌| 821/861 [00:19<00:00, 44.31it/s] 96%|█████████▌| 826/861 [00:19<00:00, 44.48it/s] 97%|█████████▋| 831/861 [00:19<00:00, 44.75it/s] 97%|█████████▋| 836/861 [00:19<00:00, 44.91it/s] 98%|█████████▊| 841/861 [00:20<00:00, 45.03it/s] 98%|█████████▊| 846/861 [00:20<00:00, 35.83it/s] 99%|█████████▉| 851/861 [00:20<00:00, 38.13it/s] 99%|█████████▉| 856/861 [00:20<00:00, 40.04it/s]100%|██████████| 861/861 [00:20<00:00, 41.50it/s]100%|██████████| 861/861 [00:20<00:00, 41.73it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:06:16,753 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:16,753 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:16,753 >>   eval_loss               =     1.0229
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:16,753 >>   eval_runtime            = 0:00:20.65
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:16,753 >>   eval_samples            =       6884
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:16,753 >>   eval_samples_per_second =    333.325
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:16,753 >>   eval_steps_per_second   =      41.69
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:06:16,753 >>   perplexity              =     2.7812
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:46,274 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:46,424 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:46,424 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:46,424 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:46,424 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:06:48,466 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:06:48,467 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:06:49,648 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:06:51,202 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:06:51,202 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:55,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:55,247 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:55,247 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:55,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:55,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:06:56,314 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:06:56,315 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:06:57,687 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:06:59,077 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:06:59,429 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/dev.jsonl', 'labels': ['composer', 'date of birth', 'opposite of', 'shares border with', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17767
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17867, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:04,  1.37it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.44it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.38it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:10,  1.60it/s]Extractor Predicting: 16it [00:11,  1.41it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:12,  1.51it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:14,  1.39it/s]Extractor Predicting: 22it [00:15,  1.47it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.54it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:18,  1.25it/s]Extractor Predicting: 27it [00:18,  1.32it/s]Extractor Predicting: 28it [00:19,  1.40it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.37it/s]Extractor Predicting: 31it [00:21,  1.28it/s]Extractor Predicting: 32it [00:22,  1.36it/s]Extractor Predicting: 33it [00:22,  1.44it/s]Extractor Predicting: 34it [00:23,  1.48it/s]Extractor Predicting: 35it [00:24,  1.50it/s]Extractor Predicting: 36it [00:25,  1.39it/s]Extractor Predicting: 37it [00:25,  1.42it/s]Extractor Predicting: 38it [00:26,  1.48it/s]Extractor Predicting: 39it [00:26,  1.52it/s]Extractor Predicting: 40it [00:27,  1.54it/s]Extractor Predicting: 41it [00:28,  1.48it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:29,  1.55it/s]Extractor Predicting: 44it [00:30,  1.58it/s]Extractor Predicting: 45it [00:30,  1.61it/s]Extractor Predicting: 46it [00:31,  1.54it/s]Extractor Predicting: 47it [00:32,  1.53it/s]Extractor Predicting: 48it [00:32,  1.56it/s]Extractor Predicting: 49it [00:33,  1.56it/s]Extractor Predicting: 50it [00:33,  1.57it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.50it/s]Extractor Predicting: 53it [00:36,  1.50it/s]Extractor Predicting: 54it [00:36,  1.42it/s]Extractor Predicting: 55it [00:37,  1.45it/s]Extractor Predicting: 56it [00:38,  1.42it/s]Extractor Predicting: 57it [00:38,  1.48it/s]Extractor Predicting: 58it [00:39,  1.51it/s]Extractor Predicting: 59it [00:40,  1.48it/s]Extractor Predicting: 60it [00:40,  1.49it/s]Extractor Predicting: 61it [00:41,  1.51it/s]Extractor Predicting: 62it [00:42,  1.51it/s]Extractor Predicting: 63it [00:42,  1.51it/s]Extractor Predicting: 64it [00:43,  1.51it/s]Extractor Predicting: 65it [00:44,  1.53it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:45,  1.52it/s]Extractor Predicting: 68it [00:46,  1.53it/s]Extractor Predicting: 69it [00:46,  1.53it/s]Extractor Predicting: 70it [00:47,  1.52it/s]Extractor Predicting: 71it [00:48,  1.32it/s]Extractor Predicting: 72it [00:49,  1.38it/s]Extractor Predicting: 73it [00:49,  1.43it/s]Extractor Predicting: 74it [00:50,  1.47it/s]Extractor Predicting: 75it [00:50,  1.52it/s]Extractor Predicting: 76it [00:51,  1.44it/s]Extractor Predicting: 77it [00:52,  1.39it/s]Extractor Predicting: 78it [00:53,  1.44it/s]Extractor Predicting: 79it [00:53,  1.51it/s]Extractor Predicting: 80it [00:54,  1.55it/s]Extractor Predicting: 81it [00:54,  1.60it/s]Extractor Predicting: 82it [00:55,  1.53it/s]Extractor Predicting: 83it [00:56,  1.54it/s]Extractor Predicting: 84it [00:56,  1.58it/s]Extractor Predicting: 85it [00:57,  1.62it/s]Extractor Predicting: 86it [00:58,  1.64it/s]Extractor Predicting: 87it [00:58,  1.53it/s]Extractor Predicting: 88it [00:59,  1.58it/s]Extractor Predicting: 89it [00:59,  1.60it/s]Extractor Predicting: 90it [01:00,  1.65it/s]Extractor Predicting: 91it [01:01,  1.67it/s]Extractor Predicting: 92it [01:01,  1.56it/s]Extractor Predicting: 93it [01:02,  1.58it/s]Extractor Predicting: 94it [01:03,  1.63it/s]Extractor Predicting: 95it [01:03,  1.69it/s]Extractor Predicting: 96it [01:04,  1.66it/s]Extractor Predicting: 97it [01:04,  1.65it/s]Extractor Predicting: 98it [01:05,  1.66it/s]Extractor Predicting: 99it [01:05,  1.68it/s]Extractor Predicting: 100it [01:06,  1.69it/s]Extractor Predicting: 101it [01:07,  1.68it/s]Extractor Predicting: 102it [01:07,  1.67it/s]Extractor Predicting: 103it [01:08,  1.56it/s]Extractor Predicting: 104it [01:09,  1.59it/s]Extractor Predicting: 105it [01:09,  1.62it/s]Extractor Predicting: 106it [01:10,  1.65it/s]Extractor Predicting: 107it [01:10,  1.64it/s]Extractor Predicting: 108it [01:11,  1.65it/s]Extractor Predicting: 109it [01:12,  1.66it/s]Extractor Predicting: 110it [01:12,  1.70it/s]Extractor Predicting: 111it [01:13,  1.72it/s]Extractor Predicting: 112it [01:13,  1.68it/s]Extractor Predicting: 113it [01:14,  1.67it/s]Extractor Predicting: 114it [01:15,  1.62it/s]Extractor Predicting: 115it [01:15,  1.68it/s]Extractor Predicting: 116it [01:16,  1.66it/s]Extractor Predicting: 117it [01:16,  1.65it/s]Extractor Predicting: 118it [01:17,  1.65it/s]Extractor Predicting: 119it [01:18,  1.66it/s]Extractor Predicting: 120it [01:18,  1.70it/s]Extractor Predicting: 121it [01:19,  1.70it/s]Extractor Predicting: 122it [01:19,  1.64it/s]Extractor Predicting: 123it [01:20,  1.69it/s]Extractor Predicting: 124it [01:21,  1.66it/s]Extractor Predicting: 125it [01:21,  1.67it/s]Extractor Predicting: 126it [01:22,  1.69it/s]Extractor Predicting: 127it [01:22,  1.72it/s]Extractor Predicting: 128it [01:23,  1.66it/s]Extractor Predicting: 129it [01:24,  1.70it/s]Extractor Predicting: 130it [01:24,  1.72it/s]Extractor Predicting: 131it [01:25,  1.72it/s]Extractor Predicting: 132it [01:25,  1.75it/s]Extractor Predicting: 133it [01:26,  1.62it/s]Extractor Predicting: 134it [01:27,  1.65it/s]Extractor Predicting: 135it [01:27,  1.67it/s]Extractor Predicting: 136it [01:28,  1.68it/s]Extractor Predicting: 137it [01:28,  1.67it/s]Extractor Predicting: 138it [01:29,  1.64it/s]Extractor Predicting: 139it [01:30,  1.62it/s]Extractor Predicting: 140it [01:30,  1.62it/s]Extractor Predicting: 141it [01:31,  1.66it/s]Extractor Predicting: 142it [01:31,  1.65it/s]Extractor Predicting: 143it [01:32,  1.58it/s]Extractor Predicting: 144it [01:33,  1.57it/s]Extractor Predicting: 145it [01:33,  1.62it/s]Extractor Predicting: 146it [01:34,  1.64it/s]Extractor Predicting: 147it [01:34,  1.65it/s]Extractor Predicting: 148it [01:35,  1.57it/s]Extractor Predicting: 149it [01:36,  1.64it/s]Extractor Predicting: 150it [01:36,  1.60it/s]Extractor Predicting: 151it [01:37,  1.56it/s]Extractor Predicting: 152it [01:38,  1.53it/s]Extractor Predicting: 153it [01:39,  1.43it/s]Extractor Predicting: 154it [01:39,  1.38it/s]Extractor Predicting: 155it [01:40,  1.42it/s]Extractor Predicting: 156it [01:41,  1.28it/s]Extractor Predicting: 157it [01:42,  1.20it/s]Extractor Predicting: 158it [01:43,  1.26it/s]Extractor Predicting: 159it [01:43,  1.30it/s]Extractor Predicting: 160it [01:44,  1.32it/s]Extractor Predicting: 161it [01:45,  1.12it/s]Extractor Predicting: 162it [01:46,  1.17it/s]Extractor Predicting: 163it [01:47,  1.23it/s]Extractor Predicting: 164it [01:47,  1.28it/s]Extractor Predicting: 165it [01:48,  1.22it/s]Extractor Predicting: 166it [01:49,  1.27it/s]Extractor Predicting: 167it [01:50,  1.30it/s]Extractor Predicting: 168it [01:50,  1.34it/s]Extractor Predicting: 169it [01:51,  1.29it/s]Extractor Predicting: 170it [01:52,  1.32it/s]Extractor Predicting: 171it [01:53,  1.35it/s]Extractor Predicting: 172it [01:53,  1.35it/s]Extractor Predicting: 173it [01:54,  1.38it/s]Extractor Predicting: 174it [01:55,  1.42it/s]Extractor Predicting: 175it [01:56,  1.41it/s]Extractor Predicting: 176it [01:56,  1.40it/s]Extractor Predicting: 177it [01:57,  1.32it/s]Extractor Predicting: 178it [01:58,  1.34it/s]Extractor Predicting: 179it [01:59,  1.36it/s]Extractor Predicting: 180it [01:59,  1.37it/s]Extractor Predicting: 181it [02:00,  1.34it/s]Extractor Predicting: 182it [02:01,  1.36it/s]Extractor Predicting: 183it [02:01,  1.39it/s]Extractor Predicting: 184it [02:02,  1.42it/s]Extractor Predicting: 185it [02:03,  1.45it/s]Extractor Predicting: 186it [02:04,  1.38it/s]Extractor Predicting: 187it [02:04,  1.38it/s]Extractor Predicting: 188it [02:05,  1.43it/s]Extractor Predicting: 189it [02:06,  1.39it/s]Extractor Predicting: 190it [02:06,  1.39it/s]Extractor Predicting: 191it [02:07,  1.31it/s]Extractor Predicting: 192it [02:08,  1.30it/s]Extractor Predicting: 193it [02:09,  1.35it/s]Extractor Predicting: 194it [02:09,  1.40it/s]Extractor Predicting: 195it [02:10,  1.41it/s]Extractor Predicting: 196it [02:11,  1.36it/s]Extractor Predicting: 197it [02:12,  1.40it/s]Extractor Predicting: 198it [02:12,  1.43it/s]Extractor Predicting: 199it [02:13,  1.47it/s]Extractor Predicting: 200it [02:14,  1.48it/s]Extractor Predicting: 201it [02:14,  1.39it/s]Extractor Predicting: 202it [02:15,  1.42it/s]Extractor Predicting: 203it [02:16,  1.46it/s]Extractor Predicting: 204it [02:16,  1.47it/s]Extractor Predicting: 205it [02:17,  1.52it/s]Extractor Predicting: 206it [02:18,  1.33it/s]Extractor Predicting: 207it [02:19,  1.39it/s]Extractor Predicting: 208it [02:19,  1.43it/s]Extractor Predicting: 209it [02:20,  1.49it/s]Extractor Predicting: 210it [02:20,  1.50it/s]Extractor Predicting: 211it [02:21,  1.42it/s]Extractor Predicting: 212it [02:22,  1.45it/s]Extractor Predicting: 213it [02:23,  1.48it/s]Extractor Predicting: 214it [02:23,  1.50it/s]Extractor Predicting: 215it [02:24,  1.54it/s]Extractor Predicting: 216it [02:25,  1.33it/s]Extractor Predicting: 217it [02:25,  1.38it/s]Extractor Predicting: 218it [02:26,  1.45it/s]Extractor Predicting: 219it [02:27,  1.44it/s]Extractor Predicting: 220it [02:27,  1.45it/s]Extractor Predicting: 221it [02:28,  1.44it/s]Extractor Predicting: 222it [02:29,  1.46it/s]Extractor Predicting: 223it [02:29,  1.51it/s]Extractor Predicting: 224it [02:30,  1.51it/s]Extractor Predicting: 225it [02:31,  1.53it/s]Extractor Predicting: 226it [02:31,  1.50it/s]Extractor Predicting: 227it [02:32,  1.50it/s]Extractor Predicting: 228it [02:33,  1.51it/s]Extractor Predicting: 229it [02:33,  1.52it/s]Extractor Predicting: 230it [02:34,  1.52it/s]Extractor Predicting: 231it [02:35,  1.48it/s]Extractor Predicting: 232it [02:35,  1.51it/s]Extractor Predicting: 233it [02:36,  1.50it/s]Extractor Predicting: 234it [02:37,  1.51it/s]Extractor Predicting: 235it [02:37,  1.49it/s]Extractor Predicting: 236it [02:38,  1.47it/s]Extractor Predicting: 237it [02:39,  1.44it/s]Extractor Predicting: 237it [02:39,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:15,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:15,297 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:15,298 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:15,298 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:15,298 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:10:16,716 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:10:16,717 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:10:17,517 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:10:18,889 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:10:19,000 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:22,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:23,176 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:23,176 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:23,176 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:23,176 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:10:24,713 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:10:24,714 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:10:26,235 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:10:27,051 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:10:27,051 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.42151162790697677,
  "recall": 0.0421266705403835,
  "score": 0.07659799260433175,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13534
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13634, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.16it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:04,  1.58it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:05,  1.70it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:08,  1.67it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.68it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:15,  1.69it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.66it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:20,  1.50it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:21,  1.57it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.67it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:25,  1.56it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:26,  1.56it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:27,  1.57it/s]Extractor Predicting: 46it [00:28,  1.53it/s]Extractor Predicting: 47it [00:29,  1.54it/s]Extractor Predicting: 48it [00:29,  1.49it/s]Extractor Predicting: 49it [00:30,  1.51it/s]Extractor Predicting: 50it [00:31,  1.53it/s]Extractor Predicting: 51it [00:32,  1.30it/s]Extractor Predicting: 52it [00:32,  1.37it/s]Extractor Predicting: 53it [00:33,  1.43it/s]Extractor Predicting: 54it [00:34,  1.47it/s]Extractor Predicting: 55it [00:34,  1.42it/s]Extractor Predicting: 56it [00:35,  1.49it/s]Extractor Predicting: 57it [00:36,  1.50it/s]Extractor Predicting: 58it [00:36,  1.54it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:38,  1.26it/s]Extractor Predicting: 61it [00:39,  1.33it/s]Extractor Predicting: 62it [00:39,  1.42it/s]Extractor Predicting: 63it [00:40,  1.47it/s]Extractor Predicting: 64it [00:41,  1.39it/s]Extractor Predicting: 65it [00:41,  1.46it/s]Extractor Predicting: 66it [00:42,  1.51it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:43,  1.56it/s]Extractor Predicting: 69it [00:44,  1.50it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:45,  1.56it/s]Extractor Predicting: 72it [00:46,  1.57it/s]Extractor Predicting: 73it [00:46,  1.55it/s]Extractor Predicting: 74it [00:48,  1.18it/s]Extractor Predicting: 75it [00:48,  1.28it/s]Extractor Predicting: 76it [00:49,  1.35it/s]Extractor Predicting: 77it [00:50,  1.41it/s]Extractor Predicting: 78it [00:50,  1.42it/s]Extractor Predicting: 79it [00:51,  1.49it/s]Extractor Predicting: 80it [00:52,  1.55it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:53,  1.53it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:54,  1.56it/s]Extractor Predicting: 85it [00:55,  1.56it/s]Extractor Predicting: 86it [00:55,  1.59it/s]Extractor Predicting: 87it [00:56,  1.62it/s]Extractor Predicting: 88it [00:57,  1.54it/s]Extractor Predicting: 89it [00:57,  1.55it/s]Extractor Predicting: 90it [00:58,  1.57it/s]Extractor Predicting: 91it [00:59,  1.57it/s]Extractor Predicting: 92it [00:59,  1.60it/s]Extractor Predicting: 93it [01:00,  1.56it/s]Extractor Predicting: 94it [01:01,  1.41it/s]Extractor Predicting: 95it [01:01,  1.45it/s]Extractor Predicting: 96it [01:02,  1.52it/s]Extractor Predicting: 97it [01:03,  1.54it/s]Extractor Predicting: 98it [01:03,  1.56it/s]Extractor Predicting: 99it [01:04,  1.60it/s]Extractor Predicting: 100it [01:05,  1.51it/s]Extractor Predicting: 101it [01:05,  1.58it/s]Extractor Predicting: 102it [01:06,  1.53it/s]Extractor Predicting: 103it [01:06,  1.57it/s]Extractor Predicting: 104it [01:07,  1.60it/s]Extractor Predicting: 105it [01:08,  1.59it/s]Extractor Predicting: 106it [01:08,  1.64it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:10,  1.59it/s]Extractor Predicting: 109it [01:10,  1.56it/s]Extractor Predicting: 110it [01:11,  1.57it/s]Extractor Predicting: 111it [01:11,  1.57it/s]Extractor Predicting: 112it [01:12,  1.55it/s]Extractor Predicting: 113it [01:13,  1.60it/s]Extractor Predicting: 114it [01:13,  1.57it/s]Extractor Predicting: 115it [01:14,  1.57it/s]Extractor Predicting: 116it [01:15,  1.61it/s]Extractor Predicting: 117it [01:15,  1.58it/s]Extractor Predicting: 118it [01:16,  1.61it/s]Extractor Predicting: 119it [01:16,  1.60it/s]Extractor Predicting: 120it [01:17,  1.59it/s]Extractor Predicting: 121it [01:18,  1.54it/s]Extractor Predicting: 122it [01:19,  1.50it/s]Extractor Predicting: 123it [01:19,  1.53it/s]Extractor Predicting: 124it [01:20,  1.57it/s]Extractor Predicting: 125it [01:20,  1.61it/s]Extractor Predicting: 126it [01:21,  1.62it/s]Extractor Predicting: 127it [01:22,  1.58it/s]Extractor Predicting: 128it [01:22,  1.63it/s]Extractor Predicting: 129it [01:23,  1.63it/s]Extractor Predicting: 130it [01:23,  1.65it/s]Extractor Predicting: 131it [01:24,  1.64it/s]Extractor Predicting: 132it [01:25,  1.52it/s]Extractor Predicting: 133it [01:25,  1.54it/s]Extractor Predicting: 134it [01:26,  1.58it/s]Extractor Predicting: 135it [01:27,  1.60it/s]Extractor Predicting: 136it [01:27,  1.62it/s]Extractor Predicting: 137it [01:28,  1.50it/s]Extractor Predicting: 138it [01:29,  1.51it/s]Extractor Predicting: 139it [01:29,  1.52it/s]Extractor Predicting: 140it [01:30,  1.59it/s]Extractor Predicting: 141it [01:30,  1.59it/s]Extractor Predicting: 142it [01:31,  1.49it/s]Extractor Predicting: 143it [01:32,  1.50it/s]Extractor Predicting: 144it [01:33,  1.51it/s]Extractor Predicting: 145it [01:34,  1.32it/s]Extractor Predicting: 146it [01:34,  1.38it/s]Extractor Predicting: 147it [01:35,  1.43it/s]Extractor Predicting: 148it [01:36,  1.42it/s]Extractor Predicting: 149it [01:36,  1.38it/s]Extractor Predicting: 150it [01:37,  1.44it/s]Extractor Predicting: 151it [01:38,  1.48it/s]Extractor Predicting: 152it [01:38,  1.50it/s]Extractor Predicting: 153it [01:39,  1.53it/s]Extractor Predicting: 154it [01:40,  1.42it/s]Extractor Predicting: 155it [01:40,  1.46it/s]Extractor Predicting: 156it [01:41,  1.50it/s]Extractor Predicting: 157it [01:42,  1.48it/s]Extractor Predicting: 158it [01:42,  1.48it/s]Extractor Predicting: 159it [01:43,  1.46it/s]Extractor Predicting: 160it [01:44,  1.50it/s]Extractor Predicting: 161it [01:44,  1.54it/s]Extractor Predicting: 162it [01:45,  1.52it/s]Extractor Predicting: 163it [01:46,  1.49it/s]Extractor Predicting: 164it [01:47,  1.26it/s]Extractor Predicting: 165it [01:47,  1.35it/s]Extractor Predicting: 166it [01:48,  1.38it/s]Extractor Predicting: 167it [01:49,  1.43it/s]Extractor Predicting: 168it [01:50,  1.23it/s]Extractor Predicting: 168it [01:50,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:45,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:45,313 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:45,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:45,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:45,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:12:46,778 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:12:46,779 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:12:47,438 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:12:49,102 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:12:49,102 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:53,074 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:53,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:53,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:53,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:12:53,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:12:54,311 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:12:54,312 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:12:55,297 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:12:55,776 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:12:55,776 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2797297297297297,
  "recall": 0.05144135188866799,
  "score": 0.08690176322418136,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2754
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2854, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.46it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.42it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:11,  1.45it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 19it [00:12,  1.53it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5151515151515151,
  "recall": 0.016409266409266408,
  "score": 0.03180542563143124,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/', 'labels': ['creator', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
