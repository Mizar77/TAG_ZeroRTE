Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:20<04:49, 20.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:38<04:07, 19.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [01:01<04:09, 20.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:19<03:37, 19.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:38<03:14, 19.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:57<02:54, 19.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:17<02:35, 19.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:39<02:21, 20.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:56<01:55, 19.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:14<01:35, 19.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:32<01:14, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:51<00:56, 18.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [04:10<00:37, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:28<00:18, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:52<00:00, 20.38s/it]Generating: 100%|██████████| 15/15 [04:52<00:00, 19.53s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 100, 'raw': 160}
{'target': 600, 'success': 124, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 180, 'raw': 288}
{'target': 600, 'success': 200, 'raw': 320}
{'target': 600, 'success': 218, 'raw': 352}
{'target': 600, 'success': 240, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 307, 'raw': 480}
{'target': 600, 'success': 328, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 374, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 418, 'raw': 640}
{'target': 600, 'success': 440, 'raw': 672}
{'target': 600, 'success': 460, 'raw': 704}
{'target': 600, 'success': 487, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 569, 'raw': 864}
{'target': 600, 'success': 595, 'raw': 896}
{'target': 600, 'success': 617, 'raw': 928}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.6648706896551724, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'2012 MTV Video Music Video Awards\', \'nominated for\', \'\', \'He was in a songwriting competition on " The Simpsons " at the 2012 MTV Video Music Video Awards , winning in one of his four categories .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 402, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.7271634615384616, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : architect .', 'success_rate': 0.7877604166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 594, 'raw': 800}
{'target': 600, 'success': 616, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7403846153846154, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a new development project called " Bifurcio " , which was developed by the Swiss developers , EMC , for Bifurcio . Head Entity : Bifurcio , Tail Entity : Ericsson .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.80078125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.77375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Pristiniki\', \'member of political party\', \'\', \'In 2005 , she became the first female politician to serve in the parliament of Bulgaria , as first and leader of " Pristiniki " in the new parliament .\')'}}
['Relation : operator . Context : Later in the year , the company built a high - speed commuter railway crossing the Bordeaux via Bordeaux Station to Marseille , France . Head Entity : Marseille , Tail Entity : Ralf Rummel .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.842391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 76, 'raw': 128}
{'target': 600, 'success': 92, 'raw': 160}
{'target': 600, 'success': 111, 'raw': 192}
{'target': 600, 'success': 130, 'raw': 224}
{'target': 600, 'success': 144, 'raw': 256}
{'target': 600, 'success': 158, 'raw': 288}
{'target': 600, 'success': 176, 'raw': 320}
{'target': 600, 'success': 191, 'raw': 352}
{'target': 600, 'success': 208, 'raw': 384}
{'target': 600, 'success': 226, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 267, 'raw': 480}
{'target': 600, 'success': 284, 'raw': 512}
{'target': 600, 'success': 298, 'raw': 544}
{'target': 600, 'success': 315, 'raw': 576}
{'target': 600, 'success': 332, 'raw': 608}
{'target': 600, 'success': 349, 'raw': 640}
{'target': 600, 'success': 372, 'raw': 672}
{'target': 600, 'success': 388, 'raw': 704}
{'target': 600, 'success': 406, 'raw': 736}
{'target': 600, 'success': 424, 'raw': 768}
{'target': 600, 'success': 439, 'raw': 800}
{'target': 600, 'success': 454, 'raw': 832}
{'target': 600, 'success': 473, 'raw': 864}
{'target': 600, 'success': 490, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 530, 'raw': 960}
{'target': 600, 'success': 551, 'raw': 992}
{'target': 600, 'success': 567, 'raw': 1024}
{'target': 600, 'success': 586, 'raw': 1056}
{'target': 600, 'success': 608, 'raw': 1088}
{'prompt': 'Relation : position held .', 'success_rate': 0.5588235294117647, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 14871
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14971, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.13s/it]Extractor Estimating: 2it [00:18,  7.79s/it]Extractor Estimating: 3it [00:19,  4.82s/it]Extractor Estimating: 4it [00:20,  3.19s/it]Extractor Estimating: 5it [00:20,  2.25s/it]Extractor Estimating: 6it [00:21,  1.72s/it]Extractor Estimating: 7it [00:22,  1.38s/it]Extractor Estimating: 8it [00:22,  1.13s/it]Extractor Estimating: 9it [00:23,  1.00it/s]Extractor Estimating: 10it [00:24,  1.14it/s]Extractor Estimating: 11it [00:26,  1.33s/it]Extractor Estimating: 12it [00:27,  1.12s/it]Extractor Estimating: 13it [00:27,  1.01s/it]Extractor Estimating: 14it [00:28,  1.10it/s]Extractor Estimating: 15it [00:29,  1.18it/s]Extractor Estimating: 16it [00:30,  1.24it/s]Extractor Estimating: 17it [00:30,  1.33it/s]Extractor Estimating: 18it [00:31,  1.34it/s]Extractor Estimating: 19it [00:32,  1.43it/s]Extractor Estimating: 20it [00:32,  1.44it/s]Extractor Estimating: 21it [00:33,  1.48it/s]Extractor Estimating: 22it [00:33,  1.53it/s]Extractor Estimating: 23it [00:34,  1.51it/s]Extractor Estimating: 24it [00:35,  1.47it/s]Extractor Estimating: 25it [00:35,  1.52it/s]Extractor Estimating: 26it [00:36,  1.49it/s]Extractor Estimating: 27it [00:37,  1.49it/s]Extractor Estimating: 28it [00:37,  1.54it/s]Extractor Estimating: 29it [00:38,  1.57it/s]Extractor Estimating: 30it [00:39,  1.57it/s]Extractor Estimating: 31it [00:39,  1.60it/s]Extractor Estimating: 32it [00:40,  1.61it/s]Extractor Estimating: 33it [00:41,  1.56it/s]Extractor Estimating: 34it [00:41,  1.48it/s]Extractor Estimating: 35it [00:42,  1.54it/s]Extractor Estimating: 36it [00:43,  1.56it/s]Extractor Estimating: 37it [00:43,  1.45it/s]Extractor Estimating: 38it [00:44,  1.48it/s]Extractor Estimating: 39it [00:45,  1.49it/s]Extractor Estimating: 40it [00:45,  1.48it/s]Extractor Estimating: 41it [00:46,  1.46it/s]Extractor Estimating: 42it [00:47,  1.45it/s]Extractor Estimating: 43it [00:47,  1.46it/s]Extractor Estimating: 44it [00:48,  1.42it/s]Extractor Estimating: 45it [00:49,  1.43it/s]Extractor Estimating: 46it [00:49,  1.48it/s]Extractor Estimating: 47it [00:50,  1.49it/s]Extractor Estimating: 48it [00:51,  1.46it/s]Extractor Estimating: 49it [00:51,  1.51it/s]Extractor Estimating: 50it [00:52,  1.49it/s]Extractor Estimating: 51it [00:53,  1.47it/s]Extractor Estimating: 52it [00:54,  1.46it/s]Extractor Estimating: 53it [00:54,  1.46it/s]Extractor Estimating: 54it [00:55,  1.43it/s]Extractor Estimating: 55it [00:56,  1.40it/s]Extractor Estimating: 56it [00:56,  1.42it/s]Extractor Estimating: 57it [00:57,  1.40it/s]Extractor Estimating: 58it [00:58,  1.41it/s]Extractor Estimating: 59it [00:59,  1.39it/s]Extractor Estimating: 60it [00:59,  1.42it/s]Extractor Estimating: 61it [01:00,  1.38it/s]Extractor Estimating: 62it [01:01,  1.41it/s]Extractor Estimating: 63it [01:01,  1.39it/s]Extractor Estimating: 64it [01:02,  1.43it/s]Extractor Estimating: 65it [01:03,  1.39it/s]Extractor Estimating: 66it [01:04,  1.40it/s]Extractor Estimating: 67it [01:04,  1.40it/s]Extractor Estimating: 68it [01:05,  1.46it/s]Extractor Estimating: 69it [01:05,  1.48it/s]Extractor Estimating: 70it [01:06,  1.47it/s]Extractor Estimating: 71it [01:07,  1.45it/s]Extractor Estimating: 72it [01:08,  1.45it/s]Extractor Estimating: 73it [01:08,  1.44it/s]Extractor Estimating: 74it [01:09,  1.48it/s]Extractor Estimating: 75it [01:10,  1.46it/s]Extractor Estimating: 76it [01:10,  1.52it/s]Extractor Estimating: 77it [01:11,  1.53it/s]Extractor Estimating: 78it [01:11,  1.60it/s]Extractor Estimating: 79it [01:12,  1.61it/s]Extractor Estimating: 80it [01:13,  1.66it/s]Extractor Estimating: 81it [01:13,  1.66it/s]Extractor Estimating: 82it [01:14,  1.63it/s]Extractor Estimating: 83it [01:14,  1.64it/s]Extractor Estimating: 84it [01:15,  1.64it/s]Extractor Estimating: 85it [01:16,  1.60it/s]Extractor Estimating: 86it [01:16,  1.63it/s]Extractor Estimating: 87it [01:17,  1.62it/s]Extractor Estimating: 88it [01:17,  1.67it/s]Extractor Estimating: 89it [01:18,  1.68it/s]Extractor Estimating: 90it [01:19,  1.70it/s]Extractor Estimating: 91it [01:19,  1.65it/s]Extractor Estimating: 92it [01:20,  1.67it/s]Extractor Estimating: 93it [01:20,  1.66it/s]Extractor Estimating: 94it [01:21,  1.56it/s]Extractor Estimating: 95it [01:22,  1.38it/s]Extractor Estimating: 96it [01:23,  1.43it/s]Extractor Estimating: 97it [01:23,  1.49it/s]Extractor Estimating: 98it [01:24,  1.54it/s]Extractor Estimating: 99it [01:25,  1.57it/s]Extractor Estimating: 100it [01:25,  1.62it/s]Extractor Estimating: 101it [01:26,  1.64it/s]Extractor Estimating: 102it [01:26,  1.64it/s]Extractor Estimating: 103it [01:27,  1.65it/s]Extractor Estimating: 104it [01:28,  1.70it/s]Extractor Estimating: 105it [01:28,  1.68it/s]Extractor Estimating: 106it [01:29,  1.66it/s]Extractor Estimating: 107it [01:29,  1.71it/s]Extractor Estimating: 108it [01:30,  1.77it/s]Extractor Estimating: 109it [01:30,  1.73it/s]Extractor Estimating: 110it [01:31,  1.75it/s]Extractor Estimating: 111it [01:32,  1.74it/s]Extractor Estimating: 112it [01:32,  1.74it/s]Extractor Estimating: 113it [01:33,  1.68it/s]Extractor Estimating: 114it [01:33,  1.70it/s]Extractor Estimating: 115it [01:34,  1.70it/s]Extractor Estimating: 116it [01:35,  1.71it/s]Extractor Estimating: 117it [01:35,  1.66it/s]Extractor Estimating: 118it [01:36,  1.66it/s]Extractor Estimating: 119it [01:36,  1.60it/s]Extractor Estimating: 120it [01:37,  1.68it/s]Extractor Estimating: 121it [01:38,  1.64it/s]Extractor Estimating: 122it [01:38,  1.55it/s]Extractor Estimating: 123it [01:39,  1.61it/s]Extractor Estimating: 124it [01:39,  1.66it/s]Extractor Estimating: 125it [01:40,  1.64it/s]Extractor Estimating: 126it [01:41,  1.62it/s]Extractor Estimating: 127it [01:41,  1.58it/s]Extractor Estimating: 128it [01:42,  1.62it/s]Extractor Estimating: 129it [01:43,  1.64it/s]Extractor Estimating: 130it [01:43,  1.66it/s]Extractor Estimating: 131it [01:44,  1.63it/s]Extractor Estimating: 132it [01:44,  1.63it/s]Extractor Estimating: 133it [01:45,  1.60it/s]Extractor Estimating: 134it [01:46,  1.64it/s]Extractor Estimating: 135it [01:46,  1.58it/s]Extractor Estimating: 136it [01:47,  1.57it/s]Extractor Estimating: 137it [01:48,  1.57it/s]Extractor Estimating: 138it [01:48,  1.59it/s]Extractor Estimating: 139it [01:49,  1.58it/s]Extractor Estimating: 140it [01:49,  1.58it/s]Extractor Estimating: 141it [01:50,  1.56it/s]Extractor Estimating: 142it [01:51,  1.58it/s]Extractor Estimating: 143it [01:51,  1.63it/s]Extractor Estimating: 144it [01:52,  1.62it/s]Extractor Estimating: 145it [01:53,  1.63it/s]Extractor Estimating: 146it [01:53,  1.65it/s]Extractor Estimating: 147it [01:54,  1.61it/s]Extractor Estimating: 148it [01:54,  1.55it/s]Extractor Estimating: 149it [01:55,  1.54it/s]Extractor Estimating: 150it [01:56,  1.55it/s]Extractor Estimating: 151it [01:56,  1.51it/s]Extractor Estimating: 152it [01:57,  1.49it/s]Extractor Estimating: 153it [01:58,  1.51it/s]Extractor Estimating: 154it [01:58,  1.55it/s]Extractor Estimating: 155it [01:59,  1.58it/s]Extractor Estimating: 156it [02:00,  1.61it/s]Extractor Estimating: 157it [02:00,  1.65it/s]Extractor Estimating: 158it [02:01,  1.58it/s]Extractor Estimating: 159it [02:01,  1.65it/s]Extractor Estimating: 160it [02:02,  1.62it/s]Extractor Estimating: 161it [02:03,  1.59it/s]Extractor Estimating: 162it [02:03,  1.58it/s]Extractor Estimating: 163it [02:04,  1.51it/s]Extractor Estimating: 164it [02:05,  1.53it/s]Extractor Estimating: 165it [02:05,  1.48it/s]Extractor Estimating: 166it [02:06,  1.48it/s]Extractor Estimating: 167it [02:07,  1.53it/s]Extractor Estimating: 168it [02:08,  1.37it/s]Extractor Estimating: 169it [02:08,  1.40it/s]Extractor Estimating: 170it [02:09,  1.49it/s]Extractor Estimating: 171it [02:10,  1.46it/s]Extractor Estimating: 172it [02:10,  1.48it/s]Extractor Estimating: 173it [02:11,  1.49it/s]Extractor Estimating: 174it [02:12,  1.44it/s]Extractor Estimating: 175it [02:12,  1.49it/s]Extractor Estimating: 176it [02:13,  1.43it/s]Extractor Estimating: 177it [02:14,  1.50it/s]Extractor Estimating: 178it [02:14,  1.50it/s]Extractor Estimating: 179it [02:15,  1.56it/s]Extractor Estimating: 180it [02:15,  1.61it/s]Extractor Estimating: 181it [02:16,  1.58it/s]Extractor Estimating: 182it [02:17,  1.58it/s]Extractor Estimating: 183it [02:17,  1.52it/s]Extractor Estimating: 184it [02:18,  1.45it/s]Extractor Estimating: 185it [02:19,  1.48it/s]Extractor Estimating: 186it [02:19,  1.53it/s]Extractor Estimating: 187it [02:20,  1.55it/s]Extractor Estimating: 188it [02:21,  1.53it/s]Extractor Estimating: 189it [02:21,  1.59it/s]Extractor Estimating: 190it [02:22,  1.61it/s]Extractor Estimating: 191it [02:23,  1.56it/s]Extractor Estimating: 192it [02:23,  1.58it/s]Extractor Estimating: 193it [02:24,  1.55it/s]Extractor Estimating: 194it [02:25,  1.52it/s]Extractor Estimating: 195it [02:25,  1.53it/s]Extractor Estimating: 196it [02:26,  1.55it/s]Extractor Estimating: 197it [02:27,  1.55it/s]Extractor Estimating: 198it [02:27,  1.52it/s]Extractor Estimating: 199it [02:28,  1.48it/s]Extractor Estimating: 200it [02:29,  1.55it/s]Extractor Estimating: 201it [02:29,  1.55it/s]Extractor Estimating: 202it [02:30,  1.53it/s]Extractor Estimating: 203it [02:30,  1.54it/s]Extractor Estimating: 204it [02:31,  1.47it/s]Extractor Estimating: 205it [02:32,  1.48it/s]Extractor Estimating: 206it [02:32,  1.53it/s]Extractor Estimating: 207it [02:33,  1.45it/s]Extractor Estimating: 208it [02:34,  1.45it/s]Extractor Estimating: 209it [02:35,  1.44it/s]Extractor Estimating: 210it [02:35,  1.47it/s]Extractor Estimating: 211it [02:36,  1.52it/s]Extractor Estimating: 212it [02:37,  1.49it/s]Extractor Estimating: 213it [02:37,  1.50it/s]Extractor Estimating: 214it [02:38,  1.47it/s]Extractor Estimating: 215it [02:39,  1.53it/s]Extractor Estimating: 216it [02:39,  1.54it/s]Extractor Estimating: 217it [02:40,  1.50it/s]Extractor Estimating: 218it [02:41,  1.53it/s]Extractor Estimating: 219it [02:41,  1.52it/s]Extractor Estimating: 220it [02:42,  1.53it/s]Extractor Estimating: 221it [02:43,  1.50it/s]Extractor Estimating: 222it [02:43,  1.55it/s]Extractor Estimating: 223it [02:44,  1.59it/s]Extractor Estimating: 224it [02:44,  1.60it/s]Extractor Estimating: 225it [02:45,  1.56it/s]Extractor Estimating: 226it [02:46,  1.51it/s]Extractor Estimating: 227it [02:46,  1.50it/s]Extractor Estimating: 228it [02:47,  1.50it/s]Extractor Estimating: 229it [02:48,  1.47it/s]Extractor Estimating: 230it [02:49,  1.44it/s]Extractor Estimating: 231it [02:49,  1.43it/s]Extractor Estimating: 232it [02:50,  1.45it/s]Extractor Estimating: 233it [02:51,  1.45it/s]Extractor Estimating: 234it [02:51,  1.48it/s]Extractor Estimating: 235it [02:52,  1.46it/s]Extractor Estimating: 236it [02:53,  1.44it/s]Extractor Estimating: 237it [02:53,  1.47it/s]Extractor Estimating: 238it [02:54,  1.42it/s]Extractor Estimating: 239it [02:55,  1.38it/s]Extractor Estimating: 240it [02:56,  1.42it/s]Extractor Estimating: 241it [02:56,  1.43it/s]Extractor Estimating: 242it [02:57,  1.45it/s]Extractor Estimating: 243it [02:58,  1.46it/s]Extractor Estimating: 244it [02:58,  1.42it/s]Extractor Estimating: 245it [02:59,  1.37it/s]Extractor Estimating: 246it [03:00,  1.43it/s]Extractor Estimating: 247it [03:01,  1.33it/s]Extractor Estimating: 248it [03:01,  1.32it/s]Extractor Estimating: 249it [03:02,  1.36it/s]Extractor Estimating: 250it [03:03,  1.40it/s]Extractor Estimating: 251it [03:03,  1.50it/s]Extractor Estimating: 252it [03:04,  1.59it/s]Extractor Estimating: 253it [03:05,  1.52it/s]Extractor Estimating: 254it [03:05,  1.59it/s]Extractor Estimating: 255it [03:06,  1.64it/s]Extractor Estimating: 256it [03:06,  1.64it/s]Extractor Estimating: 257it [03:07,  1.68it/s]Extractor Estimating: 258it [03:07,  1.65it/s]Extractor Estimating: 259it [03:08,  1.63it/s]Extractor Estimating: 260it [03:09,  1.55it/s]Extractor Estimating: 261it [03:09,  1.57it/s]Extractor Estimating: 262it [03:10,  1.63it/s]Extractor Estimating: 263it [03:11,  1.61it/s]Extractor Estimating: 264it [03:11,  1.66it/s]Extractor Estimating: 265it [03:12,  1.65it/s]Extractor Estimating: 266it [03:12,  1.72it/s]Extractor Estimating: 267it [03:13,  1.71it/s]Extractor Estimating: 268it [03:13,  1.71it/s]Extractor Estimating: 269it [03:14,  1.71it/s]Extractor Estimating: 270it [03:15,  1.68it/s]Extractor Estimating: 271it [03:15,  1.68it/s]Extractor Estimating: 272it [03:16,  1.68it/s]Extractor Estimating: 273it [03:16,  1.72it/s]Extractor Estimating: 274it [03:17,  1.69it/s]Extractor Estimating: 275it [03:18,  1.70it/s]Extractor Estimating: 276it [03:18,  1.66it/s]Extractor Estimating: 277it [03:19,  1.59it/s]Extractor Estimating: 278it [03:20,  1.56it/s]Extractor Estimating: 279it [03:20,  1.53it/s]Extractor Estimating: 280it [03:21,  1.56it/s]Extractor Estimating: 281it [03:22,  1.57it/s]Extractor Estimating: 282it [03:22,  1.51it/s]Extractor Estimating: 283it [03:23,  1.52it/s]Extractor Estimating: 284it [03:24,  1.48it/s]Extractor Estimating: 285it [03:24,  1.49it/s]Extractor Estimating: 286it [03:25,  1.54it/s]Extractor Estimating: 287it [03:26,  1.55it/s]Extractor Estimating: 288it [03:26,  1.61it/s]Extractor Estimating: 289it [03:27,  1.57it/s]Extractor Estimating: 290it [03:27,  1.60it/s]Extractor Estimating: 291it [03:28,  1.61it/s]Extractor Estimating: 292it [03:29,  1.62it/s]Extractor Estimating: 293it [03:29,  1.58it/s]Extractor Estimating: 294it [03:30,  1.56it/s]Extractor Estimating: 295it [03:31,  1.56it/s]Extractor Estimating: 296it [03:31,  1.55it/s]Extractor Estimating: 297it [03:32,  1.55it/s]Extractor Estimating: 298it [03:32,  1.56it/s]Extractor Estimating: 299it [03:33,  1.56it/s]Extractor Estimating: 300it [03:34,  1.56it/s]Extractor Estimating: 301it [03:34,  1.55it/s]Extractor Estimating: 302it [03:35,  1.51it/s]Extractor Estimating: 303it [03:36,  1.50it/s]Extractor Estimating: 304it [03:36,  1.51it/s]Extractor Estimating: 305it [03:37,  1.51it/s]Extractor Estimating: 306it [03:38,  1.56it/s]Extractor Estimating: 307it [03:38,  1.56it/s]Extractor Estimating: 308it [03:39,  1.58it/s]Extractor Estimating: 309it [03:40,  1.57it/s]Extractor Estimating: 310it [03:40,  1.55it/s]Extractor Estimating: 311it [03:41,  1.50it/s]Extractor Estimating: 312it [03:42,  1.43it/s]Extractor Estimating: 313it [03:42,  1.41it/s]Extractor Estimating: 314it [03:43,  1.46it/s]Extractor Estimating: 315it [03:44,  1.45it/s]Extractor Estimating: 316it [03:44,  1.48it/s]Extractor Estimating: 317it [03:45,  1.47it/s]Extractor Estimating: 318it [03:46,  1.49it/s]Extractor Estimating: 319it [03:46,  1.55it/s]Extractor Estimating: 320it [03:47,  1.59it/s]Extractor Estimating: 321it [03:48,  1.56it/s]Extractor Estimating: 322it [03:48,  1.52it/s]Extractor Estimating: 323it [03:49,  1.52it/s]Extractor Estimating: 324it [03:50,  1.51it/s]Extractor Estimating: 325it [03:50,  1.48it/s]Extractor Estimating: 326it [03:51,  1.40it/s]Extractor Estimating: 327it [03:52,  1.38it/s]Extractor Estimating: 328it [03:53,  1.39it/s]Extractor Estimating: 329it [03:53,  1.40it/s]Extractor Estimating: 330it [03:54,  1.41it/s]Extractor Estimating: 331it [03:55,  1.41it/s]Extractor Estimating: 332it [03:55,  1.50it/s]Extractor Estimating: 333it [03:56,  1.50it/s]Extractor Estimating: 334it [03:57,  1.52it/s]Extractor Estimating: 335it [03:57,  1.49it/s]Extractor Estimating: 336it [03:58,  1.50it/s]Extractor Estimating: 337it [03:59,  1.44it/s]Extractor Estimating: 338it [03:59,  1.47it/s]Extractor Estimating: 339it [04:00,  1.48it/s]Extractor Estimating: 340it [04:01,  1.53it/s]Extractor Estimating: 341it [04:01,  1.55it/s]Extractor Estimating: 342it [04:02,  1.55it/s]Extractor Estimating: 343it [04:03,  1.55it/s]Extractor Estimating: 344it [04:03,  1.52it/s]Extractor Estimating: 345it [04:04,  1.50it/s]Extractor Estimating: 346it [04:05,  1.41it/s]Extractor Estimating: 347it [04:05,  1.43it/s]Extractor Estimating: 348it [04:06,  1.49it/s]Extractor Estimating: 349it [04:07,  1.50it/s]Extractor Estimating: 350it [04:07,  1.48it/s]Extractor Estimating: 351it [04:08,  1.51it/s]Extractor Estimating: 352it [04:09,  1.49it/s]Extractor Estimating: 353it [04:09,  1.54it/s]Extractor Estimating: 354it [04:10,  1.55it/s]Extractor Estimating: 355it [04:11,  1.55it/s]Extractor Estimating: 356it [04:11,  1.52it/s]Extractor Estimating: 357it [04:12,  1.55it/s]Extractor Estimating: 358it [04:13,  1.56it/s]Extractor Estimating: 359it [04:13,  1.57it/s]Extractor Estimating: 360it [04:14,  1.54it/s]Extractor Estimating: 361it [04:14,  1.55it/s]Extractor Estimating: 362it [04:15,  1.59it/s]Extractor Estimating: 363it [04:16,  1.55it/s]Extractor Estimating: 364it [04:16,  1.56it/s]Extractor Estimating: 365it [04:17,  1.56it/s]Extractor Estimating: 366it [04:18,  1.52it/s]Extractor Estimating: 367it [04:18,  1.51it/s]Extractor Estimating: 368it [04:19,  1.56it/s]Extractor Estimating: 369it [04:20,  1.58it/s]Extractor Estimating: 370it [04:20,  1.57it/s]Extractor Estimating: 371it [04:21,  1.55it/s]Extractor Estimating: 372it [04:21,  1.63it/s]Extractor Estimating: 373it [04:22,  1.65it/s]Extractor Estimating: 374it [04:23,  1.66it/s]Extractor Estimating: 375it [04:23,  1.58it/s]Extractor Estimating: 375it [04:23,  1.42it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7479 mean pseudo reward: 0.9648745038749349
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 27142
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27242, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27242, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.366, loss:3053.0440
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.051, loss:2220.1367
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.040, loss:1796.9537
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.052, loss:1702.6970
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.054, loss:1563.9487
>> valid entity prec:0.5039, rec:0.5758, f1:0.5375
>> valid relation prec:0.1658, rec:0.0518, f1:0.0790
>> valid relation with NER prec:0.1658, rec:0.0518, f1:0.0790
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.403, loss:1422.9436
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.056, loss:1372.1293
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.062, loss:1267.0960
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.060, loss:1183.1567
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.050, loss:1132.1871
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5614, rec:0.4763, f1:0.5154
>> valid relation prec:0.2203, rec:0.0441, f1:0.0735
>> valid relation with NER prec:0.2203, rec:0.0441, f1:0.0735
g_step 1100, step 164, avg_time 2.367, loss:1095.2276
g_step 1200, step 264, avg_time 1.058, loss:1085.1892
g_step 1300, step 52, avg_time 1.052, loss:1046.9876
g_step 1400, step 152, avg_time 1.053, loss:1006.4938
g_step 1500, step 252, avg_time 1.055, loss:984.5856
>> valid entity prec:0.5680, rec:0.4993, f1:0.5315
>> valid relation prec:0.2759, rec:0.0816, f1:0.1259
>> valid relation with NER prec:0.2759, rec:0.0816, f1:0.1259
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 40, avg_time 2.388, loss:959.7073
g_step 1700, step 140, avg_time 1.047, loss:933.6459
g_step 1800, step 240, avg_time 1.058, loss:931.2094
g_step 1900, step 28, avg_time 1.057, loss:911.1717
g_step 2000, step 128, avg_time 1.041, loss:895.6434
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5438, rec:0.5590, f1:0.5513
>> valid relation prec:0.2520, rec:0.1076, f1:0.1509
>> valid relation with NER prec:0.2520, rec:0.1076, f1:0.1509
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.379, loss:881.5832
g_step 2200, step 16, avg_time 1.064, loss:855.2865
g_step 2300, step 116, avg_time 1.056, loss:805.6083
g_step 2400, step 216, avg_time 1.053, loss:833.1721
g_step 2500, step 4, avg_time 1.057, loss:850.9382
>> valid entity prec:0.5654, rec:0.5217, f1:0.5426
>> valid relation prec:0.2119, rec:0.0784, f1:0.1145
>> valid relation with NER prec:0.2119, rec:0.0784, f1:0.1145
g_step 2600, step 104, avg_time 2.359, loss:786.1258
g_step 2700, step 204, avg_time 1.066, loss:788.9475
g_step 2800, step 304, avg_time 1.057, loss:795.9551
g_step 2900, step 92, avg_time 1.049, loss:733.3638
g_step 3000, step 192, avg_time 1.055, loss:744.1567
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5082, rec:0.5935, f1:0.5475
>> valid relation prec:0.1927, rec:0.0604, f1:0.0920
>> valid relation with NER prec:0.1927, rec:0.0604, f1:0.0920
g_step 3100, step 292, avg_time 2.380, loss:773.9755
g_step 3200, step 80, avg_time 1.048, loss:738.1600
g_step 3300, step 180, avg_time 1.067, loss:706.4414
g_step 3400, step 280, avg_time 1.056, loss:725.5580
g_step 3500, step 68, avg_time 1.043, loss:673.6874
>> valid entity prec:0.5322, rec:0.4601, f1:0.4935
>> valid relation prec:0.2024, rec:0.0633, f1:0.0964
>> valid relation with NER prec:0.2024, rec:0.0633, f1:0.0964
g_step 3600, step 168, avg_time 2.383, loss:681.5324
g_step 3700, step 268, avg_time 1.057, loss:698.3865
g_step 3800, step 56, avg_time 1.056, loss:664.2491
g_step 3900, step 156, avg_time 1.054, loss:661.4540
g_step 4000, step 256, avg_time 1.054, loss:679.6092
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5564, rec:0.4870, f1:0.5194
>> valid relation prec:0.2164, rec:0.0741, f1:0.1104
>> valid relation with NER prec:0.2164, rec:0.0741, f1:0.1104
g_step 4100, step 44, avg_time 2.369, loss:639.7881
g_step 4200, step 144, avg_time 1.058, loss:613.3927
g_step 4300, step 244, avg_time 1.054, loss:629.9051
g_step 4400, step 32, avg_time 1.053, loss:636.9974
g_step 4500, step 132, avg_time 1.063, loss:597.8924
>> valid entity prec:0.5789, rec:0.4497, f1:0.5062
>> valid relation prec:0.2028, rec:0.0704, f1:0.1045
>> valid relation with NER prec:0.2028, rec:0.0704, f1:0.1045
g_step 4600, step 232, avg_time 2.369, loss:623.0941
g_step 4700, step 20, avg_time 1.058, loss:587.3262
g_step 4800, step 120, avg_time 1.058, loss:571.2546
g_step 4900, step 220, avg_time 1.046, loss:600.8683
g_step 5000, step 8, avg_time 1.056, loss:585.3107
learning rate was adjusted to 0.0008
>> valid entity prec:0.5480, rec:0.4810, f1:0.5123
>> valid relation prec:0.1628, rec:0.0618, f1:0.0896
>> valid relation with NER prec:0.1628, rec:0.0618, f1:0.0896
g_step 5100, step 108, avg_time 2.366, loss:535.1339
g_step 5200, step 208, avg_time 1.059, loss:562.2647
g_step 5300, step 308, avg_time 1.066, loss:572.1710
g_step 5400, step 96, avg_time 1.052, loss:511.2903
g_step 5500, step 196, avg_time 1.057, loss:542.2310
>> valid entity prec:0.5418, rec:0.4536, f1:0.4938
>> valid relation prec:0.1893, rec:0.0696, f1:0.1017
>> valid relation with NER prec:0.1893, rec:0.0696, f1:0.1017
g_step 5600, step 296, avg_time 2.372, loss:564.1633
g_step 5700, step 84, avg_time 1.043, loss:516.4030
g_step 5800, step 184, avg_time 1.058, loss:487.9505
g_step 5900, step 284, avg_time 1.072, loss:530.0946
g_step 6000, step 72, avg_time 1.043, loss:488.7712
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5269, rec:0.5102, f1:0.5184
>> valid relation prec:0.1839, rec:0.0816, f1:0.1130
>> valid relation with NER prec:0.1839, rec:0.0816, f1:0.1130
g_step 6100, step 172, avg_time 2.381, loss:485.6176
g_step 6200, step 272, avg_time 1.062, loss:506.3917
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:15:54 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:15:54 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-15-54_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:15:55 - WARNING - datasets.builder -   Using custom data configuration default-8d29fcc20bc36d83
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8d29fcc20bc36d83/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:15:55,592 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:15:55,593 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:15:55,594 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:15:55,595 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:15:55,602 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:15:55,605 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:15:55,605 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:15:55,605 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:15:55,605 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:15:55,606 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:15:55,606 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:15:55,753 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:15:58,866 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:15:58,872 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8d29fcc20bc36d83/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 13:15:58 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x152c45f3bb00> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.79ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.65ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.02ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.20ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.32ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.37ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.41ba/s]100%|██████████| 8/8 [00:01<00:00,  5.21ba/s]100%|██████████| 8/8 [00:01<00:00,  4.45ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.99ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.19ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.27ba/s]100%|██████████| 4/4 [00:00<00:00,  5.30ba/s]100%|██████████| 4/4 [00:00<00:00,  4.83ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.59ba/s] 25%|██▌       | 2/8 [00:00<00:00,  8.80ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.23ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.51ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.87ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.86ba/s]100%|██████████| 8/8 [00:00<00:00, 10.18ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.88ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.37ba/s]100%|██████████| 4/4 [00:00<00:00, 10.62ba/s]
[INFO|trainer.py:414] 2023-08-28 13:16:03,058 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:16:03,083 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:16:03,083 >>   Num examples = 7513
[INFO|trainer.py:1149] 2023-08-28 13:16:03,083 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:16:03,083 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:16:03,083 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:16:03,083 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:16:03,084 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<03:00,  3.24it/s]  0%|          | 2/585 [00:00<02:52,  3.38it/s]  1%|          | 3/585 [00:00<02:50,  3.42it/s]  1%|          | 4/585 [00:01<02:48,  3.45it/s]  1%|          | 5/585 [00:01<02:47,  3.46it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:46,  3.47it/s]  1%|▏         | 8/585 [00:02<02:46,  3.47it/s]  2%|▏         | 9/585 [00:02<02:45,  3.47it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:45,  3.48it/s]  2%|▏         | 12/585 [00:03<02:44,  3.48it/s]  2%|▏         | 13/585 [00:03<02:44,  3.47it/s]  2%|▏         | 14/585 [00:04<02:44,  3.48it/s]  3%|▎         | 15/585 [00:04<02:44,  3.47it/s]  3%|▎         | 16/585 [00:04<02:43,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.47it/s]  3%|▎         | 18/585 [00:05<02:43,  3.47it/s]  3%|▎         | 19/585 [00:05<02:43,  3.47it/s]  3%|▎         | 20/585 [00:05<02:42,  3.47it/s]  4%|▎         | 21/585 [00:06<02:42,  3.47it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:41,  3.47it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:07<02:41,  3.47it/s]  5%|▍         | 27/585 [00:07<02:40,  3.47it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:08<02:40,  3.47it/s]  5%|▌         | 30/585 [00:08<02:39,  3.47it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:38,  3.47it/s]  6%|▌         | 34/585 [00:09<02:38,  3.47it/s]  6%|▌         | 35/585 [00:10<02:38,  3.47it/s]  6%|▌         | 36/585 [00:10<02:38,  3.47it/s]  6%|▋         | 37/585 [00:10<02:37,  3.47it/s]  6%|▋         | 38/585 [00:10<02:37,  3.47it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:37,  3.47it/s]  7%|▋         | 41/585 [00:11<02:36,  3.47it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:36,  3.47it/s]  8%|▊         | 44/585 [00:12<02:35,  3.47it/s]  8%|▊         | 45/585 [00:12<02:35,  3.47it/s]  8%|▊         | 46/585 [00:13<02:35,  3.47it/s]  8%|▊         | 47/585 [00:13<02:35,  3.47it/s]  8%|▊         | 48/585 [00:13<02:34,  3.47it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:14<02:34,  3.46it/s]  9%|▉         | 52/585 [00:15<02:33,  3.47it/s]  9%|▉         | 53/585 [00:15<02:33,  3.46it/s]  9%|▉         | 54/585 [00:15<02:33,  3.46it/s]  9%|▉         | 55/585 [00:15<02:32,  3.46it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:31,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.47it/s] 10%|█         | 61/585 [00:17<02:31,  3.47it/s] 11%|█         | 62/585 [00:17<02:30,  3.47it/s] 11%|█         | 63/585 [00:18<02:30,  3.47it/s] 11%|█         | 64/585 [00:18<02:30,  3.47it/s] 11%|█         | 65/585 [00:18<02:30,  3.47it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.46it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 76/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.46it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.45it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.45it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.46it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 90/585 [00:25<02:23,  3.45it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.46it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.46it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.46it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.45it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.45it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.45it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.46it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:32<02:16,  3.45it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.46it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.46it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 13:16:36,999 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:16:36,999 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:16:36,999 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.42it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.44it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.80it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.12it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.62it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.26it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.85it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.57it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.44it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.58it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.58it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.64it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.66it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.60it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.51it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.35it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.20it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.22it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.32it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.46it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.49it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.63it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.57it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.52it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.42it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.34it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.28it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.34it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.43it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.53it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.55it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.60it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.60it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.47it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.45it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.29it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.36it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.40it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.44it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.56it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.61it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.56it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.54it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.47it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.35it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.26it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.36it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.52it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.50it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.54it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.53it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.46it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.47it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.41it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.40it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.33it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.32it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.42it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.52it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.48it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.51it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.38it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.35it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.37it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.36it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.34it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.31it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.26it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.21it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.15it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.21it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.21it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.32it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.34it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.40it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.27it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.30it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.41it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.48it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.52it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.30it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.21it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.35it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.26it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.28it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.28it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:43<02:15,  3.46it/s]
100%|██████████| 437/437 [00:09<00:00, 46.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:16:46,429 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 13:16:46,445 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:16:48,926 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:16:48,942 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:16:48,955 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:51<42:39,  5.48s/it] 20%|██        | 119/585 [00:51<30:28,  3.92s/it] 21%|██        | 120/585 [00:51<21:57,  2.83s/it] 21%|██        | 121/585 [00:52<16:00,  2.07s/it] 21%|██        | 122/585 [00:52<11:51,  1.54s/it] 21%|██        | 123/585 [00:52<08:56,  1.16s/it] 21%|██        | 124/585 [00:53<06:54,  1.11it/s] 21%|██▏       | 125/585 [00:53<05:29,  1.40it/s] 22%|██▏       | 126/585 [00:53<04:29,  1.70it/s] 22%|██▏       | 127/585 [00:53<03:48,  2.01it/s] 22%|██▏       | 128/585 [00:54<03:19,  2.30it/s] 22%|██▏       | 129/585 [00:54<02:58,  2.56it/s] 22%|██▏       | 130/585 [00:54<02:46,  2.74it/s] 22%|██▏       | 131/585 [00:55<02:35,  2.92it/s] 23%|██▎       | 132/585 [00:55<02:28,  3.06it/s] 23%|██▎       | 133/585 [00:55<02:22,  3.17it/s] 23%|██▎       | 134/585 [00:56<02:18,  3.25it/s] 23%|██▎       | 135/585 [00:56<02:15,  3.31it/s] 23%|██▎       | 136/585 [00:56<02:13,  3.35it/s] 23%|██▎       | 137/585 [00:56<02:12,  3.38it/s] 24%|██▎       | 138/585 [00:57<02:11,  3.41it/s] 24%|██▍       | 139/585 [00:57<02:10,  3.42it/s] 24%|██▍       | 140/585 [00:57<02:09,  3.43it/s] 24%|██▍       | 141/585 [00:58<02:09,  3.43it/s] 24%|██▍       | 142/585 [00:58<02:08,  3.44it/s] 24%|██▍       | 143/585 [00:58<02:08,  3.45it/s] 25%|██▍       | 144/585 [00:58<02:07,  3.45it/s] 25%|██▍       | 145/585 [00:59<02:07,  3.45it/s] 25%|██▍       | 146/585 [00:59<02:07,  3.45it/s] 25%|██▌       | 147/585 [00:59<02:06,  3.45it/s] 25%|██▌       | 148/585 [01:00<02:06,  3.46it/s] 25%|██▌       | 149/585 [01:00<02:06,  3.46it/s] 26%|██▌       | 150/585 [01:00<02:05,  3.46it/s] 26%|██▌       | 151/585 [01:00<02:05,  3.46it/s] 26%|██▌       | 152/585 [01:01<02:05,  3.46it/s] 26%|██▌       | 153/585 [01:01<02:04,  3.46it/s] 26%|██▋       | 154/585 [01:01<02:05,  3.44it/s] 26%|██▋       | 155/585 [01:02<02:04,  3.45it/s] 27%|██▋       | 156/585 [01:02<02:04,  3.45it/s] 27%|██▋       | 157/585 [01:02<02:03,  3.45it/s] 27%|██▋       | 158/585 [01:02<02:03,  3.46it/s] 27%|██▋       | 159/585 [01:03<02:03,  3.46it/s] 27%|██▋       | 160/585 [01:03<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:03<02:02,  3.46it/s] 28%|██▊       | 162/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 163/585 [01:04<02:01,  3.46it/s] 28%|██▊       | 164/585 [01:04<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:05<02:01,  3.45it/s] 28%|██▊       | 166/585 [01:05<02:01,  3.44it/s] 29%|██▊       | 167/585 [01:05<02:01,  3.45it/s] 29%|██▊       | 168/585 [01:05<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:06<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:06<02:00,  3.46it/s] 29%|██▉       | 171/585 [01:06<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:07<01:59,  3.46it/s] 30%|██▉       | 173/585 [01:07<01:59,  3.46it/s] 30%|██▉       | 174/585 [01:07<01:58,  3.46it/s] 30%|██▉       | 175/585 [01:07<01:58,  3.46it/s] 30%|███       | 176/585 [01:08<01:58,  3.45it/s] 30%|███       | 177/585 [01:08<01:58,  3.45it/s] 30%|███       | 178/585 [01:08<01:57,  3.45it/s] 31%|███       | 179/585 [01:09<01:57,  3.45it/s] 31%|███       | 180/585 [01:09<01:57,  3.46it/s] 31%|███       | 181/585 [01:09<01:56,  3.46it/s] 31%|███       | 182/585 [01:09<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:10<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:10<01:56,  3.46it/s] 32%|███▏      | 185/585 [01:10<01:55,  3.46it/s] 32%|███▏      | 186/585 [01:11<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:11<01:55,  3.44it/s] 32%|███▏      | 188/585 [01:11<01:55,  3.45it/s] 32%|███▏      | 189/585 [01:11<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:12<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:12<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:12<01:53,  3.45it/s] 33%|███▎      | 193/585 [01:13<01:53,  3.45it/s] 33%|███▎      | 194/585 [01:13<01:53,  3.46it/s] 33%|███▎      | 195/585 [01:13<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:13<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:14<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:14<01:52,  3.45it/s] 34%|███▍      | 199/585 [01:14<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:15<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:15<01:51,  3.46it/s] 35%|███▍      | 202/585 [01:15<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:16<01:50,  3.46it/s] 35%|███▍      | 204/585 [01:16<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:16<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:16<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:17<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:17<01:49,  3.46it/s] 36%|███▌      | 209/585 [01:17<01:49,  3.45it/s] 36%|███▌      | 210/585 [01:18<01:48,  3.45it/s] 36%|███▌      | 211/585 [01:18<01:48,  3.45it/s] 36%|███▌      | 212/585 [01:18<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:18<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:19<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:19<01:47,  3.46it/s] 37%|███▋      | 216/585 [01:19<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:20<01:46,  3.45it/s] 37%|███▋      | 218/585 [01:20<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:20<01:45,  3.46it/s] 38%|███▊      | 220/585 [01:20<01:46,  3.44it/s] 38%|███▊      | 221/585 [01:21<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:21<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:21<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:22<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:22<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:22<01:43,  3.46it/s] 39%|███▉      | 227/585 [01:22<01:43,  3.46it/s] 39%|███▉      | 228/585 [01:23<01:43,  3.46it/s] 39%|███▉      | 229/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:23<01:42,  3.46it/s] 39%|███▉      | 231/585 [01:24<01:42,  3.44it/s] 40%|███▉      | 232/585 [01:24<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:24<01:42,  3.45it/s] 40%|████      | 234/585 [01:24<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:17:28,181 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:17:28,182 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:17:28,182 >>   Batch size = 8
{'eval_loss': 0.9638440608978271, 'eval_runtime': 9.4093, 'eval_samples_per_second': 371.23, 'eval_steps_per_second': 46.444, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.29it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.48it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.62it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.83it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.31it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.05it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.69it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.45it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.45it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.50it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.50it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.61it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.59it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.56it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.44it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.35it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.24it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.20it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.28it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.37it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.51it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.59it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.48it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.48it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.26it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.28it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.24it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.29it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.32it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.37it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.43it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.43it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.36it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.48it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.37it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.25it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.31it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.26it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.29it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.36it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.42it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.43it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.37it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.46it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.32it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.31it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.30it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.31it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.40it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.45it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.46it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.44it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.32it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.38it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.36it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.32it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.34it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.38it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.42it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.39it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.40it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.42it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.35it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.33it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.40it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.38it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.30it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.38it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.43it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.47it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.44it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.40it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.38it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.31it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.33it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.38it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.43it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.36it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.40it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.38it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.27it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.32it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.41it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.35it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.38it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.42it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:34<01:41,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:17:37,625 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 13:17:37,658 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:17:40,129 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:17:40,151 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:17:40,164 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:42<31:05,  5.33s/it] 40%|████      | 236/585 [01:42<22:15,  3.83s/it] 41%|████      | 237/585 [01:42<16:02,  2.77s/it] 41%|████      | 238/585 [01:42<11:41,  2.02s/it] 41%|████      | 239/585 [01:43<08:40,  1.50s/it] 41%|████      | 240/585 [01:43<06:32,  1.14s/it] 41%|████      | 241/585 [01:43<05:04,  1.13it/s] 41%|████▏     | 242/585 [01:44<04:02,  1.42it/s] 42%|████▏     | 243/585 [01:44<03:18,  1.72it/s] 42%|████▏     | 244/585 [01:44<02:48,  2.03it/s] 42%|████▏     | 245/585 [01:45<02:27,  2.31it/s] 42%|████▏     | 246/585 [01:45<02:12,  2.57it/s] 42%|████▏     | 247/585 [01:45<02:05,  2.69it/s] 42%|████▏     | 248/585 [01:45<01:56,  2.88it/s] 43%|████▎     | 249/585 [01:46<01:50,  3.03it/s] 43%|████▎     | 250/585 [01:46<01:46,  3.15it/s] 43%|████▎     | 251/585 [01:46<01:43,  3.24it/s] 43%|████▎     | 252/585 [01:47<01:40,  3.30it/s] 43%|████▎     | 253/585 [01:47<01:39,  3.34it/s] 43%|████▎     | 254/585 [01:47<01:38,  3.38it/s] 44%|████▎     | 255/585 [01:47<01:37,  3.40it/s] 44%|████▍     | 256/585 [01:48<01:36,  3.42it/s] 44%|████▍     | 257/585 [01:48<01:35,  3.43it/s] 44%|████▍     | 258/585 [01:48<01:35,  3.43it/s] 44%|████▍     | 259/585 [01:49<01:34,  3.44it/s] 44%|████▍     | 260/585 [01:49<01:34,  3.44it/s] 45%|████▍     | 261/585 [01:49<01:33,  3.45it/s] 45%|████▍     | 262/585 [01:49<01:33,  3.45it/s] 45%|████▍     | 263/585 [01:50<01:33,  3.45it/s] 45%|████▌     | 264/585 [01:50<01:33,  3.45it/s] 45%|████▌     | 265/585 [01:50<01:32,  3.45it/s] 45%|████▌     | 266/585 [01:51<01:32,  3.45it/s] 46%|████▌     | 267/585 [01:51<01:34,  3.37it/s] 46%|████▌     | 268/585 [01:51<01:33,  3.39it/s] 46%|████▌     | 269/585 [01:52<01:32,  3.40it/s] 46%|████▌     | 270/585 [01:52<01:32,  3.42it/s] 46%|████▋     | 271/585 [01:52<01:31,  3.42it/s] 46%|████▋     | 272/585 [01:52<01:31,  3.43it/s] 47%|████▋     | 273/585 [01:53<01:30,  3.44it/s] 47%|████▋     | 274/585 [01:53<01:30,  3.44it/s] 47%|████▋     | 275/585 [01:53<01:29,  3.44it/s] 47%|████▋     | 276/585 [01:54<01:29,  3.45it/s] 47%|████▋     | 277/585 [01:54<01:29,  3.45it/s] 48%|████▊     | 278/585 [01:54<01:28,  3.46it/s] 48%|████▊     | 279/585 [01:54<01:28,  3.45it/s] 48%|████▊     | 280/585 [01:55<01:29,  3.42it/s] 48%|████▊     | 281/585 [01:55<01:28,  3.43it/s] 48%|████▊     | 282/585 [01:55<01:28,  3.44it/s] 48%|████▊     | 283/585 [01:56<01:27,  3.44it/s] 49%|████▊     | 284/585 [01:56<01:27,  3.45it/s] 49%|████▊     | 285/585 [01:56<01:26,  3.45it/s] 49%|████▉     | 286/585 [01:56<01:26,  3.45it/s] 49%|████▉     | 287/585 [01:57<01:26,  3.45it/s] 49%|████▉     | 288/585 [01:57<01:26,  3.45it/s] 49%|████▉     | 289/585 [01:57<01:25,  3.45it/s] 50%|████▉     | 290/585 [01:58<01:25,  3.45it/s] 50%|████▉     | 291/585 [01:58<01:25,  3.44it/s] 50%|████▉     | 292/585 [01:58<01:25,  3.44it/s] 50%|█████     | 293/585 [01:58<01:24,  3.44it/s] 50%|█████     | 294/585 [01:59<01:24,  3.45it/s] 50%|█████     | 295/585 [01:59<01:24,  3.45it/s] 51%|█████     | 296/585 [01:59<01:23,  3.45it/s] 51%|█████     | 297/585 [02:00<01:23,  3.45it/s] 51%|█████     | 298/585 [02:00<01:23,  3.45it/s] 51%|█████     | 299/585 [02:00<01:22,  3.45it/s] 51%|█████▏    | 300/585 [02:01<01:22,  3.45it/s] 51%|█████▏    | 301/585 [02:01<01:22,  3.45it/s] 52%|█████▏    | 302/585 [02:01<01:22,  3.44it/s] 52%|█████▏    | 303/585 [02:01<01:21,  3.44it/s] 52%|█████▏    | 304/585 [02:02<01:21,  3.45it/s] 52%|█████▏    | 305/585 [02:02<01:21,  3.45it/s] 52%|█████▏    | 306/585 [02:02<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:03<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:03<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:03<01:20,  3.45it/s] 53%|█████▎    | 310/585 [02:03<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:04<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:04<01:19,  3.45it/s] 54%|█████▎    | 313/585 [02:04<01:19,  3.44it/s] 54%|█████▎    | 314/585 [02:05<01:18,  3.44it/s] 54%|█████▍    | 315/585 [02:05<01:18,  3.45it/s] 54%|█████▍    | 316/585 [02:05<01:18,  3.45it/s] 54%|█████▍    | 317/585 [02:05<01:17,  3.45it/s] 54%|█████▍    | 318/585 [02:06<01:17,  3.45it/s] 55%|█████▍    | 319/585 [02:06<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:06<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:07<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:07<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:07<01:15,  3.45it/s] 55%|█████▌    | 324/585 [02:07<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:08<01:15,  3.44it/s] 56%|█████▌    | 326/585 [02:08<01:15,  3.44it/s] 56%|█████▌    | 327/585 [02:08<01:14,  3.44it/s] 56%|█████▌    | 328/585 [02:09<01:14,  3.45it/s] 56%|█████▌    | 329/585 [02:09<01:14,  3.44it/s] 56%|█████▋    | 330/585 [02:09<01:14,  3.44it/s] 57%|█████▋    | 331/585 [02:10<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:10<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:10<01:13,  3.45it/s] 57%|█████▋    | 334/585 [02:10<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:11<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:11<01:12,  3.44it/s] 58%|█████▊    | 337/585 [02:11<01:11,  3.45it/s] 58%|█████▊    | 338/585 [02:12<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:12<01:11,  3.45it/s] 58%|█████▊    | 340/585 [02:12<01:10,  3.45it/s] 58%|█████▊    | 341/585 [02:12<01:10,  3.45it/s] 58%|█████▊    | 342/585 [02:13<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:13<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:13<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:14<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:14<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:14<01:09,  3.45it/s] 59%|█████▉    | 348/585 [02:14<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:15<01:08,  3.45it/s] 60%|█████▉    | 350/585 [02:15<01:08,  3.45it/s] 60%|██████    | 351/585 [02:15<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:18:18,998 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:18:18,998 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:18:18,998 >>   Batch size = 8
{'eval_loss': 0.9555692672729492, 'eval_runtime': 9.4161, 'eval_samples_per_second': 370.959, 'eval_steps_per_second': 46.41, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.43it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.35it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.62it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.95it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.34it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.96it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.62it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.38it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.42it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.47it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.42it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.48it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.46it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.46it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.48it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.42it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.20it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.21it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.22it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.34it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.31it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.41it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.47it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.52it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.46it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.27it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.25it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.29it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.30it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.36it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.36it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.41it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.46it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.43it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.37it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.29it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.34it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.41it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.28it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.00it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.17it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.33it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.44it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.41it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.31it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.24it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.32it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.38it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.37it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.37it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.37it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.40it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.44it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.33it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.26it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.28it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.35it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.33it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.40it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.38it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.34it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.39it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.31it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.29it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.20it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.21it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.36it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.35it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.38it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.41it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.34it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.40it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.34it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.23it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.33it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.32it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.41it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.38it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.39it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.34it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.36it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.37it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.36it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.36it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.22it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.30it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:25<01:07,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.30it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:18:28,442 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 13:18:28,463 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:18:30,969 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:18:30,983 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:18:30,990 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:32<20:31,  5.29s/it] 60%|██████    | 353/585 [02:33<14:40,  3.79s/it] 61%|██████    | 354/585 [02:33<10:33,  2.74s/it] 61%|██████    | 355/585 [02:33<07:41,  2.01s/it] 61%|██████    | 356/585 [02:33<05:41,  1.49s/it] 61%|██████    | 357/585 [02:34<04:17,  1.13s/it] 61%|██████    | 358/585 [02:34<03:19,  1.14it/s] 61%|██████▏   | 359/585 [02:34<02:38,  1.43it/s] 62%|██████▏   | 360/585 [02:35<02:10,  1.73it/s] 62%|██████▏   | 361/585 [02:35<01:50,  2.03it/s] 62%|██████▏   | 362/585 [02:35<01:36,  2.32it/s] 62%|██████▏   | 363/585 [02:35<01:26,  2.57it/s] 62%|██████▏   | 364/585 [02:36<01:19,  2.79it/s] 62%|██████▏   | 365/585 [02:36<01:14,  2.96it/s] 63%|██████▎   | 366/585 [02:36<01:10,  3.09it/s] 63%|██████▎   | 367/585 [02:37<01:08,  3.19it/s] 63%|██████▎   | 368/585 [02:37<01:06,  3.27it/s] 63%|██████▎   | 369/585 [02:37<01:05,  3.32it/s] 63%|██████▎   | 370/585 [02:37<01:03,  3.36it/s] 63%|██████▎   | 371/585 [02:38<01:03,  3.39it/s] 64%|██████▎   | 372/585 [02:38<01:02,  3.41it/s] 64%|██████▍   | 373/585 [02:38<01:01,  3.42it/s] 64%|██████▍   | 374/585 [02:39<01:04,  3.29it/s] 64%|██████▍   | 375/585 [02:39<01:02,  3.34it/s] 64%|██████▍   | 376/585 [02:39<01:01,  3.38it/s] 64%|██████▍   | 377/585 [02:40<01:01,  3.40it/s] 65%|██████▍   | 378/585 [02:40<01:00,  3.42it/s] 65%|██████▍   | 379/585 [02:40<01:00,  3.43it/s] 65%|██████▍   | 380/585 [02:40<00:59,  3.44it/s] 65%|██████▌   | 381/585 [02:41<00:59,  3.44it/s] 65%|██████▌   | 382/585 [02:41<00:58,  3.45it/s] 65%|██████▌   | 383/585 [02:41<00:58,  3.45it/s] 66%|██████▌   | 384/585 [02:42<00:58,  3.45it/s] 66%|██████▌   | 385/585 [02:42<00:58,  3.45it/s] 66%|██████▌   | 386/585 [02:42<00:57,  3.45it/s] 66%|██████▌   | 387/585 [02:42<00:57,  3.45it/s] 66%|██████▋   | 388/585 [02:43<00:57,  3.45it/s] 66%|██████▋   | 389/585 [02:43<00:56,  3.46it/s] 67%|██████▋   | 390/585 [02:43<00:56,  3.46it/s] 67%|██████▋   | 391/585 [02:44<00:56,  3.46it/s] 67%|██████▋   | 392/585 [02:44<00:55,  3.46it/s] 67%|██████▋   | 393/585 [02:44<00:55,  3.46it/s] 67%|██████▋   | 394/585 [02:44<00:55,  3.46it/s] 68%|██████▊   | 395/585 [02:45<00:54,  3.46it/s] 68%|██████▊   | 396/585 [02:45<00:54,  3.44it/s] 68%|██████▊   | 397/585 [02:45<00:54,  3.44it/s] 68%|██████▊   | 398/585 [02:46<00:54,  3.45it/s] 68%|██████▊   | 399/585 [02:46<00:53,  3.45it/s] 68%|██████▊   | 400/585 [02:46<00:53,  3.45it/s] 69%|██████▊   | 401/585 [02:46<00:53,  3.46it/s] 69%|██████▊   | 402/585 [02:47<00:52,  3.46it/s] 69%|██████▉   | 403/585 [02:47<00:52,  3.46it/s] 69%|██████▉   | 404/585 [02:47<00:52,  3.45it/s] 69%|██████▉   | 405/585 [02:48<00:52,  3.45it/s] 69%|██████▉   | 406/585 [02:48<00:51,  3.45it/s] 70%|██████▉   | 407/585 [02:48<00:51,  3.45it/s] 70%|██████▉   | 408/585 [02:49<00:51,  3.45it/s] 70%|██████▉   | 409/585 [02:49<00:51,  3.45it/s] 70%|███████   | 410/585 [02:49<00:50,  3.45it/s] 70%|███████   | 411/585 [02:49<00:50,  3.45it/s] 70%|███████   | 412/585 [02:50<00:50,  3.45it/s] 71%|███████   | 413/585 [02:50<00:49,  3.45it/s] 71%|███████   | 414/585 [02:50<00:49,  3.45it/s] 71%|███████   | 415/585 [02:51<00:49,  3.45it/s] 71%|███████   | 416/585 [02:51<00:48,  3.46it/s] 71%|███████▏  | 417/585 [02:51<00:48,  3.46it/s] 71%|███████▏  | 418/585 [02:51<00:48,  3.44it/s] 72%|███████▏  | 419/585 [02:52<00:48,  3.44it/s] 72%|███████▏  | 420/585 [02:52<00:47,  3.45it/s] 72%|███████▏  | 421/585 [02:52<00:47,  3.45it/s] 72%|███████▏  | 422/585 [02:53<00:47,  3.45it/s] 72%|███████▏  | 423/585 [02:53<00:46,  3.45it/s] 72%|███████▏  | 424/585 [02:53<00:46,  3.45it/s] 73%|███████▎  | 425/585 [02:53<00:46,  3.45it/s] 73%|███████▎  | 426/585 [02:54<00:46,  3.45it/s] 73%|███████▎  | 427/585 [02:54<00:45,  3.45it/s] 73%|███████▎  | 428/585 [02:54<00:45,  3.45it/s] 73%|███████▎  | 429/585 [02:55<00:45,  3.44it/s] 74%|███████▎  | 430/585 [02:55<00:45,  3.44it/s] 74%|███████▎  | 431/585 [02:55<00:44,  3.45it/s] 74%|███████▍  | 432/585 [02:55<00:44,  3.45it/s] 74%|███████▍  | 433/585 [02:56<00:44,  3.45it/s] 74%|███████▍  | 434/585 [02:56<00:43,  3.45it/s] 74%|███████▍  | 435/585 [02:56<00:43,  3.45it/s] 75%|███████▍  | 436/585 [02:57<00:43,  3.45it/s] 75%|███████▍  | 437/585 [02:57<00:42,  3.45it/s] 75%|███████▍  | 438/585 [02:57<00:42,  3.45it/s] 75%|███████▌  | 439/585 [02:58<00:42,  3.45it/s] 75%|███████▌  | 440/585 [02:58<00:42,  3.44it/s] 75%|███████▌  | 441/585 [02:58<00:41,  3.45it/s] 76%|███████▌  | 442/585 [02:58<00:41,  3.45it/s] 76%|███████▌  | 443/585 [02:59<00:41,  3.45it/s] 76%|███████▌  | 444/585 [02:59<00:40,  3.45it/s] 76%|███████▌  | 445/585 [02:59<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:00<00:40,  3.45it/s] 76%|███████▋  | 447/585 [03:00<00:39,  3.45it/s] 77%|███████▋  | 448/585 [03:00<00:39,  3.45it/s] 77%|███████▋  | 449/585 [03:00<00:39,  3.45it/s] 77%|███████▋  | 450/585 [03:01<00:39,  3.45it/s] 77%|███████▋  | 451/585 [03:01<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:01<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:02<00:38,  3.45it/s] 78%|███████▊  | 454/585 [03:02<00:37,  3.45it/s] 78%|███████▊  | 455/585 [03:02<00:37,  3.45it/s] 78%|███████▊  | 456/585 [03:02<00:37,  3.45it/s] 78%|███████▊  | 457/585 [03:03<00:37,  3.45it/s] 78%|███████▊  | 458/585 [03:03<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:03<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:04<00:36,  3.45it/s] 79%|███████▉  | 461/585 [03:04<00:35,  3.45it/s] 79%|███████▉  | 462/585 [03:04<00:35,  3.44it/s] 79%|███████▉  | 463/585 [03:04<00:35,  3.44it/s] 79%|███████▉  | 464/585 [03:05<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:05<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:05<00:34,  3.45it/s] 80%|███████▉  | 467/585 [03:06<00:34,  3.45it/s] 80%|████████  | 468/585 [03:06<00:33,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:19:09,611 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:19:09,611 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:19:09,611 >>   Batch size = 8
{'eval_loss': 0.9570848345756531, 'eval_runtime': 9.424, 'eval_samples_per_second': 370.651, 'eval_steps_per_second': 46.371, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.25it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.38it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.64it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.77it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.36it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.99it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.60it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.43it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.34it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.42it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.41it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.48it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.53it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.33it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.45it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.34it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.24it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.33it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.27it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.35it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.37it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.43it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.39it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.39it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.29it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.26it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.24it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.29it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.29it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.32it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.39it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.41it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.23it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.26it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.23it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.25it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.27it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.38it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.35it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.25it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.32it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.23it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.22it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.24it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.19it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.32it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.36it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.42it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.23it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.31it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.34it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.23it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.29it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.24it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.37it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.29it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.33it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.09it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.31it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.36it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.20it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.25it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.30it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.31it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.33it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.38it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.31it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.36it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.23it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.34it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.18it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.21it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.28it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.35it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.36it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.40it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.33it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.25it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.20it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.27it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.31it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.25it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.34it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.34it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.36it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.35it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:15<00:33,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:19:19,060 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 13:19:19,077 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:19:21,272 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:19:21,287 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:19:21,295 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:23<10:03,  5.20s/it] 80%|████████  | 470/585 [03:23<07:08,  3.73s/it] 81%|████████  | 471/585 [03:23<05:07,  2.70s/it] 81%|████████  | 472/585 [03:23<03:43,  1.97s/it] 81%|████████  | 473/585 [03:24<02:44,  1.47s/it] 81%|████████  | 474/585 [03:24<02:03,  1.12s/it] 81%|████████  | 475/585 [03:24<01:35,  1.15it/s] 81%|████████▏ | 476/585 [03:25<01:15,  1.44it/s] 82%|████████▏ | 477/585 [03:25<01:01,  1.75it/s] 82%|████████▏ | 478/585 [03:25<00:52,  2.05it/s] 82%|████████▏ | 479/585 [03:25<00:45,  2.33it/s] 82%|████████▏ | 480/585 [03:26<00:40,  2.58it/s] 82%|████████▏ | 481/585 [03:26<00:37,  2.78it/s] 82%|████████▏ | 482/585 [03:26<00:34,  2.96it/s] 83%|████████▎ | 483/585 [03:27<00:32,  3.09it/s] 83%|████████▎ | 484/585 [03:27<00:31,  3.19it/s] 83%|████████▎ | 485/585 [03:27<00:30,  3.26it/s] 83%|████████▎ | 486/585 [03:28<00:29,  3.32it/s] 83%|████████▎ | 487/585 [03:28<00:29,  3.36it/s] 83%|████████▎ | 488/585 [03:28<00:28,  3.39it/s] 84%|████████▎ | 489/585 [03:28<00:28,  3.41it/s] 84%|████████▍ | 490/585 [03:29<00:27,  3.42it/s] 84%|████████▍ | 491/585 [03:29<00:27,  3.43it/s] 84%|████████▍ | 492/585 [03:29<00:28,  3.29it/s] 84%|████████▍ | 493/585 [03:30<00:27,  3.34it/s] 84%|████████▍ | 494/585 [03:30<00:27,  3.37it/s] 85%|████████▍ | 495/585 [03:30<00:26,  3.39it/s] 85%|████████▍ | 496/585 [03:30<00:26,  3.41it/s] 85%|████████▍ | 497/585 [03:31<00:25,  3.42it/s] 85%|████████▌ | 498/585 [03:31<00:25,  3.43it/s] 85%|████████▌ | 499/585 [03:31<00:25,  3.43it/s] 85%|████████▌ | 500/585 [03:32<00:24,  3.44it/s]                                                  85%|████████▌ | 500/585 [03:32<00:24,  3.44it/s] 86%|████████▌ | 501/585 [03:32<00:24,  3.44it/s] 86%|████████▌ | 502/585 [03:32<00:24,  3.45it/s] 86%|████████▌ | 503/585 [03:32<00:23,  3.44it/s] 86%|████████▌ | 504/585 [03:33<00:23,  3.44it/s] 86%|████████▋ | 505/585 [03:33<00:23,  3.44it/s] 86%|████████▋ | 506/585 [03:33<00:22,  3.44it/s] 87%|████████▋ | 507/585 [03:34<00:22,  3.45it/s] 87%|████████▋ | 508/585 [03:34<00:22,  3.45it/s] 87%|████████▋ | 509/585 [03:34<00:22,  3.45it/s] 87%|████████▋ | 510/585 [03:35<00:21,  3.45it/s] 87%|████████▋ | 511/585 [03:35<00:21,  3.45it/s] 88%|████████▊ | 512/585 [03:35<00:21,  3.45it/s] 88%|████████▊ | 513/585 [03:35<00:20,  3.45it/s] 88%|████████▊ | 514/585 [03:36<00:20,  3.44it/s] 88%|████████▊ | 515/585 [03:36<00:20,  3.44it/s] 88%|████████▊ | 516/585 [03:36<00:20,  3.44it/s] 88%|████████▊ | 517/585 [03:37<00:19,  3.45it/s] 89%|████████▊ | 518/585 [03:37<00:19,  3.45it/s] 89%|████████▊ | 519/585 [03:37<00:19,  3.45it/s] 89%|████████▉ | 520/585 [03:37<00:18,  3.45it/s] 89%|████████▉ | 521/585 [03:38<00:18,  3.45it/s] 89%|████████▉ | 522/585 [03:38<00:18,  3.45it/s] 89%|████████▉ | 523/585 [03:38<00:17,  3.45it/s] 90%|████████▉ | 524/585 [03:39<00:17,  3.45it/s] 90%|████████▉ | 525/585 [03:39<00:17,  3.45it/s] 90%|████████▉ | 526/585 [03:39<00:17,  3.45it/s] 90%|█████████ | 527/585 [03:39<00:16,  3.45it/s] 90%|█████████ | 528/585 [03:40<00:16,  3.45it/s] 90%|█████████ | 529/585 [03:40<00:16,  3.44it/s] 91%|█████████ | 530/585 [03:40<00:15,  3.45it/s] 91%|█████████ | 531/585 [03:41<00:15,  3.45it/s] 91%|█████████ | 532/585 [03:41<00:15,  3.45it/s] 91%|█████████ | 533/585 [03:41<00:15,  3.45it/s] 91%|█████████▏| 534/585 [03:41<00:14,  3.45it/s] 91%|█████████▏| 535/585 [03:42<00:14,  3.45it/s] 92%|█████████▏| 536/585 [03:42<00:14,  3.45it/s] 92%|█████████▏| 537/585 [03:42<00:13,  3.45it/s] 92%|█████████▏| 538/585 [03:43<00:13,  3.45it/s] 92%|█████████▏| 539/585 [03:43<00:13,  3.45it/s] 92%|█████████▏| 540/585 [03:43<00:13,  3.44it/s] 92%|█████████▏| 541/585 [03:43<00:12,  3.45it/s] 93%|█████████▎| 542/585 [03:44<00:12,  3.44it/s] 93%|█████████▎| 543/585 [03:44<00:12,  3.45it/s] 93%|█████████▎| 544/585 [03:44<00:11,  3.45it/s] 93%|█████████▎| 545/585 [03:45<00:11,  3.45it/s] 93%|█████████▎| 546/585 [03:45<00:11,  3.45it/s] 94%|█████████▎| 547/585 [03:45<00:11,  3.45it/s] 94%|█████████▎| 548/585 [03:46<00:10,  3.45it/s] 94%|█████████▍| 549/585 [03:46<00:10,  3.45it/s] 94%|█████████▍| 550/585 [03:46<00:10,  3.45it/s] 94%|█████████▍| 551/585 [03:46<00:09,  3.43it/s] 94%|█████████▍| 552/585 [03:47<00:09,  3.44it/s] 95%|█████████▍| 553/585 [03:47<00:09,  3.44it/s] 95%|█████████▍| 554/585 [03:47<00:09,  3.44it/s] 95%|█████████▍| 555/585 [03:48<00:08,  3.45it/s] 95%|█████████▌| 556/585 [03:48<00:08,  3.45it/s] 95%|█████████▌| 557/585 [03:48<00:08,  3.45it/s] 95%|█████████▌| 558/585 [03:48<00:07,  3.45it/s] 96%|█████████▌| 559/585 [03:49<00:07,  3.45it/s] 96%|█████████▌| 560/585 [03:49<00:07,  3.45it/s] 96%|█████████▌| 561/585 [03:49<00:06,  3.45it/s] 96%|█████████▌| 562/585 [03:50<00:06,  3.44it/s] 96%|█████████▌| 563/585 [03:50<00:06,  3.43it/s] 96%|█████████▋| 564/585 [03:50<00:06,  3.44it/s] 97%|█████████▋| 565/585 [03:50<00:05,  3.44it/s] 97%|█████████▋| 566/585 [03:51<00:05,  3.44it/s] 97%|█████████▋| 567/585 [03:51<00:05,  3.35it/s] 97%|█████████▋| 568/585 [03:51<00:05,  3.38it/s] 97%|█████████▋| 569/585 [03:52<00:04,  3.40it/s] 97%|█████████▋| 570/585 [03:52<00:04,  3.41it/s] 98%|█████████▊| 571/585 [03:52<00:04,  3.42it/s] 98%|█████████▊| 572/585 [03:53<00:03,  3.43it/s] 98%|█████████▊| 573/585 [03:53<00:03,  3.42it/s] 98%|█████████▊| 574/585 [03:53<00:03,  3.43it/s] 98%|█████████▊| 575/585 [03:53<00:02,  3.44it/s] 98%|█████████▊| 576/585 [03:54<00:02,  3.44it/s] 99%|█████████▊| 577/585 [03:54<00:02,  3.44it/s] 99%|█████████▉| 578/585 [03:54<00:02,  3.44it/s] 99%|█████████▉| 579/585 [03:55<00:01,  3.45it/s] 99%|█████████▉| 580/585 [03:55<00:01,  3.45it/s] 99%|█████████▉| 581/585 [03:55<00:01,  3.45it/s] 99%|█████████▉| 582/585 [03:55<00:00,  3.45it/s]100%|█████████▉| 583/585 [03:56<00:00,  3.45it/s]100%|█████████▉| 584/585 [03:56<00:00,  3.44it/s]100%|██████████| 585/585 [03:56<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 13:19:59,886 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:19:59,887 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:19:59,887 >>   Batch size = 8
{'eval_loss': 0.9629500508308411, 'eval_runtime': 9.4311, 'eval_samples_per_second': 370.371, 'eval_steps_per_second': 46.336, 'epoch': 4.0}
{'loss': 0.8088, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.51it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.11it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.35it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.64it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.22it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.02it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.72it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.37it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.30it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.30it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.34it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.40it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.36it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.43it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.41it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.40it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.15it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.14it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.25it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.31it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.39it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.33it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.41it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.43it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.32it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.24it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.16it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.13it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.26it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.33it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.36it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.32it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.32it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.35it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.29it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.29it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.21it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.14it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.35it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.36it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.43it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.41it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.30it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.33it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.26it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.22it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.17it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.22it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.32it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.36it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.46it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.34it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.25it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.21it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.26it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.20it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.21it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.20it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.32it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.28it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.27it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.33it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.29it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.29it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.23it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.30it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.18it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.19it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.32it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.28it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.36it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.33it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.29it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.20it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.18it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.28it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.31it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.34it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.38it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.19it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.27it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.28it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.27it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.24it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.26it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.33it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:06<00:00,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 46.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:20:09,353 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 13:20:09,377 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:20:11,835 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:20:11,862 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:20:11,869 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:20:16,797 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:20:16,799 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234 (score: 0.9555692672729492).
                                                 100%|██████████| 585/585 [04:16<00:00,  3.44it/s]100%|██████████| 585/585 [04:16<00:00,  2.28it/s]
[INFO|trainer.py:1894] 2023-08-28 13:20:20,072 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 13:20:20,092 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:20:24,417 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:20:24,626 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:20:24,727 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:20:25,352 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:25,353 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:25,353 >>   train_loss               =     0.8032
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:25,353 >>   train_runtime            = 0:04:16.97
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:25,353 >>   train_samples            =       7513
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:25,353 >>   train_samples_per_second =    146.183
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:25,353 >>   train_steps_per_second   =      2.277
{'eval_loss': 0.965408980846405, 'eval_runtime': 9.4354, 'eval_samples_per_second': 370.201, 'eval_steps_per_second': 46.315, 'epoch': 5.0}
{'train_runtime': 256.9727, 'train_samples_per_second': 146.183, 'train_steps_per_second': 2.277, 'train_loss': 0.803230716020633, 'epoch': 5.0}
08/28/2023 13:20:25 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:20:25,430 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:20:25,430 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:20:25,430 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 57.67it/s]  3%|▎         | 12/437 [00:00<00:08, 50.81it/s]  4%|▍         | 18/437 [00:00<00:08, 48.95it/s]  5%|▌         | 23/437 [00:00<00:08, 48.10it/s]  6%|▋         | 28/437 [00:00<00:08, 47.76it/s]  8%|▊         | 33/437 [00:00<00:08, 47.51it/s]  9%|▊         | 38/437 [00:00<00:08, 47.31it/s] 10%|▉         | 43/437 [00:00<00:08, 47.09it/s] 11%|█         | 48/437 [00:01<00:08, 46.94it/s] 12%|█▏        | 53/437 [00:01<00:08, 46.91it/s] 13%|█▎        | 58/437 [00:01<00:08, 46.94it/s] 14%|█▍        | 63/437 [00:01<00:07, 46.75it/s] 16%|█▌        | 68/437 [00:01<00:07, 46.74it/s] 17%|█▋        | 73/437 [00:01<00:07, 46.68it/s] 18%|█▊        | 78/437 [00:01<00:07, 46.73it/s] 19%|█▉        | 83/437 [00:01<00:07, 46.80it/s] 20%|██        | 88/437 [00:01<00:07, 46.83it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.83it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.63it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.66it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.71it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.68it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.75it/s] 28%|██▊       | 123/437 [00:02<00:06, 46.71it/s] 29%|██▉       | 128/437 [00:02<00:06, 46.75it/s] 30%|███       | 133/437 [00:02<00:06, 46.83it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.76it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.80it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.40it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.63it/s] 36%|███▌      | 158/437 [00:03<00:05, 46.66it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.65it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.66it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.69it/s] 41%|████      | 178/437 [00:03<00:05, 46.73it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.78it/s] 43%|████▎     | 188/437 [00:03<00:05, 46.74it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.65it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.69it/s] 46%|████▋     | 203/437 [00:04<00:05, 46.63it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.65it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.65it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.67it/s] 51%|█████     | 223/437 [00:04<00:04, 46.74it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.80it/s] 53%|█████▎    | 233/437 [00:04<00:04, 46.75it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.58it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.57it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.63it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.64it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.57it/s] 60%|██████    | 263/437 [00:05<00:03, 46.61it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.63it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.75it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.76it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.71it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.69it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.65it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.69it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.62it/s] 70%|███████   | 308/437 [00:06<00:02, 46.62it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.69it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.59it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.65it/s] 75%|███████▌  | 328/437 [00:06<00:02, 46.73it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.67it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.70it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.72it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.50it/s] 81%|████████  | 353/437 [00:07<00:01, 46.53it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.56it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.60it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.66it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.54it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.62it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.59it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.58it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.51it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.38it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.16it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.39it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.44it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.50it/s] 97%|█████████▋| 423/437 [00:09<00:00, 46.59it/s] 98%|█████████▊| 428/437 [00:09<00:00, 46.58it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.56it/s]100%|██████████| 437/437 [00:09<00:00, 46.76it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:20:34,797 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:34,797 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:34,797 >>   eval_loss               =     0.9556
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:34,798 >>   eval_runtime            = 0:00:09.36
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:34,798 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:34,798 >>   eval_samples_per_second =    372.907
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:34,798 >>   eval_steps_per_second   =     46.653
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:20:34,798 >>   perplexity              =     2.6002
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:42,051 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:42,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:42,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:42,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:42,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:20:42,651 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:20:42,653 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:20:43,236 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:20:44,265 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:20:44,265 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,123 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,123 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,123 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:47,124 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:20:47,759 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:20:47,760 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:20:48,335 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:20:48,471 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:20:48,471 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:06,  1.53it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:13,  1.51it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.64it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:18,  1.59it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:20,  1.50it/s]Extractor Predicting: 34it [00:21,  1.49it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:22,  1.49it/s]Extractor Predicting: 37it [00:23,  1.48it/s]Extractor Predicting: 38it [00:24,  1.47it/s]Extractor Predicting: 39it [00:24,  1.50it/s]Extractor Predicting: 40it [00:25,  1.50it/s]Extractor Predicting: 41it [00:26,  1.49it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.49it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.44it/s]Extractor Predicting: 46it [00:29,  1.44it/s]Extractor Predicting: 47it [00:30,  1.43it/s]Extractor Predicting: 48it [00:31,  1.45it/s]Extractor Predicting: 49it [00:31,  1.45it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:33,  1.47it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.49it/s]Extractor Predicting: 55it [00:35,  1.47it/s]Extractor Predicting: 56it [00:36,  1.47it/s]Extractor Predicting: 57it [00:37,  1.45it/s]Extractor Predicting: 58it [00:37,  1.45it/s]Extractor Predicting: 59it [00:38,  1.44it/s]Extractor Predicting: 60it [00:39,  1.45it/s]Extractor Predicting: 61it [00:39,  1.47it/s]Extractor Predicting: 62it [00:40,  1.46it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.50it/s]Extractor Predicting: 66it [00:43,  1.53it/s]Extractor Predicting: 67it [00:43,  1.51it/s]Extractor Predicting: 68it [00:44,  1.50it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:45,  1.48it/s]Extractor Predicting: 71it [00:46,  1.49it/s]Extractor Predicting: 72it [00:47,  1.50it/s]Extractor Predicting: 73it [00:47,  1.50it/s]Extractor Predicting: 74it [00:48,  1.52it/s]Extractor Predicting: 75it [00:49,  1.54it/s]Extractor Predicting: 76it [00:49,  1.51it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.49it/s]Extractor Predicting: 79it [00:51,  1.49it/s]Extractor Predicting: 80it [00:52,  1.48it/s]Extractor Predicting: 81it [00:53,  1.53it/s]Extractor Predicting: 82it [00:53,  1.51it/s]Extractor Predicting: 83it [00:54,  1.49it/s]Extractor Predicting: 84it [00:55,  1.50it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.58it/s]Extractor Predicting: 87it [00:57,  1.61it/s]Extractor Predicting: 88it [00:57,  1.61it/s]Extractor Predicting: 89it [00:58,  1.61it/s]Extractor Predicting: 90it [00:58,  1.63it/s]Extractor Predicting: 91it [00:59,  1.60it/s]Extractor Predicting: 92it [01:00,  1.58it/s]Extractor Predicting: 93it [01:00,  1.61it/s]Extractor Predicting: 94it [01:01,  1.63it/s]Extractor Predicting: 95it [01:01,  1.62it/s]Extractor Predicting: 96it [01:02,  1.63it/s]Extractor Predicting: 97it [01:03,  1.61it/s]Extractor Predicting: 98it [01:03,  1.57it/s]Extractor Predicting: 99it [01:04,  1.55it/s]Extractor Predicting: 100it [01:05,  1.57it/s]Extractor Predicting: 101it [01:05,  1.60it/s]Extractor Predicting: 102it [01:06,  1.59it/s]Extractor Predicting: 103it [01:07,  1.59it/s]Extractor Predicting: 104it [01:07,  1.58it/s]Extractor Predicting: 105it [01:08,  1.62it/s]Extractor Predicting: 106it [01:08,  1.59it/s]Extractor Predicting: 107it [01:09,  1.60it/s]Extractor Predicting: 108it [01:10,  1.60it/s]Extractor Predicting: 109it [01:10,  1.63it/s]Extractor Predicting: 110it [01:11,  1.62it/s]Extractor Predicting: 111it [01:12,  1.60it/s]Extractor Predicting: 112it [01:12,  1.58it/s]Extractor Predicting: 113it [01:13,  1.58it/s]Extractor Predicting: 114it [01:14,  1.54it/s]Extractor Predicting: 115it [01:14,  1.55it/s]Extractor Predicting: 116it [01:15,  1.54it/s]Extractor Predicting: 117it [01:15,  1.53it/s]Extractor Predicting: 118it [01:16,  1.59it/s]Extractor Predicting: 119it [01:17,  1.58it/s]Extractor Predicting: 120it [01:17,  1.59it/s]Extractor Predicting: 121it [01:18,  1.58it/s]Extractor Predicting: 122it [01:19,  1.56it/s]Extractor Predicting: 123it [01:19,  1.55it/s]Extractor Predicting: 124it [01:20,  1.52it/s]Extractor Predicting: 125it [01:21,  1.52it/s]Extractor Predicting: 126it [01:21,  1.41it/s]Extractor Predicting: 127it [01:22,  1.43it/s]Extractor Predicting: 128it [01:23,  1.45it/s]Extractor Predicting: 129it [01:23,  1.43it/s]Extractor Predicting: 130it [01:24,  1.45it/s]Extractor Predicting: 131it [01:25,  1.47it/s]Extractor Predicting: 132it [01:25,  1.51it/s]Extractor Predicting: 133it [01:26,  1.51it/s]Extractor Predicting: 134it [01:27,  1.50it/s]Extractor Predicting: 135it [01:27,  1.50it/s]Extractor Predicting: 136it [01:28,  1.50it/s]Extractor Predicting: 137it [01:29,  1.50it/s]Extractor Predicting: 138it [01:29,  1.50it/s]Extractor Predicting: 139it [01:30,  1.49it/s]Extractor Predicting: 140it [01:31,  1.54it/s]Extractor Predicting: 141it [01:31,  1.52it/s]Extractor Predicting: 142it [01:32,  1.59it/s]Extractor Predicting: 142it [01:32,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:28,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:28,061 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:28,062 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:28,062 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:28,062 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:22:28,352 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:22:28,353 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:22:28,615 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:22:29,647 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:22:29,648 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:30,961 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:30,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:30,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:30,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:22:30,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:22:31,277 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:22:31,280 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:22:31,544 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:22:31,700 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:22:31,700 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.27921623512946114,
  "recall": 0.11422845691382766,
  "score": 0.16212921576594883,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.50it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:08,  1.54it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.57it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.48it/s]Extractor Predicting: 31it [00:20,  1.47it/s]Extractor Predicting: 32it [00:20,  1.47it/s]Extractor Predicting: 33it [00:21,  1.49it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.53it/s]Extractor Predicting: 49it [00:31,  1.53it/s]Extractor Predicting: 50it [00:32,  1.50it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:33,  1.51it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:35,  1.52it/s]Extractor Predicting: 55it [00:35,  1.48it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:38,  1.56it/s]Extractor Predicting: 60it [00:39,  1.55it/s]Extractor Predicting: 61it [00:39,  1.56it/s]Extractor Predicting: 62it [00:40,  1.55it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 64it [00:41,  1.51it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:43,  1.52it/s]Extractor Predicting: 68it [00:44,  1.57it/s]Extractor Predicting: 69it [00:44,  1.56it/s]Extractor Predicting: 70it [00:45,  1.55it/s]Extractor Predicting: 71it [00:46,  1.54it/s]Extractor Predicting: 72it [00:46,  1.55it/s]Extractor Predicting: 73it [00:47,  1.50it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:48,  1.52it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:50,  1.51it/s]Extractor Predicting: 79it [00:51,  1.51it/s]Extractor Predicting: 80it [00:52,  1.52it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:54,  1.51it/s]Extractor Predicting: 84it [00:54,  1.52it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:56,  1.51it/s]Extractor Predicting: 88it [00:57,  1.48it/s]Extractor Predicting: 89it [00:58,  1.49it/s]Extractor Predicting: 90it [00:58,  1.51it/s]Extractor Predicting: 91it [00:59,  1.49it/s]Extractor Predicting: 92it [01:00,  1.49it/s]Extractor Predicting: 93it [01:00,  1.47it/s]Extractor Predicting: 94it [01:01,  1.45it/s]Extractor Predicting: 95it [01:02,  1.34it/s]Extractor Predicting: 96it [01:03,  1.37it/s]Extractor Predicting: 97it [01:03,  1.41it/s]Extractor Predicting: 98it [01:04,  1.42it/s]Extractor Predicting: 99it [01:05,  1.45it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:06,  1.46it/s]Extractor Predicting: 102it [01:07,  1.46it/s]Extractor Predicting: 103it [01:07,  1.48it/s]Extractor Predicting: 104it [01:08,  1.47it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:09,  1.48it/s]Extractor Predicting: 107it [01:10,  1.46it/s]Extractor Predicting: 108it [01:11,  1.45it/s]Extractor Predicting: 109it [01:11,  1.47it/s]Extractor Predicting: 110it [01:12,  1.44it/s]Extractor Predicting: 111it [01:13,  1.45it/s]Extractor Predicting: 112it [01:14,  1.48it/s]Extractor Predicting: 113it [01:14,  1.51it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:15,  1.51it/s]Extractor Predicting: 116it [01:16,  1.51it/s]Extractor Predicting: 117it [01:17,  1.53it/s]Extractor Predicting: 118it [01:17,  1.54it/s]Extractor Predicting: 119it [01:18,  1.53it/s]Extractor Predicting: 120it [01:19,  1.55it/s]Extractor Predicting: 121it [01:19,  1.54it/s]Extractor Predicting: 122it [01:20,  1.59it/s]Extractor Predicting: 123it [01:21,  1.58it/s]Extractor Predicting: 124it [01:21,  1.57it/s]Extractor Predicting: 125it [01:22,  1.56it/s]Extractor Predicting: 126it [01:23,  1.54it/s]Extractor Predicting: 127it [01:23,  1.55it/s]Extractor Predicting: 128it [01:24,  1.56it/s]Extractor Predicting: 129it [01:25,  1.51it/s]Extractor Predicting: 130it [01:25,  1.53it/s]Extractor Predicting: 131it [01:26,  1.52it/s]Extractor Predicting: 132it [01:26,  1.53it/s]Extractor Predicting: 133it [01:27,  1.52it/s]Extractor Predicting: 134it [01:28,  1.51it/s]Extractor Predicting: 135it [01:28,  1.51it/s]Extractor Predicting: 136it [01:29,  1.51it/s]Extractor Predicting: 137it [01:30,  1.51it/s]Extractor Predicting: 138it [01:30,  1.52it/s]Extractor Predicting: 139it [01:31,  1.57it/s]Extractor Predicting: 140it [01:32,  1.54it/s]Extractor Predicting: 141it [01:32,  1.53it/s]Extractor Predicting: 142it [01:33,  1.57it/s]Extractor Predicting: 143it [01:34,  1.56it/s]Extractor Predicting: 144it [01:34,  1.54it/s]Extractor Predicting: 145it [01:35,  1.55it/s]Extractor Predicting: 146it [01:36,  1.54it/s]Extractor Predicting: 147it [01:36,  1.52it/s]Extractor Predicting: 148it [01:37,  1.51it/s]Extractor Predicting: 149it [01:38,  1.50it/s]Extractor Predicting: 150it [01:38,  1.52it/s]Extractor Predicting: 151it [01:39,  1.55it/s]Extractor Predicting: 152it [01:40,  1.56it/s]Extractor Predicting: 153it [01:40,  1.55it/s]Extractor Predicting: 154it [01:41,  1.53it/s]Extractor Predicting: 155it [01:42,  1.52it/s]Extractor Predicting: 156it [01:42,  1.52it/s]Extractor Predicting: 157it [01:43,  1.50it/s]Extractor Predicting: 158it [01:44,  1.50it/s]Extractor Predicting: 159it [01:44,  1.50it/s]Extractor Predicting: 160it [01:45,  1.50it/s]Extractor Predicting: 161it [01:45,  1.53it/s]Extractor Predicting: 162it [01:46,  1.54it/s]Extractor Predicting: 163it [01:47,  1.53it/s]Extractor Predicting: 164it [01:47,  1.54it/s]Extractor Predicting: 165it [01:48,  1.51it/s]Extractor Predicting: 166it [01:49,  1.52it/s]Extractor Predicting: 167it [01:49,  1.55it/s]Extractor Predicting: 168it [01:50,  1.54it/s]Extractor Predicting: 169it [01:51,  1.53it/s]Extractor Predicting: 170it [01:51,  1.50it/s]Extractor Predicting: 171it [01:52,  1.47it/s]Extractor Predicting: 172it [01:53,  1.35it/s]Extractor Predicting: 173it [01:54,  1.39it/s]Extractor Predicting: 174it [01:54,  1.38it/s]Extractor Predicting: 175it [01:55,  1.37it/s]Extractor Predicting: 176it [01:56,  1.40it/s]Extractor Predicting: 177it [01:57,  1.41it/s]Extractor Predicting: 178it [01:57,  1.46it/s]Extractor Predicting: 179it [01:58,  1.46it/s]Extractor Predicting: 180it [01:58,  1.51it/s]Extractor Predicting: 181it [01:59,  1.50it/s]Extractor Predicting: 182it [02:00,  1.52it/s]Extractor Predicting: 183it [02:00,  1.52it/s]Extractor Predicting: 184it [02:01,  1.55it/s]Extractor Predicting: 185it [02:02,  1.57it/s]Extractor Predicting: 186it [02:02,  1.57it/s]Extractor Predicting: 187it [02:03,  1.58it/s]Extractor Predicting: 188it [02:04,  1.57it/s]Extractor Predicting: 189it [02:04,  1.57it/s]Extractor Predicting: 190it [02:05,  1.54it/s]Extractor Predicting: 191it [02:06,  1.50it/s]Extractor Predicting: 192it [02:06,  1.52it/s]Extractor Predicting: 193it [02:07,  1.56it/s]Extractor Predicting: 194it [02:07,  1.55it/s]Extractor Predicting: 195it [02:08,  1.55it/s]Extractor Predicting: 196it [02:09,  1.56it/s]Extractor Predicting: 197it [02:09,  1.57it/s]Extractor Predicting: 198it [02:10,  1.54it/s]Extractor Predicting: 199it [02:11,  1.55it/s]Extractor Predicting: 200it [02:11,  1.54it/s]Extractor Predicting: 201it [02:12,  1.54it/s]Extractor Predicting: 202it [02:13,  1.57it/s]Extractor Predicting: 203it [02:13,  1.58it/s]Extractor Predicting: 204it [02:14,  1.57it/s]Extractor Predicting: 205it [02:15,  1.51it/s]Extractor Predicting: 206it [02:15,  1.50it/s]Extractor Predicting: 207it [02:16,  1.53it/s]Extractor Predicting: 208it [02:17,  1.54it/s]Extractor Predicting: 209it [02:17,  1.49it/s]Extractor Predicting: 210it [02:18,  1.47it/s]Extractor Predicting: 211it [02:19,  1.47it/s]Extractor Predicting: 212it [02:19,  1.49it/s]Extractor Predicting: 213it [02:20,  1.52it/s]Extractor Predicting: 214it [02:21,  1.48it/s]Extractor Predicting: 215it [02:21,  1.49it/s]Extractor Predicting: 216it [02:22,  1.51it/s]Extractor Predicting: 217it [02:23,  1.53it/s]Extractor Predicting: 218it [02:23,  1.47it/s]Extractor Predicting: 219it [02:24,  1.48it/s]Extractor Predicting: 220it [02:25,  1.49it/s]Extractor Predicting: 221it [02:25,  1.45it/s]Extractor Predicting: 222it [02:26,  1.46it/s]Extractor Predicting: 223it [02:27,  1.47it/s]Extractor Predicting: 224it [02:27,  1.51it/s]Extractor Predicting: 225it [02:28,  1.50it/s]Extractor Predicting: 226it [02:29,  1.54it/s]Extractor Predicting: 227it [02:29,  1.57it/s]Extractor Predicting: 228it [02:30,  1.53it/s]Extractor Predicting: 229it [02:31,  1.53it/s]Extractor Predicting: 230it [02:31,  1.51it/s]Extractor Predicting: 231it [02:32,  1.50it/s]Extractor Predicting: 232it [02:33,  1.50it/s]Extractor Predicting: 233it [02:33,  1.54it/s]Extractor Predicting: 234it [02:34,  1.52it/s]Extractor Predicting: 235it [02:35,  1.53it/s]Extractor Predicting: 236it [02:35,  1.50it/s]Extractor Predicting: 237it [02:36,  1.50it/s]Extractor Predicting: 238it [02:37,  1.53it/s]Extractor Predicting: 239it [02:37,  1.54it/s]Extractor Predicting: 240it [02:38,  1.53it/s]Extractor Predicting: 241it [02:38,  1.52it/s]Extractor Predicting: 242it [02:39,  1.50it/s]Extractor Predicting: 243it [02:40,  1.47it/s]Extractor Predicting: 244it [02:41,  1.48it/s]Extractor Predicting: 245it [02:41,  1.53it/s]Extractor Predicting: 246it [02:42,  1.51it/s]Extractor Predicting: 247it [02:42,  1.53it/s]Extractor Predicting: 248it [02:43,  1.53it/s]Extractor Predicting: 249it [02:44,  1.53it/s]Extractor Predicting: 250it [02:44,  1.52it/s]Extractor Predicting: 251it [02:45,  1.49it/s]Extractor Predicting: 252it [02:46,  1.49it/s]Extractor Predicting: 253it [02:46,  1.50it/s]Extractor Predicting: 254it [02:47,  1.52it/s]Extractor Predicting: 255it [02:48,  1.51it/s]Extractor Predicting: 256it [02:48,  1.50it/s]Extractor Predicting: 257it [02:49,  1.52it/s]Extractor Predicting: 258it [02:50,  1.51it/s]Extractor Predicting: 259it [02:50,  1.47it/s]Extractor Predicting: 260it [02:51,  1.49it/s]Extractor Predicting: 261it [02:52,  1.51it/s]Extractor Predicting: 262it [02:52,  1.50it/s]Extractor Predicting: 263it [02:53,  1.48it/s]Extractor Predicting: 264it [02:54,  1.48it/s]Extractor Predicting: 265it [02:55,  1.47it/s]Extractor Predicting: 266it [02:55,  1.45it/s]Extractor Predicting: 267it [02:56,  1.44it/s]Extractor Predicting: 268it [02:57,  1.46it/s]Extractor Predicting: 269it [02:57,  1.46it/s]Extractor Predicting: 270it [02:58,  1.45it/s]Extractor Predicting: 271it [02:59,  1.46it/s]Extractor Predicting: 272it [02:59,  1.47it/s]Extractor Predicting: 273it [03:00,  1.45it/s]Extractor Predicting: 274it [03:01,  1.45it/s]Extractor Predicting: 275it [03:01,  1.49it/s]Extractor Predicting: 276it [03:02,  1.48it/s]Extractor Predicting: 277it [03:03,  1.33it/s]Extractor Predicting: 278it [03:04,  1.37it/s]Extractor Predicting: 279it [03:04,  1.40it/s]Extractor Predicting: 280it [03:05,  1.40it/s]Extractor Predicting: 281it [03:06,  1.41it/s]Extractor Predicting: 282it [03:06,  1.43it/s]Extractor Predicting: 283it [03:07,  1.38it/s]Extractor Predicting: 284it [03:08,  1.38it/s]Extractor Predicting: 285it [03:09,  1.39it/s]Extractor Predicting: 286it [03:09,  1.39it/s]Extractor Predicting: 287it [03:10,  1.42it/s]Extractor Predicting: 288it [03:10,  1.87it/s]Extractor Predicting: 288it [03:10,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:50,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:50,751 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:50,751 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:50,751 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:50,751 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:25:51,379 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:25:51,380 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:25:51,951 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:25:53,017 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:25:53,017 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:55,901 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:55,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:55,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:55,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:55,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:25:56,526 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:25:56,527 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:25:57,083 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:25:57,249 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:25:57,249 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5174396320429283,
  "recall": 0.19596458121643198,
  "score": 0.2842703727100442,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:01,  1.82it/s]Extractor Predicting: 3it [00:01,  1.66it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:25:59,434 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:25:59,435 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:25:59,438 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:25:59,438 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:25:59,440 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:26:02,565 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:26:02,571 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:26:02,583 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:26:02,584 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:26:02,589 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:02,591 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:02,591 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:02,591 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:02,591 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:02,591 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:02,591 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5882352941176471,
  "recall": 0.09009009009009009,
  "score": 0.15625,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:26:02,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:03,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:04,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:05,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:05,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:06,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:07,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:08,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:08,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:09,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:10,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:11,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:11,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:12,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:13,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:14,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:15,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:15,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:16,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:17,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:18,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:18,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:55, 16.85s/it][WARNING|generation_utils.py:914] 2023-08-28 13:26:19,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:20,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:21,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:21,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:22,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:23,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:24,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:25,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:26,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:26,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:27,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:28,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:28,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:29,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:30,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:30,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:31,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:32,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:33,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:33,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:34,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:33, 16.45s/it][WARNING|generation_utils.py:914] 2023-08-28 13:26:35,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:36,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:37,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:38,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:38,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:39,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:40,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:41,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:42,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:42,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:43,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:44,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:45,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:46,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:47,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:47,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:48,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:49,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:50,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:50,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:51,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:52,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:52,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:53,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:51<03:28, 17.37s/it][WARNING|generation_utils.py:914] 2023-08-28 13:26:54,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:55,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:56,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:56,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:57,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:58,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:59,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:59,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:00,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:01,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:02,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:03,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:03,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:04,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:05,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:06,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:06,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:07,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:08,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:09,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:09,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:10,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:08<03:10, 17.28s/it][WARNING|generation_utils.py:914] 2023-08-28 13:27:11,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:12,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:12,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:13,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:14,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:14,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:15,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:16,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:17,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:17,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:18,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:18,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:19,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:20,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:21,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:21,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:22,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:23,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:23,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:24,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:25,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:25,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:26,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:24<02:46, 16.70s/it][WARNING|generation_utils.py:914] 2023-08-28 13:27:27,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:28,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:28,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:29,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:30,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:31,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:32,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:33,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:33,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:34,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:35,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:35,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:36,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:37,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:38,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:39,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:40,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:40,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:41,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:42,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:43,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:44,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:44,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:42<02:35, 17.30s/it][WARNING|generation_utils.py:914] 2023-08-28 13:27:45,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:46,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:46,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:47,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:48,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:49,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:49,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:50,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:51,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:52,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:53,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:53,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:54,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:55,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:56,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:57,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:58,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:58,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:59,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:00,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:01,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:02,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:00<02:18, 17.33s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:02,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:03,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:04,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:05,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:05,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:06,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:07,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:08,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:09,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:09,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:10,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:11,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:12,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:13,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:13,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:14,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:15,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:16,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:17,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:18,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:18,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:19,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:20,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:21,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:19<02:05, 17.88s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:22,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:22,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:23,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:24,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:24,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:25,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:26,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:26,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:27,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:28,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:28,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:29,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:30,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:30,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:31,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:32,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:33,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:33,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:34,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:35,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:35,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:36,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:34<01:42, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:37,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:37,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:38,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:39,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:40,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:40,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:41,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:42,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:43,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:44,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:44,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:45,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:46,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:47,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:47,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:48,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:49,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:50,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:51,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:52,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:52,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:50<01:24, 16.81s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:53,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:54,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:55,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:55,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:56,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:57,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:58,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:58,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:59,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:00,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:01,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:01,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:02,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:03,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:04,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:04,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:05,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:06,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:07,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:08,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:09,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:09,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:07<01:07, 16.88s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:10,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:11,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:11,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:12,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:13,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:14,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:14,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:15,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:16,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:17,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:18,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:19,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:19,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:20,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:21,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:22,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:22,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:23,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:24,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:24,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:25,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:26,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:27,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:25<00:51, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:27,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:28,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:29,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:30,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:31,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:31,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:32,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:33,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:34,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:35,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:35,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:36,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:37,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:37,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:38,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:39,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:40,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:41,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:41,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:42,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:43,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:41<00:33, 16.80s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:44,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:44,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:45,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:46,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:47,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:47,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:48,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:49,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:49,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:50,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:51,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:52,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:53,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:54,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:55,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:55,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:56,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:57,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:57,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:58,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:59,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:59,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:57<00:16, 16.65s/it][WARNING|generation_utils.py:914] 2023-08-28 13:30:00,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:01,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:01,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:02,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:03,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:04,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:05,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:05,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:06,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:07,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:08,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:08,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:09,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:10,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:10,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:11,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:12,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:13,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:13,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:14,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:15,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:16,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:16,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:17,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:18,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:19,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:19,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:20,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:21,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:22,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:22,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:20<00:00, 18.59s/it]Generating: 100%|██████████| 15/15 [04:20<00:00, 17.38s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:30,138 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:30,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:30,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:30,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:30,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:30:30,789 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:30:30,790 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:30:31,509 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:30:32,580 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:30:32,580 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:35,464 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:35,471 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:35,471 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:35,471 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:35,471 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:30:36,108 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:30:36,109 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:30:36,714 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:30:36,881 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:30:36,881 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.7903645833333334, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Puerto Rico United Corinthians', 'sports season of league or competition', '', 'Michael Chiaverino , born August 15 , 1991 in Pasco , Puerto Rico , plays for Puerto Rico United Corinthians .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : architect .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8721590909090909, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8315217391304348, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : operator .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8821022727272727, 'errors': {''}}
['Relation : position held . Context : On 31 March 2014 , the Romanian national football team won their first match as the champions . Head Entity : Romania national football team , Tail Entity : forward .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 108, 'raw': 160}
{'target': 600, 'success': 123, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 160, 'raw': 256}
{'target': 600, 'success': 178, 'raw': 288}
{'target': 600, 'success': 199, 'raw': 320}
{'target': 600, 'success': 220, 'raw': 352}
{'target': 600, 'success': 238, 'raw': 384}
{'target': 600, 'success': 259, 'raw': 416}
{'target': 600, 'success': 280, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 324, 'raw': 512}
{'target': 600, 'success': 349, 'raw': 544}
{'target': 600, 'success': 361, 'raw': 576}
{'target': 600, 'success': 385, 'raw': 608}
{'target': 600, 'success': 406, 'raw': 640}
{'target': 600, 'success': 426, 'raw': 672}
{'target': 600, 'success': 445, 'raw': 704}
{'target': 600, 'success': 466, 'raw': 736}
{'target': 600, 'success': 485, 'raw': 768}
{'target': 600, 'success': 503, 'raw': 800}
{'target': 600, 'success': 525, 'raw': 832}
{'target': 600, 'success': 541, 'raw': 864}
{'target': 600, 'success': 558, 'raw': 896}
{'target': 600, 'success': 577, 'raw': 928}
{'target': 600, 'success': 592, 'raw': 960}
{'target': 600, 'success': 616, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6209677419354839, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 13304
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13404, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.42it/s]Extractor Estimating: 2it [00:01,  1.42it/s]Extractor Estimating: 3it [00:02,  1.50it/s]Extractor Estimating: 4it [00:02,  1.49it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.59it/s]Extractor Estimating: 7it [00:04,  1.55it/s]Extractor Estimating: 8it [00:05,  1.60it/s]Extractor Estimating: 9it [00:05,  1.55it/s]Extractor Estimating: 10it [00:06,  1.53it/s]Extractor Estimating: 11it [00:07,  1.51it/s]Extractor Estimating: 12it [00:07,  1.51it/s]Extractor Estimating: 13it [00:08,  1.49it/s]Extractor Estimating: 14it [00:09,  1.40it/s]Extractor Estimating: 15it [00:10,  1.39it/s]Extractor Estimating: 16it [00:10,  1.37it/s]Extractor Estimating: 17it [00:11,  1.42it/s]Extractor Estimating: 18it [00:12,  1.44it/s]Extractor Estimating: 19it [00:12,  1.45it/s]Extractor Estimating: 20it [00:13,  1.44it/s]Extractor Estimating: 21it [00:14,  1.46it/s]Extractor Estimating: 22it [00:14,  1.45it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:16,  1.53it/s]Extractor Estimating: 25it [00:16,  1.55it/s]Extractor Estimating: 26it [00:17,  1.56it/s]Extractor Estimating: 27it [00:18,  1.53it/s]Extractor Estimating: 28it [00:18,  1.52it/s]Extractor Estimating: 29it [00:19,  1.54it/s]Extractor Estimating: 30it [00:19,  1.58it/s]Extractor Estimating: 31it [00:20,  1.56it/s]Extractor Estimating: 32it [00:21,  1.51it/s]Extractor Estimating: 33it [00:22,  1.50it/s]Extractor Estimating: 34it [00:22,  1.44it/s]Extractor Estimating: 35it [00:23,  1.44it/s]Extractor Estimating: 36it [00:24,  1.51it/s]Extractor Estimating: 37it [00:24,  1.56it/s]Extractor Estimating: 38it [00:25,  1.59it/s]Extractor Estimating: 39it [00:25,  1.60it/s]Extractor Estimating: 40it [00:26,  1.59it/s]Extractor Estimating: 41it [00:27,  1.59it/s]Extractor Estimating: 42it [00:27,  1.57it/s]Extractor Estimating: 43it [00:28,  1.57it/s]Extractor Estimating: 44it [00:29,  1.53it/s]Extractor Estimating: 45it [00:29,  1.51it/s]Extractor Estimating: 46it [00:30,  1.47it/s]Extractor Estimating: 47it [00:31,  1.50it/s]Extractor Estimating: 48it [00:31,  1.51it/s]Extractor Estimating: 49it [00:32,  1.52it/s]Extractor Estimating: 50it [00:33,  1.49it/s]Extractor Estimating: 51it [00:33,  1.46it/s]Extractor Estimating: 52it [00:34,  1.42it/s]Extractor Estimating: 53it [00:35,  1.44it/s]Extractor Estimating: 54it [00:36,  1.39it/s]Extractor Estimating: 55it [00:36,  1.47it/s]Extractor Estimating: 56it [00:37,  1.45it/s]Extractor Estimating: 57it [00:39,  1.07s/it]Extractor Estimating: 58it [00:40,  1.06it/s]Extractor Estimating: 59it [00:40,  1.20it/s]Extractor Estimating: 60it [00:41,  1.27it/s]Extractor Estimating: 61it [00:41,  1.30it/s]Extractor Estimating: 62it [00:42,  1.30it/s]Extractor Estimating: 63it [00:43,  1.31it/s]Extractor Estimating: 64it [00:44,  1.37it/s]Extractor Estimating: 65it [00:44,  1.39it/s]Extractor Estimating: 66it [00:45,  1.43it/s]Extractor Estimating: 67it [00:46,  1.46it/s]Extractor Estimating: 68it [00:46,  1.47it/s]Extractor Estimating: 69it [00:47,  1.46it/s]Extractor Estimating: 70it [00:48,  1.44it/s]Extractor Estimating: 71it [00:48,  1.47it/s]Extractor Estimating: 72it [00:49,  1.43it/s]Extractor Estimating: 73it [00:50,  1.46it/s]Extractor Estimating: 74it [00:50,  1.49it/s]Extractor Estimating: 75it [00:51,  1.52it/s]Extractor Estimating: 76it [00:52,  1.52it/s]Extractor Estimating: 77it [00:52,  1.57it/s]Extractor Estimating: 78it [00:53,  1.56it/s]Extractor Estimating: 79it [00:54,  1.57it/s]Extractor Estimating: 80it [00:54,  1.62it/s]Extractor Estimating: 81it [00:55,  1.63it/s]Extractor Estimating: 82it [00:55,  1.68it/s]Extractor Estimating: 83it [00:56,  1.67it/s]Extractor Estimating: 84it [00:56,  1.68it/s]Extractor Estimating: 85it [00:57,  1.69it/s]Extractor Estimating: 86it [00:58,  1.65it/s]Extractor Estimating: 87it [00:58,  1.65it/s]Extractor Estimating: 88it [00:59,  1.64it/s]Extractor Estimating: 89it [01:00,  1.63it/s]Extractor Estimating: 90it [01:00,  1.64it/s]Extractor Estimating: 91it [01:01,  1.70it/s]Extractor Estimating: 92it [01:01,  1.64it/s]Extractor Estimating: 93it [01:02,  1.64it/s]Extractor Estimating: 94it [01:03,  1.65it/s]Extractor Estimating: 95it [01:03,  1.63it/s]Extractor Estimating: 96it [01:04,  1.48it/s]Extractor Estimating: 97it [01:05,  1.53it/s]Extractor Estimating: 98it [01:05,  1.54it/s]Extractor Estimating: 99it [01:06,  1.51it/s]Extractor Estimating: 100it [01:07,  1.56it/s]Extractor Estimating: 101it [01:07,  1.62it/s]Extractor Estimating: 102it [01:08,  1.61it/s]Extractor Estimating: 103it [01:08,  1.58it/s]Extractor Estimating: 104it [01:09,  1.51it/s]Extractor Estimating: 105it [01:10,  1.61it/s]Extractor Estimating: 106it [01:10,  1.62it/s]Extractor Estimating: 107it [01:11,  1.61it/s]Extractor Estimating: 108it [01:11,  1.64it/s]Extractor Estimating: 109it [01:12,  1.65it/s]Extractor Estimating: 110it [01:13,  1.67it/s]Extractor Estimating: 111it [01:13,  1.65it/s]Extractor Estimating: 112it [01:14,  1.69it/s]Extractor Estimating: 113it [01:14,  1.68it/s]Extractor Estimating: 114it [01:15,  1.62it/s]Extractor Estimating: 115it [01:16,  1.63it/s]Extractor Estimating: 116it [01:16,  1.63it/s]Extractor Estimating: 117it [01:17,  1.66it/s]Extractor Estimating: 118it [01:18,  1.61it/s]Extractor Estimating: 119it [01:18,  1.66it/s]Extractor Estimating: 120it [01:19,  1.61it/s]Extractor Estimating: 121it [01:19,  1.63it/s]Extractor Estimating: 122it [01:20,  1.67it/s]Extractor Estimating: 123it [01:21,  1.63it/s]Extractor Estimating: 124it [01:21,  1.67it/s]Extractor Estimating: 125it [01:22,  1.69it/s]Extractor Estimating: 126it [01:22,  1.60it/s]Extractor Estimating: 127it [01:23,  1.57it/s]Extractor Estimating: 128it [01:24,  1.61it/s]Extractor Estimating: 129it [01:24,  1.67it/s]Extractor Estimating: 130it [01:25,  1.58it/s]Extractor Estimating: 131it [01:26,  1.60it/s]Extractor Estimating: 132it [01:26,  1.63it/s]Extractor Estimating: 133it [01:27,  1.64it/s]Extractor Estimating: 134it [01:27,  1.63it/s]Extractor Estimating: 135it [01:28,  1.59it/s]Extractor Estimating: 136it [01:29,  1.57it/s]Extractor Estimating: 137it [01:29,  1.61it/s]Extractor Estimating: 138it [01:30,  1.65it/s]Extractor Estimating: 139it [01:30,  1.59it/s]Extractor Estimating: 140it [01:31,  1.61it/s]Extractor Estimating: 141it [01:32,  1.63it/s]Extractor Estimating: 142it [01:32,  1.65it/s]Extractor Estimating: 143it [01:33,  1.59it/s]Extractor Estimating: 144it [01:34,  1.59it/s]Extractor Estimating: 145it [01:34,  1.53it/s]Extractor Estimating: 146it [01:35,  1.51it/s]Extractor Estimating: 147it [01:36,  1.56it/s]Extractor Estimating: 148it [01:36,  1.56it/s]Extractor Estimating: 149it [01:37,  1.60it/s]Extractor Estimating: 150it [01:37,  1.58it/s]Extractor Estimating: 151it [01:38,  1.60it/s]Extractor Estimating: 152it [01:39,  1.56it/s]Extractor Estimating: 153it [01:39,  1.59it/s]Extractor Estimating: 154it [01:40,  1.58it/s]Extractor Estimating: 155it [01:41,  1.58it/s]Extractor Estimating: 156it [01:41,  1.62it/s]Extractor Estimating: 157it [01:42,  1.59it/s]Extractor Estimating: 158it [01:42,  1.60it/s]Extractor Estimating: 159it [01:43,  1.61it/s]Extractor Estimating: 160it [01:44,  1.56it/s]Extractor Estimating: 161it [01:44,  1.57it/s]Extractor Estimating: 162it [01:45,  1.59it/s]Extractor Estimating: 163it [01:46,  1.47it/s]Extractor Estimating: 164it [01:46,  1.53it/s]Extractor Estimating: 165it [01:47,  1.54it/s]Extractor Estimating: 166it [01:48,  1.52it/s]Extractor Estimating: 167it [01:48,  1.54it/s]Extractor Estimating: 168it [01:49,  1.47it/s]Extractor Estimating: 169it [01:50,  1.49it/s]Extractor Estimating: 170it [01:50,  1.52it/s]Extractor Estimating: 171it [01:51,  1.54it/s]Extractor Estimating: 172it [01:52,  1.52it/s]Extractor Estimating: 173it [01:52,  1.53it/s]Extractor Estimating: 174it [01:53,  1.55it/s]Extractor Estimating: 175it [01:54,  1.60it/s]Extractor Estimating: 176it [01:54,  1.62it/s]Extractor Estimating: 177it [01:55,  1.61it/s]Extractor Estimating: 178it [01:55,  1.57it/s]Extractor Estimating: 179it [01:56,  1.59it/s]Extractor Estimating: 180it [01:57,  1.58it/s]Extractor Estimating: 181it [01:57,  1.52it/s]Extractor Estimating: 182it [01:58,  1.46it/s]Extractor Estimating: 183it [01:59,  1.49it/s]Extractor Estimating: 184it [01:59,  1.52it/s]Extractor Estimating: 185it [02:00,  1.51it/s]Extractor Estimating: 186it [02:01,  1.53it/s]Extractor Estimating: 187it [02:01,  1.54it/s]Extractor Estimating: 188it [02:02,  1.58it/s]Extractor Estimating: 189it [02:03,  1.58it/s]Extractor Estimating: 190it [02:03,  1.56it/s]Extractor Estimating: 191it [02:04,  1.56it/s]Extractor Estimating: 192it [02:05,  1.50it/s]Extractor Estimating: 193it [02:05,  1.54it/s]Extractor Estimating: 194it [02:06,  1.50it/s]Extractor Estimating: 195it [02:07,  1.47it/s]Extractor Estimating: 196it [02:07,  1.47it/s]Extractor Estimating: 197it [02:08,  1.48it/s]Extractor Estimating: 198it [02:09,  1.52it/s]Extractor Estimating: 199it [02:09,  1.56it/s]Extractor Estimating: 200it [02:10,  1.54it/s]Extractor Estimating: 201it [02:11,  1.54it/s]Extractor Estimating: 202it [02:11,  1.56it/s]Extractor Estimating: 203it [02:12,  1.46it/s]Extractor Estimating: 204it [02:13,  1.49it/s]Extractor Estimating: 205it [02:13,  1.50it/s]Extractor Estimating: 206it [02:14,  1.49it/s]Extractor Estimating: 207it [02:15,  1.51it/s]Extractor Estimating: 208it [02:15,  1.51it/s]Extractor Estimating: 209it [02:16,  1.50it/s]Extractor Estimating: 210it [02:17,  1.50it/s]Extractor Estimating: 211it [02:17,  1.48it/s]Extractor Estimating: 212it [02:18,  1.51it/s]Extractor Estimating: 213it [02:19,  1.55it/s]Extractor Estimating: 214it [02:19,  1.59it/s]Extractor Estimating: 215it [02:20,  1.52it/s]Extractor Estimating: 216it [02:20,  1.52it/s]Extractor Estimating: 217it [02:21,  1.44it/s]Extractor Estimating: 218it [02:22,  1.49it/s]Extractor Estimating: 219it [02:22,  1.52it/s]Extractor Estimating: 220it [02:23,  1.55it/s]Extractor Estimating: 221it [02:24,  1.56it/s]Extractor Estimating: 222it [02:25,  1.48it/s]Extractor Estimating: 223it [02:25,  1.46it/s]Extractor Estimating: 224it [02:26,  1.49it/s]Extractor Estimating: 225it [02:26,  1.50it/s]Extractor Estimating: 226it [02:27,  1.51it/s]Extractor Estimating: 227it [02:28,  1.47it/s]Extractor Estimating: 228it [02:29,  1.46it/s]Extractor Estimating: 229it [02:29,  1.43it/s]Extractor Estimating: 230it [02:30,  1.45it/s]Extractor Estimating: 231it [02:31,  1.49it/s]Extractor Estimating: 232it [02:31,  1.44it/s]Extractor Estimating: 233it [02:32,  1.44it/s]Extractor Estimating: 234it [02:33,  1.41it/s]Extractor Estimating: 235it [02:33,  1.41it/s]Extractor Estimating: 236it [02:34,  1.39it/s]Extractor Estimating: 237it [02:35,  1.38it/s]Extractor Estimating: 238it [02:36,  1.32it/s]Extractor Estimating: 239it [02:37,  1.25it/s]Extractor Estimating: 240it [02:37,  1.29it/s]Extractor Estimating: 241it [02:38,  1.33it/s]Extractor Estimating: 242it [02:39,  1.36it/s]Extractor Estimating: 243it [02:39,  1.40it/s]Extractor Estimating: 244it [02:40,  1.38it/s]Extractor Estimating: 245it [02:41,  1.34it/s]Extractor Estimating: 246it [02:42,  1.32it/s]Extractor Estimating: 247it [02:43,  1.35it/s]Extractor Estimating: 248it [02:43,  1.34it/s]Extractor Estimating: 249it [02:44,  1.32it/s]Extractor Estimating: 250it [02:45,  1.39it/s]Extractor Estimating: 251it [02:45,  1.49it/s]Extractor Estimating: 252it [02:46,  1.59it/s]Extractor Estimating: 253it [02:46,  1.64it/s]Extractor Estimating: 254it [02:47,  1.69it/s]Extractor Estimating: 255it [02:47,  1.71it/s]Extractor Estimating: 256it [02:48,  1.74it/s]Extractor Estimating: 257it [02:49,  1.81it/s]Extractor Estimating: 258it [02:49,  1.78it/s]Extractor Estimating: 259it [02:50,  1.75it/s]Extractor Estimating: 260it [02:50,  1.71it/s]Extractor Estimating: 261it [02:51,  1.77it/s]Extractor Estimating: 262it [02:51,  1.76it/s]Extractor Estimating: 263it [02:52,  1.79it/s]Extractor Estimating: 264it [02:53,  1.78it/s]Extractor Estimating: 265it [02:53,  1.81it/s]Extractor Estimating: 266it [02:54,  1.82it/s]Extractor Estimating: 267it [02:54,  1.79it/s]Extractor Estimating: 268it [02:55,  1.81it/s]Extractor Estimating: 269it [02:55,  1.78it/s]Extractor Estimating: 270it [02:56,  1.76it/s]Extractor Estimating: 271it [02:57,  1.69it/s]Extractor Estimating: 272it [02:57,  1.73it/s]Extractor Estimating: 273it [02:58,  1.75it/s]Extractor Estimating: 274it [02:58,  1.80it/s]Extractor Estimating: 275it [02:59,  1.77it/s]Extractor Estimating: 276it [02:59,  1.75it/s]Extractor Estimating: 277it [03:00,  1.66it/s]Extractor Estimating: 278it [03:01,  1.61it/s]Extractor Estimating: 279it [03:01,  1.59it/s]Extractor Estimating: 280it [03:02,  1.61it/s]Extractor Estimating: 281it [03:03,  1.51it/s]Extractor Estimating: 282it [03:03,  1.52it/s]Extractor Estimating: 283it [03:04,  1.55it/s]Extractor Estimating: 284it [03:05,  1.51it/s]Extractor Estimating: 285it [03:05,  1.54it/s]Extractor Estimating: 286it [03:06,  1.46it/s]Extractor Estimating: 287it [03:07,  1.50it/s]Extractor Estimating: 288it [03:07,  1.52it/s]Extractor Estimating: 289it [03:08,  1.57it/s]Extractor Estimating: 290it [03:08,  1.62it/s]Extractor Estimating: 291it [03:09,  1.59it/s]Extractor Estimating: 292it [03:10,  1.58it/s]Extractor Estimating: 293it [03:10,  1.54it/s]Extractor Estimating: 294it [03:11,  1.57it/s]Extractor Estimating: 295it [03:12,  1.54it/s]Extractor Estimating: 296it [03:12,  1.57it/s]Extractor Estimating: 297it [03:13,  1.58it/s]Extractor Estimating: 298it [03:14,  1.56it/s]Extractor Estimating: 299it [03:14,  1.48it/s]Extractor Estimating: 300it [03:15,  1.46it/s]Extractor Estimating: 301it [03:16,  1.50it/s]Extractor Estimating: 302it [03:16,  1.53it/s]Extractor Estimating: 303it [03:17,  1.53it/s]Extractor Estimating: 304it [03:18,  1.53it/s]Extractor Estimating: 305it [03:18,  1.51it/s]Extractor Estimating: 306it [03:19,  1.56it/s]Extractor Estimating: 307it [03:20,  1.53it/s]Extractor Estimating: 308it [03:20,  1.51it/s]Extractor Estimating: 309it [03:21,  1.50it/s]Extractor Estimating: 310it [03:22,  1.54it/s]Extractor Estimating: 311it [03:22,  1.51it/s]Extractor Estimating: 312it [03:23,  1.57it/s]Extractor Estimating: 313it [03:23,  1.55it/s]Extractor Estimating: 314it [03:24,  1.57it/s]Extractor Estimating: 315it [03:25,  1.55it/s]Extractor Estimating: 316it [03:25,  1.54it/s]Extractor Estimating: 317it [03:26,  1.57it/s]Extractor Estimating: 318it [03:27,  1.42it/s]Extractor Estimating: 319it [03:28,  1.41it/s]Extractor Estimating: 320it [03:28,  1.43it/s]Extractor Estimating: 321it [03:29,  1.42it/s]Extractor Estimating: 322it [03:30,  1.50it/s]Extractor Estimating: 323it [03:30,  1.52it/s]Extractor Estimating: 324it [03:31,  1.54it/s]Extractor Estimating: 325it [03:31,  1.54it/s]Extractor Estimating: 326it [03:32,  1.50it/s]Extractor Estimating: 327it [03:33,  1.49it/s]Extractor Estimating: 328it [03:34,  1.50it/s]Extractor Estimating: 329it [03:34,  1.51it/s]Extractor Estimating: 330it [03:35,  1.53it/s]Extractor Estimating: 331it [03:35,  1.54it/s]Extractor Estimating: 332it [03:36,  1.54it/s]Extractor Estimating: 333it [03:37,  1.49it/s]Extractor Estimating: 334it [03:37,  1.55it/s]Extractor Estimating: 335it [03:38,  1.50it/s]Extractor Estimating: 336it [03:39,  1.52it/s]Extractor Estimating: 337it [03:39,  1.49it/s]Extractor Estimating: 338it [03:40,  1.48it/s]Extractor Estimating: 339it [03:41,  1.42it/s]Extractor Estimating: 340it [03:42,  1.41it/s]Extractor Estimating: 341it [03:42,  1.46it/s]Extractor Estimating: 342it [03:43,  1.49it/s]Extractor Estimating: 343it [03:44,  1.49it/s]Extractor Estimating: 344it [03:44,  1.45it/s]Extractor Estimating: 345it [03:45,  1.49it/s]Extractor Estimating: 346it [03:46,  1.49it/s]Extractor Estimating: 347it [03:46,  1.50it/s]Extractor Estimating: 348it [03:47,  1.53it/s]Extractor Estimating: 349it [03:48,  1.54it/s]Extractor Estimating: 350it [03:48,  1.59it/s]Extractor Estimating: 351it [03:49,  1.58it/s]Extractor Estimating: 352it [03:49,  1.56it/s]Extractor Estimating: 353it [03:50,  1.57it/s]Extractor Estimating: 354it [03:51,  1.58it/s]Extractor Estimating: 355it [03:51,  1.55it/s]Extractor Estimating: 356it [03:52,  1.54it/s]Extractor Estimating: 357it [03:53,  1.57it/s]Extractor Estimating: 358it [03:53,  1.63it/s]Extractor Estimating: 359it [03:54,  1.60it/s]Extractor Estimating: 360it [03:54,  1.58it/s]Extractor Estimating: 361it [03:55,  1.57it/s]Extractor Estimating: 362it [03:56,  1.56it/s]Extractor Estimating: 363it [03:56,  1.59it/s]Extractor Estimating: 364it [03:57,  1.58it/s]Extractor Estimating: 365it [03:58,  1.58it/s]Extractor Estimating: 366it [03:58,  1.60it/s]Extractor Estimating: 367it [03:59,  1.58it/s]Extractor Estimating: 368it [04:00,  1.61it/s]Extractor Estimating: 369it [04:00,  1.68it/s]Extractor Estimating: 370it [04:01,  1.60it/s]Extractor Estimating: 371it [04:01,  1.51it/s]Extractor Estimating: 372it [04:02,  1.53it/s]Extractor Estimating: 373it [04:03,  1.49it/s]Extractor Estimating: 374it [04:04,  1.46it/s]Extractor Estimating: 375it [04:04,  1.52it/s]Extractor Estimating: 375it [04:04,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:55,594 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:55,597 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:55,597 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:55,597 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:55,598 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:34:56,222 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:34:56,223 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:34:56,816 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:34:57,883 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:34:57,883 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:59,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:59,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:59,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:59,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:59,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:35:00,499 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:35:00,500 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:35:01,068 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:35:01,240 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:35:01,241 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 15:53:29,377 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 15:53:29,399 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7526 mean pseudo reward: 0.9643129981866718
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 24971
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25071, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25071, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.071, loss:769.4400
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.064, loss:734.0357
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.076, loss:756.6173
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 86, avg_time 1.072, loss:725.0333
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 186, avg_time 1.062, loss:712.2844
>> valid entity prec:0.5819, rec:0.4772, f1:0.5244
>> valid relation prec:0.2310, rec:0.0756, f1:0.1139
>> valid relation with NER prec:0.2310, rec:0.0756, f1:0.1139
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 286, avg_time 2.398, loss:737.5169
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 72, avg_time 1.069, loss:700.9225
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 172, avg_time 1.071, loss:721.1191
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 272, avg_time 1.065, loss:732.8009
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 58, avg_time 1.074, loss:718.7862
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5229, rec:0.4637, f1:0.4915
>> valid relation prec:0.1767, rec:0.0578, f1:0.0871
>> valid relation with NER prec:0.1767, rec:0.0578, f1:0.0871
g_step 1100, step 158, avg_time 2.389, loss:740.0924
g_step 1200, step 258, avg_time 1.069, loss:703.3168
g_step 1300, step 44, avg_time 1.062, loss:684.3524
g_step 1400, step 144, avg_time 1.076, loss:678.7417
g_step 1500, step 244, avg_time 1.071, loss:699.3917
>> valid entity prec:0.5922, rec:0.5073, f1:0.5465
>> valid relation prec:0.2066, rec:0.0696, f1:0.1041
>> valid relation with NER prec:0.2066, rec:0.0696, f1:0.1041
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 30, avg_time 2.393, loss:670.7678
g_step 1700, step 130, avg_time 1.071, loss:651.8630
g_step 1800, step 230, avg_time 1.073, loss:662.6499
g_step 1900, step 16, avg_time 1.057, loss:615.8981
g_step 2000, step 116, avg_time 1.070, loss:606.0521
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5052, rec:0.5041, f1:0.5046
>> valid relation prec:0.1803, rec:0.0739, f1:0.1048
>> valid relation with NER prec:0.1803, rec:0.0739, f1:0.1048
g_step 2100, step 216, avg_time 2.392, loss:644.5687
g_step 2200, step 2, avg_time 1.072, loss:605.7120
g_step 2300, step 102, avg_time 1.079, loss:562.5341
g_step 2400, step 202, avg_time 1.054, loss:595.5124
g_step 2500, step 302, avg_time 1.078, loss:588.0574
>> valid entity prec:0.5597, rec:0.5192, f1:0.5387
>> valid relation prec:0.1944, rec:0.0770, f1:0.1103
>> valid relation with NER prec:0.1944, rec:0.0770, f1:0.1103
g_step 2600, step 88, avg_time 2.392, loss:561.4590
g_step 2700, step 188, avg_time 1.081, loss:561.7090
g_step 2800, step 288, avg_time 1.058, loss:575.9978
g_step 2900, step 74, avg_time 1.060, loss:536.3395
g_step 3000, step 174, avg_time 1.071, loss:526.5727
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5276, rec:0.5270, f1:0.5273
>> valid relation prec:0.1942, rec:0.0870, f1:0.1202
>> valid relation with NER prec:0.1942, rec:0.0870, f1:0.1202
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 274, avg_time 2.390, loss:546.8331
g_step 3200, step 60, avg_time 1.068, loss:518.0488
g_step 3300, step 160, avg_time 1.070, loss:516.7647
g_step 3400, step 260, avg_time 1.075, loss:505.7005
g_step 3500, step 46, avg_time 1.081, loss:497.8217
>> valid entity prec:0.5348, rec:0.4653, f1:0.4976
>> valid relation prec:0.1818, rec:0.0713, f1:0.1024
>> valid relation with NER prec:0.1818, rec:0.0713, f1:0.1024
g_step 3600, step 146, avg_time 2.397, loss:469.5651
g_step 3700, step 246, avg_time 1.063, loss:533.3928
g_step 3800, step 32, avg_time 1.061, loss:478.9242
g_step 3900, step 132, avg_time 1.070, loss:456.9995
g_step 4000, step 232, avg_time 1.062, loss:483.2469
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5384, rec:0.4869, f1:0.5114
>> valid relation prec:0.2028, rec:0.0784, f1:0.1131
>> valid relation with NER prec:0.2028, rec:0.0784, f1:0.1131
g_step 4100, step 18, avg_time 2.392, loss:495.3754
g_step 4200, step 118, avg_time 1.086, loss:435.1930
g_step 4300, step 218, avg_time 1.068, loss:444.7508
g_step 4400, step 4, avg_time 1.069, loss:475.1631
g_step 4500, step 104, avg_time 1.064, loss:413.9564
>> valid entity prec:0.5138, rec:0.5211, f1:0.5174
>> valid relation prec:0.1938, rec:0.0845, f1:0.1176
>> valid relation with NER prec:0.1938, rec:0.0845, f1:0.1176
g_step 4600, step 204, avg_time 2.390, loss:425.7175
g_step 4700, step 304, avg_time 1.076, loss:452.5649
g_step 4800, step 90, avg_time 1.072, loss:420.5409
g_step 4900, step 190, avg_time 1.081, loss:392.2909
g_step 5000, step 290, avg_time 1.056, loss:449.0197
learning rate was adjusted to 0.0008
>> valid entity prec:0.5661, rec:0.4773, f1:0.5179
>> valid relation prec:0.2089, rec:0.0741, f1:0.1094
>> valid relation with NER prec:0.2089, rec:0.0741, f1:0.1094
g_step 5100, step 76, avg_time 2.395, loss:399.6140
g_step 5200, step 176, avg_time 1.063, loss:412.0924
g_step 5300, step 276, avg_time 1.069, loss:403.7763
g_step 5400, step 62, avg_time 1.067, loss:377.1573
g_step 5500, step 162, avg_time 1.069, loss:378.8075
>> valid entity prec:0.5540, rec:0.4956, f1:0.5232
>> valid relation prec:0.1864, rec:0.0908, f1:0.1221
>> valid relation with NER prec:0.1864, rec:0.0908, f1:0.1221
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 262, avg_time 2.404, loss:391.7111
g_step 5700, step 48, avg_time 1.056, loss:372.1710
g_step 5800, step 148, avg_time 1.069, loss:379.6200
g_step 5900, step 248, avg_time 1.082, loss:370.0465
g_step 6000, step 34, avg_time 1.069, loss:358.9790
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5767, rec:0.4654, f1:0.5152
>> valid relation prec:0.1887, rec:0.0845, f1:0.1167
>> valid relation with NER prec:0.1887, rec:0.0845, f1:0.1167
g_step 6100, step 134, avg_time 2.391, loss:337.7332
g_step 6200, step 234, avg_time 1.075, loss:362.5943
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 15:53:29 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 15:53:29 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_15-53-29_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 15:53:30 - WARNING - datasets.builder -   Using custom data configuration default-f983eef22a256633
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f983eef22a256633/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 15:53:30,728 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:53:30,729 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:53:30,730 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:53:30,731 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:53:30,739 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:53:30,745 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:53:30,745 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:53:30,745 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:53:30,745 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:53:30,745 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:53:30,745 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 15:53:30,898 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:53:34,015 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 15:53:34,019 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f983eef22a256633/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.05ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.88ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.19ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.34ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.42ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.47ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.51ba/s]100%|██████████| 8/8 [00:01<00:00,  5.23ba/s]100%|██████████| 8/8 [00:01<00:00,  4.57ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.85ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.12ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.57ba/s]100%|██████████| 4/4 [00:00<00:00,  4.62ba/s]100%|██████████| 4/4 [00:00<00:00,  4.11ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.64ba/s] 25%|██▌       | 2/8 [00:00<00:00,  8.65ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.67ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.95ba/s]100%|██████████| 8/8 [00:00<00:00, 11.00ba/s]100%|██████████| 8/8 [00:00<00:00, 10.31ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.03ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.59ba/s]100%|██████████| 4/4 [00:00<00:00, 10.83ba/s]
[INFO|trainer.py:414] 2023-08-28 15:53:38,350 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 15:53:38,372 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 15:53:38,372 >>   Num examples = 7552
[INFO|trainer.py:1149] 2023-08-28 15:53:38,372 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 15:53:38,372 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 15:53:38,372 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 15:53:38,372 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 15:53:38,372 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:59,  3.28it/s]  0%|          | 2/590 [00:00<02:53,  3.40it/s]  1%|          | 3/590 [00:00<02:50,  3.44it/s]  1%|          | 4/590 [00:01<02:49,  3.46it/s]  1%|          | 5/590 [00:01<02:48,  3.47it/s]  1%|          | 6/590 [00:01<02:48,  3.47it/s]  1%|          | 7/590 [00:02<02:48,  3.47it/s]  1%|▏         | 8/590 [00:02<02:47,  3.47it/s]  2%|▏         | 9/590 [00:02<02:47,  3.47it/s]  2%|▏         | 10/590 [00:02<02:46,  3.48it/s]  2%|▏         | 11/590 [00:03<02:46,  3.48it/s]  2%|▏         | 12/590 [00:03<02:46,  3.48it/s]  2%|▏         | 13/590 [00:03<02:45,  3.48it/s]  2%|▏         | 14/590 [00:04<02:45,  3.48it/s]  3%|▎         | 15/590 [00:04<02:45,  3.48it/s]  3%|▎         | 16/590 [00:04<02:44,  3.48it/s]  3%|▎         | 17/590 [00:04<02:44,  3.48it/s]  3%|▎         | 18/590 [00:05<03:15,  2.93it/s]  3%|▎         | 19/590 [00:05<03:05,  3.08it/s]  3%|▎         | 20/590 [00:05<02:58,  3.19it/s]  4%|▎         | 21/590 [00:06<02:54,  3.27it/s]  4%|▎         | 22/590 [00:06<02:50,  3.33it/s]  4%|▍         | 23/590 [00:06<02:48,  3.37it/s]  4%|▍         | 24/590 [00:07<02:46,  3.40it/s]  4%|▍         | 25/590 [00:07<02:44,  3.43it/s]  4%|▍         | 26/590 [00:07<02:43,  3.44it/s]  5%|▍         | 27/590 [00:07<02:42,  3.45it/s]  5%|▍         | 28/590 [00:08<02:42,  3.45it/s]  5%|▍         | 29/590 [00:08<02:42,  3.46it/s]  5%|▌         | 30/590 [00:08<02:41,  3.46it/s]  5%|▌         | 31/590 [00:09<02:41,  3.47it/s]  5%|▌         | 32/590 [00:09<02:40,  3.47it/s]  6%|▌         | 33/590 [00:09<02:40,  3.47it/s]  6%|▌         | 34/590 [00:09<02:40,  3.47it/s]  6%|▌         | 35/590 [00:10<02:39,  3.47it/s]  6%|▌         | 36/590 [00:10<02:39,  3.47it/s]  6%|▋         | 37/590 [00:10<02:39,  3.47it/s]  6%|▋         | 38/590 [00:11<02:38,  3.47it/s]  7%|▋         | 39/590 [00:11<02:39,  3.45it/s]  7%|▋         | 40/590 [00:11<02:38,  3.46it/s]  7%|▋         | 41/590 [00:11<02:38,  3.47it/s]  7%|▋         | 42/590 [00:12<02:38,  3.47it/s]  7%|▋         | 43/590 [00:12<02:37,  3.47it/s]  7%|▋         | 44/590 [00:12<02:37,  3.47it/s]  8%|▊         | 45/590 [00:13<02:37,  3.47it/s]  8%|▊         | 46/590 [00:13<02:36,  3.47it/s]  8%|▊         | 47/590 [00:13<02:36,  3.47it/s]  8%|▊         | 48/590 [00:14<02:36,  3.47it/s]  8%|▊         | 49/590 [00:14<02:35,  3.47it/s]  8%|▊         | 50/590 [00:14<02:36,  3.45it/s]  9%|▊         | 51/590 [00:14<02:35,  3.46it/s]  9%|▉         | 52/590 [00:15<02:35,  3.46it/s]  9%|▉         | 53/590 [00:15<02:34,  3.47it/s]  9%|▉         | 54/590 [00:15<02:34,  3.47it/s]  9%|▉         | 55/590 [00:16<02:34,  3.47it/s]  9%|▉         | 56/590 [00:16<02:33,  3.47it/s] 10%|▉         | 57/590 [00:16<02:33,  3.47it/s] 10%|▉         | 58/590 [00:16<02:33,  3.47it/s] 10%|█         | 59/590 [00:17<02:32,  3.47it/s] 10%|█         | 60/590 [00:17<02:32,  3.47it/s] 10%|█         | 61/590 [00:17<02:32,  3.46it/s] 11%|█         | 62/590 [00:18<02:32,  3.47it/s] 11%|█         | 63/590 [00:18<02:32,  3.47it/s] 11%|█         | 64/590 [00:18<02:31,  3.47it/s] 11%|█         | 65/590 [00:18<02:31,  3.47it/s] 11%|█         | 66/590 [00:19<02:31,  3.47it/s] 11%|█▏        | 67/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 68/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 69/590 [00:20<02:30,  3.47it/s] 12%|█▏        | 70/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 71/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 72/590 [00:20<02:31,  3.42it/s] 12%|█▏        | 73/590 [00:21<02:30,  3.44it/s] 13%|█▎        | 74/590 [00:21<02:29,  3.44it/s] 13%|█▎        | 75/590 [00:21<02:29,  3.45it/s] 13%|█▎        | 76/590 [00:22<02:28,  3.45it/s] 13%|█▎        | 77/590 [00:22<02:28,  3.46it/s] 13%|█▎        | 78/590 [00:22<02:28,  3.46it/s] 13%|█▎        | 79/590 [00:22<02:27,  3.46it/s] 14%|█▎        | 80/590 [00:23<02:27,  3.46it/s] 14%|█▎        | 81/590 [00:23<02:27,  3.46it/s] 14%|█▍        | 82/590 [00:23<02:26,  3.46it/s] 14%|█▍        | 83/590 [00:24<02:26,  3.46it/s] 14%|█▍        | 84/590 [00:24<02:26,  3.46it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.46it/s] 15%|█▍        | 86/590 [00:24<02:26,  3.45it/s] 15%|█▍        | 87/590 [00:25<02:25,  3.45it/s] 15%|█▍        | 88/590 [00:25<02:25,  3.46it/s] 15%|█▌        | 89/590 [00:25<02:24,  3.46it/s] 15%|█▌        | 90/590 [00:26<02:24,  3.46it/s] 15%|█▌        | 91/590 [00:26<02:24,  3.46it/s] 16%|█▌        | 92/590 [00:26<02:23,  3.46it/s] 16%|█▌        | 93/590 [00:27<02:23,  3.46it/s] 16%|█▌        | 94/590 [00:27<02:23,  3.46it/s] 16%|█▌        | 95/590 [00:27<02:23,  3.46it/s] 16%|█▋        | 96/590 [00:27<02:22,  3.46it/s] 16%|█▋        | 97/590 [00:28<02:23,  3.45it/s] 17%|█▋        | 98/590 [00:28<02:22,  3.45it/s] 17%|█▋        | 99/590 [00:28<02:22,  3.45it/s] 17%|█▋        | 100/590 [00:29<02:21,  3.46it/s] 17%|█▋        | 101/590 [00:29<02:21,  3.46it/s] 17%|█▋        | 102/590 [00:29<02:21,  3.46it/s] 17%|█▋        | 103/590 [00:29<02:20,  3.46it/s] 18%|█▊        | 104/590 [00:30<02:20,  3.46it/s] 18%|█▊        | 105/590 [00:30<02:20,  3.46it/s] 18%|█▊        | 106/590 [00:30<02:19,  3.46it/s] 18%|█▊        | 107/590 [00:31<02:19,  3.46it/s] 18%|█▊        | 108/590 [00:31<02:19,  3.45it/s] 18%|█▊        | 109/590 [00:31<02:19,  3.45it/s] 19%|█▊        | 110/590 [00:31<02:18,  3.45it/s] 19%|█▉        | 111/590 [00:32<02:18,  3.46it/s] 19%|█▉        | 112/590 [00:32<02:18,  3.46it/s] 19%|█▉        | 113/590 [00:32<02:17,  3.46it/s] 19%|█▉        | 114/590 [00:33<02:17,  3.46it/s] 19%|█▉        | 115/590 [00:33<02:17,  3.46it/s] 20%|█▉        | 116/590 [00:33<02:17,  3.46it/s] 20%|█▉        | 117/590 [00:33<02:16,  3.46it/s] 20%|██        | 118/590 [00:34<02:16,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 15:54:12,628 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:54:12,628 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:54:12,628 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.22it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.29it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.67it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.90it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.50it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.01it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.81it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.50it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.48it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.43it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.40it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.51it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.55it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.50it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.56it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.37it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.27it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.27it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.31it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.30it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.35it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.43it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.52it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.47it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.39it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.33it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.27it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.28it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.17it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.28it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.27it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.36it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.47it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.37it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.40it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.31it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.26it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.22it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.27it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.32it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.41it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.42it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.46it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.36it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.38it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.28it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.26it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.20it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.29it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.36it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.43it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.27it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.45it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.31it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.31it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.35it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.31it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.20it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.30it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.33it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.40it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.45it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.31it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.29it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.35it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.29it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.36it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.27it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.24it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.27it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.40it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.39it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.34it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.27it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.28it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.30it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.32it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.33it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.29it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.33it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.35it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.33it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.26it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.29it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.29it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.28it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.28it/s][A 20%|██        | 118/590 [00:43<02:16,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:54:22,061 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-28 15:54:22,073 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:54:24,597 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:54:24,622 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:54:24,630 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:51<41:46,  5.32s/it] 20%|██        | 120/590 [00:51<29:51,  3.81s/it] 21%|██        | 121/590 [00:51<21:32,  2.76s/it] 21%|██        | 122/590 [00:52<15:43,  2.02s/it] 21%|██        | 123/590 [00:52<11:39,  1.50s/it] 21%|██        | 124/590 [00:52<08:48,  1.13s/it] 21%|██        | 125/590 [00:53<06:49,  1.14it/s] 21%|██▏       | 126/590 [00:53<05:26,  1.42it/s] 22%|██▏       | 127/590 [00:53<04:27,  1.73it/s] 22%|██▏       | 128/590 [00:53<03:47,  2.03it/s] 22%|██▏       | 129/590 [00:54<03:18,  2.32it/s] 22%|██▏       | 130/590 [00:54<02:58,  2.58it/s] 22%|██▏       | 131/590 [00:54<02:44,  2.79it/s] 22%|██▏       | 132/590 [00:55<02:34,  2.97it/s] 23%|██▎       | 133/590 [00:55<02:27,  3.10it/s] 23%|██▎       | 134/590 [00:55<02:23,  3.18it/s] 23%|██▎       | 135/590 [00:55<02:19,  3.26it/s] 23%|██▎       | 136/590 [00:56<02:16,  3.32it/s] 23%|██▎       | 137/590 [00:56<02:14,  3.36it/s] 23%|██▎       | 138/590 [00:56<02:13,  3.39it/s] 24%|██▎       | 139/590 [00:57<02:12,  3.41it/s] 24%|██▎       | 140/590 [00:57<02:11,  3.43it/s] 24%|██▍       | 141/590 [00:57<02:10,  3.43it/s] 24%|██▍       | 142/590 [00:57<02:10,  3.44it/s] 24%|██▍       | 143/590 [00:58<02:09,  3.45it/s] 24%|██▍       | 144/590 [00:58<02:09,  3.45it/s] 25%|██▍       | 145/590 [00:58<02:09,  3.44it/s] 25%|██▍       | 146/590 [00:59<02:08,  3.44it/s] 25%|██▍       | 147/590 [00:59<02:08,  3.45it/s] 25%|██▌       | 148/590 [00:59<02:08,  3.45it/s] 25%|██▌       | 149/590 [00:59<02:07,  3.45it/s] 25%|██▌       | 150/590 [01:00<02:07,  3.45it/s] 26%|██▌       | 151/590 [01:00<02:07,  3.46it/s] 26%|██▌       | 152/590 [01:00<02:06,  3.46it/s] 26%|██▌       | 153/590 [01:01<02:06,  3.46it/s] 26%|██▌       | 154/590 [01:01<02:06,  3.46it/s] 26%|██▋       | 155/590 [01:01<02:05,  3.46it/s] 26%|██▋       | 156/590 [01:02<02:05,  3.45it/s] 27%|██▋       | 157/590 [01:02<02:05,  3.45it/s] 27%|██▋       | 158/590 [01:02<02:05,  3.45it/s] 27%|██▋       | 159/590 [01:02<02:04,  3.45it/s] 27%|██▋       | 160/590 [01:03<02:04,  3.45it/s] 27%|██▋       | 161/590 [01:03<02:04,  3.46it/s] 27%|██▋       | 162/590 [01:03<02:03,  3.46it/s] 28%|██▊       | 163/590 [01:04<02:03,  3.46it/s] 28%|██▊       | 164/590 [01:04<02:03,  3.46it/s] 28%|██▊       | 165/590 [01:04<02:02,  3.46it/s] 28%|██▊       | 166/590 [01:04<02:02,  3.46it/s] 28%|██▊       | 167/590 [01:05<02:02,  3.44it/s] 28%|██▊       | 168/590 [01:05<02:02,  3.45it/s] 29%|██▊       | 169/590 [01:05<02:01,  3.45it/s] 29%|██▉       | 170/590 [01:06<02:01,  3.45it/s] 29%|██▉       | 171/590 [01:06<02:01,  3.45it/s] 29%|██▉       | 172/590 [01:06<02:01,  3.45it/s] 29%|██▉       | 173/590 [01:06<02:00,  3.46it/s] 29%|██▉       | 174/590 [01:07<02:00,  3.45it/s] 30%|██▉       | 175/590 [01:07<02:00,  3.46it/s] 30%|██▉       | 176/590 [01:07<01:59,  3.45it/s] 30%|███       | 177/590 [01:08<01:59,  3.45it/s] 30%|███       | 178/590 [01:08<01:59,  3.44it/s] 30%|███       | 179/590 [01:08<01:59,  3.44it/s] 31%|███       | 180/590 [01:08<01:58,  3.45it/s] 31%|███       | 181/590 [01:09<01:58,  3.45it/s] 31%|███       | 182/590 [01:09<01:58,  3.45it/s] 31%|███       | 183/590 [01:09<01:57,  3.45it/s] 31%|███       | 184/590 [01:10<01:57,  3.45it/s] 31%|███▏      | 185/590 [01:10<01:57,  3.45it/s] 32%|███▏      | 186/590 [01:10<01:57,  3.45it/s] 32%|███▏      | 187/590 [01:10<01:56,  3.45it/s] 32%|███▏      | 188/590 [01:11<01:56,  3.46it/s] 32%|███▏      | 189/590 [01:11<01:56,  3.45it/s] 32%|███▏      | 190/590 [01:11<01:56,  3.44it/s] 32%|███▏      | 191/590 [01:12<01:55,  3.45it/s] 33%|███▎      | 192/590 [01:12<01:55,  3.45it/s] 33%|███▎      | 193/590 [01:12<01:55,  3.45it/s] 33%|███▎      | 194/590 [01:13<01:54,  3.45it/s] 33%|███▎      | 195/590 [01:13<01:54,  3.45it/s] 33%|███▎      | 196/590 [01:13<01:54,  3.45it/s] 33%|███▎      | 197/590 [01:13<01:53,  3.45it/s] 34%|███▎      | 198/590 [01:14<01:53,  3.46it/s] 34%|███▎      | 199/590 [01:14<01:53,  3.46it/s] 34%|███▍      | 200/590 [01:14<01:53,  3.44it/s] 34%|███▍      | 201/590 [01:15<01:52,  3.44it/s] 34%|███▍      | 202/590 [01:15<01:52,  3.45it/s] 34%|███▍      | 203/590 [01:15<01:52,  3.45it/s] 35%|███▍      | 204/590 [01:15<01:51,  3.45it/s] 35%|███▍      | 205/590 [01:16<01:51,  3.45it/s] 35%|███▍      | 206/590 [01:16<01:51,  3.45it/s] 35%|███▌      | 207/590 [01:16<01:50,  3.45it/s] 35%|███▌      | 208/590 [01:17<01:50,  3.46it/s] 35%|███▌      | 209/590 [01:17<01:50,  3.45it/s] 36%|███▌      | 210/590 [01:17<01:50,  3.45it/s] 36%|███▌      | 211/590 [01:17<01:50,  3.44it/s] 36%|███▌      | 212/590 [01:18<01:49,  3.45it/s] 36%|███▌      | 213/590 [01:18<01:49,  3.45it/s] 36%|███▋      | 214/590 [01:18<01:48,  3.45it/s] 36%|███▋      | 215/590 [01:19<01:48,  3.45it/s] 37%|███▋      | 216/590 [01:19<01:48,  3.45it/s] 37%|███▋      | 217/590 [01:19<01:48,  3.45it/s] 37%|███▋      | 218/590 [01:19<01:47,  3.45it/s] 37%|███▋      | 219/590 [01:20<01:47,  3.45it/s] 37%|███▋      | 220/590 [01:20<01:47,  3.45it/s] 37%|███▋      | 221/590 [01:20<01:46,  3.45it/s] 38%|███▊      | 222/590 [01:21<01:47,  3.44it/s] 38%|███▊      | 223/590 [01:21<01:46,  3.44it/s] 38%|███▊      | 224/590 [01:21<01:46,  3.45it/s] 38%|███▊      | 225/590 [01:22<01:45,  3.45it/s] 38%|███▊      | 226/590 [01:22<01:45,  3.45it/s] 38%|███▊      | 227/590 [01:22<01:45,  3.45it/s] 39%|███▊      | 228/590 [01:22<01:44,  3.45it/s] 39%|███▉      | 229/590 [01:23<01:44,  3.45it/s] 39%|███▉      | 230/590 [01:23<01:44,  3.45it/s] 39%|███▉      | 231/590 [01:23<01:44,  3.45it/s] 39%|███▉      | 232/590 [01:24<01:43,  3.45it/s] 39%|███▉      | 233/590 [01:24<01:43,  3.45it/s] 40%|███▉      | 234/590 [01:24<01:43,  3.45it/s] 40%|███▉      | 235/590 [01:24<01:42,  3.45it/s] 40%|████      | 236/590 [01:25<01:42,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 15:55:03,575 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:55:03,575 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:55:03,575 >>   Batch size = 8
{'eval_loss': 0.9580700993537903, 'eval_runtime': 9.4237, 'eval_samples_per_second': 370.662, 'eval_steps_per_second': 46.372, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.73it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.31it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.60it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.85it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.09it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.74it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.37it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.31it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.24it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.27it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.42it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.48it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.51it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.45it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.42it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.17it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.20it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.24it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.27it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.37it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.48it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.52it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.48it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.32it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.06it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.20it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.22it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.27it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.28it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.34it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.43it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.45it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.37it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.28it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.18it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.18it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.17it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.27it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.37it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.38it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.45it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.32it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.31it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.16it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.20it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.30it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.38it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.39it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.38it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.39it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.39it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.31it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.32it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.20it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.31it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.40it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.37it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.37it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.38it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.39it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.35it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.30it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.22it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.20it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.36it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.41it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.38it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.30it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.36it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.36it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.32it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.25it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.27it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.29it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.29it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.40it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.37it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.43it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.35it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.30it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.33it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.28it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.23it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.36it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.33it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.33it/s][A 40%|████      | 236/590 [01:34<01:42,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:55:13,026 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-28 15:55:13,044 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:55:15,425 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:55:15,442 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:55:15,451 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:42<32:02,  5.45s/it] 40%|████      | 238/590 [01:42<22:52,  3.90s/it] 41%|████      | 239/590 [01:43<16:28,  2.82s/it] 41%|████      | 240/590 [01:43<12:00,  2.06s/it] 41%|████      | 241/590 [01:43<08:52,  1.53s/it] 41%|████      | 242/590 [01:44<06:42,  1.16s/it] 41%|████      | 243/590 [01:44<05:10,  1.12it/s] 41%|████▏     | 244/590 [01:44<04:06,  1.40it/s] 42%|████▏     | 245/590 [01:44<03:22,  1.71it/s] 42%|████▏     | 246/590 [01:45<02:51,  2.01it/s] 42%|████▏     | 247/590 [01:45<02:29,  2.30it/s] 42%|████▏     | 248/590 [01:45<02:13,  2.56it/s] 42%|████▏     | 249/590 [01:46<02:03,  2.77it/s] 42%|████▏     | 250/590 [01:46<01:55,  2.95it/s] 43%|████▎     | 251/590 [01:46<01:49,  3.08it/s] 43%|████▎     | 252/590 [01:47<01:46,  3.19it/s] 43%|████▎     | 253/590 [01:47<01:43,  3.26it/s] 43%|████▎     | 254/590 [01:47<01:41,  3.32it/s] 43%|████▎     | 255/590 [01:47<01:39,  3.36it/s] 43%|████▎     | 256/590 [01:48<01:38,  3.39it/s] 44%|████▎     | 257/590 [01:48<01:37,  3.41it/s] 44%|████▎     | 258/590 [01:48<01:37,  3.42it/s] 44%|████▍     | 259/590 [01:49<01:36,  3.43it/s] 44%|████▍     | 260/590 [01:49<01:36,  3.43it/s] 44%|████▍     | 261/590 [01:49<01:35,  3.43it/s] 44%|████▍     | 262/590 [01:49<01:35,  3.44it/s] 45%|████▍     | 263/590 [01:50<01:34,  3.45it/s] 45%|████▍     | 264/590 [01:50<01:34,  3.45it/s] 45%|████▍     | 265/590 [01:50<01:34,  3.45it/s] 45%|████▌     | 266/590 [01:51<01:33,  3.45it/s] 45%|████▌     | 267/590 [01:51<01:36,  3.34it/s] 45%|████▌     | 268/590 [01:51<01:35,  3.37it/s] 46%|████▌     | 269/590 [01:51<01:34,  3.40it/s] 46%|████▌     | 270/590 [01:52<01:33,  3.41it/s] 46%|████▌     | 271/590 [01:52<01:33,  3.40it/s] 46%|████▌     | 272/590 [01:52<01:33,  3.42it/s] 46%|████▋     | 273/590 [01:53<01:32,  3.43it/s] 46%|████▋     | 274/590 [01:53<01:31,  3.44it/s] 47%|████▋     | 275/590 [01:53<01:31,  3.44it/s] 47%|████▋     | 276/590 [01:54<01:31,  3.45it/s] 47%|████▋     | 277/590 [01:54<01:30,  3.45it/s] 47%|████▋     | 278/590 [01:54<01:30,  3.44it/s] 47%|████▋     | 279/590 [01:54<01:30,  3.45it/s] 47%|████▋     | 280/590 [01:55<01:29,  3.45it/s] 48%|████▊     | 281/590 [01:55<01:29,  3.45it/s] 48%|████▊     | 282/590 [01:55<01:29,  3.43it/s] 48%|████▊     | 283/590 [01:56<01:29,  3.44it/s] 48%|████▊     | 284/590 [01:56<01:28,  3.45it/s] 48%|████▊     | 285/590 [01:56<01:28,  3.45it/s] 48%|████▊     | 286/590 [01:56<01:28,  3.45it/s] 49%|████▊     | 287/590 [01:57<01:27,  3.45it/s] 49%|████▉     | 288/590 [01:57<01:27,  3.46it/s] 49%|████▉     | 289/590 [01:57<01:27,  3.46it/s] 49%|████▉     | 290/590 [01:58<01:26,  3.45it/s] 49%|████▉     | 291/590 [01:58<01:26,  3.46it/s] 49%|████▉     | 292/590 [01:58<01:26,  3.45it/s] 50%|████▉     | 293/590 [01:58<01:26,  3.42it/s] 50%|████▉     | 294/590 [01:59<01:26,  3.43it/s] 50%|█████     | 295/590 [01:59<01:25,  3.44it/s] 50%|█████     | 296/590 [01:59<01:25,  3.44it/s] 50%|█████     | 297/590 [02:00<01:25,  3.45it/s] 51%|█████     | 298/590 [02:00<01:24,  3.45it/s] 51%|█████     | 299/590 [02:00<01:24,  3.45it/s] 51%|█████     | 300/590 [02:00<01:24,  3.45it/s] 51%|█████     | 301/590 [02:01<01:23,  3.45it/s] 51%|█████     | 302/590 [02:01<01:23,  3.45it/s] 51%|█████▏    | 303/590 [02:01<01:23,  3.45it/s] 52%|█████▏    | 304/590 [02:02<01:23,  3.43it/s] 52%|█████▏    | 305/590 [02:02<01:22,  3.44it/s] 52%|█████▏    | 306/590 [02:02<01:22,  3.44it/s] 52%|█████▏    | 307/590 [02:02<01:22,  3.45it/s] 52%|█████▏    | 308/590 [02:03<01:21,  3.45it/s] 52%|█████▏    | 309/590 [02:03<01:21,  3.45it/s] 53%|█████▎    | 310/590 [02:03<01:21,  3.45it/s] 53%|█████▎    | 311/590 [02:04<01:20,  3.45it/s] 53%|█████▎    | 312/590 [02:04<01:20,  3.45it/s] 53%|█████▎    | 313/590 [02:04<01:20,  3.45it/s] 53%|█████▎    | 314/590 [02:05<01:19,  3.45it/s] 53%|█████▎    | 315/590 [02:05<01:19,  3.45it/s] 54%|█████▎    | 316/590 [02:05<01:19,  3.45it/s] 54%|█████▎    | 317/590 [02:05<01:19,  3.45it/s] 54%|█████▍    | 318/590 [02:06<01:18,  3.46it/s] 54%|█████▍    | 319/590 [02:06<01:18,  3.45it/s] 54%|█████▍    | 320/590 [02:06<01:18,  3.46it/s] 54%|█████▍    | 321/590 [02:07<01:17,  3.45it/s] 55%|█████▍    | 322/590 [02:07<01:17,  3.44it/s] 55%|█████▍    | 323/590 [02:07<01:17,  3.44it/s] 55%|█████▍    | 324/590 [02:07<01:17,  3.45it/s] 55%|█████▌    | 325/590 [02:08<01:16,  3.45it/s] 55%|█████▌    | 326/590 [02:08<01:16,  3.45it/s] 55%|█████▌    | 327/590 [02:08<01:16,  3.45it/s] 56%|█████▌    | 328/590 [02:09<01:15,  3.45it/s] 56%|█████▌    | 329/590 [02:09<01:15,  3.45it/s] 56%|█████▌    | 330/590 [02:09<01:15,  3.45it/s] 56%|█████▌    | 331/590 [02:09<01:14,  3.45it/s] 56%|█████▋    | 332/590 [02:10<01:14,  3.45it/s] 56%|█████▋    | 333/590 [02:10<01:14,  3.44it/s] 57%|█████▋    | 334/590 [02:10<01:14,  3.45it/s] 57%|█████▋    | 335/590 [02:11<01:13,  3.45it/s] 57%|█████▋    | 336/590 [02:11<01:13,  3.45it/s] 57%|█████▋    | 337/590 [02:11<01:13,  3.45it/s] 57%|█████▋    | 338/590 [02:11<01:12,  3.45it/s] 57%|█████▋    | 339/590 [02:12<01:12,  3.45it/s] 58%|█████▊    | 340/590 [02:12<01:12,  3.45it/s] 58%|█████▊    | 341/590 [02:12<01:12,  3.45it/s] 58%|█████▊    | 342/590 [02:13<01:11,  3.45it/s] 58%|█████▊    | 343/590 [02:13<01:11,  3.45it/s] 58%|█████▊    | 344/590 [02:13<01:11,  3.42it/s] 58%|█████▊    | 345/590 [02:14<01:11,  3.43it/s] 59%|█████▊    | 346/590 [02:14<01:10,  3.44it/s] 59%|█████▉    | 347/590 [02:14<01:10,  3.44it/s] 59%|█████▉    | 348/590 [02:14<01:10,  3.45it/s] 59%|█████▉    | 349/590 [02:15<01:09,  3.45it/s] 59%|█████▉    | 350/590 [02:15<01:09,  3.45it/s] 59%|█████▉    | 351/590 [02:15<01:09,  3.45it/s] 60%|█████▉    | 352/590 [02:16<01:08,  3.45it/s] 60%|█████▉    | 353/590 [02:16<01:08,  3.45it/s] 60%|██████    | 354/590 [02:16<01:08,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 15:55:55,002 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:55:55,002 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:55:55,002 >>   Batch size = 8
{'eval_loss': 0.9700155258178711, 'eval_runtime': 9.4273, 'eval_samples_per_second': 370.52, 'eval_steps_per_second': 46.355, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.30it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.61it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.56it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.95it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.46it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.05it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.64it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.40it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.29it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.31it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.43it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.43it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.40it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.45it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.51it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.49it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.29it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.23it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.25it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.29it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.38it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.44it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.38it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.50it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.42it/s][A
 30%|███       | 133/437 [00:02<00:06, 45.96it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.02it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.14it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.16it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.18it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.38it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.36it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.43it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.38it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.31it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.35it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.18it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.23it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.34it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.34it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.41it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.44it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.47it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.36it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.35it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.24it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.27it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.35it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.38it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.37it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.43it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.43it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.39it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.36it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.27it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.27it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.33it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.27it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.38it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.43it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.38it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.38it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.25it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.31it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.26it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.39it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.28it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.36it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.34it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.39it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.32it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.29it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.26it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.37it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.30it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.25it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.30it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.33it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.41it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.36it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.32it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.34it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.31it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.26it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.31it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.26it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.26it/s][A 60%|██████    | 354/590 [02:26<01:08,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:56:04,457 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-28 15:56:04,479 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:56:07,102 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:56:07,121 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:56:07,138 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:35<22:30,  5.75s/it] 60%|██████    | 356/590 [02:35<16:04,  4.12s/it] 61%|██████    | 357/590 [02:35<11:32,  2.97s/it] 61%|██████    | 358/590 [02:36<08:22,  2.17s/it] 61%|██████    | 359/590 [02:36<06:10,  1.60s/it] 61%|██████    | 360/590 [02:36<04:38,  1.21s/it] 61%|██████    | 361/590 [02:36<03:33,  1.07it/s] 61%|██████▏   | 362/590 [02:37<02:48,  1.35it/s] 62%|██████▏   | 363/590 [02:37<02:17,  1.65it/s] 62%|██████▏   | 364/590 [02:37<01:55,  1.96it/s] 62%|██████▏   | 365/590 [02:38<01:39,  2.25it/s] 62%|██████▏   | 366/590 [02:38<01:28,  2.52it/s] 62%|██████▏   | 367/590 [02:38<01:23,  2.67it/s] 62%|██████▏   | 368/590 [02:38<01:17,  2.87it/s] 63%|██████▎   | 369/590 [02:39<01:13,  3.02it/s] 63%|██████▎   | 370/590 [02:39<01:09,  3.15it/s] 63%|██████▎   | 371/590 [02:39<01:07,  3.24it/s] 63%|██████▎   | 372/590 [02:40<01:06,  3.30it/s] 63%|██████▎   | 373/590 [02:40<01:04,  3.35it/s] 63%|██████▎   | 374/590 [02:40<01:03,  3.38it/s] 64%|██████▎   | 375/590 [02:40<01:03,  3.41it/s] 64%|██████▎   | 376/590 [02:41<01:02,  3.43it/s] 64%|██████▍   | 377/590 [02:41<01:01,  3.44it/s] 64%|██████▍   | 378/590 [02:41<01:01,  3.44it/s] 64%|██████▍   | 379/590 [02:42<01:01,  3.45it/s] 64%|██████▍   | 380/590 [02:42<01:00,  3.45it/s] 65%|██████▍   | 381/590 [02:42<01:00,  3.46it/s] 65%|██████▍   | 382/590 [02:42<01:00,  3.46it/s] 65%|██████▍   | 383/590 [02:43<00:59,  3.46it/s] 65%|██████▌   | 384/590 [02:43<00:59,  3.46it/s] 65%|██████▌   | 385/590 [02:43<00:59,  3.46it/s] 65%|██████▌   | 386/590 [02:44<00:58,  3.47it/s] 66%|██████▌   | 387/590 [02:44<00:58,  3.46it/s] 66%|██████▌   | 388/590 [02:44<00:58,  3.47it/s] 66%|██████▌   | 389/590 [02:45<00:58,  3.45it/s] 66%|██████▌   | 390/590 [02:45<00:57,  3.45it/s] 66%|██████▋   | 391/590 [02:45<00:57,  3.45it/s] 66%|██████▋   | 392/590 [02:45<00:57,  3.45it/s] 67%|██████▋   | 393/590 [02:46<00:57,  3.46it/s] 67%|██████▋   | 394/590 [02:46<00:56,  3.46it/s] 67%|██████▋   | 395/590 [02:46<00:56,  3.46it/s] 67%|██████▋   | 396/590 [02:47<00:56,  3.46it/s] 67%|██████▋   | 397/590 [02:47<00:55,  3.46it/s] 67%|██████▋   | 398/590 [02:47<00:55,  3.46it/s] 68%|██████▊   | 399/590 [02:47<00:55,  3.46it/s] 68%|██████▊   | 400/590 [02:48<00:55,  3.44it/s] 68%|██████▊   | 401/590 [02:48<00:54,  3.45it/s] 68%|██████▊   | 402/590 [02:48<00:54,  3.45it/s] 68%|██████▊   | 403/590 [02:49<00:54,  3.45it/s] 68%|██████▊   | 404/590 [02:49<00:53,  3.46it/s] 69%|██████▊   | 405/590 [02:49<00:53,  3.46it/s] 69%|██████▉   | 406/590 [02:49<00:53,  3.46it/s] 69%|██████▉   | 407/590 [02:50<00:52,  3.45it/s] 69%|██████▉   | 408/590 [02:50<00:52,  3.46it/s] 69%|██████▉   | 409/590 [02:50<00:52,  3.46it/s] 69%|██████▉   | 410/590 [02:51<00:52,  3.44it/s] 70%|██████▉   | 411/590 [02:51<00:53,  3.34it/s] 70%|██████▉   | 412/590 [02:51<00:52,  3.36it/s] 70%|███████   | 413/590 [02:51<00:52,  3.39it/s] 70%|███████   | 414/590 [02:52<00:51,  3.41it/s] 70%|███████   | 415/590 [02:52<00:51,  3.42it/s] 71%|███████   | 416/590 [02:52<00:50,  3.43it/s] 71%|███████   | 417/590 [02:53<00:50,  3.44it/s] 71%|███████   | 418/590 [02:53<00:49,  3.44it/s] 71%|███████   | 419/590 [02:53<00:49,  3.45it/s] 71%|███████   | 420/590 [02:54<00:49,  3.45it/s] 71%|███████▏  | 421/590 [02:54<00:48,  3.45it/s] 72%|███████▏  | 422/590 [02:54<00:48,  3.44it/s] 72%|███████▏  | 423/590 [02:54<00:48,  3.44it/s] 72%|███████▏  | 424/590 [02:55<00:48,  3.45it/s] 72%|███████▏  | 425/590 [02:55<00:47,  3.45it/s] 72%|███████▏  | 426/590 [02:55<00:47,  3.46it/s] 72%|███████▏  | 427/590 [02:56<00:47,  3.45it/s] 73%|███████▎  | 428/590 [02:56<00:46,  3.46it/s] 73%|███████▎  | 429/590 [02:56<00:46,  3.45it/s] 73%|███████▎  | 430/590 [02:56<00:46,  3.46it/s] 73%|███████▎  | 431/590 [02:57<00:46,  3.45it/s] 73%|███████▎  | 432/590 [02:57<00:45,  3.46it/s] 73%|███████▎  | 433/590 [02:57<00:45,  3.46it/s] 74%|███████▎  | 434/590 [02:58<00:45,  3.46it/s] 74%|███████▎  | 435/590 [02:58<00:44,  3.46it/s] 74%|███████▍  | 436/590 [02:58<00:45,  3.42it/s] 74%|███████▍  | 437/590 [02:58<00:44,  3.43it/s] 74%|███████▍  | 438/590 [02:59<00:44,  3.44it/s] 74%|███████▍  | 439/590 [02:59<00:43,  3.44it/s] 75%|███████▍  | 440/590 [02:59<00:43,  3.45it/s] 75%|███████▍  | 441/590 [03:00<00:43,  3.45it/s] 75%|███████▍  | 442/590 [03:00<00:42,  3.45it/s] 75%|███████▌  | 443/590 [03:00<00:42,  3.45it/s] 75%|███████▌  | 444/590 [03:00<00:42,  3.45it/s] 75%|███████▌  | 445/590 [03:01<00:41,  3.45it/s] 76%|███████▌  | 446/590 [03:01<00:41,  3.46it/s] 76%|███████▌  | 447/590 [03:01<00:41,  3.44it/s] 76%|███████▌  | 448/590 [03:02<00:41,  3.45it/s] 76%|███████▌  | 449/590 [03:02<00:40,  3.45it/s] 76%|███████▋  | 450/590 [03:02<00:40,  3.45it/s] 76%|███████▋  | 451/590 [03:02<00:40,  3.45it/s] 77%|███████▋  | 452/590 [03:03<00:40,  3.45it/s] 77%|███████▋  | 453/590 [03:03<00:39,  3.45it/s] 77%|███████▋  | 454/590 [03:03<00:39,  3.45it/s] 77%|███████▋  | 455/590 [03:04<00:39,  3.45it/s] 77%|███████▋  | 456/590 [03:04<00:38,  3.45it/s] 77%|███████▋  | 457/590 [03:04<00:38,  3.45it/s] 78%|███████▊  | 458/590 [03:05<00:38,  3.41it/s] 78%|███████▊  | 459/590 [03:05<00:38,  3.42it/s] 78%|███████▊  | 460/590 [03:05<00:37,  3.43it/s] 78%|███████▊  | 461/590 [03:05<00:37,  3.44it/s] 78%|███████▊  | 462/590 [03:06<00:37,  3.44it/s] 78%|███████▊  | 463/590 [03:06<00:36,  3.45it/s] 79%|███████▊  | 464/590 [03:06<00:36,  3.45it/s] 79%|███████▉  | 465/590 [03:07<00:36,  3.45it/s] 79%|███████▉  | 466/590 [03:07<00:36,  3.44it/s] 79%|███████▉  | 467/590 [03:07<00:35,  3.45it/s] 79%|███████▉  | 468/590 [03:07<00:35,  3.45it/s] 79%|███████▉  | 469/590 [03:08<00:35,  3.41it/s] 80%|███████▉  | 470/590 [03:08<00:35,  3.42it/s] 80%|███████▉  | 471/590 [03:08<00:34,  3.43it/s] 80%|████████  | 472/590 [03:09<00:34,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 15:56:47,480 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:56:47,481 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:56:47,481 >>   Batch size = 8
{'eval_loss': 0.9841604828834534, 'eval_runtime': 9.4267, 'eval_samples_per_second': 370.545, 'eval_steps_per_second': 46.358, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.84it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.36it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.58it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.94it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.42it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.12it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.77it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.46it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.29it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.29it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.41it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.52it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.54it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.59it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.55it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.44it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.31it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.27it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.25it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.26it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.27it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.45it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.54it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.47it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.48it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.35it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.33it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.23it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.24it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.25it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.37it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.43it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.53it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.42it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.35it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.34it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.33it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.27it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.19it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.34it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.43it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.48it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.50it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.37it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.39it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.31it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.26it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.17it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.29it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.34it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.45it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.36it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.41it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.38it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.33it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.34it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.24it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.32it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.40it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.46it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.34it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.43it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.33it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.30it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.31it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.29it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.28it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.37it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.38it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.41it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.38it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.30it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.34it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.33it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.30it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.27it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.32it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.38it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.36it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.40it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.38it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.33it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.31it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.30it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.34it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.36it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.36it/s][A 80%|████████  | 472/590 [03:18<00:34,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:56:56,913 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-28 15:56:56,942 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:56:59,745 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:56:59,764 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:56:59,775 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:28<11:43,  6.01s/it] 80%|████████  | 474/590 [03:28<08:18,  4.30s/it] 81%|████████  | 475/590 [03:29<05:55,  3.09s/it] 81%|████████  | 476/590 [03:29<04:16,  2.25s/it] 81%|████████  | 477/590 [03:29<03:08,  1.66s/it] 81%|████████  | 478/590 [03:29<02:20,  1.25s/it] 81%|████████  | 479/590 [03:30<01:46,  1.04it/s] 81%|████████▏ | 480/590 [03:30<01:23,  1.32it/s] 82%|████████▏ | 481/590 [03:30<01:07,  1.62it/s] 82%|████████▏ | 482/590 [03:31<00:56,  1.92it/s] 82%|████████▏ | 483/590 [03:31<00:48,  2.22it/s] 82%|████████▏ | 484/590 [03:31<00:42,  2.49it/s] 82%|████████▏ | 485/590 [03:31<00:39,  2.64it/s] 82%|████████▏ | 486/590 [03:32<00:36,  2.84it/s] 83%|████████▎ | 487/590 [03:32<00:34,  3.00it/s] 83%|████████▎ | 488/590 [03:32<00:32,  3.13it/s] 83%|████████▎ | 489/590 [03:33<00:31,  3.22it/s] 83%|████████▎ | 490/590 [03:33<00:30,  3.30it/s] 83%|████████▎ | 491/590 [03:33<00:29,  3.34it/s] 83%|████████▎ | 492/590 [03:33<00:28,  3.38it/s] 84%|████████▎ | 493/590 [03:34<00:28,  3.41it/s] 84%|████████▎ | 494/590 [03:34<00:28,  3.42it/s] 84%|████████▍ | 495/590 [03:34<00:27,  3.44it/s] 84%|████████▍ | 496/590 [03:35<00:27,  3.43it/s] 84%|████████▍ | 497/590 [03:35<00:27,  3.44it/s] 84%|████████▍ | 498/590 [03:35<00:26,  3.45it/s] 85%|████████▍ | 499/590 [03:36<00:26,  3.45it/s] 85%|████████▍ | 500/590 [03:36<00:26,  3.46it/s]                                                  85%|████████▍ | 500/590 [03:36<00:26,  3.46it/s] 85%|████████▍ | 501/590 [03:36<00:25,  3.46it/s] 85%|████████▌ | 502/590 [03:36<00:25,  3.46it/s] 85%|████████▌ | 503/590 [03:37<00:25,  3.46it/s] 85%|████████▌ | 504/590 [03:37<00:24,  3.46it/s] 86%|████████▌ | 505/590 [03:37<00:24,  3.46it/s] 86%|████████▌ | 506/590 [03:38<00:24,  3.46it/s] 86%|████████▌ | 507/590 [03:38<00:24,  3.46it/s] 86%|████████▌ | 508/590 [03:38<00:23,  3.46it/s] 86%|████████▋ | 509/590 [03:38<00:23,  3.46it/s] 86%|████████▋ | 510/590 [03:39<00:23,  3.46it/s] 87%|████████▋ | 511/590 [03:39<00:22,  3.46it/s] 87%|████████▋ | 512/590 [03:39<00:22,  3.47it/s] 87%|████████▋ | 513/590 [03:40<00:22,  3.46it/s] 87%|████████▋ | 514/590 [03:40<00:21,  3.46it/s] 87%|████████▋ | 515/590 [03:40<00:21,  3.46it/s] 87%|████████▋ | 516/590 [03:40<00:21,  3.46it/s] 88%|████████▊ | 517/590 [03:41<00:21,  3.46it/s] 88%|████████▊ | 518/590 [03:41<00:20,  3.45it/s] 88%|████████▊ | 519/590 [03:41<00:20,  3.45it/s] 88%|████████▊ | 520/590 [03:42<00:20,  3.45it/s] 88%|████████▊ | 521/590 [03:42<00:19,  3.46it/s] 88%|████████▊ | 522/590 [03:42<00:19,  3.46it/s] 89%|████████▊ | 523/590 [03:42<00:19,  3.46it/s] 89%|████████▉ | 524/590 [03:43<00:19,  3.46it/s] 89%|████████▉ | 525/590 [03:43<00:18,  3.46it/s] 89%|████████▉ | 526/590 [03:43<00:18,  3.46it/s] 89%|████████▉ | 527/590 [03:44<00:18,  3.46it/s] 89%|████████▉ | 528/590 [03:44<00:17,  3.46it/s] 90%|████████▉ | 529/590 [03:44<00:17,  3.45it/s] 90%|████████▉ | 530/590 [03:44<00:17,  3.45it/s] 90%|█████████ | 531/590 [03:45<00:17,  3.46it/s] 90%|█████████ | 532/590 [03:45<00:16,  3.45it/s] 90%|█████████ | 533/590 [03:45<00:16,  3.46it/s] 91%|█████████ | 534/590 [03:46<00:16,  3.46it/s] 91%|█████████ | 535/590 [03:46<00:15,  3.46it/s] 91%|█████████ | 536/590 [03:46<00:15,  3.45it/s] 91%|█████████ | 537/590 [03:47<00:15,  3.45it/s] 91%|█████████ | 538/590 [03:47<00:15,  3.45it/s] 91%|█████████▏| 539/590 [03:47<00:14,  3.46it/s] 92%|█████████▏| 540/590 [03:47<00:14,  3.46it/s] 92%|█████████▏| 541/590 [03:48<00:14,  3.46it/s] 92%|█████████▏| 542/590 [03:48<00:13,  3.46it/s] 92%|█████████▏| 543/590 [03:48<00:13,  3.46it/s] 92%|█████████▏| 544/590 [03:49<00:13,  3.46it/s] 92%|█████████▏| 545/590 [03:49<00:13,  3.46it/s] 93%|█████████▎| 546/590 [03:49<00:12,  3.46it/s] 93%|█████████▎| 547/590 [03:49<00:12,  3.45it/s] 93%|█████████▎| 548/590 [03:50<00:12,  3.45it/s] 93%|█████████▎| 549/590 [03:50<00:11,  3.45it/s] 93%|█████████▎| 550/590 [03:50<00:11,  3.46it/s] 93%|█████████▎| 551/590 [03:51<00:11,  3.45it/s] 94%|█████████▎| 552/590 [03:51<00:10,  3.46it/s] 94%|█████████▎| 553/590 [03:51<00:10,  3.46it/s] 94%|█████████▍| 554/590 [03:51<00:10,  3.46it/s] 94%|█████████▍| 555/590 [03:52<00:10,  3.46it/s] 94%|█████████▍| 556/590 [03:52<00:09,  3.46it/s] 94%|█████████▍| 557/590 [03:52<00:09,  3.46it/s] 95%|█████████▍| 558/590 [03:53<00:09,  3.45it/s] 95%|█████████▍| 559/590 [03:53<00:08,  3.45it/s] 95%|█████████▍| 560/590 [03:53<00:08,  3.45it/s] 95%|█████████▌| 561/590 [03:53<00:08,  3.45it/s] 95%|█████████▌| 562/590 [03:54<00:08,  3.45it/s] 95%|█████████▌| 563/590 [03:54<00:07,  3.45it/s] 96%|█████████▌| 564/590 [03:54<00:07,  3.45it/s] 96%|█████████▌| 565/590 [03:55<00:07,  3.46it/s] 96%|█████████▌| 566/590 [03:55<00:06,  3.45it/s] 96%|█████████▌| 567/590 [03:55<00:06,  3.45it/s] 96%|█████████▋| 568/590 [03:55<00:06,  3.45it/s] 96%|█████████▋| 569/590 [03:56<00:06,  3.43it/s] 97%|█████████▋| 570/590 [03:56<00:05,  3.44it/s] 97%|█████████▋| 571/590 [03:56<00:05,  3.44it/s] 97%|█████████▋| 572/590 [03:57<00:05,  3.45it/s] 97%|█████████▋| 573/590 [03:57<00:04,  3.45it/s] 97%|█████████▋| 574/590 [03:57<00:04,  3.45it/s] 97%|█████████▋| 575/590 [03:58<00:04,  3.45it/s] 98%|█████████▊| 576/590 [03:58<00:04,  3.46it/s] 98%|█████████▊| 577/590 [03:58<00:03,  3.45it/s] 98%|█████████▊| 578/590 [03:58<00:03,  3.45it/s] 98%|█████████▊| 579/590 [03:59<00:03,  3.45it/s] 98%|█████████▊| 580/590 [03:59<00:02,  3.44it/s] 98%|█████████▊| 581/590 [03:59<00:02,  3.44it/s] 99%|█████████▊| 582/590 [04:00<00:02,  3.45it/s] 99%|█████████▉| 583/590 [04:00<00:02,  3.45it/s] 99%|█████████▉| 584/590 [04:00<00:01,  3.45it/s] 99%|█████████▉| 585/590 [04:00<00:01,  3.45it/s] 99%|█████████▉| 586/590 [04:01<00:01,  3.45it/s] 99%|█████████▉| 587/590 [04:01<00:00,  3.45it/s]100%|█████████▉| 588/590 [04:01<00:00,  3.46it/s]100%|█████████▉| 589/590 [04:02<00:00,  3.45it/s]100%|██████████| 590/590 [04:02<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 15:57:40,735 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:57:40,735 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:57:40,735 >>   Batch size = 8
{'eval_loss': 0.9910905957221985, 'eval_runtime': 9.4208, 'eval_samples_per_second': 370.773, 'eval_steps_per_second': 46.386, 'epoch': 4.0}
{'loss': 0.7127, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.29it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.36it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.68it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.93it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.47it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.97it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.66it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.48it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.35it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.41it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.48it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.50it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.62it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.57it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.49it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.29it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.21it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.24it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.13it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.25it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.37it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.42it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.54it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.47it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.31it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.24it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.10it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.17it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.22it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.36it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.42it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.45it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.46it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.39it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.38it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.22it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.12it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.26it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.31it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.36it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.42it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.42it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.39it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.40it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.33it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.14it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.13it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.25it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.28it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.38it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.48it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.36it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.40it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.33it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.31it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.17it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.21it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.31it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.34it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.40it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.42it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.39it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.36it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.34it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.23it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.22it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.29it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.31it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.38it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.38it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.37it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.36it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.30it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.27it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.19it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.26it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.21it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.29it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.37it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.30it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.37it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.32it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.30it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.32it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.24it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.29it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.29it/s][A100%|██████████| 590/590 [04:11<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:57:50,177 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-28 15:57:50,200 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:57:53,729 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:57:53,740 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:57:53,758 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 15:58:01,010 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 15:58:01,015 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118 (score: 0.9580700993537903).
                                                 100%|██████████| 590/590 [04:28<00:00,  3.46it/s]100%|██████████| 590/590 [04:28<00:00,  2.19it/s]
[INFO|trainer.py:1894] 2023-08-28 15:58:07,177 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 15:58:07,192 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:58:09,922 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:58:09,947 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:58:09,957 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:58:10,161 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:10,162 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:10,162 >>   train_loss               =     0.7077
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:10,162 >>   train_runtime            = 0:04:28.79
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:10,162 >>   train_samples            =       7552
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:10,162 >>   train_samples_per_second =    140.478
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:10,162 >>   train_steps_per_second   =      2.195
{'eval_loss': 0.9936475157737732, 'eval_runtime': 9.4265, 'eval_samples_per_second': 370.551, 'eval_steps_per_second': 46.359, 'epoch': 5.0}
{'train_runtime': 268.7964, 'train_samples_per_second': 140.478, 'train_steps_per_second': 2.195, 'train_loss': 0.7077319969565181, 'epoch': 5.0}
08/28/2023 15:58:10 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 15:58:10,206 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:58:10,206 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:58:10,206 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.19it/s]  3%|▎         | 12/437 [00:00<00:08, 50.95it/s]  4%|▍         | 18/437 [00:00<00:08, 48.91it/s]  5%|▌         | 23/437 [00:00<00:08, 48.16it/s]  6%|▋         | 28/437 [00:00<00:08, 47.62it/s]  8%|▊         | 33/437 [00:00<00:08, 47.34it/s]  9%|▊         | 38/437 [00:00<00:08, 47.26it/s] 10%|▉         | 43/437 [00:00<00:08, 47.14it/s] 11%|█         | 48/437 [00:01<00:08, 47.08it/s] 12%|█▏        | 53/437 [00:01<00:08, 46.97it/s] 13%|█▎        | 58/437 [00:01<00:08, 46.91it/s] 14%|█▍        | 63/437 [00:01<00:07, 46.96it/s] 16%|█▌        | 68/437 [00:01<00:07, 46.94it/s] 17%|█▋        | 73/437 [00:01<00:07, 46.83it/s] 18%|█▊        | 78/437 [00:01<00:07, 46.87it/s] 19%|█▉        | 83/437 [00:01<00:07, 46.81it/s] 20%|██        | 88/437 [00:01<00:07, 46.84it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.87it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.88it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.85it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.81it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.86it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.83it/s] 28%|██▊       | 123/437 [00:02<00:06, 46.72it/s] 29%|██▉       | 128/437 [00:02<00:06, 46.77it/s] 30%|███       | 133/437 [00:02<00:06, 46.77it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.74it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.78it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.74it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.80it/s] 36%|███▌      | 158/437 [00:03<00:05, 46.85it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.82it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.72it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.80it/s] 41%|████      | 178/437 [00:03<00:05, 46.82it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.80it/s] 43%|████▎     | 188/437 [00:03<00:05, 46.83it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.76it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.81it/s] 46%|████▋     | 203/437 [00:04<00:05, 46.79it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.82it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.81it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.70it/s] 51%|█████     | 223/437 [00:04<00:04, 46.68it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.64it/s] 53%|█████▎    | 233/437 [00:04<00:04, 46.74it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.81it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.69it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.73it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.77it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.79it/s] 60%|██████    | 263/437 [00:05<00:03, 46.81it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.69it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.73it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.76it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.66it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.75it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.76it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.71it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.80it/s] 70%|███████   | 308/437 [00:06<00:02, 46.85it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.77it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.70it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.78it/s] 75%|███████▌  | 328/437 [00:06<00:02, 46.79it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.70it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.73it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.69it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.71it/s] 81%|████████  | 353/437 [00:07<00:01, 46.77it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.75it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.66it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.67it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.74it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.74it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.72it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.70it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.73it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.67it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.70it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.69it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.67it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.68it/s] 97%|█████████▋| 423/437 [00:09<00:00, 46.78it/s] 98%|█████████▊| 428/437 [00:09<00:00, 46.64it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.70it/s]100%|██████████| 437/437 [00:09<00:00, 46.87it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:58:19,553 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:19,553 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:19,553 >>   eval_loss               =     0.9581
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:19,553 >>   eval_runtime            = 0:00:09.34
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:19,553 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:19,553 >>   eval_samples_per_second =    373.713
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:19,553 >>   eval_steps_per_second   =     46.754
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:58:19,553 >>   perplexity              =     2.6067
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:26,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:26,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:26,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:26,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:26,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:58:26,894 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:58:26,895 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:58:27,459 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:58:28,502 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:58:28,502 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:31,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:31,351 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:31,351 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:31,352 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:58:31,352 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:58:32,119 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:58:32,120 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:58:32,702 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:58:32,865 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:58:32,865 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.57it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:06,  1.54it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:13,  1.52it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.65it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.67it/s]Extractor Predicting: 29it [00:18,  1.64it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:20,  1.40it/s]Extractor Predicting: 34it [00:21,  1.40it/s]Extractor Predicting: 35it [00:22,  1.43it/s]Extractor Predicting: 36it [00:23,  1.44it/s]Extractor Predicting: 37it [00:23,  1.45it/s]Extractor Predicting: 38it [00:24,  1.45it/s]Extractor Predicting: 39it [00:25,  1.48it/s]Extractor Predicting: 40it [00:25,  1.48it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:27,  1.50it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:31,  1.48it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:33,  1.50it/s]Extractor Predicting: 52it [00:33,  1.48it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:35,  1.48it/s]Extractor Predicting: 56it [00:36,  1.48it/s]Extractor Predicting: 57it [00:37,  1.45it/s]Extractor Predicting: 58it [00:37,  1.46it/s]Extractor Predicting: 59it [00:38,  1.45it/s]Extractor Predicting: 60it [00:39,  1.45it/s]Extractor Predicting: 61it [00:39,  1.47it/s]Extractor Predicting: 62it [00:40,  1.47it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:41,  1.53it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.54it/s]Extractor Predicting: 67it [00:43,  1.52it/s]Extractor Predicting: 68it [00:44,  1.51it/s]Extractor Predicting: 69it [00:45,  1.50it/s]Extractor Predicting: 70it [00:45,  1.48it/s]Extractor Predicting: 71it [00:46,  1.49it/s]Extractor Predicting: 72it [00:47,  1.51it/s]Extractor Predicting: 73it [00:47,  1.50it/s]Extractor Predicting: 74it [00:48,  1.53it/s]Extractor Predicting: 75it [00:49,  1.54it/s]Extractor Predicting: 76it [00:49,  1.51it/s]Extractor Predicting: 77it [00:50,  1.51it/s]Extractor Predicting: 78it [00:51,  1.50it/s]Extractor Predicting: 79it [00:51,  1.50it/s]Extractor Predicting: 80it [00:52,  1.48it/s]Extractor Predicting: 81it [00:53,  1.53it/s]Extractor Predicting: 82it [00:53,  1.52it/s]Extractor Predicting: 83it [00:54,  1.49it/s]Extractor Predicting: 84it [00:55,  1.50it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.58it/s]Extractor Predicting: 87it [00:56,  1.62it/s]Extractor Predicting: 88it [00:57,  1.62it/s]Extractor Predicting: 89it [00:58,  1.61it/s]Extractor Predicting: 90it [00:58,  1.62it/s]Extractor Predicting: 91it [00:59,  1.60it/s]Extractor Predicting: 92it [01:00,  1.58it/s]Extractor Predicting: 93it [01:00,  1.61it/s]Extractor Predicting: 94it [01:01,  1.63it/s]Extractor Predicting: 95it [01:01,  1.62it/s]Extractor Predicting: 96it [01:02,  1.63it/s]Extractor Predicting: 97it [01:03,  1.61it/s]Extractor Predicting: 98it [01:03,  1.58it/s]Extractor Predicting: 99it [01:04,  1.55it/s]Extractor Predicting: 100it [01:05,  1.57it/s]Extractor Predicting: 101it [01:05,  1.61it/s]Extractor Predicting: 102it [01:06,  1.59it/s]Extractor Predicting: 103it [01:06,  1.59it/s]Extractor Predicting: 104it [01:07,  1.59it/s]Extractor Predicting: 105it [01:08,  1.62it/s]Extractor Predicting: 106it [01:08,  1.59it/s]Extractor Predicting: 107it [01:09,  1.61it/s]Extractor Predicting: 108it [01:10,  1.61it/s]Extractor Predicting: 109it [01:10,  1.64it/s]Extractor Predicting: 110it [01:11,  1.61it/s]Extractor Predicting: 111it [01:11,  1.60it/s]Extractor Predicting: 112it [01:12,  1.58it/s]Extractor Predicting: 113it [01:13,  1.58it/s]Extractor Predicting: 114it [01:13,  1.54it/s]Extractor Predicting: 115it [01:14,  1.53it/s]Extractor Predicting: 116it [01:15,  1.53it/s]Extractor Predicting: 117it [01:15,  1.52it/s]Extractor Predicting: 118it [01:16,  1.58it/s]Extractor Predicting: 119it [01:17,  1.57it/s]Extractor Predicting: 120it [01:17,  1.58it/s]Extractor Predicting: 121it [01:18,  1.57it/s]Extractor Predicting: 122it [01:19,  1.43it/s]Extractor Predicting: 123it [01:19,  1.46it/s]Extractor Predicting: 124it [01:20,  1.46it/s]Extractor Predicting: 125it [01:21,  1.46it/s]Extractor Predicting: 126it [01:21,  1.49it/s]Extractor Predicting: 127it [01:22,  1.48it/s]Extractor Predicting: 128it [01:23,  1.48it/s]Extractor Predicting: 129it [01:23,  1.45it/s]Extractor Predicting: 130it [01:24,  1.47it/s]Extractor Predicting: 131it [01:25,  1.49it/s]Extractor Predicting: 132it [01:25,  1.52it/s]Extractor Predicting: 133it [01:26,  1.52it/s]Extractor Predicting: 134it [01:27,  1.52it/s]Extractor Predicting: 135it [01:27,  1.52it/s]Extractor Predicting: 136it [01:28,  1.52it/s]Extractor Predicting: 137it [01:29,  1.50it/s]Extractor Predicting: 138it [01:29,  1.51it/s]Extractor Predicting: 139it [01:30,  1.50it/s]Extractor Predicting: 140it [01:31,  1.55it/s]Extractor Predicting: 141it [01:31,  1.52it/s]Extractor Predicting: 142it [01:32,  1.59it/s]Extractor Predicting: 142it [01:32,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:13,569 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:13,577 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:13,577 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:13,577 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:13,577 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:00:14,182 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:00:14,183 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:00:14,750 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:00:15,790 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:00:15,790 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:18,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:18,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:18,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:18,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:18,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:00:19,317 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:00:19,318 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:00:19,902 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:00:20,048 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:00:20,048 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.23710407239819004,
  "recall": 0.07500715717148583,
  "score": 0.11396259243149195,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:08,  1.52it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.58it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.48it/s]Extractor Predicting: 31it [00:20,  1.47it/s]Extractor Predicting: 32it [00:20,  1.48it/s]Extractor Predicting: 33it [00:21,  1.49it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.54it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:33,  1.53it/s]Extractor Predicting: 53it [00:34,  1.53it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:37,  1.57it/s]Extractor Predicting: 59it [00:38,  1.57it/s]Extractor Predicting: 60it [00:38,  1.56it/s]Extractor Predicting: 61it [00:39,  1.56it/s]Extractor Predicting: 62it [00:40,  1.56it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:42,  1.52it/s]Extractor Predicting: 67it [00:43,  1.54it/s]Extractor Predicting: 68it [00:44,  1.58it/s]Extractor Predicting: 69it [00:44,  1.57it/s]Extractor Predicting: 70it [00:45,  1.56it/s]Extractor Predicting: 71it [00:46,  1.55it/s]Extractor Predicting: 72it [00:46,  1.54it/s]Extractor Predicting: 73it [00:47,  1.50it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:48,  1.52it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:50,  1.49it/s]Extractor Predicting: 78it [00:50,  1.51it/s]Extractor Predicting: 79it [00:51,  1.51it/s]Extractor Predicting: 80it [00:52,  1.52it/s]Extractor Predicting: 81it [00:52,  1.53it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:54,  1.51it/s]Extractor Predicting: 84it [00:54,  1.52it/s]Extractor Predicting: 85it [00:55,  1.54it/s]Extractor Predicting: 86it [00:55,  1.51it/s]Extractor Predicting: 87it [00:56,  1.51it/s]Extractor Predicting: 88it [00:57,  1.47it/s]Extractor Predicting: 89it [00:58,  1.49it/s]Extractor Predicting: 90it [00:58,  1.51it/s]Extractor Predicting: 91it [00:59,  1.49it/s]Extractor Predicting: 92it [01:00,  1.49it/s]Extractor Predicting: 93it [01:00,  1.47it/s]Extractor Predicting: 94it [01:01,  1.45it/s]Extractor Predicting: 95it [01:02,  1.47it/s]Extractor Predicting: 96it [01:02,  1.47it/s]Extractor Predicting: 97it [01:03,  1.47it/s]Extractor Predicting: 98it [01:04,  1.46it/s]Extractor Predicting: 99it [01:04,  1.48it/s]Extractor Predicting: 100it [01:05,  1.49it/s]Extractor Predicting: 101it [01:06,  1.48it/s]Extractor Predicting: 102it [01:06,  1.48it/s]Extractor Predicting: 103it [01:07,  1.49it/s]Extractor Predicting: 104it [01:08,  1.49it/s]Extractor Predicting: 105it [01:08,  1.48it/s]Extractor Predicting: 106it [01:09,  1.49it/s]Extractor Predicting: 107it [01:10,  1.47it/s]Extractor Predicting: 108it [01:10,  1.46it/s]Extractor Predicting: 109it [01:11,  1.47it/s]Extractor Predicting: 110it [01:12,  1.45it/s]Extractor Predicting: 111it [01:12,  1.45it/s]Extractor Predicting: 112it [01:13,  1.49it/s]Extractor Predicting: 113it [01:14,  1.52it/s]Extractor Predicting: 114it [01:14,  1.53it/s]Extractor Predicting: 115it [01:15,  1.52it/s]Extractor Predicting: 116it [01:16,  1.52it/s]Extractor Predicting: 117it [01:16,  1.54it/s]Extractor Predicting: 118it [01:17,  1.55it/s]Extractor Predicting: 119it [01:18,  1.54it/s]Extractor Predicting: 120it [01:18,  1.40it/s]Extractor Predicting: 121it [01:19,  1.44it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:20,  1.52it/s]Extractor Predicting: 124it [01:21,  1.53it/s]Extractor Predicting: 125it [01:22,  1.54it/s]Extractor Predicting: 126it [01:22,  1.52it/s]Extractor Predicting: 127it [01:23,  1.53it/s]Extractor Predicting: 128it [01:24,  1.55it/s]Extractor Predicting: 129it [01:24,  1.51it/s]Extractor Predicting: 130it [01:25,  1.53it/s]Extractor Predicting: 131it [01:26,  1.51it/s]Extractor Predicting: 132it [01:26,  1.53it/s]Extractor Predicting: 133it [01:27,  1.52it/s]Extractor Predicting: 134it [01:28,  1.52it/s]Extractor Predicting: 135it [01:28,  1.55it/s]Extractor Predicting: 136it [01:29,  1.55it/s]Extractor Predicting: 137it [01:30,  1.54it/s]Extractor Predicting: 138it [01:30,  1.54it/s]Extractor Predicting: 139it [01:31,  1.57it/s]Extractor Predicting: 140it [01:31,  1.55it/s]Extractor Predicting: 141it [01:32,  1.53it/s]Extractor Predicting: 142it [01:33,  1.58it/s]Extractor Predicting: 143it [01:33,  1.57it/s]Extractor Predicting: 144it [01:34,  1.54it/s]Extractor Predicting: 145it [01:35,  1.55it/s]Extractor Predicting: 146it [01:35,  1.55it/s]Extractor Predicting: 147it [01:36,  1.52it/s]Extractor Predicting: 148it [01:37,  1.52it/s]Extractor Predicting: 149it [01:37,  1.51it/s]Extractor Predicting: 150it [01:38,  1.53it/s]Extractor Predicting: 151it [01:39,  1.55it/s]Extractor Predicting: 152it [01:39,  1.56it/s]Extractor Predicting: 153it [01:40,  1.56it/s]Extractor Predicting: 154it [01:41,  1.54it/s]Extractor Predicting: 155it [01:41,  1.52it/s]Extractor Predicting: 156it [01:42,  1.51it/s]Extractor Predicting: 157it [01:43,  1.49it/s]Extractor Predicting: 158it [01:43,  1.49it/s]Extractor Predicting: 159it [01:44,  1.50it/s]Extractor Predicting: 160it [01:45,  1.50it/s]Extractor Predicting: 161it [01:45,  1.53it/s]Extractor Predicting: 162it [01:46,  1.54it/s]Extractor Predicting: 163it [01:46,  1.54it/s]Extractor Predicting: 164it [01:47,  1.54it/s]Extractor Predicting: 165it [01:48,  1.44it/s]Extractor Predicting: 166it [01:49,  1.47it/s]Extractor Predicting: 167it [01:49,  1.51it/s]Extractor Predicting: 168it [01:50,  1.51it/s]Extractor Predicting: 169it [01:51,  1.50it/s]Extractor Predicting: 170it [01:51,  1.49it/s]Extractor Predicting: 171it [01:52,  1.46it/s]Extractor Predicting: 172it [01:53,  1.47it/s]Extractor Predicting: 173it [01:53,  1.48it/s]Extractor Predicting: 174it [01:54,  1.44it/s]Extractor Predicting: 175it [01:55,  1.39it/s]Extractor Predicting: 176it [01:55,  1.42it/s]Extractor Predicting: 177it [01:56,  1.43it/s]Extractor Predicting: 178it [01:57,  1.48it/s]Extractor Predicting: 179it [01:57,  1.48it/s]Extractor Predicting: 180it [01:58,  1.52it/s]Extractor Predicting: 181it [01:59,  1.51it/s]Extractor Predicting: 182it [01:59,  1.53it/s]Extractor Predicting: 183it [02:00,  1.52it/s]Extractor Predicting: 184it [02:01,  1.56it/s]Extractor Predicting: 185it [02:01,  1.57it/s]Extractor Predicting: 186it [02:02,  1.58it/s]Extractor Predicting: 187it [02:02,  1.59it/s]Extractor Predicting: 188it [02:03,  1.57it/s]Extractor Predicting: 189it [02:04,  1.57it/s]Extractor Predicting: 190it [02:04,  1.55it/s]Extractor Predicting: 191it [02:05,  1.50it/s]Extractor Predicting: 192it [02:06,  1.53it/s]Extractor Predicting: 193it [02:06,  1.56it/s]Extractor Predicting: 194it [02:07,  1.56it/s]Extractor Predicting: 195it [02:08,  1.56it/s]Extractor Predicting: 196it [02:08,  1.58it/s]Extractor Predicting: 197it [02:09,  1.58it/s]Extractor Predicting: 198it [02:10,  1.55it/s]Extractor Predicting: 199it [02:10,  1.56it/s]Extractor Predicting: 200it [02:11,  1.55it/s]Extractor Predicting: 201it [02:12,  1.56it/s]Extractor Predicting: 202it [02:12,  1.58it/s]Extractor Predicting: 203it [02:13,  1.59it/s]Extractor Predicting: 204it [02:13,  1.58it/s]Extractor Predicting: 205it [02:14,  1.53it/s]Extractor Predicting: 206it [02:15,  1.53it/s]Extractor Predicting: 207it [02:15,  1.55it/s]Extractor Predicting: 208it [02:16,  1.57it/s]Extractor Predicting: 209it [02:17,  1.51it/s]Extractor Predicting: 210it [02:17,  1.49it/s]Extractor Predicting: 211it [02:18,  1.50it/s]Extractor Predicting: 212it [02:19,  1.52it/s]Extractor Predicting: 213it [02:19,  1.54it/s]Extractor Predicting: 214it [02:20,  1.50it/s]Extractor Predicting: 215it [02:21,  1.50it/s]Extractor Predicting: 216it [02:21,  1.53it/s]Extractor Predicting: 217it [02:22,  1.54it/s]Extractor Predicting: 218it [02:23,  1.48it/s]Extractor Predicting: 219it [02:23,  1.49it/s]Extractor Predicting: 220it [02:24,  1.50it/s]Extractor Predicting: 221it [02:25,  1.47it/s]Extractor Predicting: 222it [02:25,  1.47it/s]Extractor Predicting: 223it [02:26,  1.48it/s]Extractor Predicting: 224it [02:27,  1.52it/s]Extractor Predicting: 225it [02:27,  1.51it/s]Extractor Predicting: 226it [02:28,  1.56it/s]Extractor Predicting: 227it [02:29,  1.57it/s]Extractor Predicting: 228it [02:29,  1.55it/s]Extractor Predicting: 229it [02:30,  1.55it/s]Extractor Predicting: 230it [02:31,  1.52it/s]Extractor Predicting: 231it [02:31,  1.51it/s]Extractor Predicting: 232it [02:32,  1.52it/s]Extractor Predicting: 233it [02:32,  1.56it/s]Extractor Predicting: 234it [02:33,  1.54it/s]Extractor Predicting: 235it [02:34,  1.54it/s]Extractor Predicting: 236it [02:34,  1.51it/s]Extractor Predicting: 237it [02:35,  1.51it/s]Extractor Predicting: 238it [02:36,  1.54it/s]Extractor Predicting: 239it [02:36,  1.55it/s]Extractor Predicting: 240it [02:37,  1.54it/s]Extractor Predicting: 241it [02:38,  1.53it/s]Extractor Predicting: 242it [02:38,  1.51it/s]Extractor Predicting: 243it [02:39,  1.48it/s]Extractor Predicting: 244it [02:40,  1.47it/s]Extractor Predicting: 245it [02:41,  1.38it/s]Extractor Predicting: 246it [02:41,  1.40it/s]Extractor Predicting: 247it [02:42,  1.45it/s]Extractor Predicting: 248it [02:43,  1.48it/s]Extractor Predicting: 249it [02:43,  1.47it/s]Extractor Predicting: 250it [02:44,  1.48it/s]Extractor Predicting: 251it [02:45,  1.46it/s]Extractor Predicting: 252it [02:45,  1.48it/s]Extractor Predicting: 253it [02:46,  1.49it/s]Extractor Predicting: 254it [02:47,  1.52it/s]Extractor Predicting: 255it [02:47,  1.52it/s]Extractor Predicting: 256it [02:48,  1.51it/s]Extractor Predicting: 257it [02:49,  1.53it/s]Extractor Predicting: 258it [02:49,  1.52it/s]Extractor Predicting: 259it [02:50,  1.48it/s]Extractor Predicting: 260it [02:51,  1.50it/s]Extractor Predicting: 261it [02:51,  1.52it/s]Extractor Predicting: 262it [02:52,  1.50it/s]Extractor Predicting: 263it [02:53,  1.49it/s]Extractor Predicting: 264it [02:53,  1.49it/s]Extractor Predicting: 265it [02:54,  1.48it/s]Extractor Predicting: 266it [02:55,  1.46it/s]Extractor Predicting: 267it [02:55,  1.44it/s]Extractor Predicting: 268it [02:56,  1.46it/s]Extractor Predicting: 269it [02:57,  1.46it/s]Extractor Predicting: 270it [02:57,  1.45it/s]Extractor Predicting: 271it [02:58,  1.46it/s]Extractor Predicting: 272it [02:59,  1.47it/s]Extractor Predicting: 273it [02:59,  1.46it/s]Extractor Predicting: 274it [03:00,  1.46it/s]Extractor Predicting: 275it [03:01,  1.49it/s]Extractor Predicting: 276it [03:01,  1.49it/s]Extractor Predicting: 277it [03:02,  1.47it/s]Extractor Predicting: 278it [03:03,  1.47it/s]Extractor Predicting: 279it [03:04,  1.47it/s]Extractor Predicting: 280it [03:04,  1.46it/s]Extractor Predicting: 281it [03:05,  1.46it/s]Extractor Predicting: 282it [03:06,  1.48it/s]Extractor Predicting: 283it [03:06,  1.43it/s]Extractor Predicting: 284it [03:07,  1.42it/s]Extractor Predicting: 285it [03:08,  1.42it/s]Extractor Predicting: 286it [03:08,  1.41it/s]Extractor Predicting: 287it [03:09,  1.43it/s]Extractor Predicting: 288it [03:09,  1.89it/s]Extractor Predicting: 288it [03:09,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:35,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:35,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:35,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:35,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:35,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:03:36,321 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:03:36,322 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:03:36,584 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:03:37,650 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:03:37,650 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:39,027 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:39,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:39,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:39,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:03:39,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:03:39,375 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:03:39,376 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:03:39,640 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:03:39,796 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:03:39,796 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.6017156862745098,
  "recall": 0.14254608796632312,
  "score": 0.23048937918084736,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:01,  1.80it/s]Extractor Predicting: 3it [00:01,  1.65it/s]
[INFO|configuration_utils.py:515] 2023-08-28 16:03:41,978 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:03:41,979 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:03:41,982 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:03:41,983 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 16:03:41,985 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:03:45,162 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 16:03:45,167 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 16:03:45,185 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:03:45,186 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:03:45,194 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:03:45,198 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:03:45,198 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:03:45,198 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:03:45,198 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:03:45,198 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:03:45,198 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6666666666666666,
  "recall": 0.05405405405405406,
  "score": 0.1,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 16:03:45,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:46,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:46,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:47,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:48,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:49,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:50,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:50,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:51,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:52,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:53,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:54,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:54,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:55,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:56,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:57,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:58,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:59,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:03:59,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:00,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:01,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:02,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:17<04:04, 17.45s/it][WARNING|generation_utils.py:914] 2023-08-28 16:04:02,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:03,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:04,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:05,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:06,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:06,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:07,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:08,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:09,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:10,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:11,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:11,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:12,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:13,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:14,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:14,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:15,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:16,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:17,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:17,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:18,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:34<03:41, 17.05s/it][WARNING|generation_utils.py:914] 2023-08-28 16:04:19,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:20,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:21,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:22,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:23,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:23,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:24,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:25,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:26,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:26,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:27,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:28,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:29,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:29,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:30,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:31,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:31,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:32,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:33,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:34,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:34,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:35,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:36,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:51<03:28, 17.37s/it][WARNING|generation_utils.py:914] 2023-08-28 16:04:37,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:38,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:38,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:39,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:40,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:41,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:42,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:42,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:43,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:44,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:45,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:46,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:47,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:47,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:49,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:49,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:50,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:51,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:52,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:53,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:53,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:09<03:12, 17.49s/it][WARNING|generation_utils.py:914] 2023-08-28 16:04:55,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:55,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:56,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:57,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:57,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:58,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:04:59,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:00,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:00,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:01,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:02,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:03,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:03,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:04,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:05,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:05,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:06,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:07,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:08,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:08,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:09,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:10,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:10,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:26<02:51, 17.15s/it][WARNING|generation_utils.py:914] 2023-08-28 16:05:11,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:12,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:13,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:13,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:14,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:15,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:16,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:16,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:17,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:18,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:19,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:20,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:20,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:21,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:22,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:23,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:23,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:24,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:25,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:26,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:26,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:27,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:43<02:34, 17.12s/it][WARNING|generation_utils.py:914] 2023-08-28 16:05:28,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:29,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:30,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:31,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:31,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:32,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:33,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:33,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:34,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:35,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:36,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:37,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:37,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:38,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:39,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:40,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:41,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:42,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:42,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:43,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:44,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:45,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:46,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:01<02:19, 17.48s/it][WARNING|generation_utils.py:914] 2023-08-28 16:05:46,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:47,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:48,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:49,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:49,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:50,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:51,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:52,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:52,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:53,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:54,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:55,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:55,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:56,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:57,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:58,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:59,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:00,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:01,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:01,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:02,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:03,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:04,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:04,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:20<02:05, 17.93s/it][WARNING|generation_utils.py:914] 2023-08-28 16:06:05,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:06,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:07,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:07,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:08,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:09,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:10,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:10,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:11,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:11,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:12,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:13,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:14,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:14,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:15,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:16,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:16,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:17,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:18,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:18,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:19,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:34<01:41, 16.86s/it][WARNING|generation_utils.py:914] 2023-08-28 16:06:20,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:21,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:22,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:22,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:23,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:24,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:25,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:26,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:26,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:27,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:28,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:28,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:29,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:30,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:31,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:31,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:32,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:33,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:34,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:34,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:35,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:50<01:22, 16.58s/it][WARNING|generation_utils.py:914] 2023-08-28 16:06:36,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:36,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:37,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:38,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:39,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:39,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:40,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:41,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:41,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:42,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:43,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:43,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:44,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:45,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:46,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:46,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:47,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:48,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:48,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:49,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:04<01:03, 15.84s/it][WARNING|generation_utils.py:914] 2023-08-28 16:06:50,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:51,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:51,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:52,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:53,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:53,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:54,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:55,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:56,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:56,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:57,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:58,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:59,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:00,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:00,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:01,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:02,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:02,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:03,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:04,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:05,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:05,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:21<00:47, 15.91s/it][WARNING|generation_utils.py:914] 2023-08-28 16:07:06,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:07,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:07,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:08,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:09,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:10,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:11,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:12,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:12,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:13,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:14,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:15,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:16,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:17,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:17,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:18,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:20,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:20,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:21,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:22,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:23,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:24,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:39<00:33, 16.66s/it][WARNING|generation_utils.py:914] 2023-08-28 16:07:24,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:25,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:26,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:27,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:28,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:28,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:29,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:30,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:30,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:31,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:32,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:33,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:33,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:34,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:35,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:35,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:36,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:37,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:38,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:39,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:39,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:55<00:16, 16.40s/it][WARNING|generation_utils.py:914] 2023-08-28 16:07:40,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:41,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:42,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:42,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:43,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:44,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:45,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:45,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:46,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:47,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:48,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:48,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:49,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:50,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:51,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:51,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:52,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:53,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:54,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:54,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:55,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:56,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:56,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:58,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:58,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:59,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:00,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:01,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:01,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:02,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:17<00:00, 18.25s/it]Generating: 100%|██████████| 15/15 [04:17<00:00, 17.19s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:09,744 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:09,752 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:09,752 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:09,752 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:09,752 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:08:10,343 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:08:10,344 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:08:10,926 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:08:11,978 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:08:11,978 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:14,852 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:14,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:14,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:14,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:14,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:08:15,477 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:08:15,479 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:08:16,060 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:08:16,215 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:08:16,215 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8607954545454546, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9241071428571429, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.8464673913043478, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : architect .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7877604166666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9270833333333334, 'errors': {'', '(\'A Trip to Remember\', \'follows\', \'\', \'As their first season premiered , " A Trip to Remember " was nominated for an Emmy for Best Drama Series .\')'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : On 31 March 2014 , the Brazilian national squad returned to their squad for the 2018 Copa Centurio , having already won a place in the 1–0 victory against Paraguay . Head Entity : Paraguay , Tail Entity : position held .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 167, 'raw': 256}
{'target': 600, 'success': 186, 'raw': 288}
{'target': 600, 'success': 207, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 250, 'raw': 384}
{'target': 600, 'success': 268, 'raw': 416}
{'target': 600, 'success': 286, 'raw': 448}
{'target': 600, 'success': 303, 'raw': 480}
{'target': 600, 'success': 323, 'raw': 512}
{'target': 600, 'success': 346, 'raw': 544}
{'target': 600, 'success': 368, 'raw': 576}
{'target': 600, 'success': 390, 'raw': 608}
{'target': 600, 'success': 407, 'raw': 640}
{'target': 600, 'success': 430, 'raw': 672}
{'target': 600, 'success': 450, 'raw': 704}
{'target': 600, 'success': 469, 'raw': 736}
{'target': 600, 'success': 493, 'raw': 768}
{'target': 600, 'success': 510, 'raw': 800}
{'target': 600, 'success': 532, 'raw': 832}
{'target': 600, 'success': 552, 'raw': 864}
{'target': 600, 'success': 573, 'raw': 896}
{'target': 600, 'success': 587, 'raw': 928}
{'target': 600, 'success': 603, 'raw': 960}
{'prompt': 'Relation : position held .', 'success_rate': 0.628125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 12152
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12252, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.45it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.49it/s]Extractor Estimating: 5it [00:03,  1.49it/s]Extractor Estimating: 6it [00:03,  1.55it/s]Extractor Estimating: 7it [00:04,  1.52it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:06,  1.45it/s]Extractor Estimating: 10it [00:06,  1.50it/s]Extractor Estimating: 11it [00:07,  1.52it/s]Extractor Estimating: 12it [00:08,  1.46it/s]Extractor Estimating: 13it [00:08,  1.53it/s]Extractor Estimating: 14it [00:09,  1.52it/s]Extractor Estimating: 15it [00:10,  1.45it/s]Extractor Estimating: 16it [00:10,  1.43it/s]Extractor Estimating: 17it [00:11,  1.42it/s]Extractor Estimating: 18it [00:12,  1.44it/s]Extractor Estimating: 19it [00:12,  1.43it/s]Extractor Estimating: 20it [00:13,  1.44it/s]Extractor Estimating: 21it [00:14,  1.50it/s]Extractor Estimating: 22it [00:14,  1.49it/s]Extractor Estimating: 23it [00:15,  1.48it/s]Extractor Estimating: 24it [00:16,  1.47it/s]Extractor Estimating: 25it [00:16,  1.47it/s]Extractor Estimating: 26it [00:17,  1.45it/s]Extractor Estimating: 27it [00:18,  1.46it/s]Extractor Estimating: 28it [00:18,  1.48it/s]Extractor Estimating: 29it [00:19,  1.48it/s]Extractor Estimating: 30it [00:20,  1.47it/s]Extractor Estimating: 31it [00:21,  1.49it/s]Extractor Estimating: 32it [00:21,  1.54it/s]Extractor Estimating: 33it [00:22,  1.55it/s]Extractor Estimating: 34it [00:22,  1.50it/s]Extractor Estimating: 35it [00:23,  1.52it/s]Extractor Estimating: 36it [00:24,  1.48it/s]Extractor Estimating: 37it [00:25,  1.47it/s]Extractor Estimating: 38it [00:25,  1.51it/s]Extractor Estimating: 39it [00:26,  1.45it/s]Extractor Estimating: 40it [00:26,  1.51it/s]Extractor Estimating: 41it [00:27,  1.56it/s]Extractor Estimating: 42it [00:28,  1.57it/s]Extractor Estimating: 43it [00:28,  1.53it/s]Extractor Estimating: 44it [00:29,  1.48it/s]Extractor Estimating: 45it [00:30,  1.52it/s]Extractor Estimating: 46it [00:30,  1.51it/s]Extractor Estimating: 47it [00:31,  1.47it/s]Extractor Estimating: 48it [00:32,  1.53it/s]Extractor Estimating: 49it [00:32,  1.46it/s]Extractor Estimating: 50it [00:33,  1.51it/s]Extractor Estimating: 51it [00:34,  1.33it/s]Extractor Estimating: 52it [00:35,  1.38it/s]Extractor Estimating: 53it [00:35,  1.44it/s]Extractor Estimating: 54it [00:36,  1.46it/s]Extractor Estimating: 55it [00:37,  1.47it/s]Extractor Estimating: 56it [00:37,  1.47it/s]Extractor Estimating: 57it [00:38,  1.50it/s]Extractor Estimating: 58it [00:39,  1.53it/s]Extractor Estimating: 59it [00:39,  1.45it/s]Extractor Estimating: 60it [00:40,  1.50it/s]Extractor Estimating: 61it [00:41,  1.49it/s]Extractor Estimating: 62it [00:41,  1.49it/s]Extractor Estimating: 63it [00:42,  1.50it/s]Extractor Estimating: 64it [00:43,  1.49it/s]Extractor Estimating: 65it [00:43,  1.51it/s]Extractor Estimating: 66it [00:44,  1.50it/s]Extractor Estimating: 67it [00:45,  1.48it/s]Extractor Estimating: 68it [00:45,  1.48it/s]Extractor Estimating: 69it [00:46,  1.45it/s]Extractor Estimating: 70it [00:47,  1.45it/s]Extractor Estimating: 71it [00:47,  1.46it/s]Extractor Estimating: 72it [00:48,  1.47it/s]Extractor Estimating: 73it [00:49,  1.38it/s]Extractor Estimating: 74it [00:50,  1.39it/s]Extractor Estimating: 75it [00:50,  1.41it/s]Extractor Estimating: 76it [00:51,  1.45it/s]Extractor Estimating: 77it [00:52,  1.47it/s]Extractor Estimating: 78it [00:52,  1.55it/s]Extractor Estimating: 79it [00:53,  1.54it/s]Extractor Estimating: 80it [00:54,  1.56it/s]Extractor Estimating: 81it [00:54,  1.57it/s]Extractor Estimating: 82it [00:55,  1.62it/s]Extractor Estimating: 83it [00:55,  1.62it/s]Extractor Estimating: 84it [00:56,  1.63it/s]Extractor Estimating: 85it [00:57,  1.62it/s]Extractor Estimating: 86it [00:57,  1.59it/s]Extractor Estimating: 87it [00:58,  1.60it/s]Extractor Estimating: 88it [00:58,  1.56it/s]Extractor Estimating: 89it [00:59,  1.58it/s]Extractor Estimating: 90it [01:00,  1.54it/s]Extractor Estimating: 91it [01:00,  1.57it/s]Extractor Estimating: 92it [01:01,  1.59it/s]Extractor Estimating: 93it [01:02,  1.62it/s]Extractor Estimating: 94it [01:02,  1.63it/s]Extractor Estimating: 95it [01:03,  1.70it/s]Extractor Estimating: 96it [01:03,  1.65it/s]Extractor Estimating: 97it [01:04,  1.60it/s]Extractor Estimating: 98it [01:05,  1.54it/s]Extractor Estimating: 99it [01:05,  1.61it/s]Extractor Estimating: 100it [01:06,  1.55it/s]Extractor Estimating: 101it [01:07,  1.59it/s]Extractor Estimating: 102it [01:07,  1.63it/s]Extractor Estimating: 103it [01:08,  1.63it/s]Extractor Estimating: 104it [01:08,  1.69it/s]Extractor Estimating: 105it [01:09,  1.72it/s]Extractor Estimating: 106it [01:10,  1.71it/s]Extractor Estimating: 107it [01:10,  1.69it/s]Extractor Estimating: 108it [01:11,  1.69it/s]Extractor Estimating: 109it [01:11,  1.57it/s]Extractor Estimating: 110it [01:12,  1.55it/s]Extractor Estimating: 111it [01:13,  1.57it/s]Extractor Estimating: 112it [01:13,  1.60it/s]Extractor Estimating: 113it [01:14,  1.62it/s]Extractor Estimating: 114it [01:15,  1.65it/s]Extractor Estimating: 115it [01:15,  1.66it/s]Extractor Estimating: 116it [01:16,  1.64it/s]Extractor Estimating: 117it [01:16,  1.66it/s]Extractor Estimating: 118it [01:17,  1.62it/s]Extractor Estimating: 119it [01:18,  1.63it/s]Extractor Estimating: 120it [01:18,  1.62it/s]Extractor Estimating: 121it [01:19,  1.63it/s]Extractor Estimating: 122it [01:19,  1.61it/s]Extractor Estimating: 123it [01:20,  1.60it/s]Extractor Estimating: 124it [01:21,  1.59it/s]Extractor Estimating: 125it [01:21,  1.62it/s]Extractor Estimating: 126it [01:22,  1.56it/s]Extractor Estimating: 127it [01:23,  1.60it/s]Extractor Estimating: 128it [01:23,  1.63it/s]Extractor Estimating: 129it [01:24,  1.61it/s]Extractor Estimating: 130it [01:24,  1.60it/s]Extractor Estimating: 131it [01:25,  1.66it/s]Extractor Estimating: 132it [01:26,  1.65it/s]Extractor Estimating: 133it [01:26,  1.64it/s]Extractor Estimating: 134it [01:27,  1.66it/s]Extractor Estimating: 135it [01:27,  1.68it/s]Extractor Estimating: 136it [01:28,  1.62it/s]Extractor Estimating: 137it [01:29,  1.62it/s]Extractor Estimating: 138it [01:29,  1.63it/s]Extractor Estimating: 139it [01:30,  1.62it/s]Extractor Estimating: 140it [01:30,  1.65it/s]Extractor Estimating: 141it [01:31,  1.65it/s]Extractor Estimating: 142it [01:32,  1.63it/s]Extractor Estimating: 143it [01:32,  1.58it/s]Extractor Estimating: 144it [01:33,  1.57it/s]Extractor Estimating: 145it [01:34,  1.59it/s]Extractor Estimating: 146it [01:34,  1.60it/s]Extractor Estimating: 147it [01:35,  1.60it/s]Extractor Estimating: 148it [01:36,  1.59it/s]Extractor Estimating: 149it [01:36,  1.59it/s]Extractor Estimating: 150it [01:37,  1.59it/s]Extractor Estimating: 151it [01:37,  1.61it/s]Extractor Estimating: 152it [01:38,  1.59it/s]Extractor Estimating: 153it [01:39,  1.52it/s]Extractor Estimating: 154it [01:39,  1.52it/s]Extractor Estimating: 155it [01:40,  1.59it/s]Extractor Estimating: 156it [01:41,  1.59it/s]Extractor Estimating: 157it [01:41,  1.63it/s]Extractor Estimating: 158it [01:42,  1.64it/s]Extractor Estimating: 159it [01:42,  1.61it/s]Extractor Estimating: 160it [01:43,  1.60it/s]Extractor Estimating: 161it [01:44,  1.59it/s]Extractor Estimating: 162it [01:44,  1.60it/s]Extractor Estimating: 163it [01:45,  1.56it/s]Extractor Estimating: 164it [01:46,  1.54it/s]Extractor Estimating: 165it [01:46,  1.56it/s]Extractor Estimating: 166it [01:47,  1.55it/s]Extractor Estimating: 167it [01:48,  1.59it/s]Extractor Estimating: 168it [01:48,  1.52it/s]Extractor Estimating: 169it [01:49,  1.56it/s]Extractor Estimating: 170it [01:49,  1.57it/s]Extractor Estimating: 171it [01:50,  1.58it/s]Extractor Estimating: 172it [01:51,  1.56it/s]Extractor Estimating: 173it [01:51,  1.58it/s]Extractor Estimating: 174it [01:52,  1.58it/s]Extractor Estimating: 175it [01:53,  1.56it/s]Extractor Estimating: 176it [01:53,  1.57it/s]Extractor Estimating: 177it [01:54,  1.49it/s]Extractor Estimating: 178it [01:55,  1.58it/s]Extractor Estimating: 179it [01:55,  1.61it/s]Extractor Estimating: 180it [01:56,  1.57it/s]Extractor Estimating: 181it [01:57,  1.54it/s]Extractor Estimating: 182it [01:57,  1.58it/s]Extractor Estimating: 183it [01:58,  1.58it/s]Extractor Estimating: 184it [01:59,  1.39it/s]Extractor Estimating: 185it [01:59,  1.48it/s]Extractor Estimating: 186it [02:00,  1.52it/s]Extractor Estimating: 187it [02:01,  1.51it/s]Extractor Estimating: 188it [02:01,  1.53it/s]Extractor Estimating: 189it [02:02,  1.54it/s]Extractor Estimating: 190it [02:03,  1.52it/s]Extractor Estimating: 191it [02:03,  1.50it/s]Extractor Estimating: 192it [02:04,  1.49it/s]Extractor Estimating: 193it [02:05,  1.48it/s]Extractor Estimating: 194it [02:05,  1.46it/s]Extractor Estimating: 195it [02:06,  1.48it/s]Extractor Estimating: 196it [02:07,  1.50it/s]Extractor Estimating: 197it [02:07,  1.48it/s]Extractor Estimating: 198it [02:08,  1.52it/s]Extractor Estimating: 199it [02:09,  1.51it/s]Extractor Estimating: 200it [02:09,  1.50it/s]Extractor Estimating: 201it [02:10,  1.53it/s]Extractor Estimating: 202it [02:11,  1.50it/s]Extractor Estimating: 203it [02:11,  1.44it/s]Extractor Estimating: 204it [02:12,  1.50it/s]Extractor Estimating: 205it [02:13,  1.47it/s]Extractor Estimating: 206it [02:13,  1.52it/s]Extractor Estimating: 207it [02:14,  1.54it/s]Extractor Estimating: 208it [02:14,  1.58it/s]Extractor Estimating: 209it [02:15,  1.56it/s]Extractor Estimating: 210it [02:16,  1.59it/s]Extractor Estimating: 211it [02:16,  1.53it/s]Extractor Estimating: 212it [02:17,  1.52it/s]Extractor Estimating: 213it [02:18,  1.52it/s]Extractor Estimating: 214it [02:18,  1.48it/s]Extractor Estimating: 215it [02:19,  1.51it/s]Extractor Estimating: 216it [02:20,  1.52it/s]Extractor Estimating: 217it [02:20,  1.51it/s]Extractor Estimating: 218it [02:21,  1.49it/s]Extractor Estimating: 219it [02:22,  1.52it/s]Extractor Estimating: 220it [02:22,  1.50it/s]Extractor Estimating: 221it [02:23,  1.47it/s]Extractor Estimating: 222it [02:24,  1.47it/s]Extractor Estimating: 223it [02:25,  1.48it/s]Extractor Estimating: 224it [02:25,  1.47it/s]Extractor Estimating: 225it [02:26,  1.48it/s]Extractor Estimating: 226it [02:27,  1.47it/s]Extractor Estimating: 227it [02:27,  1.39it/s]Extractor Estimating: 228it [02:28,  1.43it/s]Extractor Estimating: 229it [02:29,  1.46it/s]Extractor Estimating: 230it [02:29,  1.44it/s]Extractor Estimating: 231it [02:30,  1.44it/s]Extractor Estimating: 232it [02:31,  1.37it/s]Extractor Estimating: 233it [02:32,  1.37it/s]Extractor Estimating: 234it [02:32,  1.42it/s]Extractor Estimating: 235it [02:33,  1.43it/s]Extractor Estimating: 236it [02:34,  1.41it/s]Extractor Estimating: 237it [02:34,  1.37it/s]Extractor Estimating: 238it [02:35,  1.33it/s]Extractor Estimating: 239it [02:36,  1.37it/s]Extractor Estimating: 240it [02:37,  1.39it/s]Extractor Estimating: 241it [02:37,  1.41it/s]Extractor Estimating: 242it [02:38,  1.36it/s]Extractor Estimating: 243it [02:39,  1.40it/s]Extractor Estimating: 244it [02:39,  1.43it/s]Extractor Estimating: 245it [02:40,  1.46it/s]Extractor Estimating: 246it [02:41,  1.46it/s]Extractor Estimating: 247it [02:41,  1.45it/s]Extractor Estimating: 248it [02:42,  1.44it/s]Extractor Estimating: 249it [02:43,  1.42it/s]Extractor Estimating: 250it [02:44,  1.43it/s]Extractor Estimating: 251it [02:44,  1.55it/s]Extractor Estimating: 252it [02:45,  1.60it/s]Extractor Estimating: 253it [02:45,  1.73it/s]Extractor Estimating: 254it [02:46,  1.76it/s]Extractor Estimating: 255it [02:46,  1.77it/s]Extractor Estimating: 256it [02:47,  1.83it/s]Extractor Estimating: 257it [02:47,  1.81it/s]Extractor Estimating: 258it [02:48,  1.81it/s]Extractor Estimating: 259it [02:49,  1.63it/s]Extractor Estimating: 260it [02:49,  1.71it/s]Extractor Estimating: 261it [02:50,  1.73it/s]Extractor Estimating: 262it [02:50,  1.76it/s]Extractor Estimating: 263it [02:51,  1.75it/s]Extractor Estimating: 264it [02:51,  1.81it/s]Extractor Estimating: 265it [02:52,  1.76it/s]Extractor Estimating: 266it [02:53,  1.78it/s]Extractor Estimating: 267it [02:53,  1.79it/s]Extractor Estimating: 268it [02:54,  1.87it/s]Extractor Estimating: 269it [02:54,  1.82it/s]Extractor Estimating: 270it [02:55,  1.85it/s]Extractor Estimating: 271it [02:55,  1.93it/s]Extractor Estimating: 272it [02:56,  1.90it/s]Extractor Estimating: 273it [02:56,  1.88it/s]Extractor Estimating: 274it [02:57,  1.86it/s]Extractor Estimating: 275it [02:57,  1.87it/s]Extractor Estimating: 276it [02:58,  1.75it/s]Extractor Estimating: 277it [02:59,  1.74it/s]Extractor Estimating: 278it [02:59,  1.68it/s]Extractor Estimating: 279it [03:00,  1.62it/s]Extractor Estimating: 280it [03:01,  1.57it/s]Extractor Estimating: 281it [03:01,  1.56it/s]Extractor Estimating: 282it [03:02,  1.58it/s]Extractor Estimating: 283it [03:02,  1.62it/s]Extractor Estimating: 284it [03:03,  1.56it/s]Extractor Estimating: 285it [03:04,  1.55it/s]Extractor Estimating: 286it [03:04,  1.52it/s]Extractor Estimating: 287it [03:05,  1.56it/s]Extractor Estimating: 288it [03:06,  1.53it/s]Extractor Estimating: 289it [03:06,  1.53it/s]Extractor Estimating: 290it [03:07,  1.54it/s]Extractor Estimating: 291it [03:08,  1.56it/s]Extractor Estimating: 292it [03:08,  1.52it/s]Extractor Estimating: 293it [03:09,  1.48it/s]Extractor Estimating: 294it [03:10,  1.50it/s]Extractor Estimating: 295it [03:10,  1.50it/s]Extractor Estimating: 296it [03:11,  1.49it/s]Extractor Estimating: 297it [03:12,  1.53it/s]Extractor Estimating: 298it [03:12,  1.58it/s]Extractor Estimating: 299it [03:13,  1.55it/s]Extractor Estimating: 300it [03:14,  1.51it/s]Extractor Estimating: 301it [03:14,  1.50it/s]Extractor Estimating: 302it [03:15,  1.53it/s]Extractor Estimating: 303it [03:15,  1.57it/s]Extractor Estimating: 304it [03:16,  1.53it/s]Extractor Estimating: 305it [03:17,  1.59it/s]Extractor Estimating: 306it [03:17,  1.63it/s]Extractor Estimating: 307it [03:18,  1.54it/s]Extractor Estimating: 308it [03:19,  1.55it/s]Extractor Estimating: 309it [03:19,  1.51it/s]Extractor Estimating: 310it [03:20,  1.50it/s]Extractor Estimating: 311it [03:21,  1.51it/s]Extractor Estimating: 312it [03:21,  1.43it/s]Extractor Estimating: 313it [03:22,  1.44it/s]Extractor Estimating: 314it [03:23,  1.45it/s]Extractor Estimating: 315it [03:24,  1.44it/s]Extractor Estimating: 316it [03:24,  1.46it/s]Extractor Estimating: 317it [03:25,  1.52it/s]Extractor Estimating: 318it [03:25,  1.58it/s]Extractor Estimating: 319it [03:26,  1.54it/s]Extractor Estimating: 320it [03:27,  1.53it/s]Extractor Estimating: 321it [03:27,  1.47it/s]Extractor Estimating: 322it [03:28,  1.54it/s]Extractor Estimating: 323it [03:29,  1.54it/s]Extractor Estimating: 324it [03:29,  1.57it/s]Extractor Estimating: 325it [03:30,  1.58it/s]Extractor Estimating: 326it [03:31,  1.59it/s]Extractor Estimating: 327it [03:31,  1.54it/s]Extractor Estimating: 328it [03:32,  1.50it/s]Extractor Estimating: 329it [03:33,  1.46it/s]Extractor Estimating: 330it [03:33,  1.45it/s]Extractor Estimating: 331it [03:34,  1.49it/s]Extractor Estimating: 332it [03:35,  1.50it/s]Extractor Estimating: 333it [03:35,  1.48it/s]Extractor Estimating: 334it [03:36,  1.45it/s]Extractor Estimating: 335it [03:37,  1.47it/s]Extractor Estimating: 336it [03:37,  1.50it/s]Extractor Estimating: 337it [03:38,  1.51it/s]Extractor Estimating: 338it [03:39,  1.48it/s]Extractor Estimating: 339it [03:39,  1.48it/s]Extractor Estimating: 340it [03:40,  1.43it/s]Extractor Estimating: 341it [03:41,  1.43it/s]Extractor Estimating: 342it [03:42,  1.47it/s]Extractor Estimating: 343it [03:42,  1.52it/s]Extractor Estimating: 344it [03:43,  1.53it/s]Extractor Estimating: 345it [03:43,  1.55it/s]Extractor Estimating: 346it [03:44,  1.37it/s]Extractor Estimating: 347it [03:45,  1.39it/s]Extractor Estimating: 348it [03:46,  1.40it/s]Extractor Estimating: 349it [03:46,  1.41it/s]Extractor Estimating: 350it [03:47,  1.46it/s]Extractor Estimating: 351it [03:48,  1.51it/s]Extractor Estimating: 352it [03:48,  1.45it/s]Extractor Estimating: 353it [03:49,  1.47it/s]Extractor Estimating: 354it [03:50,  1.48it/s]Extractor Estimating: 355it [03:50,  1.48it/s]Extractor Estimating: 356it [03:51,  1.50it/s]Extractor Estimating: 357it [03:52,  1.52it/s]Extractor Estimating: 358it [03:52,  1.53it/s]Extractor Estimating: 359it [03:53,  1.51it/s]Extractor Estimating: 360it [03:54,  1.56it/s]Extractor Estimating: 361it [03:54,  1.55it/s]Extractor Estimating: 362it [03:55,  1.55it/s]Extractor Estimating: 363it [03:56,  1.55it/s]Extractor Estimating: 364it [03:56,  1.54it/s]Extractor Estimating: 365it [03:57,  1.55it/s]Extractor Estimating: 366it [03:57,  1.57it/s]Extractor Estimating: 367it [03:58,  1.59it/s]Extractor Estimating: 368it [03:59,  1.58it/s]Extractor Estimating: 369it [03:59,  1.52it/s]Extractor Estimating: 370it [04:00,  1.46it/s]Extractor Estimating: 371it [04:01,  1.46it/s]Extractor Estimating: 372it [04:02,  1.47it/s]Extractor Estimating: 373it [04:02,  1.51it/s]Extractor Estimating: 374it [04:03,  1.57it/s]Extractor Estimating: 375it [04:03,  1.48it/s]Extractor Estimating: 375it [04:03,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:34,104 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:34,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:34,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:34,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:34,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:12:34,417 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:12:34,418 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:12:34,749 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:12:35,809 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:12:35,809 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:38,666 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:38,671 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:38,671 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:38,671 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:38,671 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:12:39,328 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:12:39,330 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:12:39,914 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:12:40,080 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:12:40,080 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 18:33:59,654 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 18:33:59,681 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7501 mean pseudo reward: 0.9428502647878438
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 22783
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22883, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22883, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.095, loss:641.1353
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.098, loss:577.6286
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.116, loss:613.7985
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.091, loss:555.0997
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.102, loss:588.1149
>> valid entity prec:0.5321, rec:0.5106, f1:0.5211
>> valid relation prec:0.1899, rec:0.0779, f1:0.1105
>> valid relation with NER prec:0.1899, rec:0.0779, f1:0.1105
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.434, loss:592.4862
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.089, loss:542.7961
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.097, loss:574.8586
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.104, loss:600.6116
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.106, loss:542.7813
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5588, rec:0.5332, f1:0.5457
>> valid relation prec:0.2181, rec:0.0948, f1:0.1321
>> valid relation with NER prec:0.2181, rec:0.0948, f1:0.1321
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.438, loss:577.7482
g_step 1200, step 261, avg_time 1.099, loss:607.9130
g_step 1300, step 48, avg_time 1.096, loss:571.6015
g_step 1400, step 148, avg_time 1.097, loss:536.2376
g_step 1500, step 248, avg_time 1.095, loss:555.2674
>> valid entity prec:0.5656, rec:0.5206, f1:0.5422
>> valid relation prec:0.1632, rec:0.0756, f1:0.1033
>> valid relation with NER prec:0.1632, rec:0.0756, f1:0.1033
g_step 1600, step 35, avg_time 2.415, loss:527.8191
g_step 1700, step 135, avg_time 1.101, loss:491.7360
g_step 1800, step 235, avg_time 1.109, loss:532.0010
g_step 1900, step 22, avg_time 1.098, loss:517.3064
g_step 2000, step 122, avg_time 1.102, loss:510.2932
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5203, rec:0.5503, f1:0.5349
>> valid relation prec:0.1709, rec:0.0750, f1:0.1043
>> valid relation with NER prec:0.1709, rec:0.0750, f1:0.1043
g_step 2100, step 222, avg_time 2.427, loss:496.6218
g_step 2200, step 9, avg_time 1.103, loss:497.4111
g_step 2300, step 109, avg_time 1.108, loss:462.5532
g_step 2400, step 209, avg_time 1.100, loss:461.0357
g_step 2500, step 309, avg_time 1.099, loss:481.5410
>> valid entity prec:0.5515, rec:0.4720, f1:0.5087
>> valid relation prec:0.1635, rec:0.0719, f1:0.0998
>> valid relation with NER prec:0.1635, rec:0.0719, f1:0.0998
g_step 2600, step 96, avg_time 2.411, loss:426.2817
g_step 2700, step 196, avg_time 1.101, loss:448.3345
g_step 2800, step 296, avg_time 1.106, loss:472.5988
g_step 2900, step 83, avg_time 1.100, loss:430.3271
g_step 3000, step 183, avg_time 1.097, loss:446.0957
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5289, rec:0.4867, f1:0.5069
>> valid relation prec:0.1598, rec:0.0810, f1:0.1075
>> valid relation with NER prec:0.1598, rec:0.0810, f1:0.1075
g_step 3100, step 283, avg_time 2.420, loss:434.0829
g_step 3200, step 70, avg_time 1.107, loss:401.5614
g_step 3300, step 170, avg_time 1.090, loss:414.0553
g_step 3400, step 270, avg_time 1.100, loss:428.9874
g_step 3500, step 57, avg_time 1.111, loss:367.6237
>> valid entity prec:0.5604, rec:0.4779, f1:0.5159
>> valid relation prec:0.1574, rec:0.0704, f1:0.0973
>> valid relation with NER prec:0.1574, rec:0.0704, f1:0.0973
g_step 3600, step 157, avg_time 2.409, loss:389.4302
g_step 3700, step 257, avg_time 1.104, loss:402.7774
g_step 3800, step 44, avg_time 1.088, loss:389.8978
g_step 3900, step 144, avg_time 1.102, loss:376.7965
g_step 4000, step 244, avg_time 1.111, loss:393.1150
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5621, rec:0.4639, f1:0.5083
>> valid relation prec:0.1695, rec:0.0661, f1:0.0951
>> valid relation with NER prec:0.1695, rec:0.0661, f1:0.0951
g_step 4100, step 31, avg_time 2.428, loss:371.5682
g_step 4200, step 131, avg_time 1.108, loss:350.6252
g_step 4300, step 231, avg_time 1.083, loss:362.8178
g_step 4400, step 18, avg_time 1.122, loss:364.8945
g_step 4500, step 118, avg_time 1.109, loss:319.4995
>> valid entity prec:0.5485, rec:0.4575, f1:0.4989
>> valid relation prec:0.1384, rec:0.0624, f1:0.0860
>> valid relation with NER prec:0.1384, rec:0.0624, f1:0.0860
g_step 4600, step 218, avg_time 2.418, loss:364.9206
g_step 4700, step 5, avg_time 1.101, loss:356.3622
g_step 4800, step 105, avg_time 1.096, loss:319.2095
g_step 4900, step 205, avg_time 1.109, loss:342.9355
g_step 5000, step 305, avg_time 1.102, loss:360.1510
learning rate was adjusted to 0.0008
>> valid entity prec:0.5337, rec:0.4683, f1:0.4989
>> valid relation prec:0.1499, rec:0.0644, f1:0.0901
>> valid relation with NER prec:0.1499, rec:0.0644, f1:0.0901
g_step 5100, step 92, avg_time 2.413, loss:297.9686
g_step 5200, step 192, avg_time 1.090, loss:309.4668
g_step 5300, step 292, avg_time 1.100, loss:332.4063
g_step 5400, step 79, avg_time 1.092, loss:287.3221
g_step 5500, step 179, avg_time 1.115, loss:291.8008
>> valid entity prec:0.5477, rec:0.4791, f1:0.5111
>> valid relation prec:0.1858, rec:0.0770, f1:0.1089
>> valid relation with NER prec:0.1858, rec:0.0770, f1:0.1089
g_step 5600, step 279, avg_time 2.419, loss:305.3807
g_step 5700, step 66, avg_time 1.100, loss:293.7260
g_step 5800, step 166, avg_time 1.100, loss:276.5881
g_step 5900, step 266, avg_time 1.114, loss:308.1274
g_step 6000, step 53, avg_time 1.091, loss:280.2054
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5494, rec:0.4604, f1:0.5010
>> valid relation prec:0.1566, rec:0.0770, f1:0.1032
>> valid relation with NER prec:0.1566, rec:0.0770, f1:0.1032
g_step 6100, step 153, avg_time 2.422, loss:281.4982
g_step 6200, step 253, avg_time 1.093, loss:300.9974
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:33:59 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:33:59 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-33-59_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:34:00 - WARNING - datasets.builder -   Using custom data configuration default-1ed37cbc3d93a05c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1ed37cbc3d93a05c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:34:01,485 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:34:01,486 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:34:01,487 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:34:01,488 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:34:01,546 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:01,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:01,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:01,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:01,676 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:01,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:01,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:34:01,854 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:34:04,989 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:34:04,993 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1ed37cbc3d93a05c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.06ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.01ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.62ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.94ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.17ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.33ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.42ba/s]100%|██████████| 8/8 [00:01<00:00,  5.25ba/s]100%|██████████| 8/8 [00:01<00:00,  4.32ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.09ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.71ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.99ba/s]100%|██████████| 4/4 [00:00<00:00,  5.06ba/s]100%|██████████| 4/4 [00:00<00:00,  4.44ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.50ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.23ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.71ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.92ba/s]100%|██████████| 8/8 [00:00<00:00, 10.34ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.08ba/s] 50%|█████     | 2/4 [00:00<00:00,  7.84ba/s]100%|██████████| 4/4 [00:00<00:00, 10.79ba/s]100%|██████████| 4/4 [00:00<00:00,  9.75ba/s]
[INFO|trainer.py:414] 2023-08-28 18:34:09,556 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:34:09,575 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:34:09,575 >>   Num examples = 7510
[INFO|trainer.py:1149] 2023-08-28 18:34:09,575 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:34:09,575 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:34:09,575 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:34:09,575 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:34:09,576 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<03:00,  3.24it/s]  0%|          | 2/585 [00:00<02:52,  3.37it/s]  1%|          | 3/585 [00:00<02:50,  3.42it/s]  1%|          | 4/585 [00:01<02:48,  3.44it/s]  1%|          | 5/585 [00:01<02:47,  3.45it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:46,  3.46it/s]  1%|▏         | 8/585 [00:02<02:46,  3.47it/s]  2%|▏         | 9/585 [00:02<02:46,  3.47it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:45,  3.47it/s]  2%|▏         | 12/585 [00:03<02:48,  3.41it/s]  2%|▏         | 13/585 [00:03<02:46,  3.43it/s]  2%|▏         | 14/585 [00:04<02:46,  3.44it/s]  3%|▎         | 15/585 [00:04<02:45,  3.45it/s]  3%|▎         | 16/585 [00:04<02:44,  3.46it/s]  3%|▎         | 17/585 [00:04<02:44,  3.46it/s]  3%|▎         | 18/585 [00:05<02:43,  3.46it/s]  3%|▎         | 19/585 [00:05<02:43,  3.46it/s]  3%|▎         | 20/585 [00:05<02:43,  3.46it/s]  4%|▎         | 21/585 [00:06<02:42,  3.46it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:46,  3.37it/s]  4%|▍         | 24/585 [00:06<02:45,  3.39it/s]  4%|▍         | 25/585 [00:07<02:44,  3.41it/s]  4%|▍         | 26/585 [00:07<02:43,  3.43it/s]  5%|▍         | 27/585 [00:07<02:42,  3.44it/s]  5%|▍         | 28/585 [00:08<02:41,  3.45it/s]  5%|▍         | 29/585 [00:08<02:41,  3.45it/s]  5%|▌         | 30/585 [00:08<02:40,  3.46it/s]  5%|▌         | 31/585 [00:09<02:40,  3.46it/s]  5%|▌         | 32/585 [00:09<02:39,  3.46it/s]  6%|▌         | 33/585 [00:09<02:39,  3.46it/s]  6%|▌         | 34/585 [00:09<02:48,  3.27it/s]  6%|▌         | 35/585 [00:10<02:45,  3.33it/s]  6%|▌         | 36/585 [00:10<02:43,  3.37it/s]  6%|▋         | 37/585 [00:10<02:41,  3.39it/s]  6%|▋         | 38/585 [00:11<02:40,  3.41it/s]  7%|▋         | 39/585 [00:11<02:39,  3.43it/s]  7%|▋         | 40/585 [00:11<02:38,  3.44it/s]  7%|▋         | 41/585 [00:11<02:38,  3.44it/s]  7%|▋         | 42/585 [00:12<02:37,  3.45it/s]  7%|▋         | 43/585 [00:12<02:36,  3.45it/s]  8%|▊         | 44/585 [00:12<02:36,  3.46it/s]  8%|▊         | 45/585 [00:13<02:36,  3.46it/s]  8%|▊         | 46/585 [00:13<02:35,  3.46it/s]  8%|▊         | 47/585 [00:13<02:51,  3.13it/s]  8%|▊         | 48/585 [00:14<02:46,  3.22it/s]  8%|▊         | 49/585 [00:14<02:42,  3.29it/s]  9%|▊         | 50/585 [00:14<02:40,  3.34it/s]  9%|▊         | 51/585 [00:14<02:38,  3.38it/s]  9%|▉         | 52/585 [00:15<02:36,  3.40it/s]  9%|▉         | 53/585 [00:15<02:35,  3.42it/s]  9%|▉         | 54/585 [00:15<02:34,  3.43it/s]  9%|▉         | 55/585 [00:16<02:34,  3.44it/s] 10%|▉         | 56/585 [00:16<02:33,  3.45it/s] 10%|▉         | 57/585 [00:16<02:32,  3.45it/s] 10%|▉         | 58/585 [00:16<02:32,  3.45it/s] 10%|█         | 59/585 [00:17<02:32,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.46it/s] 11%|█         | 62/585 [00:18<02:31,  3.46it/s] 11%|█         | 63/585 [00:18<02:30,  3.46it/s] 11%|█         | 64/585 [00:18<02:30,  3.46it/s] 11%|█         | 65/585 [00:18<02:30,  3.44it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.45it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.45it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:20<02:29,  3.46it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:21<02:28,  3.46it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.46it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.46it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 79/585 [00:23<02:26,  3.46it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.45it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 82/585 [00:23<02:27,  3.42it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 86/585 [00:25<02:24,  3.45it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:27<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.46it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.46it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.46it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.45it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.42it/s] 17%|█▋        | 101/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 102/585 [00:29<02:20,  3.44it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.44it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.45it/s] 18%|█▊        | 107/585 [00:31<02:29,  3.20it/s] 18%|█▊        | 108/585 [00:31<02:25,  3.27it/s] 19%|█▊        | 109/585 [00:31<02:23,  3.32it/s] 19%|█▉        | 110/585 [00:32<02:21,  3.36it/s] 19%|█▉        | 111/585 [00:32<02:19,  3.39it/s] 19%|█▉        | 112/585 [00:32<02:18,  3.41it/s] 19%|█▉        | 113/585 [00:32<02:17,  3.42it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.43it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.44it/s] 20%|█▉        | 116/585 [00:33<02:16,  3.44it/s] 20%|██        | 117/585 [00:34<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 18:34:43,756 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:34:43,756 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:34:43,756 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.50it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.48it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.61it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.96it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.39it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.10it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.93it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.56it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.50it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.59it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.60it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.51it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.57it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.53it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.55it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.66it/s][A
 20%|██        | 88/437 [00:01<00:07, 44.58it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 45.14it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 45.67it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 45.94it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.06it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.29it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.32it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.41it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.33it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.37it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.50it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.48it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.53it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.54it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.56it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.47it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.52it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.50it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.49it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.56it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.54it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.34it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.58it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.59it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.55it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.36it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.39it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.48it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 42.45it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 43.61it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 44.40it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 45.07it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 45.53it/s][A
 58%|█████▊    | 253/437 [00:05<00:04, 45.87it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.13it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.13it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.15it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.24it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.34it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.44it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.44it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.48it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.53it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.61it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.49it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.42it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.44it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 41.05it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 42.62it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 43.70it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 44.50it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 45.10it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 45.57it/s][A
 81%|████████  | 353/437 [00:07<00:01, 45.88it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.04it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.00it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.01it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.15it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.31it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.41it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.45it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.46it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.45it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.53it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.39it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.36it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.40it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.43it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.47it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.53it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:43<02:15,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.53it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:34:53,398 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 18:34:53,492 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:34:57,164 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:34:57,187 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:34:57,197 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:57<56:21,  7.24s/it] 20%|██        | 119/585 [00:57<40:10,  5.17s/it] 21%|██        | 120/585 [00:58<28:44,  3.71s/it] 21%|██        | 121/585 [00:58<20:44,  2.68s/it] 21%|██        | 122/585 [00:58<15:09,  1.96s/it] 21%|██        | 123/585 [00:59<11:15,  1.46s/it] 21%|██        | 124/585 [00:59<08:31,  1.11s/it] 21%|██▏       | 125/585 [00:59<06:37,  1.16it/s] 22%|██▏       | 126/585 [00:59<05:17,  1.45it/s] 22%|██▏       | 127/585 [01:00<04:21,  1.75it/s] 22%|██▏       | 128/585 [01:00<03:42,  2.06it/s] 22%|██▏       | 129/585 [01:00<03:14,  2.34it/s] 22%|██▏       | 130/585 [01:01<03:48,  1.99it/s] 22%|██▏       | 131/585 [01:01<03:18,  2.28it/s] 23%|██▎       | 132/585 [01:02<02:58,  2.54it/s] 23%|██▎       | 133/585 [01:02<02:43,  2.76it/s] 23%|██▎       | 134/585 [01:02<02:33,  2.94it/s] 23%|██▎       | 135/585 [01:02<02:26,  3.08it/s] 23%|██▎       | 136/585 [01:03<02:21,  3.18it/s] 23%|██▎       | 137/585 [01:03<02:17,  3.26it/s] 24%|██▎       | 138/585 [01:03<02:14,  3.31it/s] 24%|██▍       | 139/585 [01:04<02:12,  3.35it/s] 24%|██▍       | 140/585 [01:04<03:05,  2.40it/s] 24%|██▍       | 141/585 [01:05<02:48,  2.64it/s] 24%|██▍       | 142/585 [01:05<02:35,  2.84it/s] 24%|██▍       | 143/585 [01:05<02:27,  3.00it/s] 25%|██▍       | 144/585 [01:05<02:20,  3.13it/s] 25%|██▍       | 145/585 [01:06<02:16,  3.22it/s] 25%|██▍       | 146/585 [01:06<02:13,  3.29it/s] 25%|██▌       | 147/585 [01:06<02:11,  3.33it/s] 25%|██▌       | 148/585 [01:07<02:09,  3.37it/s] 25%|██▌       | 149/585 [01:07<02:29,  2.92it/s] 26%|██▌       | 150/585 [01:07<02:21,  3.07it/s] 26%|██▌       | 151/585 [01:08<02:16,  3.17it/s] 26%|██▌       | 152/585 [01:08<02:13,  3.25it/s] 26%|██▌       | 153/585 [01:08<02:10,  3.31it/s] 26%|██▋       | 154/585 [01:09<02:08,  3.35it/s] 26%|██▋       | 155/585 [01:09<02:07,  3.39it/s] 27%|██▋       | 156/585 [01:09<02:05,  3.41it/s] 27%|██▋       | 157/585 [01:09<02:05,  3.42it/s] 27%|██▋       | 158/585 [01:10<02:04,  3.43it/s] 27%|██▋       | 159/585 [01:10<02:09,  3.28it/s] 27%|██▋       | 160/585 [01:10<02:07,  3.33it/s] 28%|██▊       | 161/585 [01:11<02:05,  3.37it/s] 28%|██▊       | 162/585 [01:11<02:04,  3.39it/s] 28%|██▊       | 163/585 [01:11<02:03,  3.41it/s] 28%|██▊       | 164/585 [01:11<02:02,  3.42it/s] 28%|██▊       | 165/585 [01:12<02:02,  3.44it/s] 28%|██▊       | 166/585 [01:12<02:01,  3.44it/s] 29%|██▊       | 167/585 [01:12<02:01,  3.44it/s] 29%|██▊       | 168/585 [01:13<02:01,  3.44it/s] 29%|██▉       | 169/585 [01:13<02:00,  3.45it/s] 29%|██▉       | 170/585 [01:13<02:00,  3.45it/s] 29%|██▉       | 171/585 [01:13<01:59,  3.45it/s] 29%|██▉       | 172/585 [01:14<01:59,  3.45it/s] 30%|██▉       | 173/585 [01:14<01:59,  3.46it/s] 30%|██▉       | 174/585 [01:14<01:58,  3.45it/s] 30%|██▉       | 175/585 [01:15<01:59,  3.42it/s] 30%|███       | 176/585 [01:15<01:59,  3.43it/s] 30%|███       | 177/585 [01:15<01:58,  3.44it/s] 30%|███       | 178/585 [01:16<01:58,  3.44it/s] 31%|███       | 179/585 [01:16<01:57,  3.45it/s] 31%|███       | 180/585 [01:16<01:57,  3.45it/s] 31%|███       | 181/585 [01:16<01:57,  3.45it/s] 31%|███       | 182/585 [01:17<01:56,  3.45it/s] 31%|███▏      | 183/585 [01:17<01:56,  3.45it/s] 31%|███▏      | 184/585 [01:17<01:56,  3.45it/s] 32%|███▏      | 185/585 [01:18<01:55,  3.46it/s] 32%|███▏      | 186/585 [01:18<01:57,  3.38it/s] 32%|███▏      | 187/585 [01:18<01:56,  3.41it/s] 32%|███▏      | 188/585 [01:18<01:56,  3.42it/s] 32%|███▏      | 189/585 [01:19<01:55,  3.43it/s] 32%|███▏      | 190/585 [01:19<01:54,  3.44it/s] 33%|███▎      | 191/585 [01:19<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:20<01:53,  3.45it/s] 33%|███▎      | 193/585 [01:20<01:53,  3.45it/s] 33%|███▎      | 194/585 [01:20<01:53,  3.45it/s] 33%|███▎      | 195/585 [01:20<01:52,  3.45it/s] 34%|███▎      | 196/585 [01:21<01:52,  3.45it/s] 34%|███▎      | 197/585 [01:21<01:55,  3.37it/s] 34%|███▍      | 198/585 [01:21<01:53,  3.40it/s] 34%|███▍      | 199/585 [01:22<01:53,  3.41it/s] 34%|███▍      | 200/585 [01:22<01:52,  3.42it/s] 34%|███▍      | 201/585 [01:22<01:51,  3.43it/s] 35%|███▍      | 202/585 [01:23<01:51,  3.44it/s] 35%|███▍      | 203/585 [01:23<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:23<01:50,  3.44it/s] 35%|███▌      | 205/585 [01:23<01:50,  3.44it/s] 35%|███▌      | 206/585 [01:24<01:49,  3.45it/s] 35%|███▌      | 207/585 [01:24<01:49,  3.45it/s] 36%|███▌      | 208/585 [01:24<01:50,  3.41it/s] 36%|███▌      | 209/585 [01:25<01:49,  3.42it/s] 36%|███▌      | 210/585 [01:25<01:49,  3.43it/s] 36%|███▌      | 211/585 [01:25<01:48,  3.44it/s] 36%|███▌      | 212/585 [01:25<01:48,  3.44it/s] 36%|███▋      | 213/585 [01:26<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:26<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:26<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:27<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:27<01:46,  3.45it/s] 37%|███▋      | 218/585 [01:27<01:46,  3.45it/s] 37%|███▋      | 219/585 [01:27<01:47,  3.40it/s] 38%|███▊      | 220/585 [01:28<01:46,  3.42it/s] 38%|███▊      | 221/585 [01:28<01:46,  3.43it/s] 38%|███▊      | 222/585 [01:28<01:45,  3.43it/s] 38%|███▊      | 223/585 [01:29<01:45,  3.44it/s] 38%|███▊      | 224/585 [01:29<01:44,  3.44it/s] 38%|███▊      | 225/585 [01:29<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:29<01:44,  3.45it/s] 39%|███▉      | 227/585 [01:30<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:30<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:30<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:31<01:43,  3.41it/s] 39%|███▉      | 231/585 [01:31<01:43,  3.42it/s] 40%|███▉      | 232/585 [01:31<01:42,  3.43it/s] 40%|███▉      | 233/585 [01:32<01:42,  3.44it/s] 40%|████      | 234/585 [01:32<01:41,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 18:35:41,934 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:35:41,934 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:35:41,934 >>   Batch size = 8
{'eval_loss': 0.977833092212677, 'eval_runtime': 9.5359, 'eval_samples_per_second': 366.302, 'eval_steps_per_second': 45.827, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.10it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.40it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.63it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.92it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.10it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.91it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.51it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.42it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.45it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.49it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.48it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.56it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.50it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.50it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.53it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.49it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.36it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.48it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.53it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.56it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.54it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.55it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.59it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.51it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.50it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.39it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.40it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.43it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.49it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.54it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.54it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.64it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.47it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.49it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.47it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.46it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.41it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.40it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.41it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.51it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.56it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.60it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.47it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.42it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.46it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.45it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.40it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.46it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.49it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.47it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.55it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.45it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.50it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.34it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.38it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.32it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.40it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.49it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.41it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.48it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.51it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.35it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.31it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.38it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.44it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.37it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.37it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.38it/s][A
 81%|████████  | 353/437 [00:07<00:02, 39.87it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 41.64it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 43.04it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 43.89it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 44.72it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 45.25it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.61it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 45.79it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 45.78it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 45.95it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 45.85it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.24it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.36it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.34it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.45it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.51it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.31it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:41<01:41,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 46.31it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:35:51,464 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 18:35:51,627 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:35:57,568 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:35:57,583 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:35:57,595 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:58<47:12,  8.09s/it] 40%|████      | 236/585 [01:58<33:31,  5.76s/it] 41%|████      | 237/585 [01:59<23:54,  4.12s/it] 41%|████      | 238/585 [01:59<17:11,  2.97s/it] 41%|████      | 239/585 [01:59<12:29,  2.17s/it] 41%|████      | 240/585 [02:00<09:13,  1.60s/it] 41%|████      | 241/585 [02:00<06:55,  1.21s/it] 41%|████▏     | 242/585 [02:00<05:19,  1.07it/s] 42%|████▏     | 243/585 [02:00<04:12,  1.35it/s] 42%|████▏     | 244/585 [02:01<03:26,  1.66it/s] 42%|████▏     | 245/585 [02:01<02:53,  1.96it/s] 42%|████▏     | 246/585 [02:01<02:30,  2.26it/s] 42%|████▏     | 247/585 [02:02<02:19,  2.42it/s] 42%|████▏     | 248/585 [02:02<02:06,  2.66it/s] 43%|████▎     | 249/585 [02:02<01:57,  2.86it/s] 43%|████▎     | 250/585 [02:03<01:51,  3.01it/s] 43%|████▎     | 251/585 [02:03<01:46,  3.13it/s] 43%|████▎     | 252/585 [02:03<01:43,  3.22it/s] 43%|████▎     | 253/585 [02:03<01:40,  3.29it/s] 43%|████▎     | 254/585 [02:04<01:39,  3.34it/s] 44%|████▎     | 255/585 [02:04<01:37,  3.37it/s] 44%|████▍     | 256/585 [02:04<01:36,  3.40it/s] 44%|████▍     | 257/585 [02:05<01:35,  3.42it/s] 44%|████▍     | 258/585 [02:05<01:43,  3.15it/s] 44%|████▍     | 259/585 [02:05<01:40,  3.24it/s] 44%|████▍     | 260/585 [02:06<01:38,  3.30it/s] 45%|████▍     | 261/585 [02:06<01:36,  3.35it/s] 45%|████▍     | 262/585 [02:06<01:35,  3.38it/s] 45%|████▍     | 263/585 [02:06<01:34,  3.40it/s] 45%|████▌     | 264/585 [02:07<01:33,  3.42it/s] 45%|████▌     | 265/585 [02:07<01:33,  3.43it/s] 45%|████▌     | 266/585 [02:07<01:32,  3.44it/s] 46%|████▌     | 267/585 [02:08<01:32,  3.45it/s] 46%|████▌     | 268/585 [02:08<01:31,  3.45it/s] 46%|████▌     | 269/585 [02:08<01:35,  3.32it/s] 46%|████▌     | 270/585 [02:08<01:33,  3.36it/s] 46%|████▋     | 271/585 [02:09<01:32,  3.39it/s] 46%|████▋     | 272/585 [02:09<01:31,  3.41it/s] 47%|████▋     | 273/585 [02:09<01:31,  3.43it/s] 47%|████▋     | 274/585 [02:10<01:30,  3.44it/s] 47%|████▋     | 275/585 [02:10<01:29,  3.45it/s] 47%|████▋     | 276/585 [02:10<01:29,  3.45it/s] 47%|████▋     | 277/585 [02:10<01:29,  3.45it/s] 48%|████▊     | 278/585 [02:11<01:28,  3.46it/s] 48%|████▊     | 279/585 [02:11<01:28,  3.46it/s] 48%|████▊     | 280/585 [02:11<01:31,  3.32it/s] 48%|████▊     | 281/585 [02:12<01:30,  3.36it/s] 48%|████▊     | 282/585 [02:12<01:29,  3.39it/s] 48%|████▊     | 283/585 [02:12<01:28,  3.41it/s] 49%|████▊     | 284/585 [02:13<01:27,  3.43it/s] 49%|████▊     | 285/585 [02:13<01:27,  3.44it/s] 49%|████▉     | 286/585 [02:13<01:45,  2.84it/s] 49%|████▉     | 287/585 [02:14<01:39,  3.00it/s] 49%|████▉     | 288/585 [02:14<01:35,  3.12it/s] 49%|████▉     | 289/585 [02:14<01:31,  3.22it/s] 50%|████▉     | 290/585 [02:14<01:29,  3.29it/s] 50%|████▉     | 291/585 [02:15<01:28,  3.34it/s] 50%|████▉     | 292/585 [02:15<01:26,  3.37it/s] 50%|█████     | 293/585 [02:15<01:26,  3.39it/s] 50%|█████     | 294/585 [02:16<01:25,  3.41it/s] 50%|█████     | 295/585 [02:16<01:24,  3.43it/s] 51%|█████     | 296/585 [02:16<01:24,  3.41it/s] 51%|█████     | 297/585 [02:16<01:23,  3.43it/s] 51%|█████     | 298/585 [02:17<01:23,  3.44it/s] 51%|█████     | 299/585 [02:17<01:23,  3.44it/s] 51%|█████▏    | 300/585 [02:17<01:22,  3.45it/s] 51%|█████▏    | 301/585 [02:18<01:22,  3.45it/s] 52%|█████▏    | 302/585 [02:18<01:21,  3.45it/s] 52%|█████▏    | 303/585 [02:18<01:21,  3.45it/s] 52%|█████▏    | 304/585 [02:19<01:21,  3.46it/s] 52%|█████▏    | 305/585 [02:19<01:21,  3.46it/s] 52%|█████▏    | 306/585 [02:19<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:19<01:22,  3.36it/s] 53%|█████▎    | 308/585 [02:20<01:21,  3.39it/s] 53%|█████▎    | 309/585 [02:20<01:20,  3.41it/s] 53%|█████▎    | 310/585 [02:20<01:20,  3.42it/s] 53%|█████▎    | 311/585 [02:21<01:19,  3.43it/s] 53%|█████▎    | 312/585 [02:21<01:19,  3.44it/s] 54%|█████▎    | 313/585 [02:21<01:18,  3.45it/s] 54%|█████▎    | 314/585 [02:21<01:18,  3.45it/s] 54%|█████▍    | 315/585 [02:22<01:18,  3.45it/s] 54%|█████▍    | 316/585 [02:22<01:17,  3.45it/s] 54%|█████▍    | 317/585 [02:22<01:17,  3.45it/s] 54%|█████▍    | 318/585 [02:23<01:17,  3.44it/s] 55%|█████▍    | 319/585 [02:23<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:23<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:23<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:24<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:24<01:15,  3.45it/s] 55%|█████▌    | 324/585 [02:24<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:25<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:25<01:14,  3.46it/s] 56%|█████▌    | 327/585 [02:25<01:14,  3.46it/s] 56%|█████▌    | 328/585 [02:25<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:26<01:14,  3.45it/s] 56%|█████▋    | 330/585 [02:26<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:26<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:27<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:27<01:12,  3.45it/s] 57%|█████▋    | 334/585 [02:27<01:12,  3.46it/s] 57%|█████▋    | 335/585 [02:28<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:28<01:12,  3.45it/s] 58%|█████▊    | 337/585 [02:28<01:11,  3.46it/s] 58%|█████▊    | 338/585 [02:28<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:29<01:11,  3.45it/s] 58%|█████▊    | 340/585 [02:29<01:11,  3.44it/s] 58%|█████▊    | 341/585 [02:29<01:10,  3.44it/s] 58%|█████▊    | 342/585 [02:30<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:30<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:30<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:30<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:31<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:31<01:08,  3.45it/s] 59%|█████▉    | 348/585 [02:31<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:32<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:32<01:08,  3.45it/s] 60%|██████    | 351/585 [02:32<01:08,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 18:36:42,284 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:36:42,284 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:36:42,284 >>   Batch size = 8
{'eval_loss': 0.9925274848937988, 'eval_runtime': 9.5102, 'eval_samples_per_second': 367.29, 'eval_steps_per_second': 45.951, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.14it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.19it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.55it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.87it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.45it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.17it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.92it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.60it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.55it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.64it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.62it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.59it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.63it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.56it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.64it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.66it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.45it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.46it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.57it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.48it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.57it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.62it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.57it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.59it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.61it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.51it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.50it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.48it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.52it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.59it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.59it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.57it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.61it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.59it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.55it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.56it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.56it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.47it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.52it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.57it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.60it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.62it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.61it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.45it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.47it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.61it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.52it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.51it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.51it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.47it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.55it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.55it/s][A
 61%|██████▏   | 268/437 [00:05<00:04, 41.37it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 42.76it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 43.85it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 44.63it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 45.18it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 45.56it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 45.81it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.07it/s][A
 70%|███████   | 308/437 [00:06<00:02, 45.97it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.06it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.24it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.35it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.32it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.45it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.51it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.52it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.45it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.34it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.24it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.25it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.28it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.37it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.38it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.49it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.52it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.56it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.38it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.37it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 43.22it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 44.15it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 44.85it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 45.34it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 45.66it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 45.95it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 45.95it/s][A 60%|██████    | 351/585 [02:42<01:08,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:36:51,833 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 18:36:51,868 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:36:55,291 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:36:55,313 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:36:55,325 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:55<27:22,  7.05s/it] 60%|██████    | 353/585 [02:55<19:25,  5.02s/it] 61%|██████    | 354/585 [02:56<13:52,  3.60s/it] 61%|██████    | 355/585 [02:56<09:59,  2.61s/it] 61%|██████    | 356/585 [02:56<07:17,  1.91s/it] 61%|██████    | 357/585 [02:56<05:24,  1.43s/it] 61%|██████    | 358/585 [02:57<04:06,  1.08s/it] 61%|██████▏   | 359/585 [02:57<03:11,  1.18it/s] 62%|██████▏   | 360/585 [02:57<02:32,  1.47it/s] 62%|██████▏   | 361/585 [02:58<02:05,  1.78it/s] 62%|██████▏   | 362/585 [02:58<01:46,  2.09it/s] 62%|██████▏   | 363/585 [02:58<01:33,  2.37it/s] 62%|██████▏   | 364/585 [02:58<01:27,  2.52it/s] 62%|██████▏   | 365/585 [02:59<01:20,  2.75it/s] 63%|██████▎   | 366/585 [02:59<01:14,  2.93it/s] 63%|██████▎   | 367/585 [02:59<01:11,  3.07it/s] 63%|██████▎   | 368/585 [03:00<01:08,  3.18it/s] 63%|██████▎   | 369/585 [03:00<01:06,  3.26it/s] 63%|██████▎   | 370/585 [03:00<01:04,  3.32it/s] 63%|██████▎   | 371/585 [03:01<01:03,  3.36it/s] 64%|██████▎   | 372/585 [03:01<01:02,  3.39it/s] 64%|██████▍   | 373/585 [03:01<01:02,  3.40it/s] 64%|██████▍   | 374/585 [03:01<01:01,  3.42it/s] 64%|██████▍   | 375/585 [03:02<01:22,  2.55it/s] 64%|██████▍   | 376/585 [03:02<01:15,  2.77it/s] 64%|██████▍   | 377/585 [03:03<01:10,  2.95it/s] 65%|██████▍   | 378/585 [03:03<01:07,  3.08it/s] 65%|██████▍   | 379/585 [03:03<01:04,  3.19it/s] 65%|██████▍   | 380/585 [03:03<01:02,  3.26it/s] 65%|██████▌   | 381/585 [03:04<01:01,  3.32it/s] 65%|██████▌   | 382/585 [03:04<01:00,  3.36it/s] 65%|██████▌   | 383/585 [03:04<00:59,  3.38it/s] 66%|██████▌   | 384/585 [03:05<00:59,  3.41it/s] 66%|██████▌   | 385/585 [03:05<01:13,  2.71it/s] 66%|██████▌   | 386/585 [03:05<01:08,  2.90it/s] 66%|██████▌   | 387/585 [03:06<01:05,  3.04it/s] 66%|██████▋   | 388/585 [03:06<01:02,  3.16it/s] 66%|██████▋   | 389/585 [03:06<01:00,  3.24it/s] 67%|██████▋   | 390/585 [03:07<00:58,  3.31it/s] 67%|██████▋   | 391/585 [03:07<00:57,  3.35it/s] 67%|██████▋   | 392/585 [03:07<00:57,  3.38it/s] 67%|██████▋   | 393/585 [03:07<00:56,  3.41it/s] 67%|██████▋   | 394/585 [03:08<00:55,  3.42it/s] 68%|██████▊   | 395/585 [03:08<01:02,  3.03it/s] 68%|██████▊   | 396/585 [03:08<01:00,  3.15it/s] 68%|██████▊   | 397/585 [03:09<00:58,  3.23it/s] 68%|██████▊   | 398/585 [03:09<00:56,  3.29it/s] 68%|██████▊   | 399/585 [03:09<00:55,  3.34it/s] 68%|██████▊   | 400/585 [03:10<00:54,  3.37it/s] 69%|██████▊   | 401/585 [03:10<00:54,  3.40it/s] 69%|██████▊   | 402/585 [03:10<00:53,  3.42it/s] 69%|██████▉   | 403/585 [03:10<00:53,  3.43it/s] 69%|██████▉   | 404/585 [03:11<00:52,  3.44it/s] 69%|██████▉   | 405/585 [03:11<01:01,  2.94it/s] 69%|██████▉   | 406/585 [03:12<00:58,  3.08it/s] 70%|██████▉   | 407/585 [03:12<00:55,  3.18it/s] 70%|██████▉   | 408/585 [03:12<00:54,  3.26it/s] 70%|██████▉   | 409/585 [03:12<00:53,  3.32it/s] 70%|███████   | 410/585 [03:13<00:52,  3.36it/s] 70%|███████   | 411/585 [03:13<00:51,  3.39it/s] 70%|███████   | 412/585 [03:13<00:50,  3.41it/s] 71%|███████   | 413/585 [03:14<00:50,  3.42it/s] 71%|███████   | 414/585 [03:14<00:49,  3.43it/s] 71%|███████   | 415/585 [03:14<00:57,  2.97it/s] 71%|███████   | 416/585 [03:15<01:03,  2.67it/s] 71%|███████▏  | 417/585 [03:15<00:58,  2.86it/s] 71%|███████▏  | 418/585 [03:15<00:55,  3.02it/s] 72%|███████▏  | 419/585 [03:16<00:52,  3.14it/s] 72%|███████▏  | 420/585 [03:16<00:51,  3.23it/s] 72%|███████▏  | 421/585 [03:16<00:49,  3.29it/s] 72%|███████▏  | 422/585 [03:16<00:48,  3.34it/s] 72%|███████▏  | 423/585 [03:17<00:47,  3.38it/s] 72%|███████▏  | 424/585 [03:17<00:47,  3.40it/s] 73%|███████▎  | 425/585 [03:17<00:46,  3.42it/s] 73%|███████▎  | 426/585 [03:18<00:48,  3.29it/s] 73%|███████▎  | 427/585 [03:18<00:47,  3.34it/s] 73%|███████▎  | 428/585 [03:18<00:46,  3.38it/s] 73%|███████▎  | 429/585 [03:19<00:45,  3.40it/s] 74%|███████▎  | 430/585 [03:19<00:45,  3.42it/s] 74%|███████▎  | 431/585 [03:19<00:44,  3.43it/s] 74%|███████▍  | 432/585 [03:19<00:44,  3.43it/s] 74%|███████▍  | 433/585 [03:20<00:44,  3.44it/s] 74%|███████▍  | 434/585 [03:20<00:43,  3.44it/s] 74%|███████▍  | 435/585 [03:20<00:43,  3.45it/s] 75%|███████▍  | 436/585 [03:21<00:43,  3.45it/s] 75%|███████▍  | 437/585 [03:21<00:43,  3.44it/s] 75%|███████▍  | 438/585 [03:21<00:42,  3.44it/s] 75%|███████▌  | 439/585 [03:21<00:42,  3.45it/s] 75%|███████▌  | 440/585 [03:22<00:42,  3.45it/s] 75%|███████▌  | 441/585 [03:22<00:41,  3.45it/s] 76%|███████▌  | 442/585 [03:22<00:41,  3.45it/s] 76%|███████▌  | 443/585 [03:23<00:41,  3.45it/s] 76%|███████▌  | 444/585 [03:23<00:40,  3.45it/s] 76%|███████▌  | 445/585 [03:23<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:23<00:40,  3.45it/s] 76%|███████▋  | 447/585 [03:24<00:39,  3.45it/s] 77%|███████▋  | 448/585 [03:24<00:40,  3.39it/s] 77%|███████▋  | 449/585 [03:24<00:39,  3.41it/s] 77%|███████▋  | 450/585 [03:25<00:39,  3.42it/s] 77%|███████▋  | 451/585 [03:25<00:39,  3.43it/s] 77%|███████▋  | 452/585 [03:25<00:38,  3.44it/s] 77%|███████▋  | 453/585 [03:26<00:38,  3.44it/s] 78%|███████▊  | 454/585 [03:26<00:38,  3.45it/s] 78%|███████▊  | 455/585 [03:26<00:37,  3.45it/s] 78%|███████▊  | 456/585 [03:26<00:37,  3.45it/s] 78%|███████▊  | 457/585 [03:27<00:37,  3.45it/s] 78%|███████▊  | 458/585 [03:27<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:27<00:36,  3.43it/s] 79%|███████▊  | 460/585 [03:28<00:36,  3.44it/s] 79%|███████▉  | 461/585 [03:28<00:35,  3.44it/s] 79%|███████▉  | 462/585 [03:28<00:35,  3.45it/s] 79%|███████▉  | 463/585 [03:28<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:29<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:29<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:29<00:34,  3.45it/s] 80%|███████▉  | 467/585 [03:30<00:34,  3.45it/s] 80%|████████  | 468/585 [03:30<00:33,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 18:37:39,988 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:37:39,988 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:37:39,988 >>   Batch size = 8
{'eval_loss': 1.0047729015350342, 'eval_runtime': 9.511, 'eval_samples_per_second': 367.258, 'eval_steps_per_second': 45.947, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  2%|▏         | 7/437 [00:00<00:07, 58.54it/s][A
  3%|▎         | 13/437 [00:00<00:08, 51.01it/s][A
  4%|▍         | 19/437 [00:00<00:08, 49.01it/s][A
  5%|▌         | 24/437 [00:00<00:08, 48.20it/s][A
  7%|▋         | 29/437 [00:00<00:08, 47.63it/s][A
  8%|▊         | 34/437 [00:00<00:08, 47.32it/s][A
  9%|▉         | 39/437 [00:00<00:08, 47.03it/s][A
 10%|█         | 44/437 [00:00<00:08, 46.64it/s][A
 11%|█         | 49/437 [00:01<00:08, 46.61it/s][A
 12%|█▏        | 54/437 [00:01<00:08, 46.75it/s][A
 14%|█▎        | 59/437 [00:01<00:08, 46.63it/s][A
 15%|█▍        | 64/437 [00:01<00:07, 46.67it/s][A
 16%|█▌        | 69/437 [00:01<00:07, 46.70it/s][A
 17%|█▋        | 74/437 [00:01<00:07, 46.54it/s][A
 18%|█▊        | 79/437 [00:01<00:07, 46.49it/s][A
 19%|█▉        | 84/437 [00:01<00:07, 46.48it/s][A
 20%|██        | 89/437 [00:01<00:07, 46.32it/s][A
 22%|██▏       | 94/437 [00:01<00:07, 46.43it/s][A
 23%|██▎       | 99/437 [00:02<00:07, 46.50it/s][A
 24%|██▍       | 104/437 [00:02<00:07, 46.45it/s][A
 25%|██▍       | 109/437 [00:02<00:07, 46.59it/s][A
 26%|██▌       | 114/437 [00:02<00:06, 46.54it/s][A
 27%|██▋       | 119/437 [00:02<00:06, 46.50it/s][A
 28%|██▊       | 124/437 [00:02<00:06, 46.44it/s][A
 30%|██▉       | 129/437 [00:02<00:06, 46.46it/s][A
 31%|███       | 134/437 [00:02<00:06, 46.28it/s][A
 32%|███▏      | 139/437 [00:02<00:06, 46.31it/s][A
 33%|███▎      | 144/437 [00:03<00:06, 46.42it/s][A
 34%|███▍      | 149/437 [00:03<00:07, 39.81it/s][A
 35%|███▌      | 154/437 [00:03<00:06, 41.64it/s][A
 36%|███▋      | 159/437 [00:03<00:06, 43.03it/s][A
 38%|███▊      | 164/437 [00:03<00:06, 44.05it/s][A
 39%|███▊      | 169/437 [00:03<00:05, 44.81it/s][A
 40%|███▉      | 174/437 [00:03<00:05, 45.31it/s][A
 41%|████      | 179/437 [00:03<00:05, 45.56it/s][A
 42%|████▏     | 184/437 [00:03<00:05, 45.99it/s][A
 43%|████▎     | 189/437 [00:04<00:05, 45.65it/s][A
 44%|████▍     | 194/437 [00:04<00:05, 45.76it/s][A
 46%|████▌     | 199/437 [00:04<00:05, 45.99it/s][A
 47%|████▋     | 204/437 [00:04<00:05, 46.11it/s][A
 48%|████▊     | 209/437 [00:04<00:04, 46.24it/s][A
 49%|████▉     | 214/437 [00:04<00:04, 46.47it/s][A
 50%|█████     | 219/437 [00:04<00:04, 46.45it/s][A
 51%|█████▏    | 224/437 [00:04<00:04, 46.52it/s][A
 52%|█████▏    | 229/437 [00:04<00:04, 46.45it/s][A
 54%|█████▎    | 234/437 [00:05<00:04, 46.19it/s][A
 55%|█████▍    | 239/437 [00:05<00:04, 46.17it/s][A
 56%|█████▌    | 244/437 [00:05<00:04, 46.11it/s][A
 57%|█████▋    | 249/437 [00:05<00:04, 46.20it/s][A
 58%|█████▊    | 254/437 [00:05<00:03, 46.27it/s][A
 59%|█████▉    | 259/437 [00:05<00:03, 46.46it/s][A
 60%|██████    | 264/437 [00:05<00:03, 46.45it/s][A
 62%|██████▏   | 269/437 [00:05<00:03, 46.50it/s][A
 63%|██████▎   | 274/437 [00:05<00:03, 46.48it/s][A
 64%|██████▍   | 279/437 [00:06<00:03, 46.29it/s][A
 65%|██████▍   | 284/437 [00:06<00:03, 46.27it/s][A
 66%|██████▌   | 289/437 [00:06<00:03, 46.22it/s][A
 67%|██████▋   | 294/437 [00:06<00:03, 46.17it/s][A
 68%|██████▊   | 299/437 [00:06<00:02, 46.31it/s][A
 70%|██████▉   | 304/437 [00:06<00:02, 46.38it/s][A
 71%|███████   | 309/437 [00:06<00:02, 46.50it/s][A
 72%|███████▏  | 314/437 [00:06<00:02, 46.45it/s][A
 73%|███████▎  | 319/437 [00:06<00:02, 46.38it/s][A
 74%|███████▍  | 324/437 [00:07<00:02, 46.38it/s][A
 75%|███████▌  | 329/437 [00:07<00:02, 46.26it/s][A
 76%|███████▋  | 334/437 [00:07<00:02, 46.28it/s][A
 78%|███████▊  | 339/437 [00:07<00:02, 46.21it/s][A
 79%|███████▊  | 344/437 [00:07<00:02, 46.23it/s][A
 80%|███████▉  | 349/437 [00:07<00:01, 46.33it/s][A
 81%|████████  | 354/437 [00:07<00:01, 46.33it/s][A
 82%|████████▏ | 359/437 [00:07<00:01, 46.42it/s][A
 83%|████████▎ | 364/437 [00:07<00:01, 46.50it/s][A
 84%|████████▍ | 369/437 [00:07<00:01, 46.38it/s][A
 86%|████████▌ | 374/437 [00:08<00:01, 46.28it/s][A
 87%|████████▋ | 379/437 [00:08<00:01, 46.33it/s][A
 88%|████████▊ | 384/437 [00:08<00:01, 46.34it/s][A
 89%|████████▉ | 389/437 [00:08<00:01, 46.20it/s][A
 90%|█████████ | 394/437 [00:08<00:00, 46.32it/s][A
 91%|█████████▏| 399/437 [00:08<00:00, 46.43it/s][A
 92%|█████████▏| 404/437 [00:08<00:00, 46.36it/s][A
 94%|█████████▎| 409/437 [00:08<00:00, 46.47it/s][A
 95%|█████████▍| 414/437 [00:08<00:00, 46.45it/s][A
 96%|█████████▌| 419/437 [00:09<00:00, 46.25it/s][A
 97%|█████████▋| 424/437 [00:09<00:00, 46.25it/s][A
 98%|█████████▊| 429/437 [00:09<00:00, 46.27it/s][A
 99%|█████████▉| 434/437 [00:09<00:00, 46.24it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:39<00:33,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:37:49,525 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 18:37:49,549 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:37:53,520 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:37:53,631 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:37:53,666 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:53<13:58,  7.23s/it] 80%|████████  | 470/585 [03:54<09:52,  5.15s/it] 81%|████████  | 471/585 [03:54<07:00,  3.69s/it] 81%|████████  | 472/585 [03:54<05:01,  2.67s/it] 81%|████████  | 473/585 [03:54<03:39,  1.96s/it] 81%|████████  | 474/585 [03:55<02:41,  1.46s/it] 81%|████████  | 475/585 [03:55<02:01,  1.11s/it] 81%|████████▏ | 476/585 [03:55<01:33,  1.16it/s] 82%|████████▏ | 477/585 [03:56<01:14,  1.45it/s] 82%|████████▏ | 478/585 [03:56<01:00,  1.76it/s] 82%|████████▏ | 479/585 [03:56<00:51,  2.06it/s] 82%|████████▏ | 480/585 [03:56<00:44,  2.35it/s] 82%|████████▏ | 481/585 [03:57<00:40,  2.59it/s] 82%|████████▏ | 482/585 [03:57<00:36,  2.80it/s] 83%|████████▎ | 483/585 [03:57<00:34,  2.97it/s] 83%|████████▎ | 484/585 [03:58<00:32,  3.10it/s] 83%|████████▎ | 485/585 [03:58<00:31,  3.20it/s] 83%|████████▎ | 486/585 [03:58<00:30,  3.28it/s] 83%|████████▎ | 487/585 [03:58<00:29,  3.33it/s] 83%|████████▎ | 488/585 [03:59<00:28,  3.37it/s] 84%|████████▎ | 489/585 [03:59<00:28,  3.40it/s] 84%|████████▍ | 490/585 [03:59<00:27,  3.42it/s] 84%|████████▍ | 491/585 [04:00<00:27,  3.43it/s] 84%|████████▍ | 492/585 [04:00<00:27,  3.41it/s] 84%|████████▍ | 493/585 [04:00<00:26,  3.42it/s] 84%|████████▍ | 494/585 [04:01<00:26,  3.44it/s] 85%|████████▍ | 495/585 [04:01<00:26,  3.44it/s] 85%|████████▍ | 496/585 [04:01<00:25,  3.44it/s] 85%|████████▍ | 497/585 [04:01<00:25,  3.45it/s] 85%|████████▌ | 498/585 [04:02<00:25,  3.45it/s] 85%|████████▌ | 499/585 [04:02<00:24,  3.45it/s] 85%|████████▌ | 500/585 [04:02<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [04:02<00:24,  3.46it/s] 86%|████████▌ | 501/585 [04:03<00:24,  3.45it/s] 86%|████████▌ | 502/585 [04:03<00:24,  3.46it/s] 86%|████████▌ | 503/585 [04:03<00:26,  3.10it/s] 86%|████████▌ | 504/585 [04:04<00:25,  3.20it/s] 86%|████████▋ | 505/585 [04:04<00:24,  3.27it/s] 86%|████████▋ | 506/585 [04:04<00:23,  3.33it/s] 87%|████████▋ | 507/585 [04:04<00:23,  3.37it/s] 87%|████████▋ | 508/585 [04:05<00:22,  3.39it/s] 87%|████████▋ | 509/585 [04:05<00:22,  3.40it/s] 87%|████████▋ | 510/585 [04:05<00:21,  3.42it/s] 87%|████████▋ | 511/585 [04:06<00:21,  3.43it/s] 88%|████████▊ | 512/585 [04:06<00:21,  3.44it/s] 88%|████████▊ | 513/585 [04:06<00:21,  3.29it/s] 88%|████████▊ | 514/585 [04:06<00:21,  3.34it/s] 88%|████████▊ | 515/585 [04:07<00:20,  3.37it/s] 88%|████████▊ | 516/585 [04:07<00:20,  3.40it/s] 88%|████████▊ | 517/585 [04:07<00:19,  3.41it/s] 89%|████████▊ | 518/585 [04:08<00:19,  3.43it/s] 89%|████████▊ | 519/585 [04:08<00:19,  3.43it/s] 89%|████████▉ | 520/585 [04:08<00:18,  3.44it/s] 89%|████████▉ | 521/585 [04:08<00:18,  3.44it/s] 89%|████████▉ | 522/585 [04:09<00:18,  3.44it/s] 89%|████████▉ | 523/585 [04:09<00:18,  3.44it/s] 90%|████████▉ | 524/585 [04:09<00:17,  3.41it/s] 90%|████████▉ | 525/585 [04:10<00:17,  3.42it/s] 90%|████████▉ | 526/585 [04:10<00:17,  3.43it/s] 90%|█████████ | 527/585 [04:10<00:16,  3.44it/s] 90%|█████████ | 528/585 [04:11<00:16,  3.44it/s] 90%|█████████ | 529/585 [04:11<00:16,  3.44it/s] 91%|█████████ | 530/585 [04:11<00:15,  3.44it/s] 91%|█████████ | 531/585 [04:11<00:15,  3.45it/s] 91%|█████████ | 532/585 [04:12<00:15,  3.45it/s] 91%|█████████ | 533/585 [04:12<00:15,  3.45it/s] 91%|█████████▏| 534/585 [04:12<00:14,  3.45it/s] 91%|█████████▏| 535/585 [04:13<00:16,  3.07it/s] 92%|█████████▏| 536/585 [04:13<00:15,  3.17it/s] 92%|█████████▏| 537/585 [04:13<00:14,  3.25it/s] 92%|█████████▏| 538/585 [04:14<00:14,  3.31it/s] 92%|█████████▏| 539/585 [04:14<00:13,  3.36it/s] 92%|█████████▏| 540/585 [04:14<00:13,  3.38it/s] 92%|█████████▏| 541/585 [04:14<00:12,  3.41it/s] 93%|█████████▎| 542/585 [04:15<00:12,  3.42it/s] 93%|█████████▎| 543/585 [04:15<00:12,  3.43it/s] 93%|█████████▎| 544/585 [04:15<00:11,  3.44it/s] 93%|█████████▎| 545/585 [04:16<00:12,  3.20it/s] 93%|█████████▎| 546/585 [04:16<00:13,  2.99it/s] 94%|█████████▎| 547/585 [04:16<00:12,  3.11it/s] 94%|█████████▎| 548/585 [04:17<00:11,  3.21it/s] 94%|█████████▍| 549/585 [04:17<00:10,  3.28it/s] 94%|█████████▍| 550/585 [04:17<00:10,  3.33it/s] 94%|█████████▍| 551/585 [04:17<00:10,  3.37it/s] 94%|█████████▍| 552/585 [04:18<00:09,  3.39it/s] 95%|█████████▍| 553/585 [04:18<00:09,  3.41it/s] 95%|█████████▍| 554/585 [04:18<00:09,  3.42it/s] 95%|█████████▍| 555/585 [04:19<00:08,  3.44it/s] 95%|█████████▌| 556/585 [04:19<00:10,  2.65it/s] 95%|█████████▌| 557/585 [04:20<00:09,  2.85it/s] 95%|█████████▌| 558/585 [04:20<00:08,  3.01it/s] 96%|█████████▌| 559/585 [04:20<00:08,  3.13it/s] 96%|█████████▌| 560/585 [04:20<00:07,  3.22it/s] 96%|█████████▌| 561/585 [04:21<00:07,  3.29it/s] 96%|█████████▌| 562/585 [04:21<00:06,  3.34it/s] 96%|█████████▌| 563/585 [04:21<00:06,  3.37it/s] 96%|█████████▋| 564/585 [04:22<00:06,  3.39it/s] 97%|█████████▋| 565/585 [04:22<00:05,  3.41it/s] 97%|█████████▋| 566/585 [04:22<00:05,  3.40it/s] 97%|█████████▋| 567/585 [04:22<00:05,  3.41it/s] 97%|█████████▋| 568/585 [04:23<00:04,  3.43it/s] 97%|█████████▋| 569/585 [04:23<00:04,  3.43it/s] 97%|█████████▋| 570/585 [04:23<00:04,  3.44it/s] 98%|█████████▊| 571/585 [04:24<00:04,  3.44it/s] 98%|█████████▊| 572/585 [04:24<00:03,  3.45it/s] 98%|█████████▊| 573/585 [04:24<00:03,  3.45it/s] 98%|█████████▊| 574/585 [04:24<00:03,  3.45it/s] 98%|█████████▊| 575/585 [04:25<00:02,  3.45it/s] 98%|█████████▊| 576/585 [04:25<00:02,  3.45it/s] 99%|█████████▊| 577/585 [04:25<00:02,  3.43it/s] 99%|█████████▉| 578/585 [04:26<00:02,  3.44it/s] 99%|█████████▉| 579/585 [04:26<00:01,  3.44it/s] 99%|█████████▉| 580/585 [04:26<00:01,  3.45it/s] 99%|█████████▉| 581/585 [04:26<00:01,  3.45it/s] 99%|█████████▉| 582/585 [04:27<00:00,  3.45it/s]100%|█████████▉| 583/585 [04:27<00:00,  3.45it/s]100%|█████████▉| 584/585 [04:27<00:00,  3.45it/s]100%|██████████| 585/585 [04:28<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 18:38:37,713 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:38:37,713 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:38:37,713 >>   Batch size = 8
{'eval_loss': 1.0169800519943237, 'eval_runtime': 9.5203, 'eval_samples_per_second': 366.9, 'eval_steps_per_second': 45.902, 'epoch': 4.0}
{'loss': 0.6351, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.91it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.28it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.46it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.80it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.26it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.06it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.90it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.70it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.62it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.68it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.51it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.49it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.58it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.57it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.47it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.59it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.53it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.38it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.55it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.50it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.50it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.51it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.57it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.32it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.51it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.50it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.39it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.47it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.38it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.35it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.42it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.51it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.41it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.45it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.31it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.34it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.34it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.31it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.35it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.39it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.46it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.41it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.42it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.30it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.35it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.43it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.46it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.32it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.37it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.45it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.40it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.39it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.43it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.29it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.40it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.39it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.36it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.36it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.40it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.40it/s][A
 70%|███████   | 308/437 [00:06<00:02, 45.80it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.05it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.03it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.17it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.23it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.28it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.33it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.29it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.31it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.34it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.38it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.32it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.32it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.29it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.27it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.31it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.36it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.31it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.36it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.42it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.35it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.41it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.29it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.26it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.31it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.36it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.36it/s][A100%|██████████| 585/585 [04:37<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:38:47,145 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 18:38:47,160 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:38:52,149 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:38:52,306 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:38:52,347 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:39:01,716 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:39:01,720 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117 (score: 0.977833092212677).
                                                 100%|██████████| 585/585 [04:58<00:00,  3.45it/s]100%|██████████| 585/585 [04:58<00:00,  1.96it/s]
[INFO|trainer.py:1894] 2023-08-28 18:39:08,460 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 18:39:08,506 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:39:16,631 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:39:16,654 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:39:16,670 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:39:16,869 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:16,870 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:16,870 >>   train_loss               =     0.6311
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:16,870 >>   train_runtime            = 0:04:58.84
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:16,870 >>   train_samples            =       7510
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:16,870 >>   train_samples_per_second =    125.649
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:16,870 >>   train_steps_per_second   =      1.958
{'eval_loss': 1.0229052305221558, 'eval_runtime': 9.4174, 'eval_samples_per_second': 370.909, 'eval_steps_per_second': 46.403, 'epoch': 5.0}
{'train_runtime': 298.8484, 'train_samples_per_second': 125.649, 'train_steps_per_second': 1.958, 'train_loss': 0.6310672205737513, 'epoch': 5.0}
08/28/2023 18:39:16 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:39:16,920 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:39:16,921 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:39:16,921 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 57.22it/s]  3%|▎         | 12/437 [00:00<00:08, 50.43it/s]  4%|▍         | 18/437 [00:00<00:08, 48.88it/s]  5%|▌         | 23/437 [00:00<00:08, 48.20it/s]  6%|▋         | 28/437 [00:00<00:08, 47.78it/s]  8%|▊         | 33/437 [00:00<00:08, 47.46it/s]  9%|▊         | 38/437 [00:00<00:08, 47.21it/s] 10%|▉         | 43/437 [00:00<00:08, 47.07it/s] 11%|█         | 48/437 [00:01<00:08, 47.09it/s] 12%|█▏        | 53/437 [00:01<00:08, 46.99it/s] 13%|█▎        | 58/437 [00:01<00:08, 46.96it/s] 14%|█▍        | 63/437 [00:01<00:07, 46.97it/s] 16%|█▌        | 68/437 [00:01<00:07, 46.99it/s] 17%|█▋        | 73/437 [00:01<00:07, 46.90it/s] 18%|█▊        | 78/437 [00:01<00:07, 46.95it/s] 19%|█▉        | 83/437 [00:01<00:07, 46.93it/s] 20%|██        | 88/437 [00:01<00:07, 46.89it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.78it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.87it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.85it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.86it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.89it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.90it/s] 28%|██▊       | 123/437 [00:02<00:06, 46.92it/s] 29%|██▉       | 128/437 [00:02<00:06, 46.93it/s] 30%|███       | 133/437 [00:02<00:09, 32.04it/s] 32%|███▏      | 138/437 [00:03<00:08, 35.33it/s] 33%|███▎      | 143/437 [00:03<00:07, 38.12it/s] 34%|███▍      | 148/437 [00:03<00:07, 40.36it/s] 35%|███▌      | 153/437 [00:03<00:06, 41.85it/s] 36%|███▌      | 158/437 [00:03<00:06, 43.22it/s] 37%|███▋      | 163/437 [00:03<00:06, 44.18it/s] 38%|███▊      | 168/437 [00:03<00:05, 44.86it/s] 40%|███▉      | 173/437 [00:03<00:05, 45.39it/s] 41%|████      | 178/437 [00:03<00:05, 45.84it/s] 42%|████▏     | 183/437 [00:04<00:05, 46.08it/s] 43%|████▎     | 188/437 [00:04<00:05, 46.18it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.33it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.44it/s] 46%|████▋     | 203/437 [00:04<00:05, 46.56it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.63it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.63it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.64it/s] 51%|█████     | 223/437 [00:04<00:04, 46.67it/s] 52%|█████▏    | 228/437 [00:05<00:04, 46.67it/s] 53%|█████▎    | 233/437 [00:05<00:04, 46.68it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.60it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.48it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.54it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.60it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.65it/s] 60%|██████    | 263/437 [00:05<00:03, 46.50it/s] 61%|██████▏   | 268/437 [00:06<00:05, 29.74it/s] 62%|██████▏   | 273/437 [00:06<00:04, 33.31it/s] 64%|██████▎   | 278/437 [00:06<00:04, 36.45it/s] 65%|██████▍   | 283/437 [00:06<00:03, 39.03it/s] 66%|██████▌   | 288/437 [00:06<00:03, 41.04it/s] 67%|██████▋   | 293/437 [00:06<00:03, 42.60it/s] 68%|██████▊   | 298/437 [00:06<00:03, 43.70it/s] 69%|██████▉   | 303/437 [00:06<00:03, 44.55it/s] 70%|███████   | 308/437 [00:06<00:02, 45.17it/s] 72%|███████▏  | 313/437 [00:07<00:02, 45.60it/s] 73%|███████▎  | 318/437 [00:07<00:02, 45.87it/s] 74%|███████▍  | 323/437 [00:07<00:02, 45.95it/s] 75%|███████▌  | 328/437 [00:07<00:02, 46.18it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.38it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.49it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.40it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.47it/s] 81%|████████  | 353/437 [00:07<00:01, 46.46it/s] 82%|████████▏ | 358/437 [00:08<00:01, 46.55it/s] 83%|████████▎ | 363/437 [00:08<00:01, 46.61it/s] 84%|████████▍ | 368/437 [00:08<00:01, 46.57it/s] 85%|████████▌ | 373/437 [00:08<00:01, 46.58it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.48it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.48it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.45it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.50it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.42it/s] 92%|█████████▏| 403/437 [00:09<00:01, 19.04it/s] 93%|█████████▎| 408/437 [00:09<00:01, 23.12it/s] 95%|█████████▍| 413/437 [00:09<00:00, 27.26it/s] 96%|█████████▌| 418/437 [00:09<00:00, 31.16it/s] 97%|█████████▋| 423/437 [00:09<00:00, 34.61it/s] 98%|█████████▊| 428/437 [00:10<00:00, 37.50it/s] 99%|█████████▉| 433/437 [00:10<00:00, 39.86it/s]100%|██████████| 437/437 [00:10<00:00, 42.73it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:39:27,178 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:27,178 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:27,178 >>   eval_loss               =     0.9778
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:27,178 >>   eval_runtime            = 0:00:10.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:27,178 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:27,178 >>   eval_samples_per_second =    340.794
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:27,178 >>   eval_steps_per_second   =     42.636
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:27,178 >>   perplexity              =     2.6587
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:46,364 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:46,458 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:46,458 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:46,458 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:46,458 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:39:47,523 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:39:47,524 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:39:48,128 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:39:49,235 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:39:49,235 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:52,282 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:52,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:52,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:52,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:52,308 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:39:53,178 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:39:53,179 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:39:53,740 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:39:53,930 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:39:53,930 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:17,  1.63it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:18,  1.64it/s]Extractor Predicting: 30it [00:18,  1.59it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:20,  1.51it/s]Extractor Predicting: 34it [00:21,  1.49it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.49it/s]Extractor Predicting: 37it [00:23,  1.48it/s]Extractor Predicting: 38it [00:24,  1.47it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:25,  1.49it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:27,  1.50it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:31,  1.48it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:33,  1.50it/s]Extractor Predicting: 52it [00:33,  1.48it/s]Extractor Predicting: 53it [00:34,  1.42it/s]Extractor Predicting: 54it [00:35,  1.43it/s]Extractor Predicting: 55it [00:35,  1.43it/s]Extractor Predicting: 56it [00:36,  1.44it/s]Extractor Predicting: 57it [00:37,  1.43it/s]Extractor Predicting: 58it [00:38,  1.41it/s]Extractor Predicting: 59it [00:38,  1.41it/s]Extractor Predicting: 60it [00:39,  1.42it/s]Extractor Predicting: 61it [00:40,  1.33it/s]Extractor Predicting: 62it [00:40,  1.37it/s]Extractor Predicting: 63it [00:42,  1.19it/s]Extractor Predicting: 64it [00:42,  1.29it/s]Extractor Predicting: 65it [00:43,  1.33it/s]Extractor Predicting: 66it [00:44,  1.40it/s]Extractor Predicting: 67it [00:44,  1.41it/s]Extractor Predicting: 68it [00:45,  1.42it/s]Extractor Predicting: 69it [00:46,  1.43it/s]Extractor Predicting: 70it [00:46,  1.43it/s]Extractor Predicting: 71it [00:47,  1.44it/s]Extractor Predicting: 72it [00:48,  1.46it/s]Extractor Predicting: 73it [00:48,  1.46it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:50,  1.51it/s]Extractor Predicting: 76it [00:50,  1.49it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:52,  1.47it/s]Extractor Predicting: 79it [00:52,  1.47it/s]Extractor Predicting: 80it [00:53,  1.46it/s]Extractor Predicting: 81it [00:54,  1.51it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:55,  1.46it/s]Extractor Predicting: 84it [00:56,  1.47it/s]Extractor Predicting: 85it [00:56,  1.50it/s]Extractor Predicting: 86it [00:57,  1.56it/s]Extractor Predicting: 87it [00:58,  1.58it/s]Extractor Predicting: 88it [00:58,  1.59it/s]Extractor Predicting: 89it [00:59,  1.58it/s]Extractor Predicting: 90it [00:59,  1.60it/s]Extractor Predicting: 91it [01:00,  1.58it/s]Extractor Predicting: 92it [01:01,  1.56it/s]Extractor Predicting: 93it [01:01,  1.59it/s]Extractor Predicting: 94it [01:02,  1.62it/s]Extractor Predicting: 95it [01:03,  1.61it/s]Extractor Predicting: 96it [01:03,  1.62it/s]Extractor Predicting: 97it [01:04,  1.59it/s]Extractor Predicting: 98it [01:05,  1.54it/s]Extractor Predicting: 99it [01:05,  1.52it/s]Extractor Predicting: 100it [01:06,  1.55it/s]Extractor Predicting: 101it [01:06,  1.59it/s]Extractor Predicting: 102it [01:07,  1.58it/s]Extractor Predicting: 103it [01:08,  1.58it/s]Extractor Predicting: 104it [01:08,  1.58it/s]Extractor Predicting: 105it [01:09,  1.61it/s]Extractor Predicting: 106it [01:10,  1.58it/s]Extractor Predicting: 107it [01:10,  1.59it/s]Extractor Predicting: 108it [01:11,  1.57it/s]Extractor Predicting: 109it [01:11,  1.61it/s]Extractor Predicting: 110it [01:12,  1.60it/s]Extractor Predicting: 111it [01:13,  1.59it/s]Extractor Predicting: 112it [01:13,  1.57it/s]Extractor Predicting: 113it [01:14,  1.57it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:15,  1.55it/s]Extractor Predicting: 116it [01:16,  1.54it/s]Extractor Predicting: 117it [01:17,  1.53it/s]Extractor Predicting: 118it [01:17,  1.59it/s]Extractor Predicting: 119it [01:18,  1.57it/s]Extractor Predicting: 120it [01:18,  1.59it/s]Extractor Predicting: 121it [01:19,  1.57it/s]Extractor Predicting: 122it [01:20,  1.56it/s]Extractor Predicting: 123it [01:20,  1.55it/s]Extractor Predicting: 124it [01:21,  1.52it/s]Extractor Predicting: 125it [01:22,  1.52it/s]Extractor Predicting: 126it [01:22,  1.53it/s]Extractor Predicting: 127it [01:23,  1.51it/s]Extractor Predicting: 128it [01:24,  1.49it/s]Extractor Predicting: 129it [01:25,  1.46it/s]Extractor Predicting: 130it [01:25,  1.48it/s]Extractor Predicting: 131it [01:26,  1.49it/s]Extractor Predicting: 132it [01:26,  1.53it/s]Extractor Predicting: 133it [01:27,  1.50it/s]Extractor Predicting: 134it [01:28,  1.51it/s]Extractor Predicting: 135it [01:28,  1.51it/s]Extractor Predicting: 136it [01:29,  1.51it/s]Extractor Predicting: 137it [01:30,  1.50it/s]Extractor Predicting: 138it [01:31,  1.42it/s]Extractor Predicting: 139it [01:31,  1.44it/s]Extractor Predicting: 140it [01:32,  1.49it/s]Extractor Predicting: 141it [01:33,  1.48it/s]Extractor Predicting: 142it [01:33,  1.55it/s]Extractor Predicting: 142it [01:33,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:42,473 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:42,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:42,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:42,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:42,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:41:42,790 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:41:42,791 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:41:43,061 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:41:44,117 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:41:44,117 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:45,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:45,599 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:45,599 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:45,599 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:41:45,599 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:41:45,924 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:41:45,925 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:41:46,204 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:41:46,355 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:41:46,355 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.24436860068259386,
  "recall": 0.10249069567706842,
  "score": 0.1444130697862041,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.50it/s]Extractor Predicting: 30it [00:19,  1.46it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:20,  1.46it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:22,  1.49it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.54it/s]Extractor Predicting: 38it [00:24,  1.44it/s]Extractor Predicting: 39it [00:25,  1.45it/s]Extractor Predicting: 40it [00:26,  1.47it/s]Extractor Predicting: 41it [00:26,  1.49it/s]Extractor Predicting: 42it [00:27,  1.49it/s]Extractor Predicting: 43it [00:28,  1.50it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:31,  1.52it/s]Extractor Predicting: 49it [00:32,  1.51it/s]Extractor Predicting: 50it [00:32,  1.46it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.47it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.53it/s]Extractor Predicting: 58it [00:38,  1.55it/s]Extractor Predicting: 59it [00:38,  1.56it/s]Extractor Predicting: 60it [00:39,  1.55it/s]Extractor Predicting: 61it [00:40,  1.55it/s]Extractor Predicting: 62it [00:40,  1.55it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:43,  1.52it/s]Extractor Predicting: 68it [00:44,  1.56it/s]Extractor Predicting: 69it [00:45,  1.55it/s]Extractor Predicting: 70it [00:45,  1.55it/s]Extractor Predicting: 71it [00:46,  1.53it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:47,  1.49it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:49,  1.52it/s]Extractor Predicting: 76it [00:49,  1.52it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.51it/s]Extractor Predicting: 79it [00:51,  1.50it/s]Extractor Predicting: 80it [00:52,  1.51it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:54,  1.50it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.45it/s]Extractor Predicting: 87it [00:57,  1.47it/s]Extractor Predicting: 88it [00:57,  1.45it/s]Extractor Predicting: 89it [00:58,  1.47it/s]Extractor Predicting: 90it [00:59,  1.50it/s]Extractor Predicting: 91it [00:59,  1.48it/s]Extractor Predicting: 92it [01:00,  1.48it/s]Extractor Predicting: 93it [01:01,  1.46it/s]Extractor Predicting: 94it [01:02,  1.44it/s]Extractor Predicting: 95it [01:02,  1.45it/s]Extractor Predicting: 96it [01:03,  1.45it/s]Extractor Predicting: 97it [01:04,  1.47it/s]Extractor Predicting: 98it [01:04,  1.46it/s]Extractor Predicting: 99it [01:05,  1.48it/s]Extractor Predicting: 100it [01:06,  1.48it/s]Extractor Predicting: 101it [01:06,  1.47it/s]Extractor Predicting: 102it [01:07,  1.47it/s]Extractor Predicting: 103it [01:08,  1.48it/s]Extractor Predicting: 104it [01:08,  1.47it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:10,  1.46it/s]Extractor Predicting: 108it [01:11,  1.45it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:12,  1.44it/s]Extractor Predicting: 111it [01:13,  1.44it/s]Extractor Predicting: 112it [01:14,  1.47it/s]Extractor Predicting: 113it [01:14,  1.50it/s]Extractor Predicting: 114it [01:15,  1.51it/s]Extractor Predicting: 115it [01:16,  1.51it/s]Extractor Predicting: 116it [01:16,  1.51it/s]Extractor Predicting: 117it [01:17,  1.51it/s]Extractor Predicting: 118it [01:18,  1.53it/s]Extractor Predicting: 119it [01:18,  1.52it/s]Extractor Predicting: 120it [01:19,  1.54it/s]Extractor Predicting: 121it [01:20,  1.54it/s]Extractor Predicting: 122it [01:20,  1.58it/s]Extractor Predicting: 123it [01:21,  1.58it/s]Extractor Predicting: 124it [01:22,  1.57it/s]Extractor Predicting: 125it [01:22,  1.57it/s]Extractor Predicting: 126it [01:23,  1.55it/s]Extractor Predicting: 127it [01:24,  1.55it/s]Extractor Predicting: 128it [01:24,  1.56it/s]Extractor Predicting: 129it [01:25,  1.53it/s]Extractor Predicting: 130it [01:25,  1.55it/s]Extractor Predicting: 131it [01:26,  1.52it/s]Extractor Predicting: 132it [01:27,  1.54it/s]Extractor Predicting: 133it [01:27,  1.53it/s]Extractor Predicting: 134it [01:28,  1.39it/s]Extractor Predicting: 135it [01:29,  1.46it/s]Extractor Predicting: 136it [01:30,  1.47it/s]Extractor Predicting: 137it [01:30,  1.48it/s]Extractor Predicting: 138it [01:31,  1.49it/s]Extractor Predicting: 139it [01:31,  1.54it/s]Extractor Predicting: 140it [01:32,  1.52it/s]Extractor Predicting: 141it [01:33,  1.50it/s]Extractor Predicting: 142it [01:33,  1.54it/s]Extractor Predicting: 143it [01:34,  1.54it/s]Extractor Predicting: 144it [01:35,  1.51it/s]Extractor Predicting: 145it [01:35,  1.52it/s]Extractor Predicting: 146it [01:36,  1.50it/s]Extractor Predicting: 147it [01:37,  1.49it/s]Extractor Predicting: 148it [01:38,  1.49it/s]Extractor Predicting: 149it [01:38,  1.48it/s]Extractor Predicting: 150it [01:39,  1.50it/s]Extractor Predicting: 151it [01:39,  1.53it/s]Extractor Predicting: 152it [01:40,  1.54it/s]Extractor Predicting: 153it [01:41,  1.54it/s]Extractor Predicting: 154it [01:41,  1.52it/s]Extractor Predicting: 155it [01:42,  1.52it/s]Extractor Predicting: 156it [01:43,  1.50it/s]Extractor Predicting: 157it [01:43,  1.48it/s]Extractor Predicting: 158it [01:44,  1.47it/s]Extractor Predicting: 159it [01:45,  1.48it/s]Extractor Predicting: 160it [01:45,  1.49it/s]Extractor Predicting: 161it [01:46,  1.50it/s]Extractor Predicting: 162it [01:47,  1.52it/s]Extractor Predicting: 163it [01:47,  1.52it/s]Extractor Predicting: 164it [01:48,  1.53it/s]Extractor Predicting: 165it [01:49,  1.50it/s]Extractor Predicting: 166it [01:49,  1.52it/s]Extractor Predicting: 167it [01:50,  1.54it/s]Extractor Predicting: 168it [01:51,  1.53it/s]Extractor Predicting: 169it [01:51,  1.52it/s]Extractor Predicting: 170it [01:52,  1.50it/s]Extractor Predicting: 171it [01:53,  1.46it/s]Extractor Predicting: 172it [01:53,  1.47it/s]Extractor Predicting: 173it [01:54,  1.48it/s]Extractor Predicting: 174it [01:55,  1.44it/s]Extractor Predicting: 175it [01:56,  1.40it/s]Extractor Predicting: 176it [01:56,  1.43it/s]Extractor Predicting: 177it [01:57,  1.43it/s]Extractor Predicting: 178it [01:58,  1.47it/s]Extractor Predicting: 179it [01:58,  1.48it/s]Extractor Predicting: 180it [01:59,  1.52it/s]Extractor Predicting: 181it [02:00,  1.51it/s]Extractor Predicting: 182it [02:00,  1.53it/s]Extractor Predicting: 183it [02:01,  1.53it/s]Extractor Predicting: 184it [02:01,  1.56it/s]Extractor Predicting: 185it [02:02,  1.58it/s]Extractor Predicting: 186it [02:03,  1.58it/s]Extractor Predicting: 187it [02:03,  1.59it/s]Extractor Predicting: 188it [02:04,  1.57it/s]Extractor Predicting: 189it [02:05,  1.58it/s]Extractor Predicting: 190it [02:05,  1.55it/s]Extractor Predicting: 191it [02:06,  1.50it/s]Extractor Predicting: 192it [02:07,  1.52it/s]Extractor Predicting: 193it [02:07,  1.56it/s]Extractor Predicting: 194it [02:08,  1.55it/s]Extractor Predicting: 195it [02:09,  1.55it/s]Extractor Predicting: 196it [02:09,  1.57it/s]Extractor Predicting: 197it [02:10,  1.58it/s]Extractor Predicting: 198it [02:10,  1.55it/s]Extractor Predicting: 199it [02:11,  1.55it/s]Extractor Predicting: 200it [02:12,  1.55it/s]Extractor Predicting: 201it [02:12,  1.56it/s]Extractor Predicting: 202it [02:13,  1.58it/s]Extractor Predicting: 203it [02:14,  1.59it/s]Extractor Predicting: 204it [02:14,  1.57it/s]Extractor Predicting: 205it [02:15,  1.53it/s]Extractor Predicting: 206it [02:16,  1.53it/s]Extractor Predicting: 207it [02:16,  1.54it/s]Extractor Predicting: 208it [02:17,  1.55it/s]Extractor Predicting: 209it [02:18,  1.50it/s]Extractor Predicting: 210it [02:18,  1.49it/s]Extractor Predicting: 211it [02:19,  1.50it/s]Extractor Predicting: 212it [02:20,  1.36it/s]Extractor Predicting: 213it [02:20,  1.42it/s]Extractor Predicting: 214it [02:21,  1.41it/s]Extractor Predicting: 215it [02:22,  1.43it/s]Extractor Predicting: 216it [02:23,  1.47it/s]Extractor Predicting: 217it [02:23,  1.49it/s]Extractor Predicting: 218it [02:24,  1.44it/s]Extractor Predicting: 219it [02:25,  1.43it/s]Extractor Predicting: 220it [02:25,  1.44it/s]Extractor Predicting: 221it [02:26,  1.42it/s]Extractor Predicting: 222it [02:27,  1.43it/s]Extractor Predicting: 223it [02:27,  1.44it/s]Extractor Predicting: 224it [02:28,  1.47it/s]Extractor Predicting: 225it [02:29,  1.47it/s]Extractor Predicting: 226it [02:29,  1.52it/s]Extractor Predicting: 227it [02:30,  1.55it/s]Extractor Predicting: 228it [02:31,  1.52it/s]Extractor Predicting: 229it [02:31,  1.52it/s]Extractor Predicting: 230it [02:32,  1.49it/s]Extractor Predicting: 231it [02:33,  1.49it/s]Extractor Predicting: 232it [02:33,  1.49it/s]Extractor Predicting: 233it [02:34,  1.54it/s]Extractor Predicting: 234it [02:35,  1.51it/s]Extractor Predicting: 235it [02:35,  1.52it/s]Extractor Predicting: 236it [02:36,  1.49it/s]Extractor Predicting: 237it [02:37,  1.50it/s]Extractor Predicting: 238it [02:37,  1.52it/s]Extractor Predicting: 239it [02:38,  1.53it/s]Extractor Predicting: 240it [02:39,  1.52it/s]Extractor Predicting: 241it [02:39,  1.52it/s]Extractor Predicting: 242it [02:40,  1.50it/s]Extractor Predicting: 243it [02:41,  1.47it/s]Extractor Predicting: 244it [02:41,  1.48it/s]Extractor Predicting: 245it [02:42,  1.53it/s]Extractor Predicting: 246it [02:43,  1.52it/s]Extractor Predicting: 247it [02:43,  1.54it/s]Extractor Predicting: 248it [02:44,  1.54it/s]Extractor Predicting: 249it [02:45,  1.53it/s]Extractor Predicting: 250it [02:45,  1.53it/s]Extractor Predicting: 251it [02:46,  1.49it/s]Extractor Predicting: 252it [02:47,  1.50it/s]Extractor Predicting: 253it [02:47,  1.51it/s]Extractor Predicting: 254it [02:48,  1.51it/s]Extractor Predicting: 255it [02:49,  1.51it/s]Extractor Predicting: 256it [02:49,  1.50it/s]Extractor Predicting: 257it [02:50,  1.52it/s]Extractor Predicting: 258it [02:50,  1.52it/s]Extractor Predicting: 259it [02:51,  1.48it/s]Extractor Predicting: 260it [02:52,  1.50it/s]Extractor Predicting: 261it [02:52,  1.52it/s]Extractor Predicting: 262it [02:53,  1.50it/s]Extractor Predicting: 263it [02:54,  1.49it/s]Extractor Predicting: 264it [02:55,  1.48it/s]Extractor Predicting: 265it [02:55,  1.47it/s]Extractor Predicting: 266it [02:56,  1.45it/s]Extractor Predicting: 267it [02:57,  1.44it/s]Extractor Predicting: 268it [02:57,  1.45it/s]Extractor Predicting: 269it [02:58,  1.46it/s]Extractor Predicting: 270it [02:59,  1.44it/s]Extractor Predicting: 271it [02:59,  1.46it/s]Extractor Predicting: 272it [03:00,  1.46it/s]Extractor Predicting: 273it [03:01,  1.44it/s]Extractor Predicting: 274it [03:01,  1.45it/s]Extractor Predicting: 275it [03:02,  1.48it/s]Extractor Predicting: 276it [03:03,  1.48it/s]Extractor Predicting: 277it [03:03,  1.46it/s]Extractor Predicting: 278it [03:04,  1.47it/s]Extractor Predicting: 279it [03:05,  1.47it/s]Extractor Predicting: 280it [03:06,  1.45it/s]Extractor Predicting: 281it [03:06,  1.44it/s]Extractor Predicting: 282it [03:07,  1.47it/s]Extractor Predicting: 283it [03:08,  1.42it/s]Extractor Predicting: 284it [03:08,  1.41it/s]Extractor Predicting: 285it [03:09,  1.40it/s]Extractor Predicting: 286it [03:10,  1.40it/s]Extractor Predicting: 287it [03:10,  1.42it/s]Extractor Predicting: 288it [03:11,  1.88it/s]Extractor Predicting: 288it [03:11,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:10,611 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:10,658 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:10,658 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:10,658 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:10,658 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:45:11,510 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:45:11,511 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:45:12,078 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:45:13,136 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:45:13,136 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:16,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:16,039 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:16,039 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:16,039 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:16,039 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:45:16,747 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:45:16,749 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:45:17,340 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:45:17,495 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:45:17,495 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5058926233085989,
  "recall": 0.1682392219480331,
  "score": 0.25250544662309365,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:01,  1.80it/s]Extractor Predicting: 3it [00:01,  1.66it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:45:19,946 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:45:19,947 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:45:19,970 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:45:19,971 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:45:19,977 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:45:25,164 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:45:25,167 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:45:25,221 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:45:25,221 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:45:25,246 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:25,252 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:25,252 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:25,252 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:25,252 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:25,252 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:25,252 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.08108108108108109,
  "score": 0.13953488372093023,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:45:25,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:26,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:27,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:27,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:28,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:29,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:30,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:30,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:31,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:32,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:33,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:34,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:34,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:35,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:36,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:37,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:38,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:38,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:39,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:40,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:41,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:53, 16.67s/it][WARNING|generation_utils.py:914] 2023-08-28 18:45:42,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:42,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:43,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:44,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:44,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:45,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:46,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:46,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:47,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:48,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:49,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:49,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:50,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:51,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:51,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:52,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:53,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:54,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:54,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:55,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:19, 15.36s/it][WARNING|generation_utils.py:914] 2023-08-28 18:45:56,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:57,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:58,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:58,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:59,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:00,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:01,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:01,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:02,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:03,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:04,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:04,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:05,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:06,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:07,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:07,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:08,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:09,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:10,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:11,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:11,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:12,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:47<03:10, 15.92s/it][WARNING|generation_utils.py:914] 2023-08-28 18:46:13,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:14,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:14,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:15,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:16,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:17,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:17,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:18,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:19,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:20,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:20,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:21,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:22,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:23,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:23,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:24,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:25,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:26,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:27,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:27,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:28,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:03<02:56, 16.04s/it][WARNING|generation_utils.py:914] 2023-08-28 18:46:29,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:30,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:30,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:31,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:31,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:32,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:33,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:33,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:34,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:35,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:35,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:36,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:37,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:38,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:38,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:39,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:40,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:40,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:41,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:41,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:42,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:43,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:18<02:34, 15.48s/it][WARNING|generation_utils.py:914] 2023-08-28 18:46:43,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:44,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:45,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:45,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:46,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:47,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:48,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:49,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:49,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:50,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:51,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:52,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:52,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:53,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:54,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:55,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:55,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:56,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:57,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:57,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:58,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:34<02:19, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-28 18:46:59,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:00,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:00,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:01,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:03,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:03,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:04,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:05,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:05,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:06,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:07,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:08,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:09,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:09,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:10,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:11,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:12,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:12,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:13,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:14,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:15,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:15,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:51<02:08, 16.07s/it][WARNING|generation_utils.py:914] 2023-08-28 18:47:16,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:17,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:18,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:19,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:20,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:21,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:21,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:22,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:23,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:23,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:25,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:25,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:26,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:27,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:28,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:28,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:29,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:30,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:31,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:32,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:33,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:33,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:34,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:35,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:10<01:59, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-28 18:47:36,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:36,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:37,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:38,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:38,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:39,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:40,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:40,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:41,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:41,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:42,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:43,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:44,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:44,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:45,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:46,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:46,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:47,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:48,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:48,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:49,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:24<01:36, 16.15s/it][WARNING|generation_utils.py:914] 2023-08-28 18:47:50,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:50,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:51,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:52,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:53,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:53,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:54,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:55,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:56,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:57,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:58,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:58,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:59,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:00,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:01,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:01,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:02,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:03,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:04,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:05,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:40<01:20, 16.11s/it][WARNING|generation_utils.py:914] 2023-08-28 18:48:06,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:06,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:07,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:08,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:08,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:09,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:10,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:10,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:11,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:12,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:12,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:13,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:14,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:14,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:15,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:16,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:16,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:17,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:18,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:18,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:53<01:00, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 18:48:19,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:20,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:20,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:21,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:22,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:23,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:23,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:24,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:25,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:26,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:27,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:27,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:28,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:29,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:30,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:30,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:31,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:32,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:33,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:34,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:34,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:35,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:10<00:47, 15.80s/it][WARNING|generation_utils.py:914] 2023-08-28 18:48:36,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:37,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:38,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:38,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:39,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:40,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:41,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:42,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:42,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:43,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:44,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:45,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:45,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:46,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:47,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:47,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:48,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:49,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:50,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:50,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:51,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:27<00:31, 15.87s/it][WARNING|generation_utils.py:914] 2023-08-28 18:48:52,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:53,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:54,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:54,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:55,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:56,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:56,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:57,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:58,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:59,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:59,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:00,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:01,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:01,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:02,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:03,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:04,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:04,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:05,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:06,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:41<00:15, 15.38s/it][WARNING|generation_utils.py:914] 2023-08-28 18:49:06,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:07,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:08,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:08,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:09,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:10,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:10,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:11,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:12,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:13,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:13,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:14,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:15,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:15,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:16,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:17,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:17,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:18,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:19,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:19,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:20,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:21,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:22,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:22,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:23,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:24,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:24,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:00<00:00, 16.41s/it]Generating: 100%|██████████| 15/15 [04:00<00:00, 16.00s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:32,789 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:32,810 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:32,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:32,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:32,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:49:33,897 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:49:33,898 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:49:34,599 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:49:35,672 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:49:35,672 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:38,545 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:38,549 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:38,549 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:38,549 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:38,549 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:49:39,323 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:49:39,324 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:49:39,918 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:49:40,081 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:49:40,082 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8988095238095238, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.8551136363636364, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : architect .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : contains administrative territorial entity . Context : The city of Stuttgart is under the control of the German state of Saxony . Head Entity : Struttgart , Tail Entity : Saxony .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8863636363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7916666666666666, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.9515625, 'errors': {'', '(\'" series "\', \'follows\', \'\', \'The film was released in Japan on 20 September 1993 by Aonuma Records , which released the first and second " " series of " " films in the " series " .\')'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : operator .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : position held . Context : On 31 March 2014 , the Brazilian national squad announced a deal to sign former teammate Luiz Felipe Scolari on loan for around the next two weeks , via the loan market . Head Entity : Luiz Felipe Scolari , Tail Entity : goalkeeper .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 269, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 313, 'raw': 448}
{'target': 600, 'success': 332, 'raw': 480}
{'target': 600, 'success': 355, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 402, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 451, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 539, 'raw': 768}
{'target': 600, 'success': 563, 'raw': 800}
{'target': 600, 'success': 584, 'raw': 832}
{'target': 600, 'success': 602, 'raw': 864}
{'prompt': 'Relation : position held .', 'success_rate': 0.6967592592592593, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 10829
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10929, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.37it/s]Extractor Estimating: 2it [00:01,  1.25it/s]Extractor Estimating: 3it [00:02,  1.39it/s]Extractor Estimating: 4it [00:02,  1.39it/s]Extractor Estimating: 5it [00:03,  1.43it/s]Extractor Estimating: 6it [00:04,  1.51it/s]Extractor Estimating: 7it [00:04,  1.46it/s]Extractor Estimating: 8it [00:05,  1.46it/s]Extractor Estimating: 9it [00:06,  1.50it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:08,  1.53it/s]Extractor Estimating: 13it [00:08,  1.56it/s]Extractor Estimating: 14it [00:09,  1.52it/s]Extractor Estimating: 15it [00:10,  1.52it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:11,  1.57it/s]Extractor Estimating: 18it [00:12,  1.49it/s]Extractor Estimating: 19it [00:12,  1.49it/s]Extractor Estimating: 20it [00:13,  1.49it/s]Extractor Estimating: 21it [00:14,  1.51it/s]Extractor Estimating: 22it [00:14,  1.57it/s]Extractor Estimating: 23it [00:15,  1.54it/s]Extractor Estimating: 24it [00:16,  1.51it/s]Extractor Estimating: 25it [00:16,  1.51it/s]Extractor Estimating: 26it [00:17,  1.55it/s]Extractor Estimating: 27it [00:17,  1.56it/s]Extractor Estimating: 28it [00:18,  1.55it/s]Extractor Estimating: 29it [00:19,  1.58it/s]Extractor Estimating: 30it [00:19,  1.61it/s]Extractor Estimating: 31it [00:20,  1.64it/s]Extractor Estimating: 32it [00:20,  1.64it/s]Extractor Estimating: 33it [00:21,  1.60it/s]Extractor Estimating: 34it [00:22,  1.67it/s]Extractor Estimating: 35it [00:22,  1.65it/s]Extractor Estimating: 36it [00:23,  1.67it/s]Extractor Estimating: 37it [00:23,  1.69it/s]Extractor Estimating: 38it [00:24,  1.68it/s]Extractor Estimating: 39it [00:25,  1.67it/s]Extractor Estimating: 40it [00:25,  1.69it/s]Extractor Estimating: 41it [00:26,  1.71it/s]Extractor Estimating: 42it [00:27,  1.47it/s]Extractor Estimating: 43it [00:27,  1.54it/s]Extractor Estimating: 44it [00:28,  1.54it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:29,  1.61it/s]Extractor Estimating: 47it [00:30,  1.57it/s]Extractor Estimating: 48it [00:30,  1.63it/s]Extractor Estimating: 49it [00:31,  1.58it/s]Extractor Estimating: 50it [00:32,  1.57it/s]Extractor Estimating: 51it [00:32,  1.55it/s]Extractor Estimating: 52it [00:33,  1.45it/s]Extractor Estimating: 53it [00:34,  1.48it/s]Extractor Estimating: 54it [00:35,  1.39it/s]Extractor Estimating: 55it [00:35,  1.41it/s]Extractor Estimating: 56it [00:36,  1.43it/s]Extractor Estimating: 57it [00:37,  1.45it/s]Extractor Estimating: 58it [00:37,  1.46it/s]Extractor Estimating: 59it [00:38,  1.46it/s]Extractor Estimating: 60it [00:39,  1.48it/s]Extractor Estimating: 61it [00:39,  1.46it/s]Extractor Estimating: 62it [00:40,  1.52it/s]Extractor Estimating: 63it [00:41,  1.55it/s]Extractor Estimating: 64it [00:41,  1.59it/s]Extractor Estimating: 65it [00:42,  1.53it/s]Extractor Estimating: 66it [00:43,  1.49it/s]Extractor Estimating: 67it [00:43,  1.41it/s]Extractor Estimating: 68it [00:44,  1.44it/s]Extractor Estimating: 69it [00:45,  1.47it/s]Extractor Estimating: 70it [00:45,  1.48it/s]Extractor Estimating: 71it [00:46,  1.48it/s]Extractor Estimating: 72it [00:47,  1.46it/s]Extractor Estimating: 73it [00:47,  1.45it/s]Extractor Estimating: 74it [00:48,  1.48it/s]Extractor Estimating: 75it [00:49,  1.46it/s]Extractor Estimating: 76it [00:49,  1.50it/s]Extractor Estimating: 77it [00:50,  1.54it/s]Extractor Estimating: 78it [00:51,  1.56it/s]Extractor Estimating: 79it [00:51,  1.61it/s]Extractor Estimating: 80it [00:52,  1.65it/s]Extractor Estimating: 81it [00:52,  1.70it/s]Extractor Estimating: 82it [00:53,  1.74it/s]Extractor Estimating: 83it [00:53,  1.76it/s]Extractor Estimating: 84it [00:54,  1.74it/s]Extractor Estimating: 85it [00:55,  1.73it/s]Extractor Estimating: 86it [00:55,  1.73it/s]Extractor Estimating: 87it [00:56,  1.75it/s]Extractor Estimating: 88it [00:56,  1.79it/s]Extractor Estimating: 89it [00:57,  1.75it/s]Extractor Estimating: 90it [00:58,  1.67it/s]Extractor Estimating: 91it [00:58,  1.70it/s]Extractor Estimating: 92it [00:59,  1.70it/s]Extractor Estimating: 93it [00:59,  1.73it/s]Extractor Estimating: 94it [01:00,  1.69it/s]Extractor Estimating: 95it [01:00,  1.70it/s]Extractor Estimating: 96it [01:01,  1.67it/s]Extractor Estimating: 97it [01:02,  1.66it/s]Extractor Estimating: 98it [01:02,  1.57it/s]Extractor Estimating: 99it [01:03,  1.64it/s]Extractor Estimating: 100it [01:04,  1.66it/s]Extractor Estimating: 101it [01:04,  1.69it/s]Extractor Estimating: 102it [01:05,  1.69it/s]Extractor Estimating: 103it [01:05,  1.69it/s]Extractor Estimating: 104it [01:06,  1.67it/s]Extractor Estimating: 105it [01:06,  1.72it/s]Extractor Estimating: 106it [01:07,  1.70it/s]Extractor Estimating: 107it [01:08,  1.73it/s]Extractor Estimating: 108it [01:08,  1.73it/s]Extractor Estimating: 109it [01:09,  1.76it/s]Extractor Estimating: 110it [01:09,  1.77it/s]Extractor Estimating: 111it [01:10,  1.69it/s]Extractor Estimating: 112it [01:11,  1.69it/s]Extractor Estimating: 113it [01:11,  1.62it/s]Extractor Estimating: 114it [01:12,  1.60it/s]Extractor Estimating: 115it [01:13,  1.59it/s]Extractor Estimating: 116it [01:13,  1.59it/s]Extractor Estimating: 117it [01:14,  1.58it/s]Extractor Estimating: 118it [01:14,  1.66it/s]Extractor Estimating: 119it [01:15,  1.66it/s]Extractor Estimating: 120it [01:16,  1.68it/s]Extractor Estimating: 121it [01:16,  1.70it/s]Extractor Estimating: 122it [01:17,  1.77it/s]Extractor Estimating: 123it [01:17,  1.72it/s]Extractor Estimating: 124it [01:18,  1.67it/s]Extractor Estimating: 125it [01:18,  1.72it/s]Extractor Estimating: 126it [01:19,  1.73it/s]Extractor Estimating: 127it [01:20,  1.70it/s]Extractor Estimating: 128it [01:20,  1.69it/s]Extractor Estimating: 129it [01:21,  1.59it/s]Extractor Estimating: 130it [01:21,  1.62it/s]Extractor Estimating: 131it [01:22,  1.66it/s]Extractor Estimating: 132it [01:23,  1.68it/s]Extractor Estimating: 133it [01:23,  1.60it/s]Extractor Estimating: 134it [01:24,  1.61it/s]Extractor Estimating: 135it [01:25,  1.61it/s]Extractor Estimating: 136it [01:25,  1.66it/s]Extractor Estimating: 137it [01:26,  1.67it/s]Extractor Estimating: 138it [01:26,  1.68it/s]Extractor Estimating: 139it [01:27,  1.69it/s]Extractor Estimating: 140it [01:27,  1.71it/s]Extractor Estimating: 141it [01:28,  1.72it/s]Extractor Estimating: 142it [01:29,  1.72it/s]Extractor Estimating: 143it [01:29,  1.67it/s]Extractor Estimating: 144it [01:30,  1.69it/s]Extractor Estimating: 145it [01:30,  1.67it/s]Extractor Estimating: 146it [01:31,  1.66it/s]Extractor Estimating: 147it [01:32,  1.69it/s]Extractor Estimating: 148it [01:32,  1.72it/s]Extractor Estimating: 149it [01:33,  1.73it/s]Extractor Estimating: 150it [01:33,  1.70it/s]Extractor Estimating: 151it [01:34,  1.73it/s]Extractor Estimating: 152it [01:34,  1.71it/s]Extractor Estimating: 153it [01:35,  1.74it/s]Extractor Estimating: 154it [01:36,  1.65it/s]Extractor Estimating: 155it [01:36,  1.58it/s]Extractor Estimating: 156it [01:37,  1.64it/s]Extractor Estimating: 157it [01:37,  1.73it/s]Extractor Estimating: 158it [01:38,  1.69it/s]Extractor Estimating: 159it [01:39,  1.66it/s]Extractor Estimating: 160it [01:39,  1.67it/s]Extractor Estimating: 161it [01:40,  1.67it/s]Extractor Estimating: 162it [01:41,  1.66it/s]Extractor Estimating: 163it [01:41,  1.67it/s]Extractor Estimating: 164it [01:42,  1.70it/s]Extractor Estimating: 165it [01:42,  1.68it/s]Extractor Estimating: 166it [01:43,  1.68it/s]Extractor Estimating: 167it [01:44,  1.66it/s]Extractor Estimating: 168it [01:44,  1.62it/s]Extractor Estimating: 169it [01:45,  1.65it/s]Extractor Estimating: 170it [01:45,  1.62it/s]Extractor Estimating: 171it [01:46,  1.64it/s]Extractor Estimating: 172it [01:47,  1.67it/s]Extractor Estimating: 173it [01:47,  1.71it/s]Extractor Estimating: 174it [01:48,  1.74it/s]Extractor Estimating: 175it [01:48,  1.73it/s]Extractor Estimating: 176it [01:49,  1.67it/s]Extractor Estimating: 177it [01:49,  1.70it/s]Extractor Estimating: 178it [01:50,  1.65it/s]Extractor Estimating: 179it [01:51,  1.62it/s]Extractor Estimating: 180it [01:51,  1.65it/s]Extractor Estimating: 181it [01:52,  1.65it/s]Extractor Estimating: 182it [01:53,  1.67it/s]Extractor Estimating: 183it [01:53,  1.63it/s]Extractor Estimating: 184it [01:54,  1.66it/s]Extractor Estimating: 185it [01:54,  1.67it/s]Extractor Estimating: 186it [01:55,  1.65it/s]Extractor Estimating: 187it [01:56,  1.61it/s]Extractor Estimating: 188it [01:56,  1.62it/s]Extractor Estimating: 189it [01:57,  1.61it/s]Extractor Estimating: 190it [01:58,  1.56it/s]Extractor Estimating: 191it [01:58,  1.60it/s]Extractor Estimating: 192it [01:59,  1.52it/s]Extractor Estimating: 193it [02:00,  1.52it/s]Extractor Estimating: 194it [02:00,  1.58it/s]Extractor Estimating: 195it [02:01,  1.58it/s]Extractor Estimating: 196it [02:01,  1.60it/s]Extractor Estimating: 197it [02:02,  1.64it/s]Extractor Estimating: 198it [02:03,  1.63it/s]Extractor Estimating: 199it [02:03,  1.66it/s]Extractor Estimating: 200it [02:04,  1.62it/s]Extractor Estimating: 201it [02:04,  1.64it/s]Extractor Estimating: 202it [02:05,  1.59it/s]Extractor Estimating: 203it [02:06,  1.56it/s]Extractor Estimating: 204it [02:06,  1.57it/s]Extractor Estimating: 205it [02:07,  1.55it/s]Extractor Estimating: 206it [02:08,  1.59it/s]Extractor Estimating: 207it [02:08,  1.57it/s]Extractor Estimating: 208it [02:09,  1.59it/s]Extractor Estimating: 209it [02:09,  1.58it/s]Extractor Estimating: 210it [02:10,  1.57it/s]Extractor Estimating: 211it [02:11,  1.64it/s]Extractor Estimating: 212it [02:11,  1.59it/s]Extractor Estimating: 213it [02:12,  1.51it/s]Extractor Estimating: 214it [02:13,  1.54it/s]Extractor Estimating: 215it [02:13,  1.61it/s]Extractor Estimating: 216it [02:14,  1.54it/s]Extractor Estimating: 217it [02:15,  1.55it/s]Extractor Estimating: 218it [02:15,  1.52it/s]Extractor Estimating: 219it [02:16,  1.47it/s]Extractor Estimating: 220it [02:17,  1.49it/s]Extractor Estimating: 221it [02:18,  1.37it/s]Extractor Estimating: 222it [02:18,  1.39it/s]Extractor Estimating: 223it [02:19,  1.47it/s]Extractor Estimating: 224it [02:19,  1.50it/s]Extractor Estimating: 225it [02:20,  1.48it/s]Extractor Estimating: 226it [02:21,  1.46it/s]Extractor Estimating: 227it [02:22,  1.46it/s]Extractor Estimating: 228it [02:22,  1.43it/s]Extractor Estimating: 229it [02:23,  1.41it/s]Extractor Estimating: 230it [02:24,  1.43it/s]Extractor Estimating: 231it [02:24,  1.41it/s]Extractor Estimating: 232it [02:25,  1.41it/s]Extractor Estimating: 233it [02:26,  1.46it/s]Extractor Estimating: 234it [02:26,  1.46it/s]Extractor Estimating: 235it [02:28,  1.05s/it]Extractor Estimating: 236it [02:29,  1.09it/s]Extractor Estimating: 237it [02:30,  1.18it/s]Extractor Estimating: 238it [02:30,  1.23it/s]Extractor Estimating: 239it [02:31,  1.31it/s]Extractor Estimating: 240it [02:32,  1.33it/s]Extractor Estimating: 241it [02:32,  1.34it/s]Extractor Estimating: 242it [02:33,  1.41it/s]Extractor Estimating: 243it [02:34,  1.44it/s]Extractor Estimating: 244it [02:35,  1.41it/s]Extractor Estimating: 245it [02:35,  1.38it/s]Extractor Estimating: 246it [02:36,  1.33it/s]Extractor Estimating: 247it [02:37,  1.35it/s]Extractor Estimating: 248it [02:38,  1.36it/s]Extractor Estimating: 249it [02:38,  1.39it/s]Extractor Estimating: 250it [02:39,  1.46it/s]Extractor Estimating: 251it [02:39,  1.59it/s]Extractor Estimating: 252it [02:40,  1.69it/s]Extractor Estimating: 253it [02:40,  1.79it/s]Extractor Estimating: 254it [02:41,  1.84it/s]Extractor Estimating: 255it [02:41,  1.88it/s]Extractor Estimating: 256it [02:42,  1.95it/s]Extractor Estimating: 257it [02:42,  1.97it/s]Extractor Estimating: 258it [02:43,  1.90it/s]Extractor Estimating: 259it [02:43,  1.90it/s]Extractor Estimating: 260it [02:44,  1.88it/s]Extractor Estimating: 261it [02:44,  1.89it/s]Extractor Estimating: 262it [02:45,  1.92it/s]Extractor Estimating: 263it [02:45,  1.96it/s]Extractor Estimating: 264it [02:46,  1.99it/s]Extractor Estimating: 265it [02:46,  1.97it/s]Extractor Estimating: 266it [02:47,  1.97it/s]Extractor Estimating: 267it [02:47,  1.99it/s]Extractor Estimating: 268it [02:48,  1.95it/s]Extractor Estimating: 269it [02:48,  1.94it/s]Extractor Estimating: 270it [02:49,  1.91it/s]Extractor Estimating: 271it [02:49,  1.98it/s]Extractor Estimating: 272it [02:50,  1.98it/s]Extractor Estimating: 273it [02:50,  2.00it/s]Extractor Estimating: 274it [02:51,  1.94it/s]Extractor Estimating: 275it [02:52,  1.95it/s]Extractor Estimating: 276it [02:52,  1.78it/s]Extractor Estimating: 277it [02:53,  1.68it/s]Extractor Estimating: 278it [02:54,  1.64it/s]Extractor Estimating: 279it [02:54,  1.61it/s]Extractor Estimating: 280it [02:55,  1.58it/s]Extractor Estimating: 281it [02:56,  1.54it/s]Extractor Estimating: 282it [02:56,  1.54it/s]Extractor Estimating: 283it [02:57,  1.60it/s]Extractor Estimating: 284it [02:57,  1.55it/s]Extractor Estimating: 285it [02:58,  1.58it/s]Extractor Estimating: 286it [02:59,  1.59it/s]Extractor Estimating: 287it [02:59,  1.60it/s]Extractor Estimating: 288it [03:00,  1.54it/s]Extractor Estimating: 289it [03:01,  1.50it/s]Extractor Estimating: 290it [03:01,  1.47it/s]Extractor Estimating: 291it [03:02,  1.53it/s]Extractor Estimating: 292it [03:03,  1.55it/s]Extractor Estimating: 293it [03:03,  1.53it/s]Extractor Estimating: 294it [03:04,  1.52it/s]Extractor Estimating: 295it [03:05,  1.51it/s]Extractor Estimating: 296it [03:05,  1.49it/s]Extractor Estimating: 297it [03:06,  1.52it/s]Extractor Estimating: 298it [03:07,  1.53it/s]Extractor Estimating: 299it [03:07,  1.43it/s]Extractor Estimating: 300it [03:08,  1.44it/s]Extractor Estimating: 301it [03:09,  1.49it/s]Extractor Estimating: 302it [03:09,  1.49it/s]Extractor Estimating: 303it [03:10,  1.52it/s]Extractor Estimating: 304it [03:11,  1.54it/s]Extractor Estimating: 305it [03:11,  1.59it/s]Extractor Estimating: 306it [03:12,  1.60it/s]Extractor Estimating: 307it [03:12,  1.61it/s]Extractor Estimating: 308it [03:13,  1.56it/s]Extractor Estimating: 309it [03:14,  1.57it/s]Extractor Estimating: 310it [03:15,  1.44it/s]Extractor Estimating: 311it [03:15,  1.46it/s]Extractor Estimating: 312it [03:16,  1.48it/s]Extractor Estimating: 313it [03:17,  1.45it/s]Extractor Estimating: 314it [03:17,  1.49it/s]Extractor Estimating: 315it [03:18,  1.48it/s]Extractor Estimating: 316it [03:18,  1.58it/s]Extractor Estimating: 317it [03:19,  1.58it/s]Extractor Estimating: 318it [03:20,  1.61it/s]Extractor Estimating: 319it [03:20,  1.62it/s]Extractor Estimating: 320it [03:21,  1.64it/s]Extractor Estimating: 321it [03:21,  1.69it/s]Extractor Estimating: 322it [03:22,  1.65it/s]Extractor Estimating: 323it [03:23,  1.60it/s]Extractor Estimating: 324it [03:23,  1.57it/s]Extractor Estimating: 325it [03:24,  1.56it/s]Extractor Estimating: 326it [03:25,  1.53it/s]Extractor Estimating: 327it [03:25,  1.54it/s]Extractor Estimating: 328it [03:26,  1.49it/s]Extractor Estimating: 329it [03:27,  1.52it/s]Extractor Estimating: 330it [03:27,  1.51it/s]Extractor Estimating: 331it [03:28,  1.52it/s]Extractor Estimating: 332it [03:29,  1.54it/s]Extractor Estimating: 333it [03:29,  1.55it/s]Extractor Estimating: 334it [03:30,  1.52it/s]Extractor Estimating: 335it [03:31,  1.55it/s]Extractor Estimating: 336it [03:31,  1.56it/s]Extractor Estimating: 337it [03:32,  1.61it/s]Extractor Estimating: 338it [03:32,  1.63it/s]Extractor Estimating: 339it [03:33,  1.66it/s]Extractor Estimating: 340it [03:34,  1.61it/s]Extractor Estimating: 341it [03:34,  1.56it/s]Extractor Estimating: 342it [03:35,  1.55it/s]Extractor Estimating: 343it [03:36,  1.55it/s]Extractor Estimating: 344it [03:36,  1.58it/s]Extractor Estimating: 345it [03:37,  1.58it/s]Extractor Estimating: 346it [03:38,  1.59it/s]Extractor Estimating: 347it [03:38,  1.60it/s]Extractor Estimating: 348it [03:39,  1.59it/s]Extractor Estimating: 349it [03:39,  1.59it/s]Extractor Estimating: 350it [03:40,  1.60it/s]Extractor Estimating: 351it [03:41,  1.59it/s]Extractor Estimating: 352it [03:41,  1.49it/s]Extractor Estimating: 353it [03:42,  1.53it/s]Extractor Estimating: 354it [03:43,  1.56it/s]Extractor Estimating: 355it [03:43,  1.54it/s]Extractor Estimating: 356it [03:44,  1.57it/s]Extractor Estimating: 357it [03:44,  1.63it/s]Extractor Estimating: 358it [03:45,  1.63it/s]Extractor Estimating: 359it [03:46,  1.61it/s]Extractor Estimating: 360it [03:46,  1.60it/s]Extractor Estimating: 361it [03:47,  1.65it/s]Extractor Estimating: 362it [03:48,  1.66it/s]Extractor Estimating: 363it [03:48,  1.67it/s]Extractor Estimating: 364it [03:49,  1.67it/s]Extractor Estimating: 365it [03:49,  1.62it/s]Extractor Estimating: 366it [03:50,  1.64it/s]Extractor Estimating: 367it [03:51,  1.64it/s]Extractor Estimating: 368it [03:51,  1.64it/s]Extractor Estimating: 369it [03:52,  1.65it/s]Extractor Estimating: 370it [03:53,  1.52it/s]Extractor Estimating: 371it [03:53,  1.48it/s]Extractor Estimating: 372it [03:54,  1.51it/s]Extractor Estimating: 373it [03:55,  1.55it/s]Extractor Estimating: 374it [03:55,  1.57it/s]Extractor Estimating: 375it [03:56,  1.60it/s]Extractor Estimating: 375it [03:56,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:48,706 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:48,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:48,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:48,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:48,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:53:49,835 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:53:49,836 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:53:50,127 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:53:51,203 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:53:51,203 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:53,058 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:53,067 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:53,068 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:53,068 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:53,068 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:53:53,855 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:53:53,856 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:53:54,132 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:53:54,321 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:53:54,321 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:14:05,135 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:14:05,159 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7496 mean pseudo reward: 0.9491752288020375
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 19941
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20041, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20041, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.107, loss:557.7140
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.090, loss:528.2245
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.075, loss:519.5789
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.086, loss:480.4054
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.089, loss:497.5825
>> valid entity prec:0.5533, rec:0.5027, f1:0.5268
>> valid relation prec:0.1946, rec:0.0810, f1:0.1144
>> valid relation with NER prec:0.1946, rec:0.0810, f1:0.1144
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.412, loss:512.7262
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.078, loss:470.0037
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.091, loss:494.4814
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.083, loss:509.3241
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.079, loss:503.8317
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5383, rec:0.4852, f1:0.5103
>> valid relation prec:0.1717, rec:0.0779, f1:0.1071
>> valid relation with NER prec:0.1717, rec:0.0779, f1:0.1071
g_step 1100, step 161, avg_time 2.425, loss:462.7034
g_step 1200, step 261, avg_time 1.090, loss:487.0513
g_step 1300, step 48, avg_time 1.085, loss:467.4609
g_step 1400, step 148, avg_time 1.093, loss:457.4410
g_step 1500, step 248, avg_time 1.093, loss:466.7828
>> valid entity prec:0.5410, rec:0.4653, f1:0.5003
>> valid relation prec:0.1812, rec:0.0595, f1:0.0896
>> valid relation with NER prec:0.1812, rec:0.0595, f1:0.0896
g_step 1600, step 35, avg_time 2.401, loss:469.2318
g_step 1700, step 135, avg_time 1.084, loss:448.0858
g_step 1800, step 235, avg_time 1.085, loss:424.8771
g_step 1900, step 22, avg_time 1.096, loss:445.6113
g_step 2000, step 122, avg_time 1.091, loss:416.7004
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5418, rec:0.4965, f1:0.5181
>> valid relation prec:0.2018, rec:0.0827, f1:0.1174
>> valid relation with NER prec:0.2018, rec:0.0827, f1:0.1174
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 222, avg_time 2.418, loss:406.2085
g_step 2200, step 9, avg_time 1.085, loss:426.4216
g_step 2300, step 109, avg_time 1.100, loss:386.7100
g_step 2400, step 209, avg_time 1.066, loss:382.0880
g_step 2500, step 309, avg_time 1.101, loss:412.9856
>> valid entity prec:0.5276, rec:0.5118, f1:0.5196
>> valid relation prec:0.1610, rec:0.0802, f1:0.1070
>> valid relation with NER prec:0.1610, rec:0.0802, f1:0.1070
g_step 2600, step 96, avg_time 2.418, loss:376.5078
g_step 2700, step 196, avg_time 1.089, loss:389.5110
g_step 2800, step 296, avg_time 1.087, loss:370.1095
g_step 2900, step 83, avg_time 1.094, loss:345.9801
g_step 3000, step 183, avg_time 1.087, loss:354.9673
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5722, rec:0.4873, f1:0.5264
>> valid relation prec:0.1867, rec:0.0827, f1:0.1147
>> valid relation with NER prec:0.1867, rec:0.0827, f1:0.1147
g_step 3100, step 283, avg_time 2.401, loss:378.9546
g_step 3200, step 70, avg_time 1.088, loss:331.2384
g_step 3300, step 170, avg_time 1.090, loss:339.8007
g_step 3400, step 270, avg_time 1.095, loss:334.7922
g_step 3500, step 57, avg_time 1.093, loss:329.2707
>> valid entity prec:0.5639, rec:0.5085, f1:0.5347
>> valid relation prec:0.1787, rec:0.0893, f1:0.1191
>> valid relation with NER prec:0.1787, rec:0.0893, f1:0.1191
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 157, avg_time 2.417, loss:325.6023
g_step 3700, step 257, avg_time 1.091, loss:335.1679
g_step 3800, step 44, avg_time 1.081, loss:314.3229
g_step 3900, step 144, avg_time 1.096, loss:314.0979
g_step 4000, step 244, avg_time 1.095, loss:310.8537
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5460, rec:0.4840, f1:0.5131
>> valid relation prec:0.1718, rec:0.0770, f1:0.1063
>> valid relation with NER prec:0.1718, rec:0.0770, f1:0.1063
g_step 4100, step 31, avg_time 2.426, loss:308.6067
g_step 4200, step 131, avg_time 1.092, loss:283.3733
g_step 4300, step 231, avg_time 1.091, loss:302.8885
g_step 4400, step 18, avg_time 1.086, loss:309.9451
g_step 4500, step 118, avg_time 1.075, loss:260.9358
>> valid entity prec:0.5321, rec:0.5025, f1:0.5169
>> valid relation prec:0.1806, rec:0.0879, f1:0.1182
>> valid relation with NER prec:0.1806, rec:0.0879, f1:0.1182
g_step 4600, step 218, avg_time 2.410, loss:297.8944
g_step 4700, step 5, avg_time 1.094, loss:303.1308
g_step 4800, step 105, avg_time 1.092, loss:257.3584
g_step 4900, step 205, avg_time 1.081, loss:274.2513
g_step 5000, step 305, avg_time 1.094, loss:292.2013
learning rate was adjusted to 0.0008
>> valid entity prec:0.5448, rec:0.5135, f1:0.5287
>> valid relation prec:0.1473, rec:0.0710, f1:0.0958
>> valid relation with NER prec:0.1473, rec:0.0710, f1:0.0958
g_step 5100, step 92, avg_time 2.422, loss:254.8634
g_step 5200, step 192, avg_time 1.090, loss:254.4549
g_step 5300, step 292, avg_time 1.095, loss:284.6092
g_step 5400, step 79, avg_time 1.097, loss:239.3590
g_step 5500, step 179, avg_time 1.083, loss:246.7765
>> valid entity prec:0.5239, rec:0.4647, f1:0.4926
>> valid relation prec:0.1678, rec:0.0802, f1:0.1085
>> valid relation with NER prec:0.1678, rec:0.0802, f1:0.1085
g_step 5600, step 279, avg_time 2.407, loss:262.6691
g_step 5700, step 66, avg_time 1.082, loss:237.3466
g_step 5800, step 166, avg_time 1.097, loss:227.0670
g_step 5900, step 266, avg_time 1.095, loss:253.2049
g_step 6000, step 53, avg_time 1.076, loss:227.9858
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5473, rec:0.5001, f1:0.5226
>> valid relation prec:0.1830, rec:0.0825, f1:0.1137
>> valid relation with NER prec:0.1830, rec:0.0825, f1:0.1137
g_step 6100, step 153, avg_time 2.414, loss:240.9911
g_step 6200, step 253, avg_time 1.083, loss:253.0545
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:14:05 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:14:05 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-14-05_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:14:06 - WARNING - datasets.builder -   Using custom data configuration default-d3992b4632fbb542
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d3992b4632fbb542/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:14:06,703 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:14:06,704 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:14:06,705 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:14:06,706 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:14:06,718 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:06,723 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:06,723 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:06,723 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:06,723 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:06,723 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:06,723 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:14:06,910 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:14:10,096 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:14:10,113 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d3992b4632fbb542/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.22ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.00ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.35ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.54ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.62ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.67ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.69ba/s]100%|██████████| 8/8 [00:01<00:00,  5.52ba/s]100%|██████████| 8/8 [00:01<00:00,  4.78ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.50ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.34ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.72ba/s]100%|██████████| 4/4 [00:00<00:00,  4.77ba/s]100%|██████████| 4/4 [00:00<00:00,  4.08ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  6.77ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.11ba/s] 50%|█████     | 4/8 [00:00<00:00,  7.98ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.09ba/s]100%|██████████| 8/8 [00:00<00:00, 10.56ba/s]100%|██████████| 8/8 [00:00<00:00,  9.63ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.22ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.55ba/s]100%|██████████| 4/4 [00:00<00:00, 10.68ba/s]
[INFO|trainer.py:414] 2023-08-28 21:14:14,495 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:14:14,507 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:14:14,507 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 21:14:14,507 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:14:14,507 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:14:14,507 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:14:14,507 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:14:14,507 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:54,  3.34it/s]  0%|          | 2/585 [00:00<02:50,  3.41it/s]  1%|          | 3/585 [00:00<02:48,  3.45it/s]  1%|          | 4/585 [00:01<02:52,  3.38it/s]  1%|          | 5/585 [00:01<02:50,  3.41it/s]  1%|          | 6/585 [00:01<02:48,  3.43it/s]  1%|          | 7/585 [00:02<02:47,  3.45it/s]  1%|▏         | 8/585 [00:02<02:46,  3.46it/s]  2%|▏         | 9/585 [00:02<02:46,  3.46it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:45,  3.47it/s]  2%|▏         | 12/585 [00:03<02:44,  3.47it/s]  2%|▏         | 13/585 [00:03<02:44,  3.47it/s]  2%|▏         | 14/585 [00:04<02:44,  3.48it/s]  3%|▎         | 15/585 [00:04<02:44,  3.46it/s]  3%|▎         | 16/585 [00:04<02:44,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.47it/s]  3%|▎         | 18/585 [00:05<02:43,  3.47it/s]  3%|▎         | 19/585 [00:05<02:42,  3.47it/s]  3%|▎         | 20/585 [00:05<02:42,  3.47it/s]  4%|▎         | 21/585 [00:06<02:42,  3.47it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:41,  3.47it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:07<02:41,  3.47it/s]  5%|▍         | 27/585 [00:07<02:40,  3.47it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:08<02:40,  3.47it/s]  5%|▌         | 30/585 [00:08<02:39,  3.47it/s]  5%|▌         | 31/585 [00:08<02:39,  3.48it/s]  5%|▌         | 32/585 [00:09<02:42,  3.41it/s]  6%|▌         | 33/585 [00:09<02:41,  3.43it/s]  6%|▌         | 34/585 [00:09<02:40,  3.44it/s]  6%|▌         | 35/585 [00:10<02:39,  3.45it/s]  6%|▌         | 36/585 [00:10<02:38,  3.46it/s]  6%|▋         | 37/585 [00:10<02:38,  3.46it/s]  6%|▋         | 38/585 [00:10<02:37,  3.46it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:37,  3.47it/s]  7%|▋         | 41/585 [00:11<02:36,  3.47it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:36,  3.47it/s]  8%|▊         | 44/585 [00:12<02:35,  3.47it/s]  8%|▊         | 45/585 [00:13<02:35,  3.47it/s]  8%|▊         | 46/585 [00:13<02:35,  3.47it/s]  8%|▊         | 47/585 [00:13<02:35,  3.47it/s]  8%|▊         | 48/585 [00:13<02:34,  3.46it/s]  8%|▊         | 49/585 [00:14<02:34,  3.46it/s]  9%|▊         | 50/585 [00:14<02:42,  3.29it/s]  9%|▊         | 51/585 [00:14<02:39,  3.34it/s]  9%|▉         | 52/585 [00:15<02:37,  3.38it/s]  9%|▉         | 53/585 [00:15<02:36,  3.40it/s]  9%|▉         | 54/585 [00:15<02:35,  3.42it/s]  9%|▉         | 55/585 [00:15<02:34,  3.43it/s] 10%|▉         | 56/585 [00:16<02:33,  3.44it/s] 10%|▉         | 57/585 [00:16<02:33,  3.44it/s] 10%|▉         | 58/585 [00:16<02:32,  3.45it/s] 10%|█         | 59/585 [00:17<02:32,  3.45it/s] 10%|█         | 60/585 [00:17<02:32,  3.45it/s] 10%|█         | 61/585 [00:17<02:31,  3.45it/s] 11%|█         | 62/585 [00:17<02:31,  3.45it/s] 11%|█         | 63/585 [00:18<02:31,  3.46it/s] 11%|█         | 64/585 [00:18<02:30,  3.46it/s] 11%|█         | 65/585 [00:18<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 69/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.46it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.45it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.46it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.46it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.45it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.46it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.46it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.46it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:29,  3.22it/s] 18%|█▊        | 104/585 [00:30<02:26,  3.29it/s] 18%|█▊        | 105/585 [00:30<02:23,  3.33it/s] 18%|█▊        | 106/585 [00:30<02:22,  3.37it/s] 18%|█▊        | 107/585 [00:31<02:20,  3.40it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.42it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.43it/s] 19%|█▉        | 110/585 [00:31<02:18,  3.44it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.45it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.45it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 21:14:48,515 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:14:48,515 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:14:48,515 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.26it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.35it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.56it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.94it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.60it/s][A
  8%|▊         | 33/437 [00:00<00:11, 35.01it/s][A
  9%|▊         | 38/437 [00:00<00:10, 37.96it/s][A
 10%|▉         | 43/437 [00:01<00:09, 40.28it/s][A
 11%|█         | 48/437 [00:01<00:09, 42.03it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 43.34it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 44.26it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 44.97it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 45.47it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 45.73it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 45.92it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.11it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.27it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 46.42it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.40it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.49it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.51it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.55it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.57it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.58it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.58it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.34it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 46.50it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.58it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.56it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.64it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.57it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.53it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.56it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.55it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.60it/s][A
 42%|████▏     | 183/437 [00:04<00:05, 46.57it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.52it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.61it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.50it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.53it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.55it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.54it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.60it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.60it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.48it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.56it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.55it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.56it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.63it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.53it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.50it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.54it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 45.06it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 45.48it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 45.84it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.10it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.15it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.34it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.40it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.43it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.42it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.44it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.44it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 46.49it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.54it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.59it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.60it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.61it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.44it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.46it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.51it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 43.77it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 44.61it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 45.21it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 45.54it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.83it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.11it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.15it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.32it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.35it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.34it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.41it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.46it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.57it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.50it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.58it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.58it/s][A 20%|██        | 117/585 [00:43<02:15,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:14:58,243 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 21:14:58,323 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:15:04,252 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:15:04,273 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:15:04,287 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:07<1:20:35, 10.36s/it] 20%|██        | 119/585 [01:08<56:59,  7.34s/it]   21%|██        | 120/585 [01:08<40:28,  5.22s/it] 21%|██        | 121/585 [01:08<28:56,  3.74s/it] 21%|██        | 122/585 [01:08<20:52,  2.71s/it] 21%|██        | 123/585 [01:09<15:14,  1.98s/it] 21%|██        | 124/585 [01:09<11:18,  1.47s/it] 21%|██▏       | 125/585 [01:09<08:33,  1.12s/it] 22%|██▏       | 126/585 [01:10<06:38,  1.15it/s] 22%|██▏       | 127/585 [01:10<05:18,  1.44it/s] 22%|██▏       | 128/585 [01:10<04:21,  1.75it/s] 22%|██▏       | 129/585 [01:10<03:42,  2.05it/s] 22%|██▏       | 130/585 [01:11<03:29,  2.17it/s] 22%|██▏       | 131/585 [01:11<03:05,  2.44it/s] 23%|██▎       | 132/585 [01:11<02:48,  2.68it/s] 23%|██▎       | 133/585 [01:12<02:37,  2.88it/s] 23%|██▎       | 134/585 [01:12<02:28,  3.03it/s] 23%|██▎       | 135/585 [01:13<03:02,  2.47it/s] 23%|██▎       | 136/585 [01:13<02:46,  2.70it/s] 23%|██▎       | 137/585 [01:13<02:34,  2.89it/s] 24%|██▎       | 138/585 [01:13<02:27,  3.04it/s] 24%|██▍       | 139/585 [01:14<02:21,  3.16it/s] 24%|██▍       | 140/585 [01:14<02:17,  3.24it/s] 24%|██▍       | 141/585 [01:14<02:14,  3.31it/s] 24%|██▍       | 142/585 [01:15<02:12,  3.35it/s] 24%|██▍       | 143/585 [01:15<02:10,  3.39it/s] 25%|██▍       | 144/585 [01:15<02:09,  3.41it/s] 25%|██▍       | 145/585 [01:16<02:16,  3.23it/s] 25%|██▍       | 146/585 [01:16<02:13,  3.30it/s] 25%|██▌       | 147/585 [01:16<02:10,  3.35it/s] 25%|██▌       | 148/585 [01:16<02:09,  3.38it/s] 25%|██▌       | 149/585 [01:17<02:07,  3.41it/s] 26%|██▌       | 150/585 [01:17<02:07,  3.42it/s] 26%|██▌       | 151/585 [01:17<02:06,  3.43it/s] 26%|██▌       | 152/585 [01:18<02:05,  3.44it/s] 26%|██▌       | 153/585 [01:18<02:05,  3.45it/s] 26%|██▋       | 154/585 [01:18<02:04,  3.45it/s] 26%|██▋       | 155/585 [01:18<02:04,  3.46it/s] 27%|██▋       | 156/585 [01:19<02:07,  3.36it/s] 27%|██▋       | 157/585 [01:19<02:06,  3.39it/s] 27%|██▋       | 158/585 [01:19<02:05,  3.41it/s] 27%|██▋       | 159/585 [01:20<02:04,  3.43it/s] 27%|██▋       | 160/585 [01:20<02:03,  3.44it/s] 28%|██▊       | 161/585 [01:20<02:03,  3.44it/s] 28%|██▊       | 162/585 [01:20<02:02,  3.45it/s] 28%|██▊       | 163/585 [01:21<02:02,  3.45it/s] 28%|██▊       | 164/585 [01:21<02:01,  3.45it/s] 28%|██▊       | 165/585 [01:21<02:01,  3.45it/s] 28%|██▊       | 166/585 [01:22<02:01,  3.45it/s] 29%|██▊       | 167/585 [01:22<02:01,  3.45it/s] 29%|██▊       | 168/585 [01:22<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:23<02:00,  3.45it/s] 29%|██▉       | 170/585 [01:23<02:00,  3.45it/s] 29%|██▉       | 171/585 [01:23<01:59,  3.45it/s] 29%|██▉       | 172/585 [01:23<01:59,  3.45it/s] 30%|██▉       | 173/585 [01:24<01:59,  3.45it/s] 30%|██▉       | 174/585 [01:24<01:58,  3.45it/s] 30%|██▉       | 175/585 [01:24<01:58,  3.45it/s] 30%|███       | 176/585 [01:25<01:58,  3.45it/s] 30%|███       | 177/585 [01:25<01:58,  3.45it/s] 30%|███       | 178/585 [01:25<02:04,  3.26it/s] 31%|███       | 179/585 [01:25<02:02,  3.32it/s] 31%|███       | 180/585 [01:26<02:00,  3.36it/s] 31%|███       | 181/585 [01:26<01:59,  3.39it/s] 31%|███       | 182/585 [01:26<01:58,  3.41it/s] 31%|███▏      | 183/585 [01:27<01:57,  3.42it/s] 31%|███▏      | 184/585 [01:27<01:56,  3.43it/s] 32%|███▏      | 185/585 [01:27<01:56,  3.44it/s] 32%|███▏      | 186/585 [01:28<01:55,  3.44it/s] 32%|███▏      | 187/585 [01:28<01:55,  3.45it/s] 32%|███▏      | 188/585 [01:28<01:55,  3.45it/s] 32%|███▏      | 189/585 [01:28<01:56,  3.39it/s] 32%|███▏      | 190/585 [01:29<01:55,  3.41it/s] 33%|███▎      | 191/585 [01:29<01:55,  3.43it/s] 33%|███▎      | 192/585 [01:29<01:54,  3.44it/s] 33%|███▎      | 193/585 [01:30<01:53,  3.44it/s] 33%|███▎      | 194/585 [01:30<01:53,  3.45it/s] 33%|███▎      | 195/585 [01:30<01:53,  3.45it/s] 34%|███▎      | 196/585 [01:30<01:52,  3.45it/s] 34%|███▎      | 197/585 [01:31<01:52,  3.45it/s] 34%|███▍      | 198/585 [01:31<01:52,  3.45it/s] 34%|███▍      | 199/585 [01:31<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:32<01:51,  3.44it/s] 34%|███▍      | 201/585 [01:32<01:51,  3.44it/s] 35%|███▍      | 202/585 [01:32<01:51,  3.45it/s] 35%|███▍      | 203/585 [01:32<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:33<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:33<01:50,  3.45it/s] 35%|███▌      | 206/585 [01:33<01:49,  3.45it/s] 35%|███▌      | 207/585 [01:34<01:49,  3.45it/s] 36%|███▌      | 208/585 [01:34<01:49,  3.45it/s] 36%|███▌      | 209/585 [01:34<01:48,  3.45it/s] 36%|███▌      | 210/585 [01:34<01:48,  3.45it/s] 36%|███▌      | 211/585 [01:35<01:49,  3.42it/s] 36%|███▌      | 212/585 [01:35<01:48,  3.43it/s] 36%|███▋      | 213/585 [01:35<01:48,  3.44it/s] 37%|███▋      | 214/585 [01:36<01:47,  3.44it/s] 37%|███▋      | 215/585 [01:36<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:36<01:47,  3.45it/s] 37%|███▋      | 217/585 [01:37<01:46,  3.45it/s] 37%|███▋      | 218/585 [01:37<01:46,  3.45it/s] 37%|███▋      | 219/585 [01:37<01:46,  3.45it/s] 38%|███▊      | 220/585 [01:37<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:38<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:38<01:47,  3.39it/s] 38%|███▊      | 223/585 [01:38<01:46,  3.41it/s] 38%|███▊      | 224/585 [01:39<01:45,  3.42it/s] 38%|███▊      | 225/585 [01:39<01:44,  3.43it/s] 39%|███▊      | 226/585 [01:39<01:44,  3.44it/s] 39%|███▉      | 227/585 [01:39<01:44,  3.44it/s] 39%|███▉      | 228/585 [01:40<01:43,  3.44it/s] 39%|███▉      | 229/585 [01:40<01:43,  3.44it/s] 39%|███▉      | 230/585 [01:40<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:41<01:42,  3.44it/s] 40%|███▉      | 232/585 [01:41<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:41<01:43,  3.39it/s] 40%|████      | 234/585 [01:41<01:43,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:15:56,521 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:15:56,522 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:15:56,522 >>   Batch size = 8
{'eval_loss': 1.0058566331863403, 'eval_runtime': 9.538, 'eval_samples_per_second': 366.22, 'eval_steps_per_second': 45.817, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.02it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.18it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.47it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.74it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.32it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.89it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.79it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.60it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.61it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.57it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.48it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.44it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.42it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.37it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.36it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.40it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.39it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.45it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.39it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.51it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.42it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.40it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.44it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.45it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.35it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.34it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.39it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.39it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.43it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.44it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.40it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.36it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.41it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.34it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.39it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.32it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.39it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.31it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.40it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.25it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.32it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.29it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.35it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.39it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.36it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.30it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.31it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.23it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.19it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.23it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.17it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.31it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.36it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.39it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.40it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.42it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.40it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.46it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.50it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.46it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.45it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.50it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.45it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.43it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.38it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.36it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.39it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 38.79it/s][A
 80%|███████▉  | 348/437 [00:07<00:02, 40.85it/s][A
 81%|████████  | 353/437 [00:07<00:01, 42.40it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 43.58it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 44.44it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 45.00it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 45.41it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 45.64it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.91it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.11it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.15it/s][A
 91%|█████████ | 398/437 [00:09<00:03, 12.35it/s][A
 92%|█████████▏| 403/437 [00:09<00:02, 15.93it/s][A
 93%|█████████▎| 408/437 [00:09<00:01, 19.85it/s][A
 95%|█████████▍| 413/437 [00:09<00:01, 23.99it/s][A
 96%|█████████▌| 418/437 [00:10<00:00, 28.05it/s][A
 97%|█████████▋| 423/437 [00:10<00:00, 31.84it/s][A
 98%|█████████▊| 428/437 [00:10<00:00, 35.12it/s][A
 99%|█████████▉| 433/437 [00:10<00:00, 37.93it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 37.93it/s][A 40%|████      | 234/585 [01:52<01:43,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:16:07,118 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 21:16:07,191 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:16:15,381 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:16:15,654 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:16:16,041 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:12<54:54,  9.41s/it] 40%|████      | 236/585 [02:12<38:49,  6.68s/it] 41%|████      | 237/585 [02:13<27:36,  4.76s/it] 41%|████      | 238/585 [02:13<19:45,  3.42s/it] 41%|████      | 239/585 [02:13<14:17,  2.48s/it] 41%|████      | 240/585 [02:14<10:28,  1.82s/it] 41%|████      | 241/585 [02:14<07:51,  1.37s/it] 41%|████▏     | 242/585 [02:14<05:58,  1.04s/it] 42%|████▏     | 243/585 [02:14<04:39,  1.22it/s] 42%|████▏     | 244/585 [02:15<03:44,  1.52it/s] 42%|████▏     | 245/585 [02:15<03:06,  1.83it/s] 42%|████▏     | 246/585 [02:15<02:39,  2.13it/s] 42%|████▏     | 247/585 [02:16<02:20,  2.41it/s] 42%|████▏     | 248/585 [02:16<02:07,  2.65it/s] 43%|████▎     | 249/585 [02:16<01:57,  2.86it/s] 43%|████▎     | 250/585 [02:17<01:51,  3.02it/s] 43%|████▎     | 251/585 [02:17<01:47,  3.12it/s] 43%|████▎     | 252/585 [02:17<01:43,  3.22it/s] 43%|████▎     | 253/585 [02:17<01:40,  3.29it/s] 43%|████▎     | 254/585 [02:18<01:38,  3.35it/s] 44%|████▎     | 255/585 [02:18<01:37,  3.38it/s] 44%|████▍     | 256/585 [02:18<01:36,  3.41it/s] 44%|████▍     | 257/585 [02:19<01:35,  3.43it/s] 44%|████▍     | 258/585 [02:19<01:34,  3.44it/s] 44%|████▍     | 259/585 [02:19<01:34,  3.45it/s] 44%|████▍     | 260/585 [02:19<01:34,  3.46it/s] 45%|████▍     | 261/585 [02:20<01:33,  3.46it/s] 45%|████▍     | 262/585 [02:20<01:35,  3.37it/s] 45%|████▍     | 263/585 [02:20<01:34,  3.40it/s] 45%|████▌     | 264/585 [02:21<01:33,  3.42it/s] 45%|████▌     | 265/585 [02:21<01:33,  3.44it/s] 45%|████▌     | 266/585 [02:21<01:32,  3.45it/s] 46%|████▌     | 267/585 [02:21<01:32,  3.45it/s] 46%|████▌     | 268/585 [02:22<01:31,  3.45it/s] 46%|████▌     | 269/585 [02:22<01:31,  3.46it/s] 46%|████▌     | 270/585 [02:22<01:30,  3.46it/s] 46%|████▋     | 271/585 [02:23<01:30,  3.46it/s] 46%|████▋     | 272/585 [02:23<01:30,  3.46it/s] 47%|████▋     | 273/585 [02:23<01:31,  3.43it/s] 47%|████▋     | 274/585 [02:23<01:30,  3.44it/s] 47%|████▋     | 275/585 [02:24<01:30,  3.44it/s] 47%|████▋     | 276/585 [02:24<01:29,  3.45it/s] 47%|████▋     | 277/585 [02:24<01:29,  3.45it/s] 48%|████▊     | 278/585 [02:25<01:28,  3.46it/s] 48%|████▊     | 279/585 [02:25<01:28,  3.46it/s] 48%|████▊     | 280/585 [02:25<01:28,  3.46it/s] 48%|████▊     | 281/585 [02:25<01:27,  3.46it/s] 48%|████▊     | 282/585 [02:26<01:27,  3.46it/s] 48%|████▊     | 283/585 [02:26<01:27,  3.46it/s] 49%|████▊     | 284/585 [02:26<01:29,  3.36it/s] 49%|████▊     | 285/585 [02:27<01:28,  3.39it/s] 49%|████▉     | 286/585 [02:27<01:27,  3.41it/s] 49%|████▉     | 287/585 [02:27<01:26,  3.43it/s] 49%|████▉     | 288/585 [02:28<01:26,  3.44it/s] 49%|████▉     | 289/585 [02:28<01:25,  3.45it/s] 50%|████▉     | 290/585 [02:28<01:25,  3.45it/s] 50%|████▉     | 291/585 [02:28<01:25,  3.45it/s] 50%|████▉     | 292/585 [02:29<01:24,  3.46it/s] 50%|█████     | 293/585 [02:29<01:24,  3.46it/s] 50%|█████     | 294/585 [02:29<01:24,  3.46it/s] 50%|█████     | 295/585 [02:30<01:24,  3.43it/s] 51%|█████     | 296/585 [02:30<01:24,  3.44it/s] 51%|█████     | 297/585 [02:30<01:23,  3.44it/s] 51%|█████     | 298/585 [02:30<01:23,  3.45it/s] 51%|█████     | 299/585 [02:31<01:22,  3.45it/s] 51%|█████▏    | 300/585 [02:31<01:22,  3.45it/s] 51%|█████▏    | 301/585 [02:31<01:22,  3.45it/s] 52%|█████▏    | 302/585 [02:32<01:21,  3.45it/s] 52%|█████▏    | 303/585 [02:32<01:21,  3.45it/s] 52%|█████▏    | 304/585 [02:32<01:21,  3.45it/s] 52%|█████▏    | 305/585 [02:32<01:21,  3.45it/s] 52%|█████▏    | 306/585 [02:33<01:40,  2.79it/s] 52%|█████▏    | 307/585 [02:33<01:33,  2.96it/s] 53%|█████▎    | 308/585 [02:34<01:29,  3.09it/s] 53%|█████▎    | 309/585 [02:34<01:26,  3.20it/s] 53%|█████▎    | 310/585 [02:34<01:24,  3.27it/s] 53%|█████▎    | 311/585 [02:34<01:22,  3.33it/s] 53%|█████▎    | 312/585 [02:35<01:21,  3.37it/s] 54%|█████▎    | 313/585 [02:35<01:20,  3.39it/s] 54%|█████▎    | 314/585 [02:35<01:19,  3.41it/s] 54%|█████▍    | 315/585 [02:36<01:18,  3.43it/s] 54%|█████▍    | 316/585 [02:36<01:25,  3.14it/s] 54%|█████▍    | 317/585 [02:36<01:23,  3.23it/s] 54%|█████▍    | 318/585 [02:37<01:21,  3.29it/s] 55%|█████▍    | 319/585 [02:37<01:19,  3.34it/s] 55%|█████▍    | 320/585 [02:37<01:18,  3.37it/s] 55%|█████▍    | 321/585 [02:37<01:17,  3.39it/s] 55%|█████▌    | 322/585 [02:38<01:17,  3.41it/s] 55%|█████▌    | 323/585 [02:38<01:16,  3.42it/s] 55%|█████▌    | 324/585 [02:38<01:16,  3.43it/s] 56%|█████▌    | 325/585 [02:39<01:15,  3.44it/s] 56%|█████▌    | 326/585 [02:39<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:40<01:43,  2.49it/s] 56%|█████▌    | 328/585 [02:40<01:34,  2.71it/s] 56%|█████▌    | 329/585 [02:40<01:28,  2.90it/s] 56%|█████▋    | 330/585 [02:40<01:23,  3.05it/s] 57%|█████▋    | 331/585 [02:41<01:20,  3.16it/s] 57%|█████▋    | 332/585 [02:41<01:17,  3.25it/s] 57%|█████▋    | 333/585 [02:41<01:16,  3.31it/s] 57%|█████▋    | 334/585 [02:42<01:14,  3.35it/s] 57%|█████▋    | 335/585 [02:42<01:13,  3.38it/s] 57%|█████▋    | 336/585 [02:42<01:13,  3.40it/s] 58%|█████▊    | 337/585 [02:43<01:52,  2.21it/s] 58%|█████▊    | 338/585 [02:43<01:39,  2.48it/s] 58%|█████▊    | 339/585 [02:44<01:30,  2.71it/s] 58%|█████▊    | 340/585 [02:44<01:24,  2.89it/s] 58%|█████▊    | 341/585 [02:44<01:20,  3.04it/s] 58%|█████▊    | 342/585 [02:44<01:16,  3.16it/s] 59%|█████▊    | 343/585 [02:45<01:14,  3.24it/s] 59%|█████▉    | 344/585 [02:45<01:12,  3.30it/s] 59%|█████▉    | 345/585 [02:45<01:11,  3.35it/s] 59%|█████▉    | 346/585 [02:46<01:10,  3.38it/s] 59%|█████▉    | 347/585 [02:46<01:09,  3.40it/s] 59%|█████▉    | 348/585 [02:46<01:09,  3.42it/s] 60%|█████▉    | 349/585 [02:46<01:08,  3.43it/s] 60%|█████▉    | 350/585 [02:47<01:08,  3.44it/s] 60%|██████    | 351/585 [02:47<01:08,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 21:17:02,052 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:17:02,052 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:17:02,052 >>   Batch size = 8
{'eval_loss': 1.0163058042526245, 'eval_runtime': 10.475, 'eval_samples_per_second': 333.462, 'eval_steps_per_second': 41.719, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.50it/s][A
  3%|▎         | 12/437 [00:00<00:09, 45.83it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.22it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.35it/s][A
  6%|▌         | 27/437 [00:00<00:08, 46.42it/s][A
  7%|▋         | 32/437 [00:00<00:08, 46.48it/s][A
  8%|▊         | 37/437 [00:00<00:08, 46.61it/s][A
 10%|▉         | 42/437 [00:00<00:08, 46.63it/s][A
 11%|█         | 47/437 [00:01<00:08, 46.57it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 46.56it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 46.56it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 46.53it/s][A
 15%|█▌        | 67/437 [00:01<00:07, 46.60it/s][A
 16%|█▋        | 72/437 [00:01<00:07, 46.59it/s][A
 18%|█▊        | 77/437 [00:01<00:07, 46.55it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 46.68it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 46.66it/s][A
 21%|██        | 92/437 [00:01<00:07, 46.58it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 46.39it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 46.38it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 46.40it/s][A
 26%|██▌       | 112/437 [00:02<00:06, 46.43it/s][A
 27%|██▋       | 117/437 [00:02<00:06, 46.33it/s][A
 28%|██▊       | 122/437 [00:02<00:06, 46.37it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 46.34it/s][A
 30%|███       | 132/437 [00:02<00:06, 46.34it/s][A
 31%|███▏      | 137/437 [00:02<00:06, 46.32it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 46.31it/s][A
 34%|███▎      | 147/437 [00:03<00:10, 27.78it/s][A
 35%|███▍      | 151/437 [00:03<00:11, 25.72it/s][A
 36%|███▌      | 156/437 [00:03<00:09, 29.90it/s][A
 37%|███▋      | 160/437 [00:03<00:08, 30.83it/s][A
 38%|███▊      | 164/437 [00:03<00:08, 31.18it/s][A
 38%|███▊      | 168/437 [00:05<00:32,  8.34it/s][A
 40%|███▉      | 173/437 [00:05<00:22, 11.54it/s][A
 41%|████      | 178/437 [00:05<00:16, 15.24it/s][A
 42%|████▏     | 183/437 [00:05<00:13, 19.36it/s][A
 43%|████▎     | 188/437 [00:05<00:10, 23.67it/s][A
 44%|████▍     | 193/437 [00:05<00:08, 27.89it/s][A
 45%|████▌     | 198/437 [00:05<00:07, 31.80it/s][A
 46%|████▋     | 203/437 [00:06<00:06, 35.21it/s][A
 48%|████▊     | 208/437 [00:06<00:06, 38.07it/s][A
 49%|████▊     | 213/437 [00:06<00:05, 40.22it/s][A
 50%|████▉     | 218/437 [00:06<00:05, 41.88it/s][A
 51%|█████     | 223/437 [00:06<00:04, 43.19it/s][A
 52%|█████▏    | 228/437 [00:06<00:05, 38.39it/s][A
 53%|█████▎    | 233/437 [00:06<00:05, 40.49it/s][A
 54%|█████▍    | 238/437 [00:06<00:04, 42.11it/s][A
 56%|█████▌    | 243/437 [00:06<00:04, 43.40it/s][A
 57%|█████▋    | 248/437 [00:07<00:04, 44.29it/s][A
 58%|█████▊    | 253/437 [00:07<00:04, 45.01it/s][A
 59%|█████▉    | 258/437 [00:07<00:03, 45.48it/s][A
 60%|██████    | 263/437 [00:07<00:03, 45.76it/s][A
 61%|██████▏   | 268/437 [00:07<00:03, 45.92it/s][A
 62%|██████▏   | 273/437 [00:07<00:03, 46.11it/s][A
 64%|██████▎   | 278/437 [00:07<00:03, 46.25it/s][A
 65%|██████▍   | 283/437 [00:07<00:03, 46.32it/s][A
 66%|██████▌   | 288/437 [00:07<00:03, 46.33it/s][A
 67%|██████▋   | 293/437 [00:08<00:03, 46.41it/s][A
 68%|██████▊   | 298/437 [00:08<00:02, 46.55it/s][A
 69%|██████▉   | 303/437 [00:08<00:02, 46.57it/s][A
 70%|███████   | 308/437 [00:08<00:02, 46.59it/s][A
 72%|███████▏  | 313/437 [00:08<00:02, 46.49it/s][A
 73%|███████▎  | 318/437 [00:08<00:02, 46.55it/s][A
 74%|███████▍  | 323/437 [00:08<00:02, 46.55it/s][A
 75%|███████▌  | 328/437 [00:08<00:02, 46.56it/s][A
 76%|███████▌  | 333/437 [00:08<00:02, 46.56it/s][A
 77%|███████▋  | 338/437 [00:09<00:02, 46.60it/s][A
 78%|███████▊  | 343/437 [00:09<00:02, 46.63it/s][A
 80%|███████▉  | 348/437 [00:09<00:01, 46.67it/s][A
 81%|████████  | 353/437 [00:09<00:01, 46.60it/s][A
 82%|████████▏ | 358/437 [00:09<00:01, 46.53it/s][A
 83%|████████▎ | 363/437 [00:09<00:01, 46.61it/s][A
 84%|████████▍ | 368/437 [00:09<00:01, 37.99it/s][A
 85%|████████▌ | 373/437 [00:09<00:01, 40.10it/s][A
 86%|████████▋ | 378/437 [00:09<00:01, 41.90it/s][A
 88%|████████▊ | 383/437 [00:10<00:01, 43.19it/s][A
 89%|████████▉ | 388/437 [00:10<00:01, 44.20it/s][A
 90%|████████▉ | 393/437 [00:10<00:00, 44.90it/s][A
 91%|█████████ | 398/437 [00:10<00:00, 45.41it/s][A
 92%|█████████▏| 403/437 [00:10<00:00, 45.79it/s][A
 93%|█████████▎| 408/437 [00:10<00:00, 45.86it/s][A
 95%|█████████▍| 413/437 [00:10<00:00, 46.09it/s][A
 96%|█████████▌| 418/437 [00:10<00:00, 46.17it/s][A
 97%|█████████▋| 423/437 [00:10<00:00, 46.34it/s][A
 98%|█████████▊| 428/437 [00:11<00:00, 46.38it/s][A
 99%|█████████▉| 433/437 [00:11<00:00, 46.50it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:11<00:00, 46.50it/s][A 60%|██████    | 351/585 [02:58<01:08,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:17:13,679 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 21:17:14,932 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:17:20,234 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:17:20,379 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:17:20,395 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:16<34:10,  8.80s/it] 60%|██████    | 353/585 [03:16<24:09,  6.25s/it] 61%|██████    | 354/585 [03:16<17:09,  4.46s/it] 61%|██████    | 355/585 [03:17<12:17,  3.21s/it] 61%|██████    | 356/585 [03:17<08:53,  2.33s/it] 61%|██████    | 357/585 [03:17<06:31,  1.72s/it] 61%|██████    | 358/585 [03:17<04:52,  1.29s/it] 61%|██████▏   | 359/585 [03:18<03:43,  1.01it/s] 62%|██████▏   | 360/585 [03:18<02:55,  1.28it/s] 62%|██████▏   | 361/585 [03:18<02:22,  1.58it/s] 62%|██████▏   | 362/585 [03:19<01:58,  1.89it/s] 62%|██████▏   | 363/585 [03:19<01:41,  2.19it/s] 62%|██████▏   | 364/585 [03:19<01:29,  2.46it/s] 62%|██████▏   | 365/585 [03:19<01:21,  2.70it/s] 63%|██████▎   | 366/585 [03:20<01:15,  2.89it/s] 63%|██████▎   | 367/585 [03:20<01:11,  3.04it/s] 63%|██████▎   | 368/585 [03:20<01:08,  3.16it/s] 63%|██████▎   | 369/585 [03:21<01:06,  3.25it/s] 63%|██████▎   | 370/585 [03:21<01:04,  3.31it/s] 63%|██████▎   | 371/585 [03:21<01:03,  3.36it/s] 64%|██████▎   | 372/585 [03:21<01:03,  3.36it/s] 64%|██████▍   | 373/585 [03:22<01:02,  3.39it/s] 64%|██████▍   | 374/585 [03:22<01:01,  3.41it/s] 64%|██████▍   | 375/585 [03:22<01:01,  3.43it/s] 64%|██████▍   | 376/585 [03:23<01:00,  3.44it/s] 64%|██████▍   | 377/585 [03:23<01:00,  3.45it/s] 65%|██████▍   | 378/585 [03:23<00:59,  3.45it/s] 65%|██████▍   | 379/585 [03:23<00:59,  3.46it/s] 65%|██████▍   | 380/585 [03:24<00:59,  3.46it/s] 65%|██████▌   | 381/585 [03:24<00:58,  3.46it/s] 65%|██████▌   | 382/585 [03:24<00:58,  3.46it/s] 65%|██████▌   | 383/585 [03:25<01:02,  3.24it/s] 66%|██████▌   | 384/585 [03:25<01:00,  3.30it/s] 66%|██████▌   | 385/585 [03:25<00:59,  3.35it/s] 66%|██████▌   | 386/585 [03:26<00:58,  3.38it/s] 66%|██████▌   | 387/585 [03:26<00:58,  3.40it/s] 66%|██████▋   | 388/585 [03:26<00:57,  3.42it/s] 66%|██████▋   | 389/585 [03:26<00:57,  3.44it/s] 67%|██████▋   | 390/585 [03:27<00:56,  3.44it/s] 67%|██████▋   | 391/585 [03:27<00:56,  3.45it/s] 67%|██████▋   | 392/585 [03:27<00:55,  3.45it/s] 67%|██████▋   | 393/585 [03:28<00:55,  3.46it/s] 67%|██████▋   | 394/585 [03:28<00:57,  3.29it/s] 68%|██████▊   | 395/585 [03:28<00:56,  3.34it/s] 68%|██████▊   | 396/585 [03:28<00:55,  3.38it/s] 68%|██████▊   | 397/585 [03:29<00:55,  3.40it/s] 68%|██████▊   | 398/585 [03:29<00:54,  3.42it/s] 68%|██████▊   | 399/585 [03:29<00:54,  3.44it/s] 68%|██████▊   | 400/585 [03:30<00:53,  3.44it/s] 69%|██████▊   | 401/585 [03:30<00:53,  3.45it/s] 69%|██████▊   | 402/585 [03:30<00:52,  3.45it/s] 69%|██████▉   | 403/585 [03:30<00:52,  3.45it/s] 69%|██████▉   | 404/585 [03:31<00:52,  3.45it/s] 69%|██████▉   | 405/585 [03:31<00:53,  3.38it/s] 69%|██████▉   | 406/585 [03:31<00:52,  3.40it/s] 70%|██████▉   | 407/585 [03:32<00:52,  3.42it/s] 70%|██████▉   | 408/585 [03:32<00:51,  3.43it/s] 70%|██████▉   | 409/585 [03:32<00:51,  3.44it/s] 70%|███████   | 410/585 [03:33<00:50,  3.44it/s] 70%|███████   | 411/585 [03:33<00:50,  3.44it/s] 70%|███████   | 412/585 [03:33<00:50,  3.45it/s] 71%|███████   | 413/585 [03:33<00:49,  3.45it/s] 71%|███████   | 414/585 [03:34<00:49,  3.46it/s] 71%|███████   | 415/585 [03:34<00:49,  3.46it/s] 71%|███████   | 416/585 [03:34<00:52,  3.20it/s] 71%|███████▏  | 417/585 [03:35<00:51,  3.27it/s] 71%|███████▏  | 418/585 [03:35<00:50,  3.32it/s] 72%|███████▏  | 419/585 [03:35<00:49,  3.36it/s] 72%|███████▏  | 420/585 [03:36<00:48,  3.39it/s] 72%|███████▏  | 421/585 [03:36<00:48,  3.41it/s] 72%|███████▏  | 422/585 [03:36<00:47,  3.42it/s] 72%|███████▏  | 423/585 [03:36<00:47,  3.43it/s] 72%|███████▏  | 424/585 [03:37<00:46,  3.44it/s] 73%|███████▎  | 425/585 [03:37<00:46,  3.45it/s] 73%|███████▎  | 426/585 [03:37<00:46,  3.45it/s] 73%|███████▎  | 427/585 [03:38<00:45,  3.44it/s] 73%|███████▎  | 428/585 [03:38<00:45,  3.44it/s] 73%|███████▎  | 429/585 [03:38<00:45,  3.45it/s] 74%|███████▎  | 430/585 [03:38<00:45,  3.44it/s] 74%|███████▎  | 431/585 [03:39<00:44,  3.44it/s] 74%|███████▍  | 432/585 [03:39<00:44,  3.44it/s] 74%|███████▍  | 433/585 [03:39<00:44,  3.44it/s] 74%|███████▍  | 434/585 [03:40<00:43,  3.45it/s] 74%|███████▍  | 435/585 [03:40<00:43,  3.45it/s] 75%|███████▍  | 436/585 [03:40<00:43,  3.45it/s] 75%|███████▍  | 437/585 [03:40<00:42,  3.46it/s] 75%|███████▍  | 438/585 [03:41<00:45,  3.25it/s] 75%|███████▌  | 439/585 [03:41<00:44,  3.31it/s] 75%|███████▌  | 440/585 [03:41<00:43,  3.36it/s] 75%|███████▌  | 441/585 [03:42<00:42,  3.39it/s] 76%|███████▌  | 442/585 [03:42<00:41,  3.41it/s] 76%|███████▌  | 443/585 [03:42<00:41,  3.42it/s] 76%|███████▌  | 444/585 [03:43<00:41,  3.44it/s] 76%|███████▌  | 445/585 [03:43<00:40,  3.44it/s] 76%|███████▌  | 446/585 [03:43<00:40,  3.45it/s] 76%|███████▋  | 447/585 [03:43<00:39,  3.45it/s] 77%|███████▋  | 448/585 [03:44<00:39,  3.45it/s] 77%|███████▋  | 449/585 [03:44<00:41,  3.30it/s] 77%|███████▋  | 450/585 [03:44<00:40,  3.34it/s] 77%|███████▋  | 451/585 [03:45<00:39,  3.38it/s] 77%|███████▋  | 452/585 [03:45<00:39,  3.40it/s] 77%|███████▋  | 453/585 [03:45<00:38,  3.42it/s] 78%|███████▊  | 454/585 [03:45<00:38,  3.43it/s] 78%|███████▊  | 455/585 [03:46<00:37,  3.44it/s] 78%|███████▊  | 456/585 [03:47<00:59,  2.15it/s] 78%|███████▊  | 457/585 [03:47<00:52,  2.43it/s] 78%|███████▊  | 458/585 [03:47<00:47,  2.67it/s] 78%|███████▊  | 459/585 [03:47<00:44,  2.86it/s] 79%|███████▊  | 460/585 [03:48<00:41,  3.02it/s] 79%|███████▉  | 461/585 [03:48<00:39,  3.14it/s] 79%|███████▉  | 462/585 [03:48<00:38,  3.23it/s] 79%|███████▉  | 463/585 [03:49<00:37,  3.29it/s] 79%|███████▉  | 464/585 [03:49<00:45,  2.67it/s] 79%|███████▉  | 465/585 [03:49<00:41,  2.87it/s] 80%|███████▉  | 466/585 [03:50<00:39,  3.02it/s] 80%|███████▉  | 467/585 [03:50<00:37,  3.14it/s] 80%|████████  | 468/585 [03:50<00:37,  3.11it/s][INFO|trainer.py:2140] 2023-08-28 21:18:05,434 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:18:05,434 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:18:05,434 >>   Batch size = 8
{'eval_loss': 1.0310983657836914, 'eval_runtime': 11.2535, 'eval_samples_per_second': 310.392, 'eval_steps_per_second': 38.832, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.72it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.12it/s][A
  4%|▍         | 18/437 [00:00<00:10, 38.44it/s][A
  5%|▌         | 23/437 [00:00<00:11, 35.80it/s][A
  6%|▌         | 27/437 [00:00<00:12, 31.78it/s][A
  7%|▋         | 32/437 [00:00<00:11, 35.70it/s][A
  8%|▊         | 37/437 [00:00<00:10, 38.62it/s][A
 10%|▉         | 42/437 [00:01<00:09, 40.86it/s][A
 11%|█         | 47/437 [00:01<00:16, 23.87it/s][A
 12%|█▏        | 51/437 [00:01<00:18, 20.68it/s][A
 12%|█▏        | 54/437 [00:02<00:22, 16.73it/s][A
 14%|█▎        | 59/437 [00:02<00:17, 21.49it/s][A
 15%|█▍        | 64/437 [00:02<00:14, 26.11it/s][A
 16%|█▌        | 69/437 [00:02<00:12, 30.41it/s][A
 17%|█▋        | 74/437 [00:02<00:10, 34.10it/s][A
 18%|█▊        | 79/437 [00:02<00:09, 37.18it/s][A
 19%|█▉        | 84/437 [00:02<00:08, 39.66it/s][A
 20%|██        | 89/437 [00:02<00:08, 41.54it/s][A
 22%|██▏       | 94/437 [00:02<00:07, 42.96it/s][A
 23%|██▎       | 99/437 [00:03<00:07, 43.87it/s][A
 24%|██▍       | 104/437 [00:03<00:07, 44.80it/s][A
 25%|██▍       | 109/437 [00:03<00:07, 45.27it/s][A
 26%|██▌       | 114/437 [00:03<00:07, 45.66it/s][A
 27%|██▋       | 119/437 [00:03<00:06, 46.00it/s][A
 28%|██▊       | 124/437 [00:03<00:06, 46.11it/s][A
 30%|██▉       | 129/437 [00:03<00:06, 46.31it/s][A
 31%|███       | 134/437 [00:03<00:06, 46.44it/s][A
 32%|███▏      | 139/437 [00:03<00:06, 46.47it/s][A
 33%|███▎      | 144/437 [00:03<00:06, 46.56it/s][A
 34%|███▍      | 149/437 [00:04<00:06, 46.48it/s][A
 35%|███▌      | 154/437 [00:04<00:06, 46.53it/s][A
 36%|███▋      | 159/437 [00:04<00:05, 46.61it/s][A
 38%|███▊      | 164/437 [00:04<00:05, 46.67it/s][A
 39%|███▊      | 169/437 [00:04<00:05, 46.51it/s][A
 40%|███▉      | 174/437 [00:04<00:06, 37.85it/s][A
 41%|████      | 179/437 [00:04<00:06, 40.05it/s][A
 42%|████▏     | 184/437 [00:04<00:06, 41.85it/s][A
 43%|████▎     | 189/437 [00:05<00:05, 43.22it/s][A
 44%|████▍     | 194/437 [00:05<00:05, 44.08it/s][A
 46%|████▌     | 199/437 [00:05<00:05, 44.84it/s][A
 47%|████▋     | 204/437 [00:05<00:05, 45.36it/s][A
 48%|████▊     | 209/437 [00:05<00:04, 45.72it/s][A
 49%|████▉     | 214/437 [00:05<00:04, 45.89it/s][A
 50%|█████     | 219/437 [00:05<00:04, 45.92it/s][A
 51%|█████▏    | 224/437 [00:05<00:04, 46.12it/s][A
 52%|█████▏    | 229/437 [00:05<00:04, 46.29it/s][A
 54%|█████▎    | 234/437 [00:05<00:04, 46.33it/s][A
 55%|█████▍    | 239/437 [00:06<00:04, 46.46it/s][A
 56%|█████▌    | 244/437 [00:06<00:04, 46.39it/s][A
 57%|█████▋    | 249/437 [00:06<00:04, 46.53it/s][A
 58%|█████▊    | 254/437 [00:06<00:03, 46.58it/s][A
 59%|█████▉    | 259/437 [00:06<00:03, 46.57it/s][A
 60%|██████    | 264/437 [00:06<00:03, 46.52it/s][A
 62%|██████▏   | 269/437 [00:06<00:03, 46.49it/s][A
 63%|██████▎   | 274/437 [00:06<00:03, 46.43it/s][A
 64%|██████▍   | 279/437 [00:06<00:03, 46.53it/s][A
 65%|██████▍   | 284/437 [00:07<00:03, 46.60it/s][A
 66%|██████▌   | 289/437 [00:07<00:03, 46.53it/s][A
 67%|██████▋   | 294/437 [00:07<00:03, 46.58it/s][A
 68%|██████▊   | 299/437 [00:07<00:02, 46.59it/s][A
 70%|██████▉   | 304/437 [00:07<00:02, 46.61it/s][A
 71%|███████   | 309/437 [00:07<00:02, 46.61it/s][A
 72%|███████▏  | 314/437 [00:07<00:02, 42.23it/s][A
 73%|███████▎  | 319/437 [00:07<00:02, 43.41it/s][A
 74%|███████▍  | 324/437 [00:07<00:02, 44.31it/s][A
 75%|███████▌  | 329/437 [00:08<00:02, 45.02it/s][A
 76%|███████▋  | 334/437 [00:08<00:02, 45.46it/s][A
 78%|███████▊  | 339/437 [00:08<00:02, 45.79it/s][A
 79%|███████▊  | 344/437 [00:08<00:02, 46.05it/s][A
 80%|███████▉  | 349/437 [00:08<00:01, 46.20it/s][A
 81%|████████  | 354/437 [00:08<00:01, 46.22it/s][A
 82%|████████▏ | 359/437 [00:08<00:01, 46.39it/s][A
 83%|████████▎ | 364/437 [00:08<00:01, 46.44it/s][A
 84%|████████▍ | 369/437 [00:08<00:01, 46.47it/s][A
 86%|████████▌ | 374/437 [00:09<00:01, 46.51it/s][A
 87%|████████▋ | 379/437 [00:09<00:01, 46.53it/s][A
 88%|████████▊ | 384/437 [00:09<00:01, 46.55it/s][A
 89%|████████▉ | 389/437 [00:09<00:01, 46.61it/s][A
 90%|█████████ | 394/437 [00:09<00:00, 46.53it/s][A
 91%|█████████▏| 399/437 [00:09<00:00, 46.51it/s][A
 92%|█████████▏| 404/437 [00:09<00:00, 46.55it/s][A
 94%|█████████▎| 409/437 [00:09<00:00, 46.50it/s][A
 95%|█████████▍| 414/437 [00:09<00:00, 46.50it/s][A
 96%|█████████▌| 419/437 [00:09<00:00, 46.50it/s][A
 97%|█████████▋| 424/437 [00:10<00:00, 46.50it/s][A
 98%|█████████▊| 429/437 [00:10<00:00, 46.56it/s][A
 99%|█████████▉| 434/437 [00:10<00:00, 46.60it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 46.60it/s][A 80%|████████  | 468/585 [04:01<00:37,  3.11it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:18:15,875 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:18:15,894 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:18:21,159 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:18:21,174 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:18:21,184 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:15<14:56,  7.72s/it] 80%|████████  | 470/585 [04:16<10:31,  5.49s/it] 81%|████████  | 471/585 [04:16<07:28,  3.93s/it] 81%|████████  | 472/585 [04:16<05:20,  2.84s/it] 81%|████████  | 473/585 [04:17<03:52,  2.07s/it] 81%|████████  | 474/585 [04:17<02:50,  1.54s/it] 81%|████████  | 475/585 [04:17<02:07,  1.16s/it] 81%|████████▏ | 476/585 [04:17<01:38,  1.11it/s] 82%|████████▏ | 477/585 [04:18<01:17,  1.39it/s] 82%|████████▏ | 478/585 [04:18<01:02,  1.70it/s] 82%|████████▏ | 479/585 [04:18<00:52,  2.01it/s] 82%|████████▏ | 480/585 [04:19<00:45,  2.30it/s] 82%|████████▏ | 481/585 [04:19<00:40,  2.55it/s] 82%|████████▏ | 482/585 [04:19<00:37,  2.77it/s] 83%|████████▎ | 483/585 [04:19<00:34,  2.95it/s] 83%|████████▎ | 484/585 [04:20<00:34,  2.93it/s] 83%|████████▎ | 485/585 [04:20<00:32,  3.07it/s] 83%|████████▎ | 486/585 [04:20<00:31,  3.18it/s] 83%|████████▎ | 487/585 [04:21<00:30,  3.26it/s] 83%|████████▎ | 488/585 [04:21<00:29,  3.32it/s] 84%|████████▎ | 489/585 [04:21<00:28,  3.36it/s] 84%|████████▍ | 490/585 [04:21<00:28,  3.39it/s] 84%|████████▍ | 491/585 [04:22<00:27,  3.41it/s] 84%|████████▍ | 492/585 [04:22<00:27,  3.42it/s] 84%|████████▍ | 493/585 [04:22<00:26,  3.44it/s] 84%|████████▍ | 494/585 [04:23<00:26,  3.44it/s] 85%|████████▍ | 495/585 [04:23<00:27,  3.33it/s] 85%|████████▍ | 496/585 [04:23<00:26,  3.37it/s] 85%|████████▍ | 497/585 [04:24<00:25,  3.40it/s] 85%|████████▌ | 498/585 [04:24<00:25,  3.42it/s] 85%|████████▌ | 499/585 [04:24<00:25,  3.43it/s] 85%|████████▌ | 500/585 [04:24<00:24,  3.44it/s]                                                  85%|████████▌ | 500/585 [04:24<00:24,  3.44it/s] 86%|████████▌ | 501/585 [04:25<00:24,  3.44it/s] 86%|████████▌ | 502/585 [04:25<00:24,  3.45it/s] 86%|████████▌ | 503/585 [04:25<00:23,  3.45it/s] 86%|████████▌ | 504/585 [04:26<00:23,  3.46it/s] 86%|████████▋ | 505/585 [04:26<00:23,  3.46it/s] 86%|████████▋ | 506/585 [04:26<00:23,  3.32it/s] 87%|████████▋ | 507/585 [04:26<00:23,  3.36it/s] 87%|████████▋ | 508/585 [04:27<00:22,  3.39it/s] 87%|████████▋ | 509/585 [04:27<00:22,  3.41it/s] 87%|████████▋ | 510/585 [04:27<00:21,  3.43it/s] 87%|████████▋ | 511/585 [04:28<00:22,  3.24it/s] 88%|████████▊ | 512/585 [04:28<00:22,  3.29it/s] 88%|████████▊ | 513/585 [04:28<00:21,  3.34it/s] 88%|████████▊ | 514/585 [04:29<00:21,  3.37it/s] 88%|████████▊ | 515/585 [04:29<00:20,  3.39it/s] 88%|████████▊ | 516/585 [04:29<00:20,  3.41it/s] 88%|████████▊ | 517/585 [04:30<00:21,  3.19it/s] 89%|████████▊ | 518/585 [04:30<00:20,  3.27it/s] 89%|████████▊ | 519/585 [04:30<00:19,  3.32it/s] 89%|████████▉ | 520/585 [04:30<00:19,  3.36it/s] 89%|████████▉ | 521/585 [04:31<00:18,  3.39it/s] 89%|████████▉ | 522/585 [04:31<00:18,  3.41it/s] 89%|████████▉ | 523/585 [04:31<00:18,  3.43it/s] 90%|████████▉ | 524/585 [04:32<00:17,  3.44it/s] 90%|████████▉ | 525/585 [04:32<00:17,  3.44it/s] 90%|████████▉ | 526/585 [04:32<00:17,  3.45it/s] 90%|█████████ | 527/585 [04:32<00:16,  3.45it/s] 90%|█████████ | 528/585 [04:33<00:17,  3.24it/s] 90%|█████████ | 529/585 [04:33<00:16,  3.30it/s] 91%|█████████ | 530/585 [04:33<00:16,  3.35it/s] 91%|█████████ | 531/585 [04:34<00:15,  3.38it/s] 91%|█████████ | 532/585 [04:34<00:15,  3.40it/s] 91%|█████████ | 533/585 [04:34<00:15,  3.42it/s] 91%|█████████▏| 534/585 [04:34<00:14,  3.43it/s] 91%|█████████▏| 535/585 [04:35<00:14,  3.44it/s] 92%|█████████▏| 536/585 [04:35<00:14,  3.44it/s] 92%|█████████▏| 537/585 [04:35<00:13,  3.45it/s] 92%|█████████▏| 538/585 [04:36<00:13,  3.45it/s] 92%|█████████▏| 539/585 [04:37<00:21,  2.10it/s] 92%|█████████▏| 540/585 [04:37<00:18,  2.39it/s] 92%|█████████▏| 541/585 [04:37<00:16,  2.63it/s] 93%|█████████▎| 542/585 [04:37<00:15,  2.83it/s] 93%|█████████▎| 543/585 [04:38<00:14,  2.99it/s] 93%|█████████▎| 544/585 [04:38<00:13,  3.12it/s] 93%|█████████▎| 545/585 [04:38<00:12,  3.22it/s] 93%|█████████▎| 546/585 [04:39<00:11,  3.29it/s] 94%|█████████▎| 547/585 [04:39<00:11,  3.34it/s] 94%|█████████▎| 548/585 [04:39<00:11,  3.29it/s] 94%|█████████▍| 549/585 [04:39<00:10,  3.34it/s] 94%|█████████▍| 550/585 [04:40<00:10,  3.37it/s] 94%|█████████▍| 551/585 [04:40<00:10,  3.40it/s] 94%|█████████▍| 552/585 [04:40<00:09,  3.42it/s] 95%|█████████▍| 553/585 [04:41<00:09,  3.43it/s] 95%|█████████▍| 554/585 [04:41<00:09,  3.44it/s] 95%|█████████▍| 555/585 [04:41<00:08,  3.44it/s] 95%|█████████▌| 556/585 [04:41<00:08,  3.45it/s] 95%|█████████▌| 557/585 [04:42<00:08,  3.45it/s] 95%|█████████▌| 558/585 [04:42<00:07,  3.45it/s] 96%|█████████▌| 559/585 [04:42<00:07,  3.31it/s] 96%|█████████▌| 560/585 [04:43<00:07,  3.35it/s] 96%|█████████▌| 561/585 [04:43<00:07,  3.39it/s] 96%|█████████▌| 562/585 [04:43<00:06,  3.41it/s] 96%|█████████▌| 563/585 [04:44<00:06,  3.42it/s] 96%|█████████▋| 564/585 [04:44<00:06,  3.43it/s] 97%|█████████▋| 565/585 [04:44<00:05,  3.44it/s] 97%|█████████▋| 566/585 [04:44<00:05,  3.45it/s] 97%|█████████▋| 567/585 [04:45<00:05,  3.45it/s] 97%|█████████▋| 568/585 [04:45<00:04,  3.45it/s] 97%|█████████▋| 569/585 [04:45<00:04,  3.46it/s] 97%|█████████▋| 570/585 [04:46<00:04,  3.24it/s] 98%|█████████▊| 571/585 [04:46<00:04,  3.30it/s] 98%|█████████▊| 572/585 [04:46<00:03,  3.34it/s] 98%|█████████▊| 573/585 [04:47<00:03,  3.38it/s] 98%|█████████▊| 574/585 [04:47<00:03,  3.40it/s] 98%|█████████▊| 575/585 [04:47<00:02,  3.42it/s] 98%|█████████▊| 576/585 [04:47<00:02,  3.21it/s] 99%|█████████▊| 577/585 [04:48<00:02,  3.28it/s] 99%|█████████▉| 578/585 [04:48<00:02,  3.33it/s] 99%|█████████▉| 579/585 [04:48<00:01,  3.37it/s] 99%|█████████▉| 580/585 [04:49<00:01,  3.39it/s] 99%|█████████▉| 581/585 [04:49<00:01,  3.41it/s] 99%|█████████▉| 582/585 [04:49<00:00,  3.43it/s]100%|█████████▉| 583/585 [04:49<00:00,  3.44it/s]100%|█████████▉| 584/585 [04:50<00:00,  3.45it/s]100%|██████████| 585/585 [04:50<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 21:19:05,060 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:19:05,060 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:19:05,061 >>   Batch size = 8
{'eval_loss': 1.0446875095367432, 'eval_runtime': 10.4116, 'eval_samples_per_second': 335.49, 'eval_steps_per_second': 41.972, 'epoch': 4.0}
{'loss': 0.5314, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  2%|▏         | 7/437 [00:00<00:07, 58.38it/s][A
  3%|▎         | 13/437 [00:00<00:08, 48.61it/s][A
  4%|▍         | 18/437 [00:00<00:08, 47.33it/s][A
  5%|▌         | 23/437 [00:00<00:08, 46.86it/s][A
  6%|▋         | 28/437 [00:00<00:08, 46.72it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.54it/s][A
  9%|▊         | 38/437 [00:00<00:08, 44.99it/s][A
 10%|▉         | 43/437 [00:00<00:08, 45.06it/s][A
 11%|█         | 48/437 [00:01<00:09, 42.03it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 43.86it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 44.48it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 44.96it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 45.48it/s][A
 17%|█▋        | 73/437 [00:01<00:09, 39.37it/s][A
 18%|█▊        | 78/437 [00:01<00:11, 30.68it/s][A
 19%|█▉        | 82/437 [00:02<00:11, 31.80it/s][A
 20%|█▉        | 86/437 [00:02<00:10, 32.95it/s][A
 21%|██        | 90/437 [00:02<00:10, 33.36it/s][A
 22%|██▏       | 95/437 [00:02<00:09, 36.85it/s][A
 23%|██▎       | 100/437 [00:02<00:08, 39.48it/s][A
 24%|██▍       | 105/437 [00:02<00:07, 41.52it/s][A
 25%|██▌       | 110/437 [00:02<00:07, 42.97it/s][A
 26%|██▋       | 115/437 [00:02<00:07, 43.96it/s][A
 27%|██▋       | 120/437 [00:02<00:07, 44.74it/s][A
 29%|██▊       | 125/437 [00:03<00:06, 45.33it/s][A
 30%|██▉       | 130/437 [00:03<00:07, 39.84it/s][A
 31%|███       | 135/437 [00:03<00:07, 41.52it/s][A
 32%|███▏      | 140/437 [00:03<00:06, 42.90it/s][A
 33%|███▎      | 145/437 [00:03<00:06, 43.98it/s][A
 34%|███▍      | 150/437 [00:03<00:06, 44.74it/s][A
 35%|███▌      | 155/437 [00:03<00:06, 45.30it/s][A
 37%|███▋      | 160/437 [00:03<00:06, 45.68it/s][A
 38%|███▊      | 165/437 [00:03<00:05, 45.90it/s][A
 39%|███▉      | 170/437 [00:04<00:05, 46.10it/s][A
 40%|████      | 175/437 [00:04<00:05, 46.20it/s][A
 41%|████      | 180/437 [00:04<00:05, 46.36it/s][A
 42%|████▏     | 185/437 [00:04<00:05, 46.39it/s][A
 43%|████▎     | 190/437 [00:04<00:05, 46.40it/s][A
 45%|████▍     | 195/437 [00:04<00:05, 46.51it/s][A
 46%|████▌     | 200/437 [00:04<00:05, 46.55it/s][A
 47%|████▋     | 205/437 [00:04<00:04, 46.43it/s][A
 48%|████▊     | 210/437 [00:04<00:04, 46.48it/s][A
 49%|████▉     | 215/437 [00:04<00:04, 46.43it/s][A
 50%|█████     | 220/437 [00:05<00:04, 46.41it/s][A
 51%|█████▏    | 225/437 [00:05<00:04, 46.46it/s][A
 53%|█████▎    | 230/437 [00:05<00:04, 46.36it/s][A
 54%|█████▍    | 235/437 [00:05<00:04, 46.50it/s][A
 55%|█████▍    | 240/437 [00:05<00:04, 46.53it/s][A
 56%|█████▌    | 245/437 [00:05<00:04, 46.54it/s][A
 57%|█████▋    | 250/437 [00:05<00:04, 46.59it/s][A
 58%|█████▊    | 255/437 [00:05<00:03, 46.58it/s][A
 59%|█████▉    | 260/437 [00:05<00:03, 46.55it/s][A
 61%|██████    | 265/437 [00:06<00:03, 46.61it/s][A
 62%|██████▏   | 270/437 [00:06<00:05, 28.99it/s][A
 63%|██████▎   | 275/437 [00:06<00:04, 32.73it/s][A
 64%|██████▍   | 280/437 [00:06<00:04, 35.96it/s][A
 65%|██████▌   | 285/437 [00:06<00:03, 38.58it/s][A
 66%|██████▋   | 290/437 [00:06<00:03, 40.66it/s][A
 68%|██████▊   | 295/437 [00:06<00:03, 42.26it/s][A
 69%|██████▊   | 300/437 [00:07<00:03, 43.45it/s][A
 70%|██████▉   | 305/437 [00:07<00:02, 44.38it/s][A
 71%|███████   | 310/437 [00:07<00:02, 44.85it/s][A
 72%|███████▏  | 315/437 [00:07<00:02, 45.38it/s][A
 73%|███████▎  | 320/437 [00:07<00:02, 45.70it/s][A
 74%|███████▍  | 325/437 [00:07<00:02, 45.94it/s][A
 76%|███████▌  | 330/437 [00:07<00:02, 46.16it/s][A
 77%|███████▋  | 335/437 [00:07<00:02, 46.29it/s][A
 78%|███████▊  | 340/437 [00:07<00:02, 46.34it/s][A
 79%|███████▉  | 345/437 [00:07<00:01, 46.30it/s][A
 80%|████████  | 350/437 [00:08<00:01, 46.39it/s][A
 81%|████████  | 355/437 [00:08<00:01, 46.45it/s][A
 82%|████████▏ | 360/437 [00:08<00:01, 46.44it/s][A
 84%|████████▎ | 365/437 [00:08<00:01, 46.46it/s][A
 85%|████████▍ | 370/437 [00:08<00:01, 46.47it/s][A
 86%|████████▌ | 375/437 [00:08<00:01, 46.56it/s][A
 87%|████████▋ | 380/437 [00:08<00:01, 46.52it/s][A
 88%|████████▊ | 385/437 [00:08<00:01, 46.53it/s][A
 89%|████████▉ | 390/437 [00:08<00:01, 46.42it/s][A
 90%|█████████ | 395/437 [00:09<00:00, 46.49it/s][A
 92%|█████████▏| 400/437 [00:09<00:00, 40.02it/s][A
 93%|█████████▎| 405/437 [00:09<00:00, 41.72it/s][A
 94%|█████████▍| 410/437 [00:09<00:00, 43.09it/s][A
 95%|█████████▍| 415/437 [00:09<00:00, 44.11it/s][A
 96%|█████████▌| 420/437 [00:09<00:00, 44.82it/s][A
 97%|█████████▋| 425/437 [00:09<00:00, 45.34it/s][A
 98%|█████████▊| 430/437 [00:09<00:00, 45.66it/s][A
100%|█████████▉| 435/437 [00:09<00:00, 45.99it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 45.99it/s][A100%|██████████| 585/585 [05:00<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:19:15,138 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 21:19:15,206 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:19:21,227 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:19:21,248 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:19:21,258 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:19:33,324 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:19:33,324 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117 (score: 1.0058566331863403).
                                                 100%|██████████| 585/585 [05:23<00:00,  3.45it/s]100%|██████████| 585/585 [05:23<00:00,  1.81it/s]
[INFO|trainer.py:1894] 2023-08-28 21:19:37,978 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 21:19:38,008 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:19:43,450 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:19:43,482 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:19:43,501 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:19:43,707 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:43,708 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:43,708 >>   train_loss               =     0.5276
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:43,708 >>   train_runtime            = 0:05:23.44
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:43,708 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:43,708 >>   train_samples_per_second =    115.925
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:43,708 >>   train_steps_per_second   =      1.809
{'eval_loss': 1.0479775667190552, 'eval_runtime': 10.055, 'eval_samples_per_second': 347.389, 'eval_steps_per_second': 43.461, 'epoch': 5.0}
{'train_runtime': 323.4431, 'train_samples_per_second': 115.925, 'train_steps_per_second': 1.809, 'train_loss': 0.5275688953888722, 'epoch': 5.0}
08/28/2023 21:19:43 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:19:43,763 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:19:43,764 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:19:43,764 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.82it/s]  3%|▎         | 12/437 [00:00<00:08, 51.34it/s]  4%|▍         | 18/437 [00:00<00:08, 49.23it/s]  5%|▌         | 23/437 [00:00<00:08, 48.50it/s]  6%|▋         | 28/437 [00:00<00:08, 48.03it/s]  8%|▊         | 33/437 [00:00<00:08, 47.76it/s]  9%|▊         | 38/437 [00:00<00:08, 47.60it/s] 10%|▉         | 43/437 [00:00<00:08, 47.47it/s] 11%|█         | 48/437 [00:00<00:08, 47.19it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.09it/s] 13%|█▎        | 58/437 [00:01<00:08, 47.12it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.10it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.20it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.14it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.17it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.16it/s] 20%|██        | 88/437 [00:01<00:07, 47.15it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.04it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.99it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.96it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.05it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.14it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.10it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.05it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.14it/s] 30%|███       | 133/437 [00:02<00:06, 45.58it/s] 32%|███▏      | 138/437 [00:02<00:06, 45.93it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.27it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.58it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.71it/s] 36%|███▌      | 158/437 [00:03<00:05, 46.85it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.90it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.98it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.86it/s] 41%|████      | 178/437 [00:03<00:05, 46.91it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.91it/s] 43%|████▎     | 188/437 [00:03<00:05, 46.98it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.87it/s] 45%|████▌     | 198/437 [00:04<00:05, 47.03it/s] 46%|████▋     | 203/437 [00:04<00:04, 47.04it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.98it/s] 49%|████▊     | 213/437 [00:04<00:04, 47.06it/s] 50%|████▉     | 218/437 [00:04<00:04, 47.02it/s] 51%|█████     | 223/437 [00:04<00:04, 46.95it/s] 52%|█████▏    | 228/437 [00:04<00:04, 47.01it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.04it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.91it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.99it/s] 57%|█████▋    | 248/437 [00:05<00:04, 47.04it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.88it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.95it/s] 60%|██████    | 263/437 [00:05<00:03, 46.99it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.91it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.96it/s] 64%|██████▎   | 278/437 [00:05<00:03, 47.04it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.92it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.97it/s] 67%|██████▋   | 293/437 [00:06<00:03, 47.04it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.92it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.84it/s] 70%|███████   | 308/437 [00:06<00:02, 46.75it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.66it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.75it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.87it/s] 75%|███████▌  | 328/437 [00:06<00:02, 46.85it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.88it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.89it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.87it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.86it/s] 81%|████████  | 353/437 [00:07<00:01, 46.90it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.88it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.89it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.89it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.76it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.80it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.90it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.91it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.89it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.88it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.90it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.90it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.93it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.90it/s] 97%|█████████▋| 423/437 [00:08<00:00, 46.93it/s] 98%|█████████▊| 428/437 [00:09<00:00, 46.99it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.97it/s]100%|██████████| 437/437 [00:09<00:00, 47.03it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:19:53,080 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:53,080 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:53,080 >>   eval_loss               =     1.0059
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:53,080 >>   eval_runtime            = 0:00:09.31
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:53,080 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:53,080 >>   eval_samples_per_second =    374.957
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:53,080 >>   eval_steps_per_second   =      46.91
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:53,080 >>   perplexity              =     2.7342
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:01,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:01,704 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:01,704 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:01,704 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:01,704 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:20:02,063 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:20:02,064 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:20:02,845 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:20:03,871 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:20:03,871 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:08,119 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:08,134 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:08,134 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:08,134 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:20:08,134 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:20:08,784 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:20:08,785 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:20:09,389 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:20:09,535 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:20:09,535 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.57it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:06,  1.51it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:17,  1.65it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:19,  1.52it/s]Extractor Predicting: 32it [00:20,  1.49it/s]Extractor Predicting: 33it [00:21,  1.47it/s]Extractor Predicting: 34it [00:21,  1.46it/s]Extractor Predicting: 35it [00:22,  1.48it/s]Extractor Predicting: 36it [00:23,  1.47it/s]Extractor Predicting: 37it [00:23,  1.45it/s]Extractor Predicting: 38it [00:24,  1.45it/s]Extractor Predicting: 39it [00:25,  1.47it/s]Extractor Predicting: 40it [00:25,  1.47it/s]Extractor Predicting: 41it [00:26,  1.47it/s]Extractor Predicting: 42it [00:27,  1.48it/s]Extractor Predicting: 43it [00:27,  1.48it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:29,  1.48it/s]Extractor Predicting: 47it [00:30,  1.46it/s]Extractor Predicting: 48it [00:31,  1.47it/s]Extractor Predicting: 49it [00:32,  1.46it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.46it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:36,  1.46it/s]Extractor Predicting: 56it [00:36,  1.47it/s]Extractor Predicting: 57it [00:37,  1.44it/s]Extractor Predicting: 58it [00:38,  1.45it/s]Extractor Predicting: 59it [00:38,  1.44it/s]Extractor Predicting: 60it [00:39,  1.44it/s]Extractor Predicting: 61it [00:40,  1.46it/s]Extractor Predicting: 62it [00:40,  1.44it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.50it/s]Extractor Predicting: 65it [00:42,  1.49it/s]Extractor Predicting: 66it [00:43,  1.52it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:44,  1.49it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.47it/s]Extractor Predicting: 71it [00:46,  1.49it/s]Extractor Predicting: 72it [00:47,  1.35it/s]Extractor Predicting: 73it [00:48,  1.39it/s]Extractor Predicting: 74it [00:49,  1.44it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.47it/s]Extractor Predicting: 77it [00:51,  1.46it/s]Extractor Predicting: 78it [00:51,  1.46it/s]Extractor Predicting: 79it [00:52,  1.47it/s]Extractor Predicting: 80it [00:53,  1.46it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:54,  1.35it/s]Extractor Predicting: 83it [00:55,  1.38it/s]Extractor Predicting: 84it [00:56,  1.42it/s]Extractor Predicting: 85it [00:56,  1.47it/s]Extractor Predicting: 86it [00:57,  1.54it/s]Extractor Predicting: 87it [00:57,  1.53it/s]Extractor Predicting: 88it [00:58,  1.55it/s]Extractor Predicting: 89it [00:59,  1.57it/s]Extractor Predicting: 90it [00:59,  1.60it/s]Extractor Predicting: 91it [01:00,  1.58it/s]Extractor Predicting: 92it [01:01,  1.53it/s]Extractor Predicting: 93it [01:01,  1.57it/s]Extractor Predicting: 94it [01:02,  1.61it/s]Extractor Predicting: 95it [01:02,  1.60it/s]Extractor Predicting: 96it [01:03,  1.62it/s]Extractor Predicting: 97it [01:04,  1.60it/s]Extractor Predicting: 98it [01:04,  1.57it/s]Extractor Predicting: 99it [01:05,  1.54it/s]Extractor Predicting: 100it [01:06,  1.57it/s]Extractor Predicting: 101it [01:06,  1.60it/s]Extractor Predicting: 102it [01:07,  1.36it/s]Extractor Predicting: 103it [01:08,  1.43it/s]Extractor Predicting: 104it [01:08,  1.47it/s]Extractor Predicting: 105it [01:09,  1.41it/s]Extractor Predicting: 106it [01:10,  1.43it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:11,  1.52it/s]Extractor Predicting: 109it [01:12,  1.57it/s]Extractor Predicting: 110it [01:12,  1.57it/s]Extractor Predicting: 111it [01:13,  1.56it/s]Extractor Predicting: 112it [01:14,  1.56it/s]Extractor Predicting: 113it [01:14,  1.56it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.53it/s]Extractor Predicting: 116it [01:16,  1.51it/s]Extractor Predicting: 117it [01:17,  1.50it/s]Extractor Predicting: 118it [01:18,  1.56it/s]Extractor Predicting: 119it [01:18,  1.55it/s]Extractor Predicting: 120it [01:19,  1.57it/s]Extractor Predicting: 121it [01:20,  1.56it/s]Extractor Predicting: 122it [01:20,  1.54it/s]Extractor Predicting: 123it [01:21,  1.54it/s]Extractor Predicting: 124it [01:22,  1.50it/s]Extractor Predicting: 125it [01:22,  1.50it/s]Extractor Predicting: 126it [01:23,  1.51it/s]Extractor Predicting: 127it [01:24,  1.49it/s]Extractor Predicting: 128it [01:24,  1.49it/s]Extractor Predicting: 129it [01:25,  1.44it/s]Extractor Predicting: 130it [01:26,  1.46it/s]Extractor Predicting: 131it [01:26,  1.48it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:28,  1.51it/s]Extractor Predicting: 134it [01:28,  1.52it/s]Extractor Predicting: 135it [01:29,  1.51it/s]Extractor Predicting: 136it [01:30,  1.51it/s]Extractor Predicting: 137it [01:30,  1.50it/s]Extractor Predicting: 138it [01:31,  1.51it/s]Extractor Predicting: 139it [01:32,  1.48it/s]Extractor Predicting: 140it [01:32,  1.53it/s]Extractor Predicting: 141it [01:33,  1.51it/s]Extractor Predicting: 142it [01:33,  1.57it/s]Extractor Predicting: 142it [01:33,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:59,081 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:59,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:59,088 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:59,088 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:59,088 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:21:59,377 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:21:59,378 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:22:00,159 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:22:01,179 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:22:01,179 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:04,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:04,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:04,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:04,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:04,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:22:04,672 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:22:04,673 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:22:05,282 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:22:05,432 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:22:05,432 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2061114439784302,
  "recall": 0.09848267964500429,
  "score": 0.1332816737698566,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.51it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.47it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:20,  1.47it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:22,  1.49it/s]Extractor Predicting: 35it [00:22,  1.47it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.52it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:29,  1.52it/s]Extractor Predicting: 47it [00:30,  1.52it/s]Extractor Predicting: 48it [00:31,  1.52it/s]Extractor Predicting: 49it [00:31,  1.52it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:33,  1.52it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:34,  1.52it/s]Extractor Predicting: 54it [00:35,  1.52it/s]Extractor Predicting: 55it [00:35,  1.48it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.53it/s]Extractor Predicting: 58it [00:37,  1.56it/s]Extractor Predicting: 59it [00:38,  1.55it/s]Extractor Predicting: 60it [00:39,  1.55it/s]Extractor Predicting: 61it [00:39,  1.56it/s]Extractor Predicting: 62it [00:40,  1.56it/s]Extractor Predicting: 63it [00:41,  1.56it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.57it/s]Extractor Predicting: 69it [00:44,  1.56it/s]Extractor Predicting: 70it [00:45,  1.55it/s]Extractor Predicting: 71it [00:46,  1.54it/s]Extractor Predicting: 72it [00:46,  1.55it/s]Extractor Predicting: 73it [00:47,  1.50it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:48,  1.52it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:50,  1.49it/s]Extractor Predicting: 78it [00:50,  1.50it/s]Extractor Predicting: 79it [00:51,  1.50it/s]Extractor Predicting: 80it [00:52,  1.51it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:54,  1.50it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.50it/s]Extractor Predicting: 87it [00:56,  1.50it/s]Extractor Predicting: 88it [00:57,  1.47it/s]Extractor Predicting: 89it [00:58,  1.48it/s]Extractor Predicting: 90it [00:58,  1.51it/s]Extractor Predicting: 91it [00:59,  1.48it/s]Extractor Predicting: 92it [01:00,  1.46it/s]Extractor Predicting: 93it [01:01,  1.45it/s]Extractor Predicting: 94it [01:01,  1.44it/s]Extractor Predicting: 95it [01:02,  1.45it/s]Extractor Predicting: 96it [01:03,  1.45it/s]Extractor Predicting: 97it [01:03,  1.47it/s]Extractor Predicting: 98it [01:04,  1.45it/s]Extractor Predicting: 99it [01:05,  1.47it/s]Extractor Predicting: 100it [01:05,  1.48it/s]Extractor Predicting: 101it [01:06,  1.48it/s]Extractor Predicting: 102it [01:07,  1.47it/s]Extractor Predicting: 103it [01:07,  1.48it/s]Extractor Predicting: 104it [01:08,  1.48it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:09,  1.48it/s]Extractor Predicting: 107it [01:10,  1.46it/s]Extractor Predicting: 108it [01:11,  1.45it/s]Extractor Predicting: 109it [01:11,  1.46it/s]Extractor Predicting: 110it [01:12,  1.44it/s]Extractor Predicting: 111it [01:13,  1.44it/s]Extractor Predicting: 112it [01:14,  1.48it/s]Extractor Predicting: 113it [01:14,  1.37it/s]Extractor Predicting: 114it [01:15,  1.42it/s]Extractor Predicting: 115it [01:16,  1.44it/s]Extractor Predicting: 116it [01:16,  1.46it/s]Extractor Predicting: 117it [01:17,  1.49it/s]Extractor Predicting: 118it [01:18,  1.50it/s]Extractor Predicting: 119it [01:18,  1.50it/s]Extractor Predicting: 120it [01:19,  1.52it/s]Extractor Predicting: 121it [01:20,  1.52it/s]Extractor Predicting: 122it [01:20,  1.57it/s]Extractor Predicting: 123it [01:21,  1.55it/s]Extractor Predicting: 124it [01:21,  1.55it/s]Extractor Predicting: 125it [01:22,  1.55it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:23,  1.53it/s]Extractor Predicting: 128it [01:24,  1.53it/s]Extractor Predicting: 129it [01:25,  1.50it/s]Extractor Predicting: 130it [01:25,  1.51it/s]Extractor Predicting: 131it [01:26,  1.50it/s]Extractor Predicting: 132it [01:27,  1.52it/s]Extractor Predicting: 133it [01:28,  1.48it/s]Extractor Predicting: 134it [01:28,  1.49it/s]Extractor Predicting: 135it [01:29,  1.53it/s]Extractor Predicting: 136it [01:29,  1.53it/s]Extractor Predicting: 137it [01:30,  1.51it/s]Extractor Predicting: 138it [01:31,  1.45it/s]Extractor Predicting: 139it [01:31,  1.51it/s]Extractor Predicting: 140it [01:32,  1.50it/s]Extractor Predicting: 141it [01:33,  1.50it/s]Extractor Predicting: 142it [01:33,  1.54it/s]Extractor Predicting: 143it [01:34,  1.54it/s]Extractor Predicting: 144it [01:35,  1.50it/s]Extractor Predicting: 145it [01:35,  1.52it/s]Extractor Predicting: 146it [01:36,  1.52it/s]Extractor Predicting: 147it [01:37,  1.50it/s]Extractor Predicting: 148it [01:37,  1.50it/s]Extractor Predicting: 149it [01:38,  1.49it/s]Extractor Predicting: 150it [01:39,  1.51it/s]Extractor Predicting: 151it [01:39,  1.54it/s]Extractor Predicting: 152it [01:40,  1.55it/s]Extractor Predicting: 153it [01:41,  1.55it/s]Extractor Predicting: 154it [01:41,  1.52it/s]Extractor Predicting: 155it [01:42,  1.51it/s]Extractor Predicting: 156it [01:43,  1.51it/s]Extractor Predicting: 157it [01:43,  1.49it/s]Extractor Predicting: 158it [01:44,  1.49it/s]Extractor Predicting: 159it [01:45,  1.48it/s]Extractor Predicting: 160it [01:45,  1.49it/s]Extractor Predicting: 161it [01:46,  1.52it/s]Extractor Predicting: 162it [01:47,  1.54it/s]Extractor Predicting: 163it [01:47,  1.53it/s]Extractor Predicting: 164it [01:48,  1.54it/s]Extractor Predicting: 165it [01:49,  1.51it/s]Extractor Predicting: 166it [01:49,  1.52it/s]Extractor Predicting: 167it [01:50,  1.54it/s]Extractor Predicting: 168it [01:51,  1.53it/s]Extractor Predicting: 169it [01:51,  1.51it/s]Extractor Predicting: 170it [01:52,  1.49it/s]Extractor Predicting: 171it [01:53,  1.46it/s]Extractor Predicting: 172it [01:53,  1.47it/s]Extractor Predicting: 173it [01:54,  1.48it/s]Extractor Predicting: 174it [01:55,  1.44it/s]Extractor Predicting: 175it [01:55,  1.40it/s]Extractor Predicting: 176it [01:56,  1.43it/s]Extractor Predicting: 177it [01:57,  1.44it/s]Extractor Predicting: 178it [01:57,  1.48it/s]Extractor Predicting: 179it [01:58,  1.47it/s]Extractor Predicting: 180it [01:59,  1.52it/s]Extractor Predicting: 181it [01:59,  1.50it/s]Extractor Predicting: 182it [02:00,  1.53it/s]Extractor Predicting: 183it [02:01,  1.53it/s]Extractor Predicting: 184it [02:02,  1.37it/s]Extractor Predicting: 185it [02:02,  1.44it/s]Extractor Predicting: 186it [02:03,  1.48it/s]Extractor Predicting: 187it [02:04,  1.52it/s]Extractor Predicting: 188it [02:04,  1.52it/s]Extractor Predicting: 189it [02:05,  1.53it/s]Extractor Predicting: 190it [02:06,  1.50it/s]Extractor Predicting: 191it [02:06,  1.47it/s]Extractor Predicting: 192it [02:07,  1.50it/s]Extractor Predicting: 193it [02:07,  1.55it/s]Extractor Predicting: 194it [02:08,  1.54it/s]Extractor Predicting: 195it [02:09,  1.50it/s]Extractor Predicting: 196it [02:09,  1.53it/s]Extractor Predicting: 197it [02:10,  1.55it/s]Extractor Predicting: 198it [02:11,  1.53it/s]Extractor Predicting: 199it [02:11,  1.54it/s]Extractor Predicting: 200it [02:12,  1.42it/s]Extractor Predicting: 201it [02:13,  1.46it/s]Extractor Predicting: 202it [02:13,  1.51it/s]Extractor Predicting: 203it [02:14,  1.54it/s]Extractor Predicting: 204it [02:15,  1.54it/s]Extractor Predicting: 205it [02:15,  1.50it/s]Extractor Predicting: 206it [02:16,  1.51it/s]Extractor Predicting: 207it [02:17,  1.54it/s]Extractor Predicting: 208it [02:17,  1.55it/s]Extractor Predicting: 209it [02:18,  1.50it/s]Extractor Predicting: 210it [02:19,  1.48it/s]Extractor Predicting: 211it [02:19,  1.49it/s]Extractor Predicting: 212it [02:20,  1.51it/s]Extractor Predicting: 213it [02:21,  1.53it/s]Extractor Predicting: 214it [02:21,  1.49it/s]Extractor Predicting: 215it [02:22,  1.49it/s]Extractor Predicting: 216it [02:23,  1.52it/s]Extractor Predicting: 217it [02:23,  1.53it/s]Extractor Predicting: 218it [02:24,  1.48it/s]Extractor Predicting: 219it [02:25,  1.49it/s]Extractor Predicting: 220it [02:25,  1.49it/s]Extractor Predicting: 221it [02:26,  1.46it/s]Extractor Predicting: 222it [02:27,  1.47it/s]Extractor Predicting: 223it [02:27,  1.47it/s]Extractor Predicting: 224it [02:28,  1.51it/s]Extractor Predicting: 225it [02:29,  1.50it/s]Extractor Predicting: 226it [02:29,  1.55it/s]Extractor Predicting: 227it [02:30,  1.57it/s]Extractor Predicting: 228it [02:31,  1.39it/s]Extractor Predicting: 229it [02:32,  1.43it/s]Extractor Predicting: 230it [02:32,  1.43it/s]Extractor Predicting: 231it [02:33,  1.44it/s]Extractor Predicting: 232it [02:34,  1.46it/s]Extractor Predicting: 233it [02:34,  1.51it/s]Extractor Predicting: 234it [02:35,  1.50it/s]Extractor Predicting: 235it [02:36,  1.51it/s]Extractor Predicting: 236it [02:36,  1.48it/s]Extractor Predicting: 237it [02:37,  1.48it/s]Extractor Predicting: 238it [02:38,  1.51it/s]Extractor Predicting: 239it [02:38,  1.53it/s]Extractor Predicting: 240it [02:39,  1.52it/s]Extractor Predicting: 241it [02:40,  1.51it/s]Extractor Predicting: 242it [02:40,  1.49it/s]Extractor Predicting: 243it [02:41,  1.46it/s]Extractor Predicting: 244it [02:42,  1.47it/s]Extractor Predicting: 245it [02:42,  1.52it/s]Extractor Predicting: 246it [02:43,  1.49it/s]Extractor Predicting: 247it [02:44,  1.52it/s]Extractor Predicting: 248it [02:44,  1.52it/s]Extractor Predicting: 249it [02:45,  1.51it/s]Extractor Predicting: 250it [02:46,  1.51it/s]Extractor Predicting: 251it [02:46,  1.46it/s]Extractor Predicting: 252it [02:47,  1.47it/s]Extractor Predicting: 253it [02:48,  1.48it/s]Extractor Predicting: 254it [02:48,  1.50it/s]Extractor Predicting: 255it [02:49,  1.50it/s]Extractor Predicting: 256it [02:50,  1.49it/s]Extractor Predicting: 257it [02:50,  1.51it/s]Extractor Predicting: 258it [02:51,  1.50it/s]Extractor Predicting: 259it [02:52,  1.46it/s]Extractor Predicting: 260it [02:52,  1.49it/s]Extractor Predicting: 261it [02:53,  1.50it/s]Extractor Predicting: 262it [02:54,  1.49it/s]Extractor Predicting: 263it [02:54,  1.48it/s]Extractor Predicting: 264it [02:55,  1.48it/s]Extractor Predicting: 265it [02:56,  1.47it/s]Extractor Predicting: 266it [02:56,  1.45it/s]Extractor Predicting: 267it [02:57,  1.43it/s]Extractor Predicting: 268it [02:58,  1.45it/s]Extractor Predicting: 269it [02:58,  1.45it/s]Extractor Predicting: 270it [02:59,  1.45it/s]Extractor Predicting: 271it [03:00,  1.44it/s]Extractor Predicting: 272it [03:01,  1.45it/s]Extractor Predicting: 273it [03:01,  1.44it/s]Extractor Predicting: 274it [03:02,  1.44it/s]Extractor Predicting: 275it [03:03,  1.48it/s]Extractor Predicting: 276it [03:03,  1.47it/s]Extractor Predicting: 277it [03:04,  1.46it/s]Extractor Predicting: 278it [03:05,  1.46it/s]Extractor Predicting: 279it [03:05,  1.46it/s]Extractor Predicting: 280it [03:06,  1.45it/s]Extractor Predicting: 281it [03:07,  1.44it/s]Extractor Predicting: 282it [03:07,  1.46it/s]Extractor Predicting: 283it [03:08,  1.42it/s]Extractor Predicting: 284it [03:09,  1.41it/s]Extractor Predicting: 285it [03:10,  1.41it/s]Extractor Predicting: 286it [03:10,  1.40it/s]Extractor Predicting: 287it [03:11,  1.42it/s]Extractor Predicting: 288it [03:11,  1.87it/s]Extractor Predicting: 288it [03:11,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:31,015 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:31,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:31,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:31,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:31,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:25:32,713 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:25:32,714 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:25:33,979 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:25:35,242 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:25:35,262 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:25:40,241 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:25:40,242 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:25:40,882 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:25:41,035 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:25:41,035 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4756006006006006,
  "recall": 0.18391638844534766,
  "score": 0.26525698733382186,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 3it [00:01,  1.78it/s]Extractor Predicting: 3it [00:01,  1.64it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:25:43,398 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:25:43,399 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:25:43,402 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:25:43,403 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:25:43,405 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:25:49,734 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:25:49,738 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:25:49,748 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:25:49,749 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:25:49,754 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:49,758 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:49,758 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:49,758 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:49,758 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:49,758 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:49,758 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6923076923076923,
  "recall": 0.08108108108108109,
  "score": 0.14516129032258066,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:25:50,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:50,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:51,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:52,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:53,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:53,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:54,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:55,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:55,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:56,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:57,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:58,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:59,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:00,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:00,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:01,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:02,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:02,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:03,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:04,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:05,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:42, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-28 21:26:05,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:06,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:07,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:08,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:08,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:10,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:10,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:11,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:12,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:12,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:14,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:15,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:16,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:16,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:17,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:18,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:18,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:20,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:20,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:21,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:32<03:30, 16.23s/it][WARNING|generation_utils.py:914] 2023-08-28 21:26:22,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:23,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:23,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:25,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:25,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:27,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:28,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:29,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:29,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:30,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:31,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:32,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:32,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:33,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:34,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:35,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:35,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:36,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:37,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:38,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:39,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:39,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:50<03:25, 17.11s/it][WARNING|generation_utils.py:914] 2023-08-28 21:26:40,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:41,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:42,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:42,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:43,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:44,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:44,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:45,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:46,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:47,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:47,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:48,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:49,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:49,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:50,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:51,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:51,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:52,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:53,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:54,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:54,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:05<02:58, 16.27s/it][WARNING|generation_utils.py:914] 2023-08-28 21:26:55,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:56,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:56,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:57,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:58,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:58,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:59,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:00,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:00,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:01,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:02,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:02,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:03,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:04,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:04,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:05,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:06,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:06,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:07,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:08,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:08,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:19<02:35, 15.51s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:09,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:10,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:11,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:12,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:13,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:13,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:14,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:15,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:16,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:16,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:17,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:18,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:19,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:19,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:20,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:21,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:21,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:22,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:23,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:23,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:34<02:17, 15.32s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:24,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:25,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:26,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:26,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:27,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:28,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:29,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:29,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:30,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:31,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:31,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:32,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:33,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:34,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:35,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:35,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:36,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:37,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:38,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:38,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:39,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:50<02:03, 15.39s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:40,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:40,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:42,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:42,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:43,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:44,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:45,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:46,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:47,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:47,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:48,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:49,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:49,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:50,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:51,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:52,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:52,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:53,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:54,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:55,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:55,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:56,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:57,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:08<01:53, 16.19s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:58,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:58,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:59,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:00,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:01,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:01,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:02,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:03,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:03,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:04,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:05,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:06,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:06,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:07,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:08,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:08,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:09,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:10,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:10,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:11,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:12,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:22<01:34, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:12,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:13,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:14,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:15,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:15,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:16,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:17,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:18,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:18,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:19,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:20,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:21,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:21,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:22,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:23,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:24,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:25,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:25,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:26,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:27,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:38<01:18, 15.78s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:28,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:29,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:29,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:30,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:31,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:31,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:32,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:32,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:33,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:34,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:34,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:35,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:35,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:36,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:36,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:37,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:38,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:38,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:39,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:40,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:50<00:58, 14.63s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:40,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:41,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:42,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:43,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:43,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:44,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:45,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:45,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:46,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:47,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:48,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:49,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:49,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:50,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:51,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:51,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:52,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:53,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:53,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:54,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:55,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:55,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:06<00:44, 14.99s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:56,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:57,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:57,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:58,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:59,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:00,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:01,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:02,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:03,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:04,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:04,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:05,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:06,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:06,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:07,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:08,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:09,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:10,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:10,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:11,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:13,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:24<00:31, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-28 21:29:14,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:14,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:15,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:16,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:17,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:17,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:18,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:19,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:20,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:21,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:21,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:22,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:23,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:24,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:25,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:25,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:26,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:27,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:27,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:28,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:39<00:15, 15.63s/it][WARNING|generation_utils.py:914] 2023-08-28 21:29:29,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:29,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:30,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:31,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:32,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:32,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:33,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:34,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:34,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:35,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:36,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:37,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:37,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:38,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:38,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:39,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:40,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:41,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:41,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:42,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:42,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:43,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:44,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:45,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:55<00:00, 15.83s/it]Generating: 100%|██████████| 15/15 [03:55<00:00, 15.71s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:52,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:52,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:52,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:52,129 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:52,129 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:29:53,401 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:29:53,402 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:29:54,176 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:29:55,860 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:29:55,950 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:02,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:02,267 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:02,267 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:02,267 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:02,267 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:30:04,359 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:30:04,360 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:30:05,720 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:30:06,121 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:30:06,121 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : architect .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9151785714285714, 'errors': {''}}
['Relation : country of origin . Context : Later in 2008 , the country became a part of a new European Union member to join the common currency , creating the European Union . Head Entity : European Union , Tail Entity : member of the common currency .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Yashiro\', \'country of origin\', \'\', \'This is the second new entry in the series of " the adventures of Yashiro " by Yūki Nishi .\')'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9360119047619048, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : operator .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.95625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.7877604166666666, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 9152
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9252, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:01,  1.57it/s]Extractor Estimating: 4it [00:02,  1.62it/s]Extractor Estimating: 5it [00:03,  1.62it/s]Extractor Estimating: 6it [00:03,  1.52it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.57it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:07,  1.58it/s]Extractor Estimating: 12it [00:07,  1.49it/s]Extractor Estimating: 13it [00:08,  1.53it/s]Extractor Estimating: 14it [00:09,  1.47it/s]Extractor Estimating: 15it [00:09,  1.41it/s]Extractor Estimating: 16it [00:10,  1.49it/s]Extractor Estimating: 17it [00:11,  1.51it/s]Extractor Estimating: 18it [00:11,  1.55it/s]Extractor Estimating: 19it [00:12,  1.59it/s]Extractor Estimating: 20it [00:12,  1.64it/s]Extractor Estimating: 21it [00:13,  1.65it/s]Extractor Estimating: 22it [00:14,  1.62it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:15,  1.64it/s]Extractor Estimating: 25it [00:16,  1.56it/s]Extractor Estimating: 26it [00:16,  1.56it/s]Extractor Estimating: 27it [00:17,  1.59it/s]Extractor Estimating: 28it [00:17,  1.59it/s]Extractor Estimating: 29it [00:18,  1.60it/s]Extractor Estimating: 30it [00:19,  1.35it/s]Extractor Estimating: 31it [00:20,  1.42it/s]Extractor Estimating: 32it [00:20,  1.50it/s]Extractor Estimating: 33it [00:21,  1.58it/s]Extractor Estimating: 34it [00:21,  1.62it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:23,  1.55it/s]Extractor Estimating: 37it [00:23,  1.60it/s]Extractor Estimating: 38it [00:24,  1.60it/s]Extractor Estimating: 39it [00:25,  1.52it/s]Extractor Estimating: 40it [00:25,  1.41it/s]Extractor Estimating: 41it [00:26,  1.45it/s]Extractor Estimating: 42it [00:27,  1.55it/s]Extractor Estimating: 43it [00:27,  1.55it/s]Extractor Estimating: 44it [00:28,  1.59it/s]Extractor Estimating: 45it [00:28,  1.64it/s]Extractor Estimating: 46it [00:29,  1.67it/s]Extractor Estimating: 47it [00:30,  1.70it/s]Extractor Estimating: 48it [00:30,  1.70it/s]Extractor Estimating: 49it [00:31,  1.64it/s]Extractor Estimating: 50it [00:31,  1.63it/s]Extractor Estimating: 51it [00:32,  1.55it/s]Extractor Estimating: 52it [00:33,  1.51it/s]Extractor Estimating: 53it [00:34,  1.54it/s]Extractor Estimating: 54it [00:34,  1.44it/s]Extractor Estimating: 55it [00:35,  1.44it/s]Extractor Estimating: 56it [00:36,  1.41it/s]Extractor Estimating: 57it [00:37,  1.38it/s]Extractor Estimating: 58it [00:37,  1.45it/s]Extractor Estimating: 59it [00:38,  1.43it/s]Extractor Estimating: 60it [00:39,  1.36it/s]Extractor Estimating: 61it [00:39,  1.33it/s]Extractor Estimating: 62it [00:40,  1.36it/s]Extractor Estimating: 63it [00:41,  1.37it/s]Extractor Estimating: 64it [00:42,  1.43it/s]Extractor Estimating: 65it [00:42,  1.43it/s]Extractor Estimating: 66it [00:43,  1.46it/s]Extractor Estimating: 67it [00:44,  1.43it/s]Extractor Estimating: 68it [00:44,  1.44it/s]Extractor Estimating: 69it [00:45,  1.45it/s]Extractor Estimating: 70it [00:46,  1.45it/s]Extractor Estimating: 71it [00:46,  1.47it/s]Extractor Estimating: 72it [00:47,  1.43it/s]Extractor Estimating: 73it [00:48,  1.39it/s]Extractor Estimating: 74it [00:49,  1.39it/s]Extractor Estimating: 75it [00:49,  1.42it/s]Extractor Estimating: 76it [00:50,  1.51it/s]Extractor Estimating: 77it [00:51,  1.41it/s]Extractor Estimating: 78it [00:51,  1.51it/s]Extractor Estimating: 79it [00:52,  1.59it/s]Extractor Estimating: 80it [00:52,  1.63it/s]Extractor Estimating: 81it [00:53,  1.66it/s]Extractor Estimating: 82it [00:53,  1.66it/s]Extractor Estimating: 83it [00:54,  1.66it/s]Extractor Estimating: 84it [00:55,  1.71it/s]Extractor Estimating: 85it [00:55,  1.77it/s]Extractor Estimating: 86it [00:56,  1.80it/s]Extractor Estimating: 87it [00:56,  1.76it/s]Extractor Estimating: 88it [00:57,  1.68it/s]Extractor Estimating: 89it [00:57,  1.72it/s]Extractor Estimating: 90it [00:58,  1.78it/s]Extractor Estimating: 91it [00:58,  1.80it/s]Extractor Estimating: 92it [00:59,  1.79it/s]Extractor Estimating: 93it [01:00,  1.81it/s]Extractor Estimating: 94it [01:00,  1.75it/s]Extractor Estimating: 95it [01:01,  1.81it/s]Extractor Estimating: 96it [01:01,  1.80it/s]Extractor Estimating: 97it [01:02,  1.77it/s]Extractor Estimating: 98it [01:02,  1.82it/s]Extractor Estimating: 99it [01:03,  1.74it/s]Extractor Estimating: 100it [01:04,  1.58it/s]Extractor Estimating: 101it [01:04,  1.66it/s]Extractor Estimating: 102it [01:05,  1.69it/s]Extractor Estimating: 103it [01:05,  1.77it/s]Extractor Estimating: 104it [01:06,  1.73it/s]Extractor Estimating: 105it [01:07,  1.70it/s]Extractor Estimating: 106it [01:07,  1.54it/s]Extractor Estimating: 107it [01:08,  1.57it/s]Extractor Estimating: 108it [01:09,  1.62it/s]Extractor Estimating: 109it [01:09,  1.64it/s]Extractor Estimating: 110it [01:10,  1.67it/s]Extractor Estimating: 111it [01:10,  1.67it/s]Extractor Estimating: 112it [01:11,  1.62it/s]Extractor Estimating: 113it [01:12,  1.58it/s]Extractor Estimating: 114it [01:12,  1.58it/s]Extractor Estimating: 115it [01:13,  1.61it/s]Extractor Estimating: 116it [01:13,  1.65it/s]Extractor Estimating: 117it [01:14,  1.65it/s]Extractor Estimating: 118it [01:15,  1.64it/s]Extractor Estimating: 119it [01:15,  1.63it/s]Extractor Estimating: 120it [01:16,  1.67it/s]Extractor Estimating: 121it [01:17,  1.64it/s]Extractor Estimating: 122it [01:17,  1.57it/s]Extractor Estimating: 123it [01:18,  1.57it/s]Extractor Estimating: 124it [01:18,  1.63it/s]Extractor Estimating: 125it [01:19,  1.60it/s]Extractor Estimating: 126it [01:20,  1.64it/s]Extractor Estimating: 127it [01:20,  1.47it/s]Extractor Estimating: 128it [01:21,  1.54it/s]Extractor Estimating: 129it [01:22,  1.59it/s]Extractor Estimating: 130it [01:22,  1.60it/s]Extractor Estimating: 131it [01:23,  1.69it/s]Extractor Estimating: 132it [01:24,  1.35it/s]Extractor Estimating: 133it [01:24,  1.48it/s]Extractor Estimating: 134it [01:25,  1.57it/s]Extractor Estimating: 135it [01:26,  1.59it/s]Extractor Estimating: 136it [01:26,  1.61it/s]Extractor Estimating: 137it [01:27,  1.45it/s]Extractor Estimating: 138it [01:28,  1.53it/s]Extractor Estimating: 139it [01:28,  1.59it/s]Extractor Estimating: 140it [01:29,  1.62it/s]Extractor Estimating: 141it [01:29,  1.65it/s]Extractor Estimating: 142it [01:30,  1.50it/s]Extractor Estimating: 143it [01:31,  1.59it/s]Extractor Estimating: 144it [01:31,  1.61it/s]Extractor Estimating: 145it [01:32,  1.71it/s]Extractor Estimating: 146it [01:32,  1.73it/s]Extractor Estimating: 147it [01:33,  1.68it/s]Extractor Estimating: 148it [01:33,  1.75it/s]Extractor Estimating: 149it [01:34,  1.73it/s]Extractor Estimating: 150it [01:35,  1.77it/s]Extractor Estimating: 151it [01:35,  1.71it/s]Extractor Estimating: 152it [01:36,  1.68it/s]Extractor Estimating: 153it [01:37,  1.55it/s]Extractor Estimating: 154it [01:37,  1.65it/s]Extractor Estimating: 155it [01:38,  1.63it/s]Extractor Estimating: 156it [01:38,  1.64it/s]Extractor Estimating: 157it [01:39,  1.69it/s]Extractor Estimating: 158it [01:39,  1.74it/s]Extractor Estimating: 159it [01:40,  1.63it/s]Extractor Estimating: 160it [01:41,  1.74it/s]Extractor Estimating: 161it [01:42,  1.43it/s]Extractor Estimating: 162it [01:42,  1.48it/s]Extractor Estimating: 163it [01:43,  1.56it/s]Extractor Estimating: 164it [01:43,  1.56it/s]Extractor Estimating: 165it [01:44,  1.61it/s]Extractor Estimating: 166it [01:45,  1.64it/s]Extractor Estimating: 167it [01:45,  1.68it/s]Extractor Estimating: 168it [01:46,  1.73it/s]Extractor Estimating: 169it [01:46,  1.75it/s]Extractor Estimating: 170it [01:47,  1.68it/s]Extractor Estimating: 171it [01:47,  1.73it/s]Extractor Estimating: 172it [01:48,  1.67it/s]Extractor Estimating: 173it [01:49,  1.73it/s]Extractor Estimating: 174it [01:49,  1.80it/s]Extractor Estimating: 175it [01:50,  1.79it/s]Extractor Estimating: 176it [01:50,  1.80it/s]Extractor Estimating: 177it [01:51,  1.69it/s]Extractor Estimating: 178it [01:52,  1.69it/s]Extractor Estimating: 179it [01:52,  1.74it/s]Extractor Estimating: 180it [01:53,  1.73it/s]Extractor Estimating: 181it [01:53,  1.74it/s]Extractor Estimating: 182it [01:54,  1.68it/s]Extractor Estimating: 183it [01:54,  1.73it/s]Extractor Estimating: 184it [01:55,  1.66it/s]Extractor Estimating: 185it [01:56,  1.67it/s]Extractor Estimating: 186it [01:56,  1.74it/s]Extractor Estimating: 187it [01:57,  1.73it/s]Extractor Estimating: 188it [01:57,  1.72it/s]Extractor Estimating: 189it [01:58,  1.69it/s]Extractor Estimating: 190it [01:59,  1.63it/s]Extractor Estimating: 191it [01:59,  1.58it/s]Extractor Estimating: 192it [02:00,  1.63it/s]Extractor Estimating: 193it [02:00,  1.66it/s]Extractor Estimating: 194it [02:01,  1.65it/s]Extractor Estimating: 195it [02:02,  1.64it/s]Extractor Estimating: 196it [02:02,  1.65it/s]Extractor Estimating: 197it [02:03,  1.66it/s]Extractor Estimating: 198it [02:03,  1.65it/s]Extractor Estimating: 199it [02:04,  1.65it/s]Extractor Estimating: 200it [02:05,  1.66it/s]Extractor Estimating: 201it [02:05,  1.56it/s]Extractor Estimating: 202it [02:06,  1.49it/s]Extractor Estimating: 203it [02:07,  1.41it/s]Extractor Estimating: 204it [02:08,  1.43it/s]Extractor Estimating: 205it [02:08,  1.46it/s]Extractor Estimating: 206it [02:09,  1.45it/s]Extractor Estimating: 207it [02:10,  1.46it/s]Extractor Estimating: 208it [02:10,  1.48it/s]Extractor Estimating: 209it [02:11,  1.44it/s]Extractor Estimating: 210it [02:12,  1.40it/s]Extractor Estimating: 211it [02:12,  1.41it/s]Extractor Estimating: 212it [02:13,  1.41it/s]Extractor Estimating: 213it [02:14,  1.43it/s]Extractor Estimating: 214it [02:15,  1.46it/s]Extractor Estimating: 215it [02:15,  1.46it/s]Extractor Estimating: 216it [02:16,  1.43it/s]Extractor Estimating: 217it [02:17,  1.46it/s]Extractor Estimating: 218it [02:17,  1.48it/s]Extractor Estimating: 219it [02:18,  1.54it/s]Extractor Estimating: 220it [02:19,  1.49it/s]Extractor Estimating: 221it [02:19,  1.43it/s]Extractor Estimating: 222it [02:20,  1.47it/s]Extractor Estimating: 223it [02:21,  1.46it/s]Extractor Estimating: 224it [02:21,  1.47it/s]Extractor Estimating: 225it [02:22,  1.47it/s]Extractor Estimating: 226it [02:23,  1.46it/s]Extractor Estimating: 227it [02:23,  1.43it/s]Extractor Estimating: 228it [02:24,  1.43it/s]Extractor Estimating: 229it [02:25,  1.39it/s]Extractor Estimating: 230it [02:26,  1.41it/s]Extractor Estimating: 231it [02:26,  1.42it/s]Extractor Estimating: 232it [02:27,  1.41it/s]Extractor Estimating: 233it [02:28,  1.38it/s]Extractor Estimating: 234it [02:28,  1.37it/s]Extractor Estimating: 235it [02:29,  1.38it/s]Extractor Estimating: 236it [02:30,  1.45it/s]Extractor Estimating: 237it [02:31,  1.35it/s]Extractor Estimating: 238it [02:31,  1.36it/s]Extractor Estimating: 239it [02:32,  1.35it/s]Extractor Estimating: 240it [02:33,  1.38it/s]Extractor Estimating: 241it [02:34,  1.33it/s]Extractor Estimating: 242it [02:34,  1.38it/s]Extractor Estimating: 243it [02:35,  1.38it/s]Extractor Estimating: 244it [02:36,  1.28it/s]Extractor Estimating: 245it [02:37,  1.29it/s]Extractor Estimating: 246it [02:37,  1.34it/s]Extractor Estimating: 247it [02:38,  1.31it/s]Extractor Estimating: 248it [02:39,  1.35it/s]Extractor Estimating: 249it [02:40,  1.35it/s]Extractor Estimating: 250it [02:40,  1.36it/s]Extractor Estimating: 251it [02:41,  1.50it/s]Extractor Estimating: 252it [02:41,  1.63it/s]Extractor Estimating: 253it [02:42,  1.73it/s]Extractor Estimating: 254it [02:42,  1.80it/s]Extractor Estimating: 255it [02:43,  1.72it/s]Extractor Estimating: 256it [02:43,  1.85it/s]Extractor Estimating: 257it [02:44,  1.95it/s]Extractor Estimating: 258it [02:44,  1.97it/s]Extractor Estimating: 259it [02:45,  2.02it/s]Extractor Estimating: 260it [02:45,  1.97it/s]Extractor Estimating: 261it [02:46,  1.94it/s]Extractor Estimating: 262it [02:46,  1.96it/s]Extractor Estimating: 263it [02:47,  2.01it/s]Extractor Estimating: 264it [02:47,  2.05it/s]Extractor Estimating: 265it [02:48,  2.07it/s]Extractor Estimating: 266it [02:48,  2.06it/s]Extractor Estimating: 267it [02:49,  2.03it/s]Extractor Estimating: 268it [02:49,  1.83it/s]Extractor Estimating: 269it [02:50,  1.92it/s]Extractor Estimating: 270it [02:50,  2.04it/s]Extractor Estimating: 271it [02:51,  1.88it/s]Extractor Estimating: 272it [02:51,  1.92it/s]Extractor Estimating: 273it [02:52,  1.98it/s]Extractor Estimating: 274it [02:53,  1.86it/s]Extractor Estimating: 275it [02:53,  1.87it/s]Extractor Estimating: 276it [02:54,  1.76it/s]Extractor Estimating: 277it [02:54,  1.73it/s]Extractor Estimating: 278it [02:55,  1.64it/s]Extractor Estimating: 279it [02:56,  1.64it/s]Extractor Estimating: 280it [02:56,  1.65it/s]Extractor Estimating: 281it [02:57,  1.60it/s]Extractor Estimating: 282it [02:58,  1.59it/s]Extractor Estimating: 283it [02:58,  1.57it/s]Extractor Estimating: 284it [02:59,  1.52it/s]Extractor Estimating: 285it [03:00,  1.50it/s]Extractor Estimating: 286it [03:00,  1.59it/s]Extractor Estimating: 287it [03:01,  1.51it/s]Extractor Estimating: 288it [03:02,  1.53it/s]Extractor Estimating: 289it [03:02,  1.57it/s]Extractor Estimating: 290it [03:03,  1.57it/s]Extractor Estimating: 291it [03:03,  1.61it/s]Extractor Estimating: 292it [03:04,  1.62it/s]Extractor Estimating: 293it [03:04,  1.68it/s]Extractor Estimating: 294it [03:05,  1.68it/s]Extractor Estimating: 295it [03:06,  1.70it/s]Extractor Estimating: 296it [03:06,  1.70it/s]Extractor Estimating: 297it [03:07,  1.68it/s]Extractor Estimating: 298it [03:07,  1.72it/s]Extractor Estimating: 299it [03:08,  1.70it/s]Extractor Estimating: 300it [03:09,  1.63it/s]Extractor Estimating: 301it [03:09,  1.63it/s]Extractor Estimating: 302it [03:10,  1.64it/s]Extractor Estimating: 303it [03:10,  1.63it/s]Extractor Estimating: 304it [03:11,  1.67it/s]Extractor Estimating: 305it [03:12,  1.67it/s]Extractor Estimating: 306it [03:12,  1.51it/s]Extractor Estimating: 307it [03:13,  1.59it/s]Extractor Estimating: 308it [03:14,  1.58it/s]Extractor Estimating: 309it [03:14,  1.60it/s]Extractor Estimating: 310it [03:15,  1.55it/s]Extractor Estimating: 311it [03:16,  1.58it/s]Extractor Estimating: 312it [03:16,  1.62it/s]Extractor Estimating: 313it [03:17,  1.64it/s]Extractor Estimating: 314it [03:17,  1.65it/s]Extractor Estimating: 315it [03:18,  1.70it/s]Extractor Estimating: 316it [03:18,  1.70it/s]Extractor Estimating: 317it [03:19,  1.66it/s]Extractor Estimating: 318it [03:20,  1.66it/s]Extractor Estimating: 319it [03:20,  1.68it/s]Extractor Estimating: 320it [03:21,  1.70it/s]Extractor Estimating: 321it [03:21,  1.71it/s]Extractor Estimating: 322it [03:22,  1.71it/s]Extractor Estimating: 323it [03:23,  1.68it/s]Extractor Estimating: 324it [03:23,  1.64it/s]Extractor Estimating: 325it [03:24,  1.60it/s]Extractor Estimating: 326it [03:25,  1.60it/s]Extractor Estimating: 327it [03:25,  1.63it/s]Extractor Estimating: 328it [03:26,  1.60it/s]Extractor Estimating: 329it [03:26,  1.59it/s]Extractor Estimating: 330it [03:27,  1.56it/s]Extractor Estimating: 331it [03:28,  1.57it/s]Extractor Estimating: 332it [03:28,  1.61it/s]Extractor Estimating: 333it [03:29,  1.60it/s]Extractor Estimating: 334it [03:30,  1.59it/s]Extractor Estimating: 335it [03:30,  1.58it/s]Extractor Estimating: 336it [03:31,  1.51it/s]Extractor Estimating: 337it [03:32,  1.39it/s]Extractor Estimating: 338it [03:32,  1.44it/s]Extractor Estimating: 339it [03:33,  1.49it/s]Extractor Estimating: 340it [03:34,  1.49it/s]Extractor Estimating: 341it [03:34,  1.47it/s]Extractor Estimating: 342it [03:35,  1.51it/s]Extractor Estimating: 343it [03:36,  1.54it/s]Extractor Estimating: 344it [03:36,  1.54it/s]Extractor Estimating: 345it [03:37,  1.56it/s]Extractor Estimating: 346it [03:38,  1.59it/s]Extractor Estimating: 347it [03:38,  1.56it/s]Extractor Estimating: 348it [03:39,  1.58it/s]Extractor Estimating: 349it [03:39,  1.59it/s]Extractor Estimating: 350it [03:40,  1.52it/s]Extractor Estimating: 351it [03:41,  1.54it/s]Extractor Estimating: 352it [03:41,  1.56it/s]Extractor Estimating: 353it [03:42,  1.60it/s]Extractor Estimating: 354it [03:43,  1.61it/s]Extractor Estimating: 355it [03:43,  1.64it/s]Extractor Estimating: 356it [03:44,  1.62it/s]Extractor Estimating: 357it [03:44,  1.64it/s]Extractor Estimating: 358it [03:45,  1.65it/s]Extractor Estimating: 359it [03:46,  1.60it/s]Extractor Estimating: 360it [03:46,  1.61it/s]Extractor Estimating: 361it [03:47,  1.63it/s]Extractor Estimating: 362it [03:48,  1.64it/s]Extractor Estimating: 363it [03:48,  1.66it/s]Extractor Estimating: 364it [03:49,  1.65it/s]Extractor Estimating: 365it [03:49,  1.62it/s]Extractor Estimating: 366it [03:50,  1.63it/s]Extractor Estimating: 367it [03:51,  1.59it/s]Extractor Estimating: 368it [03:51,  1.61it/s]Extractor Estimating: 369it [03:52,  1.66it/s]Extractor Estimating: 370it [03:52,  1.65it/s]Extractor Estimating: 371it [03:53,  1.63it/s]Extractor Estimating: 372it [03:54,  1.51it/s]Extractor Estimating: 373it [03:55,  1.49it/s]Extractor Estimating: 374it [03:55,  1.48it/s]Extractor Estimating: 375it [03:56,  1.50it/s]Extractor Estimating: 375it [03:56,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:30,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:30,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:30,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:30,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:30,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:34:31,451 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:34:31,452 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:32,893 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:33,924 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:34,195 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:38,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:38,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:38,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:38,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:38,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:34:39,351 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:34:39,353 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:39,912 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:40,065 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:40,065 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:59:07,356 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:59:07,388 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7497 mean pseudo reward: 0.9429769628774348
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 16352
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16452, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16452, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.124, loss:506.5972
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.158, loss:439.1899
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.126, loss:442.7437
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.104, loss:417.9147
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.134, loss:421.7328
>> valid entity prec:0.5347, rec:0.5157, f1:0.5250
>> valid relation prec:0.1555, rec:0.0790, f1:0.1048
>> valid relation with NER prec:0.1555, rec:0.0790, f1:0.1048
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.471, loss:439.1523
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.119, loss:372.4851
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.129, loss:386.6983
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.140, loss:432.9666
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.138, loss:384.8759
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5032, rec:0.5315, f1:0.5170
>> valid relation prec:0.1498, rec:0.0816, f1:0.1056
>> valid relation with NER prec:0.1498, rec:0.0816, f1:0.1056
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.466, loss:393.4442
g_step 1200, step 261, avg_time 1.108, loss:409.8712
g_step 1300, step 48, avg_time 1.130, loss:349.8086
g_step 1400, step 148, avg_time 1.129, loss:365.3480
g_step 1500, step 248, avg_time 1.127, loss:370.3932
>> valid entity prec:0.5201, rec:0.5169, f1:0.5185
>> valid relation prec:0.1450, rec:0.0744, f1:0.0984
>> valid relation with NER prec:0.1450, rec:0.0744, f1:0.0984
g_step 1600, step 35, avg_time 2.453, loss:369.4158
g_step 1700, step 135, avg_time 1.123, loss:356.6285
g_step 1800, step 235, avg_time 1.136, loss:350.8482
g_step 1900, step 22, avg_time 1.129, loss:341.2591
g_step 2000, step 122, avg_time 1.141, loss:322.3762
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5280, rec:0.5007, f1:0.5140
>> valid relation prec:0.1555, rec:0.0830, f1:0.1082
>> valid relation with NER prec:0.1555, rec:0.0830, f1:0.1082
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 222, avg_time 2.459, loss:338.2576
g_step 2200, step 9, avg_time 1.121, loss:345.3412
g_step 2300, step 109, avg_time 1.134, loss:303.0802
g_step 2400, step 209, avg_time 1.136, loss:314.4038
g_step 2500, step 309, avg_time 1.128, loss:339.9166
>> valid entity prec:0.5063, rec:0.5012, f1:0.5037
>> valid relation prec:0.1301, rec:0.0733, f1:0.0938
>> valid relation with NER prec:0.1301, rec:0.0733, f1:0.0938
g_step 2600, step 96, avg_time 2.449, loss:289.9547
g_step 2700, step 196, avg_time 1.122, loss:312.7452
g_step 2800, step 296, avg_time 1.124, loss:322.3965
g_step 2900, step 83, avg_time 1.111, loss:275.6825
g_step 3000, step 183, avg_time 1.139, loss:289.9751
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4957, rec:0.4946, f1:0.4951
>> valid relation prec:0.1577, rec:0.0830, f1:0.1088
>> valid relation with NER prec:0.1577, rec:0.0830, f1:0.1088
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 283, avg_time 2.467, loss:306.1790
g_step 3200, step 70, avg_time 1.127, loss:261.5752
g_step 3300, step 170, avg_time 1.121, loss:274.0768
g_step 3400, step 270, avg_time 1.131, loss:286.2613
g_step 3500, step 57, avg_time 1.143, loss:259.6796
>> valid entity prec:0.5367, rec:0.4791, f1:0.5062
>> valid relation prec:0.1653, rec:0.0879, f1:0.1148
>> valid relation with NER prec:0.1653, rec:0.0879, f1:0.1148
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 157, avg_time 2.447, loss:266.7351
g_step 3700, step 257, avg_time 1.140, loss:260.4143
g_step 3800, step 44, avg_time 1.144, loss:264.3587
g_step 3900, step 144, avg_time 1.130, loss:247.2149
g_step 4000, step 244, avg_time 1.132, loss:252.1144
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4876, rec:0.5118, f1:0.4994
>> valid relation prec:0.1594, rec:0.0859, f1:0.1116
>> valid relation with NER prec:0.1594, rec:0.0859, f1:0.1116
g_step 4100, step 31, avg_time 2.452, loss:258.0847
g_step 4200, step 131, avg_time 1.129, loss:235.9136
g_step 4300, step 231, avg_time 1.121, loss:244.9988
g_step 4400, step 18, avg_time 1.140, loss:249.5041
g_step 4500, step 118, avg_time 1.137, loss:247.3857
>> valid entity prec:0.5290, rec:0.4979, f1:0.5130
>> valid relation prec:0.1529, rec:0.0813, f1:0.1062
>> valid relation with NER prec:0.1529, rec:0.0813, f1:0.1062
g_step 4600, step 218, avg_time 2.453, loss:244.1328
g_step 4700, step 5, avg_time 1.124, loss:235.2132
g_step 4800, step 105, avg_time 1.118, loss:221.5973
g_step 4900, step 205, avg_time 1.150, loss:227.9266
g_step 5000, step 305, avg_time 1.119, loss:236.9814
learning rate was adjusted to 0.0008
>> valid entity prec:0.5213, rec:0.4601, f1:0.4888
>> valid relation prec:0.1482, rec:0.0699, f1:0.0950
>> valid relation with NER prec:0.1482, rec:0.0699, f1:0.0950
g_step 5100, step 92, avg_time 2.443, loss:207.3198
g_step 5200, step 192, avg_time 1.154, loss:215.0569
g_step 5300, step 292, avg_time 1.124, loss:227.7370
g_step 5400, step 79, avg_time 1.135, loss:203.9887
g_step 5500, step 179, avg_time 1.129, loss:210.4536
>> valid entity prec:0.5162, rec:0.4824, f1:0.4987
>> valid relation prec:0.1465, rec:0.0825, f1:0.1055
>> valid relation with NER prec:0.1465, rec:0.0825, f1:0.1055
g_step 5600, step 279, avg_time 2.449, loss:214.2448
g_step 5700, step 66, avg_time 1.119, loss:205.0744
g_step 5800, step 166, avg_time 1.125, loss:209.8614
g_step 5900, step 266, avg_time 1.136, loss:202.4049
g_step 6000, step 53, avg_time 1.137, loss:186.2827
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5073, rec:0.5038, f1:0.5056
>> valid relation prec:0.1354, rec:0.0822, f1:0.1023
>> valid relation with NER prec:0.1354, rec:0.0822, f1:0.1023
g_step 6100, step 153, avg_time 2.474, loss:190.1828
g_step 6200, step 253, avg_time 1.118, loss:196.9303
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:59:07 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:59:07 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-59-07_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:59:08 - WARNING - datasets.builder -   Using custom data configuration default-94b73ea1116c5eec
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-94b73ea1116c5eec/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:59:08,637 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:59:08,638 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:59:08,639 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:59:08,640 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:59:08,647 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:59:08,655 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:59:08,655 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:59:08,655 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:59:08,655 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:59:08,655 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:59:08,655 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:59:08,798 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:59:12,008 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:59:12,011 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-94b73ea1116c5eec/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.39ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.14ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.47ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.65ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.76ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.82ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.85ba/s]100%|██████████| 8/8 [00:01<00:00,  5.75ba/s]100%|██████████| 8/8 [00:01<00:00,  4.95ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.95ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.19ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.22ba/s]100%|██████████| 4/4 [00:00<00:00,  5.26ba/s]100%|██████████| 4/4 [00:00<00:00,  4.79ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.25ba/s] 25%|██▌       | 2/8 [00:00<00:00,  9.12ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.69ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.76ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.96ba/s]100%|██████████| 8/8 [00:00<00:00, 10.38ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.78ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.49ba/s]100%|██████████| 4/4 [00:00<00:00, 10.72ba/s]
[INFO|trainer.py:414] 2023-08-28 23:59:16,024 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:59:16,039 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:59:16,040 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 23:59:16,040 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:59:16,040 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:59:16,040 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:59:16,040 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:59:16,040 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:53,  3.36it/s]  0%|          | 2/585 [00:00<02:50,  3.42it/s]  1%|          | 3/585 [00:00<02:49,  3.44it/s]  1%|          | 4/585 [00:01<02:48,  3.45it/s]  1%|          | 5/585 [00:01<02:47,  3.46it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:46,  3.47it/s]  1%|▏         | 8/585 [00:02<02:47,  3.45it/s]  2%|▏         | 9/585 [00:02<02:46,  3.46it/s]  2%|▏         | 10/585 [00:02<02:46,  3.46it/s]  2%|▏         | 11/585 [00:03<02:45,  3.46it/s]  2%|▏         | 12/585 [00:03<02:45,  3.46it/s]  2%|▏         | 13/585 [00:03<02:45,  3.46it/s]  2%|▏         | 14/585 [00:04<02:44,  3.46it/s]  3%|▎         | 15/585 [00:04<02:44,  3.46it/s]  3%|▎         | 16/585 [00:04<02:44,  3.47it/s]  3%|▎         | 17/585 [00:04<02:44,  3.46it/s]  3%|▎         | 18/585 [00:05<02:43,  3.47it/s]  3%|▎         | 19/585 [00:05<02:43,  3.45it/s]  3%|▎         | 20/585 [00:05<02:43,  3.46it/s]  4%|▎         | 21/585 [00:06<02:43,  3.46it/s]  4%|▍         | 22/585 [00:06<02:42,  3.46it/s]  4%|▍         | 23/585 [00:06<02:42,  3.46it/s]  4%|▍         | 24/585 [00:06<02:42,  3.46it/s]  4%|▍         | 25/585 [00:07<02:41,  3.46it/s]  4%|▍         | 26/585 [00:07<02:41,  3.46it/s]  5%|▍         | 27/585 [00:07<02:40,  3.47it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:08<02:40,  3.46it/s]  5%|▌         | 30/585 [00:08<02:40,  3.45it/s]  5%|▌         | 31/585 [00:08<02:40,  3.45it/s]  5%|▌         | 32/585 [00:09<02:39,  3.46it/s]  6%|▌         | 33/585 [00:09<02:39,  3.46it/s]  6%|▌         | 34/585 [00:09<02:39,  3.46it/s]  6%|▌         | 35/585 [00:10<02:38,  3.46it/s]  6%|▌         | 36/585 [00:10<02:38,  3.47it/s]  6%|▋         | 37/585 [00:10<02:38,  3.46it/s]  6%|▋         | 38/585 [00:10<02:37,  3.46it/s]  7%|▋         | 39/585 [00:11<02:37,  3.46it/s]  7%|▋         | 40/585 [00:11<02:37,  3.46it/s]  7%|▋         | 41/585 [00:11<02:37,  3.45it/s]  7%|▋         | 42/585 [00:12<02:37,  3.45it/s]  7%|▋         | 43/585 [00:12<02:36,  3.46it/s]  8%|▊         | 44/585 [00:12<02:36,  3.46it/s]  8%|▊         | 45/585 [00:13<02:35,  3.46it/s]  8%|▊         | 46/585 [00:13<02:35,  3.46it/s]  8%|▊         | 47/585 [00:13<02:35,  3.46it/s]  8%|▊         | 48/585 [00:13<02:35,  3.46it/s]  8%|▊         | 49/585 [00:14<02:34,  3.46it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:14<02:34,  3.46it/s]  9%|▉         | 52/585 [00:15<02:34,  3.44it/s]  9%|▉         | 53/585 [00:15<02:34,  3.45it/s]  9%|▉         | 54/585 [00:15<02:33,  3.45it/s]  9%|▉         | 55/585 [00:15<02:33,  3.45it/s] 10%|▉         | 56/585 [00:16<02:33,  3.46it/s] 10%|▉         | 57/585 [00:16<02:32,  3.46it/s] 10%|▉         | 58/585 [00:16<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:32,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.46it/s] 11%|█         | 62/585 [00:17<02:31,  3.46it/s] 11%|█         | 63/585 [00:18<02:31,  3.44it/s] 11%|█         | 64/585 [00:18<02:31,  3.45it/s] 11%|█         | 65/585 [00:18<02:30,  3.45it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.45it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.45it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 69/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.43it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 76/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.44it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.44it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.44it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.45it/s] 14%|█▍        | 81/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.45it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.45it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.45it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.44it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.45it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.45it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.45it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 100/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.44it/s] 17%|█▋        | 102/585 [00:29<02:20,  3.45it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 106/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.44it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.45it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.45it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.45it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 113/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.44it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.44it/s] 20%|█▉        | 116/585 [00:33<02:16,  3.44it/s] 20%|██        | 117/585 [00:33<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 23:59:49,983 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:59:49,984 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:59:49,984 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.97it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.38it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.58it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.99it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.42it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.98it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.82it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.52it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.50it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.52it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.53it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.38it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.58it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.52it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.52it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.37it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.28it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.26it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.36it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.38it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.42it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.55it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.58it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.63it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.53it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.38it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.36it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.39it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.38it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.44it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.42it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.55it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.59it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.56it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.43it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.28it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 45.98it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.24it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.33it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.40it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.52it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.46it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.46it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.54it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.38it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.39it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.32it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.41it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.46it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.53it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.51it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.48it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.53it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.39it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.39it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.26it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.36it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.34it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.48it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.53it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.54it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.47it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.36it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.30it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.32it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.36it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.41it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.47it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.43it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.53it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.45it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.39it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.35it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.33it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.29it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.35it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.35it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.33it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.27it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.29it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.36it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.24it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.38it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.26it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.34it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.34it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:43<02:15,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:59:59,419 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 23:59:59,440 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:00:01,961 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:00:01,975 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:00:01,984 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:51<43:44,  5.62s/it] 20%|██        | 119/585 [00:52<31:18,  4.03s/it] 21%|██        | 120/585 [00:52<22:32,  2.91s/it] 21%|██        | 121/585 [00:52<16:24,  2.12s/it] 21%|██        | 122/585 [00:53<12:08,  1.57s/it] 21%|██        | 123/585 [00:53<09:08,  1.19s/it] 21%|██        | 124/585 [00:53<07:03,  1.09it/s] 21%|██▏       | 125/585 [00:54<05:35,  1.37it/s] 22%|██▏       | 126/585 [00:54<04:34,  1.67it/s] 22%|██▏       | 127/585 [00:54<03:51,  1.98it/s] 22%|██▏       | 128/585 [00:54<03:21,  2.27it/s] 22%|██▏       | 129/585 [00:55<03:00,  2.53it/s] 22%|██▏       | 130/585 [00:55<02:45,  2.74it/s] 22%|██▏       | 131/585 [00:55<02:35,  2.92it/s] 23%|██▎       | 132/585 [00:56<02:27,  3.06it/s] 23%|██▎       | 133/585 [00:56<02:22,  3.17it/s] 23%|██▎       | 134/585 [00:56<02:18,  3.25it/s] 23%|██▎       | 135/585 [00:56<02:16,  3.31it/s] 23%|██▎       | 136/585 [00:57<02:14,  3.34it/s] 23%|██▎       | 137/585 [00:57<02:12,  3.38it/s] 24%|██▎       | 138/585 [00:57<02:11,  3.40it/s] 24%|██▍       | 139/585 [00:58<02:10,  3.41it/s] 24%|██▍       | 140/585 [00:58<02:10,  3.42it/s] 24%|██▍       | 141/585 [00:58<02:09,  3.43it/s] 24%|██▍       | 142/585 [00:58<02:08,  3.44it/s] 24%|██▍       | 143/585 [00:59<02:08,  3.44it/s] 25%|██▍       | 144/585 [00:59<02:08,  3.43it/s] 25%|██▍       | 145/585 [00:59<02:08,  3.44it/s] 25%|██▍       | 146/585 [01:00<02:07,  3.44it/s] 25%|██▌       | 147/585 [01:00<02:07,  3.44it/s] 25%|██▌       | 148/585 [01:00<02:07,  3.44it/s] 25%|██▌       | 149/585 [01:00<02:06,  3.44it/s] 26%|██▌       | 150/585 [01:01<02:06,  3.44it/s] 26%|██▌       | 151/585 [01:01<02:06,  3.44it/s] 26%|██▌       | 152/585 [01:01<02:05,  3.44it/s] 26%|██▌       | 153/585 [01:02<02:05,  3.44it/s] 26%|██▋       | 154/585 [01:02<02:05,  3.45it/s] 26%|██▋       | 155/585 [01:02<02:05,  3.44it/s] 27%|██▋       | 156/585 [01:03<02:04,  3.44it/s] 27%|██▋       | 157/585 [01:03<02:04,  3.44it/s] 27%|██▋       | 158/585 [01:03<02:03,  3.44it/s] 27%|██▋       | 159/585 [01:03<02:03,  3.44it/s] 27%|██▋       | 160/585 [01:04<02:03,  3.45it/s] 28%|██▊       | 161/585 [01:04<02:03,  3.43it/s] 28%|██▊       | 162/585 [01:04<02:03,  3.44it/s] 28%|██▊       | 163/585 [01:05<02:02,  3.44it/s] 28%|██▊       | 164/585 [01:05<02:02,  3.44it/s] 28%|██▊       | 165/585 [01:05<02:01,  3.44it/s] 28%|██▊       | 166/585 [01:05<02:01,  3.44it/s] 29%|██▊       | 167/585 [01:06<02:01,  3.44it/s] 29%|██▊       | 168/585 [01:06<02:01,  3.44it/s] 29%|██▉       | 169/585 [01:06<02:00,  3.44it/s] 29%|██▉       | 170/585 [01:07<02:00,  3.44it/s] 29%|██▉       | 171/585 [01:07<02:00,  3.44it/s] 29%|██▉       | 172/585 [01:07<01:59,  3.44it/s] 30%|██▉       | 173/585 [01:07<01:59,  3.44it/s] 30%|██▉       | 174/585 [01:08<01:59,  3.44it/s] 30%|██▉       | 175/585 [01:08<01:59,  3.44it/s] 30%|███       | 176/585 [01:08<01:58,  3.44it/s] 30%|███       | 177/585 [01:09<01:59,  3.42it/s] 30%|███       | 178/585 [01:09<01:58,  3.43it/s] 31%|███       | 179/585 [01:09<01:58,  3.43it/s] 31%|███       | 180/585 [01:09<01:57,  3.44it/s] 31%|███       | 181/585 [01:10<01:57,  3.44it/s] 31%|███       | 182/585 [01:10<01:57,  3.44it/s] 31%|███▏      | 183/585 [01:10<01:56,  3.44it/s] 31%|███▏      | 184/585 [01:11<01:56,  3.45it/s] 32%|███▏      | 185/585 [01:11<01:56,  3.44it/s] 32%|███▏      | 186/585 [01:11<01:55,  3.44it/s] 32%|███▏      | 187/585 [01:12<01:55,  3.44it/s] 32%|███▏      | 188/585 [01:12<01:55,  3.43it/s] 32%|███▏      | 189/585 [01:12<01:55,  3.43it/s] 32%|███▏      | 190/585 [01:12<01:54,  3.44it/s] 33%|███▎      | 191/585 [01:13<01:54,  3.44it/s] 33%|███▎      | 192/585 [01:13<01:54,  3.44it/s] 33%|███▎      | 193/585 [01:13<01:53,  3.44it/s] 33%|███▎      | 194/585 [01:14<01:53,  3.44it/s] 33%|███▎      | 195/585 [01:14<01:53,  3.45it/s] 34%|███▎      | 196/585 [01:14<01:52,  3.44it/s] 34%|███▎      | 197/585 [01:14<01:52,  3.45it/s] 34%|███▍      | 198/585 [01:15<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:15<01:52,  3.42it/s] 34%|███▍      | 200/585 [01:15<01:52,  3.43it/s] 34%|███▍      | 201/585 [01:16<01:51,  3.43it/s] 35%|███▍      | 202/585 [01:16<01:51,  3.43it/s] 35%|███▍      | 203/585 [01:16<01:51,  3.44it/s] 35%|███▍      | 204/585 [01:16<01:50,  3.44it/s] 35%|███▌      | 205/585 [01:17<01:50,  3.44it/s] 35%|███▌      | 206/585 [01:17<01:50,  3.44it/s] 35%|███▌      | 207/585 [01:17<01:49,  3.44it/s] 36%|███▌      | 208/585 [01:18<01:49,  3.44it/s] 36%|███▌      | 209/585 [01:18<01:49,  3.44it/s] 36%|███▌      | 210/585 [01:18<01:49,  3.43it/s] 36%|███▌      | 211/585 [01:19<01:48,  3.43it/s] 36%|███▌      | 212/585 [01:19<01:48,  3.43it/s] 36%|███▋      | 213/585 [01:19<01:48,  3.44it/s] 37%|███▋      | 214/585 [01:19<01:47,  3.44it/s] 37%|███▋      | 215/585 [01:20<01:47,  3.44it/s] 37%|███▋      | 216/585 [01:20<01:47,  3.44it/s] 37%|███▋      | 217/585 [01:20<01:46,  3.44it/s] 37%|███▋      | 218/585 [01:21<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:21<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:21<01:46,  3.44it/s] 38%|███▊      | 221/585 [01:21<01:46,  3.43it/s] 38%|███▊      | 222/585 [01:22<01:45,  3.43it/s] 38%|███▊      | 223/585 [01:22<01:45,  3.43it/s] 38%|███▊      | 224/585 [01:22<01:45,  3.44it/s] 38%|███▊      | 225/585 [01:23<01:44,  3.44it/s] 39%|███▊      | 226/585 [01:23<01:44,  3.44it/s] 39%|███▉      | 227/585 [01:23<01:44,  3.44it/s] 39%|███▉      | 228/585 [01:23<01:43,  3.44it/s] 39%|███▉      | 229/585 [01:24<01:43,  3.44it/s] 39%|███▉      | 230/585 [01:24<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:24<01:42,  3.44it/s] 40%|███▉      | 232/585 [01:25<01:42,  3.43it/s] 40%|███▉      | 233/585 [01:25<01:42,  3.43it/s] 40%|████      | 234/585 [01:25<01:42,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 00:00:41,778 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:00:41,778 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:00:41,778 >>   Batch size = 8
{'eval_loss': 1.055759310722351, 'eval_runtime': 9.4218, 'eval_samples_per_second': 370.735, 'eval_steps_per_second': 46.382, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.70it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.31it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.48it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.82it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.33it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.03it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.71it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.32it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.31it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.29it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.39it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.46it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.50it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.53it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.41it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.28it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.23it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.12it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.21it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.33it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.34it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.41it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.42it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.46it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.28it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.25it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.09it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.19it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.23it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.32it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.39it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.49it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.42it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.36it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.32it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.15it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.25it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.26it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.30it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.30it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.46it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.40it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.42it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.30it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.11it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.19it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.30it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.35it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.37it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.36it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.34it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.41it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.28it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.32it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.24it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.20it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.28it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.40it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.34it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.37it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.38it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.27it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.23it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.25it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.27it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.28it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.34it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.27it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.35it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.33it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.40it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.33it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.25it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.26it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.29it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.40it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.36it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.28it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.29it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.32it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.28it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.25it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.31it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.27it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.31it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.33it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:35<01:42,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 46.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:00:51,233 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 00:00:51,245 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:00:54,170 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:00:54,200 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:00:54,215 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:43<33:00,  5.66s/it] 40%|████      | 236/585 [01:44<23:32,  4.05s/it] 41%|████      | 237/585 [01:44<16:56,  2.92s/it] 41%|████      | 238/585 [01:44<12:19,  2.13s/it] 41%|████      | 239/585 [01:45<09:06,  1.58s/it] 41%|████      | 240/585 [01:45<06:51,  1.19s/it] 41%|████      | 241/585 [01:45<05:17,  1.09it/s] 41%|████▏     | 242/585 [01:45<04:11,  1.37it/s] 42%|████▏     | 243/585 [01:46<03:25,  1.67it/s] 42%|████▏     | 244/585 [01:46<02:52,  1.97it/s] 42%|████▏     | 245/585 [01:46<02:30,  2.26it/s] 42%|████▏     | 246/585 [01:47<02:14,  2.52it/s] 42%|████▏     | 247/585 [01:47<02:03,  2.74it/s] 42%|████▏     | 248/585 [01:47<01:55,  2.92it/s] 43%|████▎     | 249/585 [01:47<01:49,  3.06it/s] 43%|████▎     | 250/585 [01:48<01:45,  3.17it/s] 43%|████▎     | 251/585 [01:48<01:43,  3.24it/s] 43%|████▎     | 252/585 [01:48<01:40,  3.30it/s] 43%|████▎     | 253/585 [01:49<01:39,  3.34it/s] 43%|████▎     | 254/585 [01:49<01:38,  3.37it/s] 44%|████▎     | 255/585 [01:49<01:37,  3.39it/s] 44%|████▍     | 256/585 [01:49<01:36,  3.41it/s] 44%|████▍     | 257/585 [01:50<01:35,  3.42it/s] 44%|████▍     | 258/585 [01:50<01:35,  3.41it/s] 44%|████▍     | 259/585 [01:50<01:35,  3.42it/s] 44%|████▍     | 260/585 [01:51<01:35,  3.42it/s] 45%|████▍     | 261/585 [01:51<01:34,  3.43it/s] 45%|████▍     | 262/585 [01:51<01:35,  3.37it/s] 45%|████▍     | 263/585 [01:52<01:35,  3.39it/s] 45%|████▌     | 264/585 [01:52<01:34,  3.41it/s] 45%|████▌     | 265/585 [01:52<01:33,  3.42it/s] 45%|████▌     | 266/585 [01:52<01:33,  3.42it/s] 46%|████▌     | 267/585 [01:53<01:32,  3.43it/s] 46%|████▌     | 268/585 [01:53<01:32,  3.44it/s] 46%|████▌     | 269/585 [01:53<01:32,  3.43it/s] 46%|████▌     | 270/585 [01:54<01:31,  3.43it/s] 46%|████▋     | 271/585 [01:54<01:31,  3.44it/s] 46%|████▋     | 272/585 [01:54<01:31,  3.44it/s] 47%|████▋     | 273/585 [01:54<01:30,  3.44it/s] 47%|████▋     | 274/585 [01:55<01:30,  3.44it/s] 47%|████▋     | 275/585 [01:55<01:30,  3.44it/s] 47%|████▋     | 276/585 [01:55<01:29,  3.44it/s] 47%|████▋     | 277/585 [01:56<01:29,  3.44it/s] 48%|████▊     | 278/585 [01:56<01:29,  3.44it/s] 48%|████▊     | 279/585 [01:56<01:28,  3.44it/s] 48%|████▊     | 280/585 [01:56<01:28,  3.44it/s] 48%|████▊     | 281/585 [01:57<01:28,  3.44it/s] 48%|████▊     | 282/585 [01:57<01:28,  3.44it/s] 48%|████▊     | 283/585 [01:57<01:27,  3.44it/s] 49%|████▊     | 284/585 [01:58<01:27,  3.44it/s] 49%|████▊     | 285/585 [01:58<01:27,  3.44it/s] 49%|████▉     | 286/585 [01:58<01:26,  3.44it/s] 49%|████▉     | 287/585 [01:59<01:26,  3.44it/s] 49%|████▉     | 288/585 [01:59<01:26,  3.44it/s] 49%|████▉     | 289/585 [01:59<01:25,  3.44it/s] 50%|████▉     | 290/585 [01:59<01:25,  3.44it/s] 50%|████▉     | 291/585 [02:00<01:25,  3.44it/s] 50%|████▉     | 292/585 [02:00<01:25,  3.44it/s] 50%|█████     | 293/585 [02:00<01:24,  3.44it/s] 50%|█████     | 294/585 [02:01<01:25,  3.40it/s] 50%|█████     | 295/585 [02:01<01:24,  3.41it/s] 51%|█████     | 296/585 [02:01<01:24,  3.42it/s] 51%|█████     | 297/585 [02:01<01:24,  3.43it/s] 51%|█████     | 298/585 [02:02<01:23,  3.43it/s] 51%|█████     | 299/585 [02:02<01:23,  3.44it/s] 51%|█████▏    | 300/585 [02:02<01:22,  3.44it/s] 51%|█████▏    | 301/585 [02:03<01:22,  3.44it/s] 52%|█████▏    | 302/585 [02:03<01:22,  3.44it/s] 52%|█████▏    | 303/585 [02:03<01:21,  3.44it/s] 52%|█████▏    | 304/585 [02:03<01:21,  3.45it/s] 52%|█████▏    | 305/585 [02:04<01:22,  3.40it/s] 52%|█████▏    | 306/585 [02:04<01:21,  3.42it/s] 52%|█████▏    | 307/585 [02:04<01:21,  3.42it/s] 53%|█████▎    | 308/585 [02:05<01:20,  3.43it/s] 53%|█████▎    | 309/585 [02:05<01:20,  3.43it/s] 53%|█████▎    | 310/585 [02:05<01:20,  3.44it/s] 53%|█████▎    | 311/585 [02:05<01:19,  3.44it/s] 53%|█████▎    | 312/585 [02:06<01:19,  3.44it/s] 54%|█████▎    | 313/585 [02:06<01:19,  3.44it/s] 54%|█████▎    | 314/585 [02:06<01:18,  3.44it/s] 54%|█████▍    | 315/585 [02:07<01:18,  3.44it/s] 54%|█████▍    | 316/585 [02:07<01:18,  3.43it/s] 54%|█████▍    | 317/585 [02:07<01:18,  3.44it/s] 54%|█████▍    | 318/585 [02:08<01:17,  3.44it/s] 55%|█████▍    | 319/585 [02:08<01:17,  3.44it/s] 55%|█████▍    | 320/585 [02:08<01:17,  3.44it/s] 55%|█████▍    | 321/585 [02:08<01:16,  3.44it/s] 55%|█████▌    | 322/585 [02:09<01:16,  3.44it/s] 55%|█████▌    | 323/585 [02:09<01:16,  3.44it/s] 55%|█████▌    | 324/585 [02:09<01:15,  3.44it/s] 56%|█████▌    | 325/585 [02:10<01:15,  3.44it/s] 56%|█████▌    | 326/585 [02:10<01:15,  3.44it/s] 56%|█████▌    | 327/585 [02:10<01:15,  3.43it/s] 56%|█████▌    | 328/585 [02:10<01:14,  3.43it/s] 56%|█████▌    | 329/585 [02:11<01:14,  3.44it/s] 56%|█████▋    | 330/585 [02:11<01:14,  3.44it/s] 57%|█████▋    | 331/585 [02:11<01:13,  3.44it/s] 57%|█████▋    | 332/585 [02:12<01:13,  3.44it/s] 57%|█████▋    | 333/585 [02:12<01:13,  3.44it/s] 57%|█████▋    | 334/585 [02:12<01:13,  3.44it/s] 57%|█████▋    | 335/585 [02:12<01:12,  3.44it/s] 57%|█████▋    | 336/585 [02:13<01:12,  3.44it/s] 58%|█████▊    | 337/585 [02:13<01:12,  3.44it/s] 58%|█████▊    | 338/585 [02:13<01:12,  3.42it/s] 58%|█████▊    | 339/585 [02:14<01:11,  3.43it/s] 58%|█████▊    | 340/585 [02:14<01:11,  3.43it/s] 58%|█████▊    | 341/585 [02:14<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:15<01:10,  3.44it/s] 59%|█████▊    | 343/585 [02:15<01:10,  3.44it/s] 59%|█████▉    | 344/585 [02:15<01:10,  3.44it/s] 59%|█████▉    | 345/585 [02:15<01:09,  3.44it/s] 59%|█████▉    | 346/585 [02:16<01:09,  3.44it/s] 59%|█████▉    | 347/585 [02:16<01:09,  3.44it/s] 59%|█████▉    | 348/585 [02:16<01:08,  3.44it/s] 60%|█████▉    | 349/585 [02:17<01:08,  3.43it/s] 60%|█████▉    | 350/585 [02:17<01:08,  3.43it/s] 60%|██████    | 351/585 [02:17<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 00:01:33,724 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:01:33,725 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:01:33,725 >>   Batch size = 8
{'eval_loss': 1.0683077573776245, 'eval_runtime': 9.4425, 'eval_samples_per_second': 369.921, 'eval_steps_per_second': 46.28, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.51it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.18it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.34it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.83it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.38it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.04it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.78it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.27it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.25it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.15it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.41it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.45it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.52it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.48it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.52it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.31it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.26it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.22it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.21it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.10it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.32it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.45it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.45it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.56it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.46it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.24it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.15it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.16it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.28it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.39it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.32it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.44it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.43it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.39it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.34it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.21it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.19it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.18it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.34it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.19it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.42it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.41it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.34it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.30it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.32it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.18it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.20it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.29it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.32it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.32it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.34it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.36it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.26it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.36it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.23it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.20it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.26it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.32it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.39it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.45it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.41it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.33it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.25it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.18it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.08it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.08it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.27it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.35it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.36it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.35it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.29it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.37it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.29it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.20it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.08it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.23it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.23it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.37it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.31it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.34it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.24it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.29it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.27it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.24it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.14it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.25it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:27<01:08,  3.43it/s]
100%|██████████| 437/437 [00:09<00:00, 46.25it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:01:43,203 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 00:01:43,241 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:01:45,751 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:01:45,766 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:01:45,773 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:35<21:06,  5.43s/it] 60%|██████    | 353/585 [02:35<15:02,  3.89s/it] 61%|██████    | 354/585 [02:35<10:49,  2.81s/it] 61%|██████    | 355/585 [02:35<07:52,  2.05s/it] 61%|██████    | 356/585 [02:36<05:49,  1.52s/it] 61%|██████    | 357/585 [02:36<04:23,  1.15s/it] 61%|██████    | 358/585 [02:36<03:23,  1.12it/s] 61%|██████▏   | 359/585 [02:37<02:41,  1.40it/s] 62%|██████▏   | 360/585 [02:37<02:12,  1.70it/s] 62%|██████▏   | 361/585 [02:37<01:51,  2.01it/s] 62%|██████▏   | 362/585 [02:37<01:37,  2.30it/s] 62%|██████▏   | 363/585 [02:38<01:26,  2.55it/s] 62%|██████▏   | 364/585 [02:38<01:20,  2.76it/s] 62%|██████▏   | 365/585 [02:38<01:14,  2.93it/s] 63%|██████▎   | 366/585 [02:39<01:11,  3.07it/s] 63%|██████▎   | 367/585 [02:39<01:08,  3.18it/s] 63%|██████▎   | 368/585 [02:39<01:06,  3.25it/s] 63%|██████▎   | 369/585 [02:40<01:05,  3.31it/s] 63%|██████▎   | 370/585 [02:40<01:04,  3.35it/s] 63%|██████▎   | 371/585 [02:40<01:03,  3.38it/s] 64%|██████▎   | 372/585 [02:40<01:02,  3.40it/s] 64%|██████▍   | 373/585 [02:41<01:02,  3.41it/s] 64%|██████▍   | 374/585 [02:41<01:01,  3.42it/s] 64%|██████▍   | 375/585 [02:41<01:01,  3.42it/s] 64%|██████▍   | 376/585 [02:42<01:00,  3.43it/s] 64%|██████▍   | 377/585 [02:42<01:00,  3.43it/s] 65%|██████▍   | 378/585 [02:42<01:00,  3.44it/s] 65%|██████▍   | 379/585 [02:42<00:59,  3.44it/s] 65%|██████▍   | 380/585 [02:43<00:59,  3.44it/s] 65%|██████▌   | 381/585 [02:43<00:59,  3.44it/s] 65%|██████▌   | 382/585 [02:43<00:58,  3.45it/s] 65%|██████▌   | 383/585 [02:44<00:58,  3.45it/s] 66%|██████▌   | 384/585 [02:44<00:58,  3.45it/s] 66%|██████▌   | 385/585 [02:44<00:58,  3.45it/s] 66%|██████▌   | 386/585 [02:44<00:57,  3.44it/s] 66%|██████▌   | 387/585 [02:45<00:57,  3.44it/s] 66%|██████▋   | 388/585 [02:45<00:57,  3.44it/s] 66%|██████▋   | 389/585 [02:45<00:56,  3.44it/s] 67%|██████▋   | 390/585 [02:46<00:56,  3.44it/s] 67%|██████▋   | 391/585 [02:46<00:56,  3.44it/s] 67%|██████▋   | 392/585 [02:46<00:56,  3.45it/s] 67%|██████▋   | 393/585 [02:46<00:55,  3.44it/s] 67%|██████▋   | 394/585 [02:47<00:55,  3.44it/s] 68%|██████▊   | 395/585 [02:47<00:55,  3.44it/s] 68%|██████▊   | 396/585 [02:47<00:54,  3.44it/s] 68%|██████▊   | 397/585 [02:48<00:54,  3.44it/s] 68%|██████▊   | 398/585 [02:48<00:54,  3.44it/s] 68%|██████▊   | 399/585 [02:48<00:54,  3.44it/s] 68%|██████▊   | 400/585 [02:49<00:53,  3.43it/s] 69%|██████▊   | 401/585 [02:49<00:53,  3.44it/s] 69%|██████▊   | 402/585 [02:49<00:53,  3.44it/s] 69%|██████▉   | 403/585 [02:49<00:52,  3.44it/s] 69%|██████▉   | 404/585 [02:50<00:52,  3.44it/s] 69%|██████▉   | 405/585 [02:50<00:52,  3.44it/s] 69%|██████▉   | 406/585 [02:50<00:52,  3.44it/s] 70%|██████▉   | 407/585 [02:51<00:51,  3.44it/s] 70%|██████▉   | 408/585 [02:51<00:51,  3.43it/s] 70%|██████▉   | 409/585 [02:51<00:51,  3.43it/s] 70%|███████   | 410/585 [02:51<00:52,  3.34it/s] 70%|███████   | 411/585 [02:52<00:51,  3.36it/s] 70%|███████   | 412/585 [02:52<00:51,  3.39it/s] 71%|███████   | 413/585 [02:52<00:50,  3.40it/s] 71%|███████   | 414/585 [02:53<00:50,  3.41it/s] 71%|███████   | 415/585 [02:53<00:49,  3.42it/s] 71%|███████   | 416/585 [02:53<00:49,  3.43it/s] 71%|███████▏  | 417/585 [02:53<00:48,  3.43it/s] 71%|███████▏  | 418/585 [02:54<00:48,  3.44it/s] 72%|███████▏  | 419/585 [02:54<00:48,  3.43it/s] 72%|███████▏  | 420/585 [02:54<00:48,  3.44it/s] 72%|███████▏  | 421/585 [02:55<00:47,  3.44it/s] 72%|███████▏  | 422/585 [02:55<00:47,  3.44it/s] 72%|███████▏  | 423/585 [02:55<00:47,  3.44it/s] 72%|███████▏  | 424/585 [02:56<00:46,  3.44it/s] 73%|███████▎  | 425/585 [02:56<00:46,  3.44it/s] 73%|███████▎  | 426/585 [02:56<00:46,  3.44it/s] 73%|███████▎  | 427/585 [02:56<00:45,  3.44it/s] 73%|███████▎  | 428/585 [02:57<00:45,  3.44it/s] 73%|███████▎  | 429/585 [02:57<00:45,  3.44it/s] 74%|███████▎  | 430/585 [02:57<00:45,  3.44it/s] 74%|███████▎  | 431/585 [02:58<00:44,  3.44it/s] 74%|███████▍  | 432/585 [02:58<00:44,  3.44it/s] 74%|███████▍  | 433/585 [02:58<00:44,  3.44it/s] 74%|███████▍  | 434/585 [02:58<00:43,  3.44it/s] 74%|███████▍  | 435/585 [02:59<00:43,  3.44it/s] 75%|███████▍  | 436/585 [02:59<00:43,  3.44it/s] 75%|███████▍  | 437/585 [02:59<00:43,  3.44it/s] 75%|███████▍  | 438/585 [03:00<00:42,  3.44it/s] 75%|███████▌  | 439/585 [03:00<00:42,  3.44it/s] 75%|███████▌  | 440/585 [03:00<00:42,  3.44it/s] 75%|███████▌  | 441/585 [03:00<00:41,  3.44it/s] 76%|███████▌  | 442/585 [03:01<00:41,  3.44it/s] 76%|███████▌  | 443/585 [03:01<00:41,  3.44it/s] 76%|███████▌  | 444/585 [03:01<00:40,  3.44it/s] 76%|███████▌  | 445/585 [03:02<00:40,  3.44it/s] 76%|███████▌  | 446/585 [03:02<00:40,  3.43it/s] 76%|███████▋  | 447/585 [03:02<00:40,  3.43it/s] 77%|███████▋  | 448/585 [03:02<00:39,  3.43it/s] 77%|███████▋  | 449/585 [03:03<00:39,  3.44it/s] 77%|███████▋  | 450/585 [03:03<00:39,  3.44it/s] 77%|███████▋  | 451/585 [03:03<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:04<00:38,  3.44it/s] 77%|███████▋  | 453/585 [03:04<00:38,  3.44it/s] 78%|███████▊  | 454/585 [03:04<00:38,  3.44it/s] 78%|███████▊  | 455/585 [03:05<00:37,  3.45it/s] 78%|███████▊  | 456/585 [03:05<00:37,  3.44it/s] 78%|███████▊  | 457/585 [03:05<00:37,  3.41it/s] 78%|███████▊  | 458/585 [03:05<00:37,  3.42it/s] 78%|███████▊  | 459/585 [03:06<00:36,  3.43it/s] 79%|███████▊  | 460/585 [03:06<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:06<00:36,  3.43it/s] 79%|███████▉  | 462/585 [03:07<00:35,  3.44it/s] 79%|███████▉  | 463/585 [03:07<00:35,  3.43it/s] 79%|███████▉  | 464/585 [03:07<00:35,  3.44it/s] 79%|███████▉  | 465/585 [03:07<00:34,  3.44it/s] 80%|███████▉  | 466/585 [03:08<00:34,  3.44it/s] 80%|███████▉  | 467/585 [03:08<00:34,  3.44it/s] 80%|████████  | 468/585 [03:08<00:34,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 00:02:24,904 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:02:24,904 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:02:24,904 >>   Batch size = 8
{'eval_loss': 1.0788260698318481, 'eval_runtime': 9.443, 'eval_samples_per_second': 369.905, 'eval_steps_per_second': 46.278, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.76it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.15it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.32it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.75it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.30it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.99it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.66it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.32it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.42it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.47it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.49it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.41it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.44it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.40it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.41it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.27it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.23it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.20it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.28it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.33it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.39it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.47it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.40it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.38it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.27it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.14it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.15it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.30it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.17it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.53it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.54it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.45it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.41it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.35it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.23it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.14it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.30it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.25it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.37it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.39it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.47it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.34it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.39it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.26it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.21it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.20it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.32it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.33it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.38it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.46it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.35it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.41it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.27it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.30it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.25it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.28it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.35it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.38it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.38it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.35it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.37it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.38it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.23it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.23it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.32it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.28it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.39it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.36it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.25it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.36it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.33it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.38it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.26it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.31it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.18it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.28it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.33it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.39it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.28it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.19it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.22it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.26it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.31it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.30it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.25it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.27it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:18<00:34,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 46.27it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:02:34,352 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 00:02:34,370 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:02:36,826 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:02:36,842 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:02:36,850 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:25<10:20,  5.35s/it] 80%|████████  | 470/585 [03:26<07:20,  3.83s/it] 81%|████████  | 471/585 [03:26<05:15,  2.77s/it] 81%|████████  | 472/585 [03:26<03:48,  2.03s/it] 81%|████████  | 473/585 [03:27<02:48,  1.51s/it] 81%|████████  | 474/585 [03:27<02:06,  1.14s/it] 81%|████████  | 475/585 [03:27<01:37,  1.13it/s] 81%|████████▏ | 476/585 [03:28<01:17,  1.41it/s] 82%|████████▏ | 477/585 [03:28<01:02,  1.72it/s] 82%|████████▏ | 478/585 [03:28<00:52,  2.02it/s] 82%|████████▏ | 479/585 [03:28<00:45,  2.31it/s] 82%|████████▏ | 480/585 [03:29<00:40,  2.56it/s] 82%|████████▏ | 481/585 [03:29<00:37,  2.77it/s] 82%|████████▏ | 482/585 [03:29<00:35,  2.94it/s] 83%|████████▎ | 483/585 [03:30<00:33,  3.08it/s] 83%|████████▎ | 484/585 [03:30<00:31,  3.18it/s] 83%|████████▎ | 485/585 [03:30<00:30,  3.26it/s] 83%|████████▎ | 486/585 [03:30<00:29,  3.31it/s] 83%|████████▎ | 487/585 [03:31<00:29,  3.35it/s] 83%|████████▎ | 488/585 [03:31<00:28,  3.38it/s] 84%|████████▎ | 489/585 [03:31<00:28,  3.40it/s] 84%|████████▍ | 490/585 [03:32<00:27,  3.41it/s] 84%|████████▍ | 491/585 [03:32<00:27,  3.42it/s] 84%|████████▍ | 492/585 [03:32<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:32<00:26,  3.43it/s] 84%|████████▍ | 494/585 [03:33<00:26,  3.42it/s] 85%|████████▍ | 495/585 [03:33<00:26,  3.43it/s] 85%|████████▍ | 496/585 [03:33<00:25,  3.43it/s] 85%|████████▍ | 497/585 [03:34<00:25,  3.44it/s] 85%|████████▌ | 498/585 [03:34<00:25,  3.44it/s] 85%|████████▌ | 499/585 [03:34<00:24,  3.44it/s] 85%|████████▌ | 500/585 [03:34<00:24,  3.44it/s]                                                  85%|████████▌ | 500/585 [03:34<00:24,  3.44it/s] 86%|████████▌ | 501/585 [03:35<00:24,  3.44it/s] 86%|████████▌ | 502/585 [03:35<00:24,  3.44it/s] 86%|████████▌ | 503/585 [03:35<00:23,  3.44it/s] 86%|████████▌ | 504/585 [03:36<00:23,  3.45it/s] 86%|████████▋ | 505/585 [03:36<00:23,  3.44it/s] 86%|████████▋ | 506/585 [03:36<00:22,  3.44it/s] 87%|████████▋ | 507/585 [03:37<00:22,  3.44it/s] 87%|████████▋ | 508/585 [03:37<00:22,  3.44it/s] 87%|████████▋ | 509/585 [03:37<00:22,  3.44it/s] 87%|████████▋ | 510/585 [03:37<00:21,  3.44it/s] 87%|████████▋ | 511/585 [03:38<00:21,  3.44it/s] 88%|████████▊ | 512/585 [03:38<00:21,  3.44it/s] 88%|████████▊ | 513/585 [03:38<00:20,  3.44it/s] 88%|████████▊ | 514/585 [03:39<00:20,  3.44it/s] 88%|████████▊ | 515/585 [03:39<00:20,  3.44it/s] 88%|████████▊ | 516/585 [03:39<00:20,  3.44it/s] 88%|████████▊ | 517/585 [03:39<00:19,  3.44it/s] 89%|████████▊ | 518/585 [03:40<00:19,  3.44it/s] 89%|████████▊ | 519/585 [03:40<00:19,  3.44it/s] 89%|████████▉ | 520/585 [03:40<00:18,  3.44it/s] 89%|████████▉ | 521/585 [03:41<00:18,  3.44it/s] 89%|████████▉ | 522/585 [03:41<00:18,  3.44it/s] 89%|████████▉ | 523/585 [03:41<00:17,  3.45it/s] 90%|████████▉ | 524/585 [03:41<00:17,  3.45it/s] 90%|████████▉ | 525/585 [03:42<00:17,  3.45it/s] 90%|████████▉ | 526/585 [03:42<00:17,  3.44it/s] 90%|█████████ | 527/585 [03:42<00:16,  3.43it/s] 90%|█████████ | 528/585 [03:43<00:16,  3.44it/s] 90%|█████████ | 529/585 [03:43<00:16,  3.44it/s] 91%|█████████ | 530/585 [03:43<00:15,  3.44it/s] 91%|█████████ | 531/585 [03:43<00:15,  3.44it/s] 91%|█████████ | 532/585 [03:44<00:15,  3.44it/s] 91%|█████████ | 533/585 [03:44<00:15,  3.44it/s] 91%|█████████▏| 534/585 [03:44<00:14,  3.44it/s] 91%|█████████▏| 535/585 [03:45<00:14,  3.44it/s] 92%|█████████▏| 536/585 [03:45<00:14,  3.44it/s] 92%|█████████▏| 537/585 [03:45<00:13,  3.44it/s] 92%|█████████▏| 538/585 [03:46<00:13,  3.44it/s] 92%|█████████▏| 539/585 [03:46<00:13,  3.43it/s] 92%|█████████▏| 540/585 [03:46<00:13,  3.43it/s] 92%|█████████▏| 541/585 [03:46<00:12,  3.44it/s] 93%|█████████▎| 542/585 [03:47<00:12,  3.43it/s] 93%|█████████▎| 543/585 [03:47<00:12,  3.44it/s] 93%|█████████▎| 544/585 [03:47<00:11,  3.44it/s] 93%|█████████▎| 545/585 [03:48<00:11,  3.44it/s] 93%|█████████▎| 546/585 [03:48<00:11,  3.44it/s] 94%|█████████▎| 547/585 [03:48<00:11,  3.44it/s] 94%|█████████▎| 548/585 [03:48<00:10,  3.44it/s] 94%|█████████▍| 549/585 [03:49<00:10,  3.43it/s] 94%|█████████▍| 550/585 [03:49<00:10,  3.43it/s] 94%|█████████▍| 551/585 [03:49<00:09,  3.43it/s] 94%|█████████▍| 552/585 [03:50<00:09,  3.44it/s] 95%|█████████▍| 553/585 [03:50<00:09,  3.44it/s] 95%|█████████▍| 554/585 [03:50<00:09,  3.44it/s] 95%|█████████▍| 555/585 [03:50<00:08,  3.44it/s] 95%|█████████▌| 556/585 [03:51<00:08,  3.44it/s] 95%|█████████▌| 557/585 [03:51<00:08,  3.44it/s] 95%|█████████▌| 558/585 [03:51<00:07,  3.44it/s] 96%|█████████▌| 559/585 [03:52<00:07,  3.34it/s] 96%|█████████▌| 560/585 [03:52<00:07,  3.35it/s] 96%|█████████▌| 561/585 [03:52<00:07,  3.38it/s] 96%|█████████▌| 562/585 [03:53<00:06,  3.40it/s] 96%|█████████▌| 563/585 [03:53<00:06,  3.41it/s] 96%|█████████▋| 564/585 [03:53<00:06,  3.42it/s] 97%|█████████▋| 565/585 [03:53<00:05,  3.42it/s] 97%|█████████▋| 566/585 [03:54<00:05,  3.43it/s] 97%|█████████▋| 567/585 [03:54<00:05,  3.44it/s] 97%|█████████▋| 568/585 [03:54<00:04,  3.44it/s] 97%|█████████▋| 569/585 [03:55<00:04,  3.44it/s] 97%|█████████▋| 570/585 [03:55<00:04,  3.44it/s] 98%|█████████▊| 571/585 [03:55<00:04,  3.42it/s] 98%|█████████▊| 572/585 [03:55<00:03,  3.42it/s] 98%|█████████▊| 573/585 [03:56<00:03,  3.43it/s] 98%|█████████▊| 574/585 [03:56<00:03,  3.43it/s] 98%|█████████▊| 575/585 [03:56<00:02,  3.44it/s] 98%|█████████▊| 576/585 [03:57<00:02,  3.44it/s] 99%|█████████▊| 577/585 [03:57<00:02,  3.44it/s] 99%|█████████▉| 578/585 [03:57<00:02,  3.44it/s] 99%|█████████▉| 579/585 [03:57<00:01,  3.44it/s] 99%|█████████▉| 580/585 [03:58<00:01,  3.44it/s] 99%|█████████▉| 581/585 [03:58<00:01,  3.44it/s] 99%|█████████▉| 582/585 [03:58<00:00,  3.44it/s]100%|█████████▉| 583/585 [03:59<00:00,  3.44it/s]100%|█████████▉| 584/585 [03:59<00:00,  3.44it/s]100%|██████████| 585/585 [03:59<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 00:03:15,770 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:03:15,771 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:03:15,771 >>   Batch size = 8
{'eval_loss': 1.0980240106582642, 'eval_runtime': 9.4388, 'eval_samples_per_second': 370.068, 'eval_steps_per_second': 46.298, 'epoch': 4.0}
{'loss': 0.4152, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.67it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.13it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.38it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.72it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.33it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.06it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.72it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.53it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.36it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.40it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.49it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.45it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.52it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.44it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.50it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.31it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.24it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.21it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.23it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.16it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.30it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 46.28it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.41it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.46it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.33it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.27it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.16it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.26it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.27it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.48it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.33it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.46it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.35it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.44it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.39it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.31it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.26it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.35it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.43it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.33it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.36it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.41it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.36it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.23it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.35it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.23it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.27it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.42it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.34it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.33it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.37it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.36it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.36it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.28it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.24it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.33it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.36it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.48it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.37it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.41it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.36it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.28it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.31it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.18it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.29it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.35it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.39it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.41it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.41it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.43it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.36it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.32it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.21it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.22it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.24it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.39it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.39it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.42it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.35it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.30it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.30it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.31it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.33it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.35it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.43it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.40it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:09<00:00,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 46.40it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:03:25,213 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 00:03:25,230 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:03:27,834 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:03:27,849 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:03:27,861 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:03:32,793 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:03:32,799 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117 (score: 1.055759310722351).
                                                 100%|██████████| 585/585 [04:18<00:00,  3.44it/s]100%|██████████| 585/585 [04:18<00:00,  2.26it/s]
[INFO|trainer.py:1894] 2023-08-29 00:03:34,735 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 00:03:34,754 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:03:37,355 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:03:37,378 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:03:37,401 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:03:37,604 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:37,604 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:37,605 >>   train_loss               =      0.412
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:37,605 >>   train_runtime            = 0:04:18.68
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:37,605 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:37,605 >>   train_samples_per_second =    144.966
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:37,605 >>   train_steps_per_second   =      2.261
{'eval_loss': 1.102099895477295, 'eval_runtime': 9.4243, 'eval_samples_per_second': 370.638, 'eval_steps_per_second': 46.37, 'epoch': 5.0}
{'train_runtime': 258.6805, 'train_samples_per_second': 144.966, 'train_steps_per_second': 2.261, 'train_loss': 0.41203412439069176, 'epoch': 5.0}
08/29/2023 00:03:37 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:03:37,653 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:03:37,653 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:03:37,653 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 57.32it/s]  3%|▎         | 12/437 [00:00<00:08, 50.90it/s]  4%|▍         | 18/437 [00:00<00:08, 48.98it/s]  5%|▌         | 23/437 [00:00<00:08, 48.26it/s]  6%|▋         | 28/437 [00:00<00:08, 47.74it/s]  8%|▊         | 33/437 [00:00<00:08, 47.39it/s]  9%|▊         | 38/437 [00:00<00:08, 47.26it/s] 10%|▉         | 43/437 [00:00<00:08, 47.02it/s] 11%|█         | 48/437 [00:01<00:08, 46.93it/s] 12%|█▏        | 53/437 [00:01<00:08, 46.88it/s] 13%|█▎        | 58/437 [00:01<00:08, 46.85it/s] 14%|█▍        | 63/437 [00:01<00:07, 46.82it/s] 16%|█▌        | 68/437 [00:01<00:07, 46.76it/s] 17%|█▋        | 73/437 [00:01<00:07, 46.84it/s] 18%|█▊        | 78/437 [00:01<00:07, 46.74it/s] 19%|█▉        | 83/437 [00:01<00:07, 46.84it/s] 20%|██        | 88/437 [00:01<00:07, 46.69it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.73it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.70it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.64it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.71it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.79it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.77it/s] 28%|██▊       | 123/437 [00:02<00:06, 46.77it/s] 29%|██▉       | 128/437 [00:02<00:06, 46.72it/s] 30%|███       | 133/437 [00:02<00:06, 46.32it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.37it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.41it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.49it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.61it/s] 36%|███▌      | 158/437 [00:03<00:05, 46.64it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.70it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.74it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.68it/s] 41%|████      | 178/437 [00:03<00:05, 46.67it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.67it/s] 43%|████▎     | 188/437 [00:04<00:05, 46.61it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.73it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.62it/s] 46%|████▋     | 203/437 [00:04<00:05, 46.66it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.74it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.75it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.74it/s] 51%|█████     | 223/437 [00:04<00:04, 46.80it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.58it/s] 53%|█████▎    | 233/437 [00:04<00:04, 46.65it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.68it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.69it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.68it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.43it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.71it/s] 60%|██████    | 263/437 [00:05<00:03, 46.73it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.80it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.73it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.76it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.62it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.70it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.63it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.63it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.58it/s] 70%|███████   | 308/437 [00:06<00:02, 46.61it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.63it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.67it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.71it/s] 75%|███████▌  | 328/437 [00:07<00:02, 46.52it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.60it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.61it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.60it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.55it/s] 81%|████████  | 353/437 [00:07<00:01, 46.44it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.49it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.56it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.59it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.53it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.52it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.55it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.60it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.51it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.58it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.62it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.52it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.57it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.46it/s] 97%|█████████▋| 423/437 [00:09<00:00, 46.50it/s] 98%|█████████▊| 428/437 [00:09<00:00, 46.57it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.58it/s]100%|██████████| 437/437 [00:09<00:00, 46.75it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:03:47,021 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:47,021 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:47,021 >>   eval_loss               =     1.0558
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:47,021 >>   eval_runtime            = 0:00:09.36
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:47,021 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:47,021 >>   eval_samples_per_second =    372.889
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:47,021 >>   eval_steps_per_second   =     46.651
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:03:47,021 >>   perplexity              =     2.8742
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:53,070 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:53,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:53,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:53,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:53,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:03:53,363 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:03:53,364 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:03:53,628 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:03:54,642 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:03:54,642 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:56,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:56,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:56,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:56,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:56,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:03:57,563 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:03:57,564 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:03:58,152 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:03:58,290 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:03:58,290 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:21,  1.47it/s]Extractor Predicting: 35it [00:22,  1.48it/s]Extractor Predicting: 36it [00:23,  1.47it/s]Extractor Predicting: 37it [00:23,  1.46it/s]Extractor Predicting: 38it [00:24,  1.46it/s]Extractor Predicting: 39it [00:25,  1.48it/s]Extractor Predicting: 40it [00:25,  1.48it/s]Extractor Predicting: 41it [00:26,  1.47it/s]Extractor Predicting: 42it [00:27,  1.48it/s]Extractor Predicting: 43it [00:28,  1.40it/s]Extractor Predicting: 44it [00:28,  1.44it/s]Extractor Predicting: 45it [00:29,  1.47it/s]Extractor Predicting: 46it [00:30,  1.45it/s]Extractor Predicting: 47it [00:30,  1.45it/s]Extractor Predicting: 48it [00:31,  1.46it/s]Extractor Predicting: 49it [00:32,  1.46it/s]Extractor Predicting: 50it [00:32,  1.46it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:34,  1.46it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:36,  1.46it/s]Extractor Predicting: 56it [00:36,  1.46it/s]Extractor Predicting: 57it [00:37,  1.44it/s]Extractor Predicting: 58it [00:38,  1.44it/s]Extractor Predicting: 59it [00:39,  1.43it/s]Extractor Predicting: 60it [00:39,  1.43it/s]Extractor Predicting: 61it [00:40,  1.45it/s]Extractor Predicting: 62it [00:41,  1.45it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.50it/s]Extractor Predicting: 65it [00:43,  1.48it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:45,  1.49it/s]Extractor Predicting: 69it [00:45,  1.48it/s]Extractor Predicting: 70it [00:46,  1.46it/s]Extractor Predicting: 71it [00:47,  1.47it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:48,  1.48it/s]Extractor Predicting: 74it [00:49,  1.51it/s]Extractor Predicting: 75it [00:49,  1.51it/s]Extractor Predicting: 76it [00:50,  1.49it/s]Extractor Predicting: 77it [00:51,  1.49it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.48it/s]Extractor Predicting: 80it [00:53,  1.46it/s]Extractor Predicting: 81it [00:53,  1.51it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:55,  1.47it/s]Extractor Predicting: 84it [00:55,  1.48it/s]Extractor Predicting: 85it [00:56,  1.51it/s]Extractor Predicting: 86it [00:57,  1.56it/s]Extractor Predicting: 87it [00:57,  1.60it/s]Extractor Predicting: 88it [00:58,  1.60it/s]Extractor Predicting: 89it [00:58,  1.60it/s]Extractor Predicting: 90it [00:59,  1.60it/s]Extractor Predicting: 91it [01:00,  1.58it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:01,  1.59it/s]Extractor Predicting: 94it [01:02,  1.62it/s]Extractor Predicting: 95it [01:02,  1.60it/s]Extractor Predicting: 96it [01:03,  1.61it/s]Extractor Predicting: 97it [01:04,  1.59it/s]Extractor Predicting: 98it [01:04,  1.56it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:05,  1.56it/s]Extractor Predicting: 101it [01:06,  1.59it/s]Extractor Predicting: 102it [01:07,  1.57it/s]Extractor Predicting: 103it [01:07,  1.57it/s]Extractor Predicting: 104it [01:08,  1.58it/s]Extractor Predicting: 105it [01:09,  1.61it/s]Extractor Predicting: 106it [01:09,  1.58it/s]Extractor Predicting: 107it [01:10,  1.59it/s]Extractor Predicting: 108it [01:10,  1.59it/s]Extractor Predicting: 109it [01:11,  1.63it/s]Extractor Predicting: 110it [01:12,  1.61it/s]Extractor Predicting: 111it [01:12,  1.59it/s]Extractor Predicting: 112it [01:13,  1.58it/s]Extractor Predicting: 113it [01:14,  1.58it/s]Extractor Predicting: 114it [01:14,  1.53it/s]Extractor Predicting: 115it [01:15,  1.51it/s]Extractor Predicting: 116it [01:16,  1.51it/s]Extractor Predicting: 117it [01:16,  1.50it/s]Extractor Predicting: 118it [01:17,  1.57it/s]Extractor Predicting: 119it [01:18,  1.56it/s]Extractor Predicting: 120it [01:18,  1.54it/s]Extractor Predicting: 121it [01:19,  1.54it/s]Extractor Predicting: 122it [01:20,  1.53it/s]Extractor Predicting: 123it [01:20,  1.53it/s]Extractor Predicting: 124it [01:21,  1.50it/s]Extractor Predicting: 125it [01:22,  1.50it/s]Extractor Predicting: 126it [01:22,  1.41it/s]Extractor Predicting: 127it [01:23,  1.42it/s]Extractor Predicting: 128it [01:24,  1.43it/s]Extractor Predicting: 129it [01:25,  1.41it/s]Extractor Predicting: 130it [01:25,  1.44it/s]Extractor Predicting: 131it [01:26,  1.46it/s]Extractor Predicting: 132it [01:26,  1.49it/s]Extractor Predicting: 133it [01:27,  1.49it/s]Extractor Predicting: 134it [01:28,  1.50it/s]Extractor Predicting: 135it [01:28,  1.50it/s]Extractor Predicting: 136it [01:29,  1.50it/s]Extractor Predicting: 137it [01:30,  1.49it/s]Extractor Predicting: 138it [01:30,  1.49it/s]Extractor Predicting: 139it [01:31,  1.47it/s]Extractor Predicting: 140it [01:32,  1.52it/s]Extractor Predicting: 141it [01:32,  1.50it/s]Extractor Predicting: 142it [01:33,  1.54it/s]Extractor Predicting: 142it [01:33,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:40,402 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:40,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:40,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:40,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:40,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:05:41,031 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:05:41,032 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:05:41,622 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:05:42,661 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:05:42,661 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:45,640 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:45,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:45,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:45,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:05:45,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:05:46,282 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:05:46,283 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:05:46,856 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:05:47,017 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:05:47,018 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.1865222623345367,
  "recall": 0.08874892642427712,
  "score": 0.12027158098933073,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.49it/s]Extractor Predicting: 30it [00:19,  1.46it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:21,  1.45it/s]Extractor Predicting: 33it [00:21,  1.47it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.52it/s]Extractor Predicting: 43it [00:28,  1.36it/s]Extractor Predicting: 44it [00:29,  1.40it/s]Extractor Predicting: 45it [00:29,  1.43it/s]Extractor Predicting: 46it [00:30,  1.47it/s]Extractor Predicting: 47it [00:31,  1.48it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:33,  1.52it/s]Extractor Predicting: 52it [00:34,  1.52it/s]Extractor Predicting: 53it [00:35,  1.52it/s]Extractor Predicting: 54it [00:35,  1.52it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:37,  1.51it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:38,  1.56it/s]Extractor Predicting: 59it [00:38,  1.56it/s]Extractor Predicting: 60it [00:39,  1.56it/s]Extractor Predicting: 61it [00:40,  1.56it/s]Extractor Predicting: 62it [00:40,  1.56it/s]Extractor Predicting: 63it [00:41,  1.56it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.53it/s]Extractor Predicting: 68it [00:44,  1.58it/s]Extractor Predicting: 69it [00:45,  1.57it/s]Extractor Predicting: 70it [00:46,  1.56it/s]Extractor Predicting: 71it [00:46,  1.55it/s]Extractor Predicting: 72it [00:47,  1.55it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:49,  1.52it/s]Extractor Predicting: 76it [00:50,  1.52it/s]Extractor Predicting: 77it [00:50,  1.36it/s]Extractor Predicting: 78it [00:51,  1.41it/s]Extractor Predicting: 79it [00:52,  1.43it/s]Extractor Predicting: 80it [00:52,  1.45it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:54,  1.45it/s]Extractor Predicting: 83it [00:54,  1.47it/s]Extractor Predicting: 84it [00:55,  1.48it/s]Extractor Predicting: 85it [00:56,  1.51it/s]Extractor Predicting: 86it [00:56,  1.48it/s]Extractor Predicting: 87it [00:57,  1.49it/s]Extractor Predicting: 88it [00:58,  1.46it/s]Extractor Predicting: 89it [00:58,  1.47it/s]Extractor Predicting: 90it [00:59,  1.49it/s]Extractor Predicting: 91it [01:00,  1.47it/s]Extractor Predicting: 92it [01:01,  1.47it/s]Extractor Predicting: 93it [01:01,  1.45it/s]Extractor Predicting: 94it [01:02,  1.43it/s]Extractor Predicting: 95it [01:03,  1.44it/s]Extractor Predicting: 96it [01:03,  1.44it/s]Extractor Predicting: 97it [01:04,  1.45it/s]Extractor Predicting: 98it [01:05,  1.45it/s]Extractor Predicting: 99it [01:05,  1.46it/s]Extractor Predicting: 100it [01:06,  1.47it/s]Extractor Predicting: 101it [01:07,  1.46it/s]Extractor Predicting: 102it [01:07,  1.46it/s]Extractor Predicting: 103it [01:08,  1.47it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:09,  1.45it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.44it/s]Extractor Predicting: 109it [01:12,  1.45it/s]Extractor Predicting: 110it [01:13,  1.43it/s]Extractor Predicting: 111it [01:14,  1.43it/s]Extractor Predicting: 112it [01:14,  1.47it/s]Extractor Predicting: 113it [01:15,  1.49it/s]Extractor Predicting: 114it [01:16,  1.50it/s]Extractor Predicting: 115it [01:16,  1.50it/s]Extractor Predicting: 116it [01:17,  1.50it/s]Extractor Predicting: 117it [01:18,  1.51it/s]Extractor Predicting: 118it [01:18,  1.53it/s]Extractor Predicting: 119it [01:19,  1.52it/s]Extractor Predicting: 120it [01:20,  1.54it/s]Extractor Predicting: 121it [01:20,  1.53it/s]Extractor Predicting: 122it [01:21,  1.58it/s]Extractor Predicting: 123it [01:21,  1.57it/s]Extractor Predicting: 124it [01:22,  1.57it/s]Extractor Predicting: 125it [01:23,  1.56it/s]Extractor Predicting: 126it [01:23,  1.54it/s]Extractor Predicting: 127it [01:24,  1.54it/s]Extractor Predicting: 128it [01:25,  1.56it/s]Extractor Predicting: 129it [01:25,  1.52it/s]Extractor Predicting: 130it [01:26,  1.54it/s]Extractor Predicting: 131it [01:27,  1.52it/s]Extractor Predicting: 132it [01:27,  1.53it/s]Extractor Predicting: 133it [01:28,  1.52it/s]Extractor Predicting: 134it [01:29,  1.53it/s]Extractor Predicting: 135it [01:29,  1.56it/s]Extractor Predicting: 136it [01:30,  1.55it/s]Extractor Predicting: 137it [01:31,  1.54it/s]Extractor Predicting: 138it [01:31,  1.54it/s]Extractor Predicting: 139it [01:32,  1.58it/s]Extractor Predicting: 140it [01:32,  1.55it/s]Extractor Predicting: 141it [01:33,  1.53it/s]Extractor Predicting: 142it [01:34,  1.57it/s]Extractor Predicting: 143it [01:34,  1.56it/s]Extractor Predicting: 144it [01:35,  1.54it/s]Extractor Predicting: 145it [01:36,  1.55it/s]Extractor Predicting: 146it [01:36,  1.55it/s]Extractor Predicting: 147it [01:37,  1.52it/s]Extractor Predicting: 148it [01:38,  1.52it/s]Extractor Predicting: 149it [01:38,  1.50it/s]Extractor Predicting: 150it [01:39,  1.53it/s]Extractor Predicting: 151it [01:40,  1.55it/s]Extractor Predicting: 152it [01:40,  1.56it/s]Extractor Predicting: 153it [01:41,  1.56it/s]Extractor Predicting: 154it [01:42,  1.54it/s]Extractor Predicting: 155it [01:42,  1.53it/s]Extractor Predicting: 156it [01:43,  1.52it/s]Extractor Predicting: 157it [01:44,  1.49it/s]Extractor Predicting: 158it [01:44,  1.49it/s]Extractor Predicting: 159it [01:45,  1.50it/s]Extractor Predicting: 160it [01:46,  1.50it/s]Extractor Predicting: 161it [01:46,  1.53it/s]Extractor Predicting: 162it [01:47,  1.54it/s]Extractor Predicting: 163it [01:47,  1.53it/s]Extractor Predicting: 164it [01:48,  1.54it/s]Extractor Predicting: 165it [01:49,  1.52it/s]Extractor Predicting: 166it [01:49,  1.53it/s]Extractor Predicting: 167it [01:50,  1.55it/s]Extractor Predicting: 168it [01:51,  1.54it/s]Extractor Predicting: 169it [01:51,  1.53it/s]Extractor Predicting: 170it [01:52,  1.51it/s]Extractor Predicting: 171it [01:53,  1.47it/s]Extractor Predicting: 172it [01:53,  1.48it/s]Extractor Predicting: 173it [01:54,  1.48it/s]Extractor Predicting: 174it [01:55,  1.44it/s]Extractor Predicting: 175it [01:56,  1.28it/s]Extractor Predicting: 176it [01:57,  1.33it/s]Extractor Predicting: 177it [01:57,  1.36it/s]Extractor Predicting: 178it [01:58,  1.41it/s]Extractor Predicting: 179it [01:59,  1.42it/s]Extractor Predicting: 180it [01:59,  1.48it/s]Extractor Predicting: 181it [02:00,  1.46it/s]Extractor Predicting: 182it [02:01,  1.50it/s]Extractor Predicting: 183it [02:01,  1.49it/s]Extractor Predicting: 184it [02:02,  1.53it/s]Extractor Predicting: 185it [02:02,  1.54it/s]Extractor Predicting: 186it [02:03,  1.55it/s]Extractor Predicting: 187it [02:04,  1.55it/s]Extractor Predicting: 188it [02:04,  1.54it/s]Extractor Predicting: 189it [02:05,  1.55it/s]Extractor Predicting: 190it [02:06,  1.52it/s]Extractor Predicting: 191it [02:06,  1.47it/s]Extractor Predicting: 192it [02:07,  1.49it/s]Extractor Predicting: 193it [02:08,  1.53it/s]Extractor Predicting: 194it [02:08,  1.52it/s]Extractor Predicting: 195it [02:09,  1.52it/s]Extractor Predicting: 196it [02:10,  1.54it/s]Extractor Predicting: 197it [02:10,  1.55it/s]Extractor Predicting: 198it [02:11,  1.52it/s]Extractor Predicting: 199it [02:12,  1.52it/s]Extractor Predicting: 200it [02:12,  1.52it/s]Extractor Predicting: 201it [02:13,  1.52it/s]Extractor Predicting: 202it [02:14,  1.55it/s]Extractor Predicting: 203it [02:14,  1.56it/s]Extractor Predicting: 204it [02:15,  1.55it/s]Extractor Predicting: 205it [02:16,  1.50it/s]Extractor Predicting: 206it [02:16,  1.50it/s]Extractor Predicting: 207it [02:17,  1.53it/s]Extractor Predicting: 208it [02:18,  1.53it/s]Extractor Predicting: 209it [02:18,  1.48it/s]Extractor Predicting: 210it [02:19,  1.47it/s]Extractor Predicting: 211it [02:20,  1.47it/s]Extractor Predicting: 212it [02:20,  1.49it/s]Extractor Predicting: 213it [02:21,  1.51it/s]Extractor Predicting: 214it [02:22,  1.47it/s]Extractor Predicting: 215it [02:22,  1.48it/s]Extractor Predicting: 216it [02:23,  1.51it/s]Extractor Predicting: 217it [02:24,  1.52it/s]Extractor Predicting: 218it [02:24,  1.46it/s]Extractor Predicting: 219it [02:25,  1.48it/s]Extractor Predicting: 220it [02:26,  1.48it/s]Extractor Predicting: 221it [02:26,  1.46it/s]Extractor Predicting: 222it [02:27,  1.46it/s]Extractor Predicting: 223it [02:28,  1.47it/s]Extractor Predicting: 224it [02:28,  1.51it/s]Extractor Predicting: 225it [02:29,  1.50it/s]Extractor Predicting: 226it [02:30,  1.54it/s]Extractor Predicting: 227it [02:30,  1.57it/s]Extractor Predicting: 228it [02:31,  1.54it/s]Extractor Predicting: 229it [02:32,  1.54it/s]Extractor Predicting: 230it [02:32,  1.51it/s]Extractor Predicting: 231it [02:33,  1.51it/s]Extractor Predicting: 232it [02:34,  1.51it/s]Extractor Predicting: 233it [02:34,  1.55it/s]Extractor Predicting: 234it [02:35,  1.53it/s]Extractor Predicting: 235it [02:36,  1.54it/s]Extractor Predicting: 236it [02:36,  1.51it/s]Extractor Predicting: 237it [02:37,  1.51it/s]Extractor Predicting: 238it [02:38,  1.53it/s]Extractor Predicting: 239it [02:38,  1.55it/s]Extractor Predicting: 240it [02:39,  1.54it/s]Extractor Predicting: 241it [02:39,  1.52it/s]Extractor Predicting: 242it [02:40,  1.50it/s]Extractor Predicting: 243it [02:41,  1.47it/s]Extractor Predicting: 244it [02:42,  1.48it/s]Extractor Predicting: 245it [02:42,  1.53it/s]Extractor Predicting: 246it [02:43,  1.51it/s]Extractor Predicting: 247it [02:43,  1.53it/s]Extractor Predicting: 248it [02:44,  1.53it/s]Extractor Predicting: 249it [02:45,  1.53it/s]Extractor Predicting: 250it [02:45,  1.52it/s]Extractor Predicting: 251it [02:46,  1.49it/s]Extractor Predicting: 252it [02:47,  1.49it/s]Extractor Predicting: 253it [02:47,  1.50it/s]Extractor Predicting: 254it [02:48,  1.52it/s]Extractor Predicting: 255it [02:49,  1.52it/s]Extractor Predicting: 256it [02:49,  1.50it/s]Extractor Predicting: 257it [02:50,  1.52it/s]Extractor Predicting: 258it [02:51,  1.52it/s]Extractor Predicting: 259it [02:51,  1.48it/s]Extractor Predicting: 260it [02:52,  1.50it/s]Extractor Predicting: 261it [02:53,  1.52it/s]Extractor Predicting: 262it [02:53,  1.50it/s]Extractor Predicting: 263it [02:54,  1.49it/s]Extractor Predicting: 264it [02:55,  1.49it/s]Extractor Predicting: 265it [02:55,  1.48it/s]Extractor Predicting: 266it [02:56,  1.46it/s]Extractor Predicting: 267it [02:57,  1.44it/s]Extractor Predicting: 268it [02:58,  1.46it/s]Extractor Predicting: 269it [02:58,  1.46it/s]Extractor Predicting: 270it [02:59,  1.45it/s]Extractor Predicting: 271it [03:00,  1.46it/s]Extractor Predicting: 272it [03:00,  1.46it/s]Extractor Predicting: 273it [03:01,  1.31it/s]Extractor Predicting: 274it [03:02,  1.35it/s]Extractor Predicting: 275it [03:03,  1.41it/s]Extractor Predicting: 276it [03:03,  1.42it/s]Extractor Predicting: 277it [03:04,  1.42it/s]Extractor Predicting: 278it [03:05,  1.42it/s]Extractor Predicting: 279it [03:05,  1.43it/s]Extractor Predicting: 280it [03:06,  1.43it/s]Extractor Predicting: 281it [03:07,  1.42it/s]Extractor Predicting: 282it [03:07,  1.45it/s]Extractor Predicting: 283it [03:08,  1.41it/s]Extractor Predicting: 284it [03:09,  1.39it/s]Extractor Predicting: 285it [03:10,  1.40it/s]Extractor Predicting: 286it [03:10,  1.39it/s]Extractor Predicting: 287it [03:11,  1.41it/s]Extractor Predicting: 288it [03:11,  1.86it/s]Extractor Predicting: 288it [03:11,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:06,609 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:06,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:06,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:06,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:06,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:09:07,214 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:09:07,215 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:09:08,015 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:09:09,061 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:09:09,065 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:12,010 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:12,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:12,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:12,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:09:12,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:09:12,668 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:09:12,671 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:09:13,290 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:09:13,453 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:09:13,453 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4646814404432133,
  "recall": 0.19480330962403833,
  "score": 0.274521836964304,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.62it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5217391304347826,
  "recall": 0.10810810810810811,
  "score": 0.17910447761194032,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:20<04:43, 20.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:37<04:02, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:58<03:53, 19.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:19<03:42, 20.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:41<03:27, 20.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [02:00<03:03, 20.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:23<02:49, 21.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:46<02:30, 21.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [03:08<02:09, 21.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:27<01:44, 20.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:45<01:20, 20.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [04:05<01:00, 20.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [04:23<00:38, 19.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:42<00:19, 19.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [05:04<00:00, 19.99s/it]Generating: 100%|██████████| 15/15 [05:04<00:00, 20.27s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 308, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 576, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 622, 'raw': 832}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7475961538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Queen', 'headquarters location', '', 'At the time , she was known as Queen Charles I of France , and was the Queen ( French origin ) of France .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.7916666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 445, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 564, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Boris Johnson', 'member of political party', '', 'The constituency of David Harsany at Southwark was represented at the 1992 General Election by Boris Johnson .')"}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 337, 'raw': 480}
{'target': 600, 'success': 358, 'raw': 512}
{'target': 600, 'success': 379, 'raw': 544}
{'target': 600, 'success': 399, 'raw': 576}
{'target': 600, 'success': 418, 'raw': 608}
{'target': 600, 'success': 442, 'raw': 640}
{'target': 600, 'success': 461, 'raw': 672}
{'target': 600, 'success': 483, 'raw': 704}
{'target': 600, 'success': 507, 'raw': 736}
{'target': 600, 'success': 527, 'raw': 768}
{'target': 600, 'success': 549, 'raw': 800}
{'target': 600, 'success': 568, 'raw': 832}
{'target': 600, 'success': 592, 'raw': 864}
{'target': 600, 'success': 614, 'raw': 896}
{'prompt': 'Relation : narrative location .', 'success_rate': 0.6852678571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : notable work . Context : Later in Life , he studied with a great deal of German and Italian poets whom at different times had had success in poetry among the people in the Empire , and with other poets , including Frédéric Bastard . Head Entity : Georges Bastard , Tail Entity : poems .\n']
['Relation : notable work . Context : Later in Life , he studied with a great deal of German and Italian poets whom at different times had had success in poetry among the people in the Empire , and with other poets , including Frédéric Bastard . Head Entity : Georges Bastard , Tail Entity : poems .\n', 'Relation : notable work . Context : James Anderson and Frederick C. Johnson ( 1721 1839 ) were the first English settlers to colonize New York City . Head Entity : James Anderson , Tail Entity : James Alfred Johnson .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : notable work .', 'success_rate': 0.75625, 'errors': {'', "('Institute of Politics', 'notable work', '', 'In 1992 , he founded the Institute of Politics .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 189, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 494, 'raw': 672}
{'target': 600, 'success': 514, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 572, 'raw': 800}
{'target': 600, 'success': 597, 'raw': 832}
{'target': 600, 'success': 621, 'raw': 864}
{'prompt': 'Relation : applies to jurisdiction .', 'success_rate': 0.71875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 177, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 223, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 274, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 319, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 492, 'raw': 672}
{'target': 600, 'success': 517, 'raw': 704}
{'target': 600, 'success': 544, 'raw': 736}
{'target': 600, 'success': 566, 'raw': 768}
{'target': 600, 'success': 589, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : family name .', 'success_rate': 0.7295673076923077, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 104, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 172, 'raw': 256}
{'target': 600, 'success': 194, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 232, 'raw': 352}
{'target': 600, 'success': 252, 'raw': 384}
{'target': 600, 'success': 272, 'raw': 416}
{'target': 600, 'success': 292, 'raw': 448}
{'target': 600, 'success': 310, 'raw': 480}
{'target': 600, 'success': 334, 'raw': 512}
{'target': 600, 'success': 354, 'raw': 544}
{'target': 600, 'success': 376, 'raw': 576}
{'target': 600, 'success': 389, 'raw': 608}
{'target': 600, 'success': 409, 'raw': 640}
{'target': 600, 'success': 431, 'raw': 672}
{'target': 600, 'success': 448, 'raw': 704}
{'target': 600, 'success': 465, 'raw': 736}
{'target': 600, 'success': 484, 'raw': 768}
{'target': 600, 'success': 504, 'raw': 800}
{'target': 600, 'success': 524, 'raw': 832}
{'target': 600, 'success': 543, 'raw': 864}
{'target': 600, 'success': 564, 'raw': 896}
{'target': 600, 'success': 583, 'raw': 928}
{'target': 600, 'success': 603, 'raw': 960}
{'prompt': 'Relation : genre .', 'success_rate': 0.628125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 137, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 234, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 534, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 583, 'raw': 800}
{'target': 600, 'success': 610, 'raw': 832}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.7331730769230769, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('languages', 'is a list of', '', 'The following languages are included in this definition , and are not considered to be included .')", 'too many values to unpack (expected 2)', "('network', 'is a list of', '', 'It has been described as a network of a single host ( .')", "('document', 'is a list of', '', 'The second most popular type of data is the number of characters in the document ( ) followed by the number ( ) then the document .')"}}
['Relation : located in the administrative territorial entity . Context : The city of Buenos Aires , in the province of Paracel , is located at the end of the Pacific Ocean on the Pacific Ocean in the Argentinian Caribbean Sea . Head Entity : Buenos Aires , Tail Entity : Argentine .\n']
['Relation : located in the administrative territorial entity . Context : The city of Buenos Aires , in the province of Paracel , is located at the end of the Pacific Ocean on the Pacific Ocean in the Argentinian Caribbean Sea . Head Entity : Buenos Aires , Tail Entity : Argentine .\n', 'Relation : located in the administrative territorial entity . Context : After the death of King Henry IV of France ( 9 January 1789 &ndash; 6 December 1765 ) , the castle was demolished as a separate structure . Head Entity : Kingdom , Tail Entity : French .\n']
['Relation : located in the administrative territorial entity . Context : The city of Buenos Aires , in the province of Paracel , is located at the end of the Pacific Ocean on the Pacific Ocean in the Argentinian Caribbean Sea . Head Entity : Buenos Aires , Tail Entity : Argentine .\n', 'Relation : located in the administrative territorial entity . Context : After the death of King Henry IV of France ( 9 January 1789 &ndash; 6 December 1765 ) , the castle was demolished as a separate structure . Head Entity : Kingdom , Tail Entity : French .\n', 'Relation : located in the administrative territorial entity . Context : This was the first to establish one of the most common forms of ethnic cleansing of Muslim people in Iran and Iraq during World War II . Head Entity : Iraq , Tail Entity : Tehran .\n']
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 135, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 254, 'raw': 352}
{'target': 600, 'success': 279, 'raw': 384}
{'target': 600, 'success': 303, 'raw': 416}
{'target': 600, 'success': 325, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 404, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 450, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 526, 'raw': 704}
{'target': 600, 'success': 552, 'raw': 736}
{'target': 600, 'success': 577, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.75375, 'errors': {'', "('Its orbit', 'located on astronomical body', '', 'Its orbit shows an eccentricity of .')"}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 544, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in Life , he played in The New Yorker magazine for two weeks while writing a series of articles for New York Daily News . Head Entity : The New Yorker , Tail Entity : Robert Ludlum .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 622, 'raw': 800}
{'prompt': 'Relation : member of .', 'success_rate': 0.7775, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('William Shatner', 'member of', '', 'In the 1990s , the project began playing with the release of album One Dance and came to the attention of producer and actor William Shatner .')"}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 325, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 434, 'raw': 608}
{'target': 600, 'success': 457, 'raw': 640}
{'target': 600, 'success': 478, 'raw': 672}
{'target': 600, 'success': 500, 'raw': 704}
{'target': 600, 'success': 522, 'raw': 736}
{'target': 600, 'success': 546, 'raw': 768}
{'target': 600, 'success': 567, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : use .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)', "('Elizabeth I', 'use', '', 'The first of her children was Elizabeth I , as the second daughter of Lord James I of England .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 15153
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15253, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.99s/it]Extractor Estimating: 2it [00:19,  8.75s/it]Extractor Estimating: 3it [00:20,  5.03s/it]Extractor Estimating: 4it [00:21,  3.34s/it]Extractor Estimating: 5it [00:21,  2.34s/it]Extractor Estimating: 6it [00:22,  1.77s/it]Extractor Estimating: 7it [00:23,  1.40s/it]Extractor Estimating: 8it [00:23,  1.14s/it]Extractor Estimating: 9it [00:24,  1.02it/s]Extractor Estimating: 10it [00:25,  1.14it/s]Extractor Estimating: 11it [00:25,  1.24it/s]Extractor Estimating: 12it [00:26,  1.34it/s]Extractor Estimating: 13it [00:26,  1.40it/s]Extractor Estimating: 14it [00:27,  1.38it/s]Extractor Estimating: 15it [00:28,  1.46it/s]Extractor Estimating: 16it [00:28,  1.52it/s]Extractor Estimating: 17it [00:30,  1.01s/it]Extractor Estimating: 18it [00:31,  1.12it/s]Extractor Estimating: 19it [00:31,  1.21it/s]Extractor Estimating: 20it [00:32,  1.31it/s]Extractor Estimating: 21it [00:33,  1.36it/s]Extractor Estimating: 22it [00:33,  1.39it/s]Extractor Estimating: 23it [00:34,  1.42it/s]Extractor Estimating: 24it [00:35,  1.46it/s]Extractor Estimating: 25it [00:35,  1.50it/s]Extractor Estimating: 26it [00:36,  1.51it/s]Extractor Estimating: 27it [00:37,  1.50it/s]Extractor Estimating: 28it [00:37,  1.54it/s]Extractor Estimating: 29it [00:38,  1.55it/s]Extractor Estimating: 30it [00:39,  1.58it/s]Extractor Estimating: 31it [00:39,  1.53it/s]Extractor Estimating: 32it [00:40,  1.54it/s]Extractor Estimating: 33it [00:41,  1.53it/s]Extractor Estimating: 34it [00:41,  1.52it/s]Extractor Estimating: 35it [00:42,  1.55it/s]Extractor Estimating: 36it [00:42,  1.58it/s]Extractor Estimating: 37it [00:43,  1.60it/s]Extractor Estimating: 38it [00:44,  1.58it/s]Extractor Estimating: 39it [00:44,  1.58it/s]Extractor Estimating: 40it [00:45,  1.62it/s]Extractor Estimating: 41it [00:46,  1.58it/s]Extractor Estimating: 42it [00:46,  1.57it/s]Extractor Estimating: 43it [00:47,  1.59it/s]Extractor Estimating: 44it [00:48,  1.55it/s]Extractor Estimating: 45it [00:48,  1.51it/s]Extractor Estimating: 46it [00:49,  1.54it/s]Extractor Estimating: 47it [00:50,  1.54it/s]Extractor Estimating: 48it [00:50,  1.51it/s]Extractor Estimating: 49it [00:51,  1.57it/s]Extractor Estimating: 50it [00:52,  1.52it/s]Extractor Estimating: 51it [00:52,  1.55it/s]Extractor Estimating: 52it [00:53,  1.61it/s]Extractor Estimating: 53it [00:53,  1.58it/s]Extractor Estimating: 54it [00:54,  1.58it/s]Extractor Estimating: 55it [00:55,  1.56it/s]Extractor Estimating: 56it [00:55,  1.58it/s]Extractor Estimating: 57it [00:56,  1.55it/s]Extractor Estimating: 58it [00:57,  1.59it/s]Extractor Estimating: 59it [00:57,  1.58it/s]Extractor Estimating: 60it [00:58,  1.55it/s]Extractor Estimating: 61it [00:59,  1.50it/s]Extractor Estimating: 62it [00:59,  1.46it/s]Extractor Estimating: 63it [01:00,  1.51it/s]Extractor Estimating: 64it [01:01,  1.50it/s]Extractor Estimating: 65it [01:01,  1.56it/s]Extractor Estimating: 66it [01:02,  1.60it/s]Extractor Estimating: 67it [01:02,  1.59it/s]Extractor Estimating: 68it [01:03,  1.59it/s]Extractor Estimating: 69it [01:04,  1.57it/s]Extractor Estimating: 70it [01:04,  1.58it/s]Extractor Estimating: 71it [01:05,  1.55it/s]Extractor Estimating: 72it [01:06,  1.53it/s]Extractor Estimating: 73it [01:06,  1.50it/s]Extractor Estimating: 74it [01:07,  1.53it/s]Extractor Estimating: 75it [01:08,  1.56it/s]Extractor Estimating: 76it [01:08,  1.51it/s]Extractor Estimating: 77it [01:09,  1.40it/s]Extractor Estimating: 78it [01:10,  1.39it/s]Extractor Estimating: 79it [01:11,  1.39it/s]Extractor Estimating: 80it [01:11,  1.42it/s]Extractor Estimating: 81it [01:12,  1.45it/s]Extractor Estimating: 82it [01:13,  1.42it/s]Extractor Estimating: 83it [01:13,  1.43it/s]Extractor Estimating: 84it [01:14,  1.46it/s]Extractor Estimating: 85it [01:15,  1.48it/s]Extractor Estimating: 86it [01:15,  1.50it/s]Extractor Estimating: 87it [01:16,  1.50it/s]Extractor Estimating: 88it [01:17,  1.51it/s]Extractor Estimating: 89it [01:17,  1.47it/s]Extractor Estimating: 90it [01:18,  1.45it/s]Extractor Estimating: 91it [01:19,  1.46it/s]Extractor Estimating: 92it [01:19,  1.46it/s]Extractor Estimating: 93it [01:20,  1.44it/s]Extractor Estimating: 94it [01:21,  1.46it/s]Extractor Estimating: 95it [01:21,  1.47it/s]Extractor Estimating: 96it [01:22,  1.47it/s]Extractor Estimating: 97it [01:23,  1.45it/s]Extractor Estimating: 98it [01:23,  1.49it/s]Extractor Estimating: 99it [01:24,  1.45it/s]Extractor Estimating: 100it [01:25,  1.46it/s]Extractor Estimating: 101it [01:26,  1.45it/s]Extractor Estimating: 102it [01:26,  1.45it/s]Extractor Estimating: 103it [01:28,  1.08s/it]Extractor Estimating: 104it [01:29,  1.04it/s]Extractor Estimating: 105it [01:30,  1.15it/s]Extractor Estimating: 106it [01:30,  1.22it/s]Extractor Estimating: 107it [01:31,  1.27it/s]Extractor Estimating: 108it [01:32,  1.30it/s]Extractor Estimating: 109it [01:33,  1.28it/s]Extractor Estimating: 110it [01:33,  1.27it/s]Extractor Estimating: 111it [01:34,  1.35it/s]Extractor Estimating: 112it [01:35,  1.33it/s]Extractor Estimating: 113it [01:35,  1.37it/s]Extractor Estimating: 114it [01:36,  1.37it/s]Extractor Estimating: 115it [01:37,  1.39it/s]Extractor Estimating: 116it [01:37,  1.44it/s]Extractor Estimating: 117it [01:38,  1.45it/s]Extractor Estimating: 118it [01:39,  1.42it/s]Extractor Estimating: 119it [01:40,  1.44it/s]Extractor Estimating: 120it [01:41,  1.08it/s]Extractor Estimating: 121it [01:42,  1.19it/s]Extractor Estimating: 122it [01:42,  1.29it/s]Extractor Estimating: 123it [01:43,  1.31it/s]Extractor Estimating: 124it [01:44,  1.39it/s]Extractor Estimating: 125it [01:44,  1.39it/s]Extractor Estimating: 126it [01:45,  1.45it/s]Extractor Estimating: 127it [01:46,  1.47it/s]Extractor Estimating: 128it [01:46,  1.53it/s]Extractor Estimating: 129it [01:47,  1.52it/s]Extractor Estimating: 130it [01:47,  1.56it/s]Extractor Estimating: 131it [01:48,  1.59it/s]Extractor Estimating: 132it [01:49,  1.64it/s]Extractor Estimating: 133it [01:49,  1.65it/s]Extractor Estimating: 134it [01:50,  1.67it/s]Extractor Estimating: 135it [01:50,  1.64it/s]Extractor Estimating: 136it [01:51,  1.61it/s]Extractor Estimating: 137it [01:52,  1.60it/s]Extractor Estimating: 138it [01:52,  1.56it/s]Extractor Estimating: 139it [01:53,  1.54it/s]Extractor Estimating: 140it [01:54,  1.57it/s]Extractor Estimating: 141it [01:54,  1.57it/s]Extractor Estimating: 142it [01:55,  1.56it/s]Extractor Estimating: 143it [01:56,  1.57it/s]Extractor Estimating: 144it [01:56,  1.57it/s]Extractor Estimating: 145it [01:57,  1.54it/s]Extractor Estimating: 146it [01:58,  1.55it/s]Extractor Estimating: 147it [01:58,  1.54it/s]Extractor Estimating: 148it [01:59,  1.52it/s]Extractor Estimating: 149it [02:00,  1.52it/s]Extractor Estimating: 150it [02:00,  1.53it/s]Extractor Estimating: 151it [02:01,  1.44it/s]Extractor Estimating: 152it [02:02,  1.46it/s]Extractor Estimating: 153it [02:02,  1.49it/s]Extractor Estimating: 154it [02:03,  1.46it/s]Extractor Estimating: 155it [02:04,  1.45it/s]Extractor Estimating: 156it [02:05,  1.33it/s]Extractor Estimating: 157it [02:05,  1.38it/s]Extractor Estimating: 158it [02:06,  1.38it/s]Extractor Estimating: 159it [02:07,  1.36it/s]Extractor Estimating: 160it [02:07,  1.35it/s]Extractor Estimating: 161it [02:08,  1.39it/s]Extractor Estimating: 162it [02:09,  1.43it/s]Extractor Estimating: 163it [02:09,  1.44it/s]Extractor Estimating: 164it [02:10,  1.45it/s]Extractor Estimating: 165it [02:11,  1.40it/s]Extractor Estimating: 166it [02:12,  1.38it/s]Extractor Estimating: 167it [02:12,  1.44it/s]Extractor Estimating: 168it [02:13,  1.48it/s]Extractor Estimating: 169it [02:14,  1.50it/s]Extractor Estimating: 170it [02:14,  1.55it/s]Extractor Estimating: 171it [02:15,  1.52it/s]Extractor Estimating: 172it [02:16,  1.51it/s]Extractor Estimating: 173it [02:16,  1.48it/s]Extractor Estimating: 174it [02:17,  1.46it/s]Extractor Estimating: 175it [02:18,  1.50it/s]Extractor Estimating: 176it [02:18,  1.52it/s]Extractor Estimating: 177it [02:19,  1.55it/s]Extractor Estimating: 178it [02:20,  1.52it/s]Extractor Estimating: 179it [02:20,  1.49it/s]Extractor Estimating: 180it [02:21,  1.51it/s]Extractor Estimating: 181it [02:21,  1.57it/s]Extractor Estimating: 182it [02:22,  1.59it/s]Extractor Estimating: 183it [02:23,  1.54it/s]Extractor Estimating: 184it [02:23,  1.57it/s]Extractor Estimating: 185it [02:24,  1.53it/s]Extractor Estimating: 186it [02:25,  1.51it/s]Extractor Estimating: 187it [02:25,  1.51it/s]Extractor Estimating: 188it [02:26,  1.51it/s]Extractor Estimating: 189it [02:27,  1.50it/s]Extractor Estimating: 190it [02:27,  1.51it/s]Extractor Estimating: 191it [02:28,  1.56it/s]Extractor Estimating: 192it [02:29,  1.55it/s]Extractor Estimating: 193it [02:29,  1.51it/s]Extractor Estimating: 194it [02:30,  1.47it/s]Extractor Estimating: 195it [02:31,  1.50it/s]Extractor Estimating: 196it [02:31,  1.52it/s]Extractor Estimating: 197it [02:32,  1.49it/s]Extractor Estimating: 198it [02:33,  1.48it/s]Extractor Estimating: 199it [02:34,  1.41it/s]Extractor Estimating: 200it [02:34,  1.45it/s]Extractor Estimating: 201it [02:35,  1.28it/s]Extractor Estimating: 202it [02:36,  1.37it/s]Extractor Estimating: 203it [02:36,  1.46it/s]Extractor Estimating: 204it [02:37,  1.44it/s]Extractor Estimating: 205it [02:38,  1.44it/s]Extractor Estimating: 206it [02:38,  1.48it/s]Extractor Estimating: 207it [02:39,  1.53it/s]Extractor Estimating: 208it [02:39,  1.63it/s]Extractor Estimating: 209it [02:40,  1.60it/s]Extractor Estimating: 210it [02:41,  1.53it/s]Extractor Estimating: 211it [02:42,  1.52it/s]Extractor Estimating: 212it [02:42,  1.59it/s]Extractor Estimating: 213it [02:43,  1.56it/s]Extractor Estimating: 214it [02:43,  1.61it/s]Extractor Estimating: 215it [02:44,  1.60it/s]Extractor Estimating: 216it [02:45,  1.54it/s]Extractor Estimating: 217it [02:45,  1.54it/s]Extractor Estimating: 218it [02:46,  1.56it/s]Extractor Estimating: 219it [02:47,  1.58it/s]Extractor Estimating: 220it [02:47,  1.49it/s]Extractor Estimating: 221it [02:48,  1.49it/s]Extractor Estimating: 222it [02:49,  1.29it/s]Extractor Estimating: 223it [02:50,  1.36it/s]Extractor Estimating: 224it [02:50,  1.45it/s]Extractor Estimating: 225it [02:51,  1.49it/s]Extractor Estimating: 226it [02:52,  1.43it/s]Extractor Estimating: 227it [02:52,  1.47it/s]Extractor Estimating: 228it [02:53,  1.52it/s]Extractor Estimating: 229it [02:53,  1.58it/s]Extractor Estimating: 230it [02:54,  1.56it/s]Extractor Estimating: 231it [02:55,  1.65it/s]Extractor Estimating: 232it [02:55,  1.61it/s]Extractor Estimating: 233it [02:56,  1.53it/s]Extractor Estimating: 234it [02:57,  1.55it/s]Extractor Estimating: 235it [02:57,  1.42it/s]Extractor Estimating: 236it [02:58,  1.50it/s]Extractor Estimating: 237it [02:59,  1.54it/s]Extractor Estimating: 238it [02:59,  1.58it/s]Extractor Estimating: 239it [03:00,  1.59it/s]Extractor Estimating: 240it [03:00,  1.67it/s]Extractor Estimating: 241it [03:01,  1.72it/s]Extractor Estimating: 242it [03:02,  1.67it/s]Extractor Estimating: 243it [03:02,  1.65it/s]Extractor Estimating: 244it [03:03,  1.66it/s]Extractor Estimating: 245it [03:03,  1.66it/s]Extractor Estimating: 246it [03:04,  1.61it/s]Extractor Estimating: 247it [03:05,  1.61it/s]Extractor Estimating: 248it [03:05,  1.64it/s]Extractor Estimating: 249it [03:06,  1.62it/s]Extractor Estimating: 250it [03:06,  1.67it/s]Extractor Estimating: 251it [03:07,  1.65it/s]Extractor Estimating: 252it [03:08,  1.67it/s]Extractor Estimating: 253it [03:08,  1.66it/s]Extractor Estimating: 254it [03:09,  1.67it/s]Extractor Estimating: 255it [03:09,  1.66it/s]Extractor Estimating: 256it [03:10,  1.68it/s]Extractor Estimating: 257it [03:11,  1.68it/s]Extractor Estimating: 258it [03:11,  1.58it/s]Extractor Estimating: 259it [03:12,  1.62it/s]Extractor Estimating: 260it [03:13,  1.58it/s]Extractor Estimating: 261it [03:13,  1.61it/s]Extractor Estimating: 262it [03:14,  1.65it/s]Extractor Estimating: 263it [03:15,  1.52it/s]Extractor Estimating: 264it [03:15,  1.56it/s]Extractor Estimating: 265it [03:16,  1.55it/s]Extractor Estimating: 266it [03:16,  1.55it/s]Extractor Estimating: 267it [03:17,  1.58it/s]Extractor Estimating: 268it [03:18,  1.59it/s]Extractor Estimating: 269it [03:18,  1.62it/s]Extractor Estimating: 270it [03:19,  1.64it/s]Extractor Estimating: 271it [03:20,  1.63it/s]Extractor Estimating: 272it [03:20,  1.65it/s]Extractor Estimating: 273it [03:21,  1.60it/s]Extractor Estimating: 274it [03:21,  1.57it/s]Extractor Estimating: 275it [03:22,  1.60it/s]Extractor Estimating: 276it [03:23,  1.58it/s]Extractor Estimating: 277it [03:23,  1.48it/s]Extractor Estimating: 278it [03:24,  1.47it/s]Extractor Estimating: 279it [03:25,  1.48it/s]Extractor Estimating: 280it [03:25,  1.52it/s]Extractor Estimating: 281it [03:26,  1.55it/s]Extractor Estimating: 282it [03:27,  1.48it/s]Extractor Estimating: 283it [03:27,  1.48it/s]Extractor Estimating: 284it [03:28,  1.48it/s]Extractor Estimating: 285it [03:29,  1.29it/s]Extractor Estimating: 286it [03:30,  1.36it/s]Extractor Estimating: 287it [03:30,  1.40it/s]Extractor Estimating: 288it [03:31,  1.46it/s]Extractor Estimating: 289it [03:32,  1.47it/s]Extractor Estimating: 290it [03:32,  1.51it/s]Extractor Estimating: 291it [03:33,  1.53it/s]Extractor Estimating: 292it [03:34,  1.56it/s]Extractor Estimating: 293it [03:34,  1.56it/s]Extractor Estimating: 294it [03:35,  1.52it/s]Extractor Estimating: 295it [03:36,  1.48it/s]Extractor Estimating: 296it [03:36,  1.47it/s]Extractor Estimating: 297it [03:37,  1.45it/s]Extractor Estimating: 298it [03:38,  1.37it/s]Extractor Estimating: 299it [03:39,  1.40it/s]Extractor Estimating: 300it [03:39,  1.45it/s]Extractor Estimating: 301it [03:40,  1.55it/s]Extractor Estimating: 302it [03:40,  1.59it/s]Extractor Estimating: 303it [03:41,  1.60it/s]Extractor Estimating: 304it [03:42,  1.61it/s]Extractor Estimating: 305it [03:42,  1.59it/s]Extractor Estimating: 306it [03:43,  1.60it/s]Extractor Estimating: 307it [03:43,  1.63it/s]Extractor Estimating: 308it [03:44,  1.66it/s]Extractor Estimating: 309it [03:45,  1.66it/s]Extractor Estimating: 310it [03:45,  1.47it/s]Extractor Estimating: 311it [03:46,  1.48it/s]Extractor Estimating: 312it [03:47,  1.55it/s]Extractor Estimating: 313it [03:47,  1.60it/s]Extractor Estimating: 314it [03:48,  1.61it/s]Extractor Estimating: 315it [03:49,  1.60it/s]Extractor Estimating: 316it [03:49,  1.63it/s]Extractor Estimating: 317it [03:50,  1.64it/s]Extractor Estimating: 318it [03:50,  1.69it/s]Extractor Estimating: 319it [03:51,  1.67it/s]Extractor Estimating: 320it [03:52,  1.63it/s]Extractor Estimating: 321it [03:52,  1.64it/s]Extractor Estimating: 322it [03:53,  1.67it/s]Extractor Estimating: 323it [03:53,  1.68it/s]Extractor Estimating: 324it [03:54,  1.67it/s]Extractor Estimating: 325it [03:54,  1.68it/s]Extractor Estimating: 326it [03:55,  1.61it/s]Extractor Estimating: 327it [03:56,  1.55it/s]Extractor Estimating: 328it [03:57,  1.54it/s]Extractor Estimating: 329it [03:57,  1.54it/s]Extractor Estimating: 330it [03:58,  1.59it/s]Extractor Estimating: 331it [03:58,  1.58it/s]Extractor Estimating: 332it [03:59,  1.60it/s]Extractor Estimating: 333it [04:00,  1.55it/s]Extractor Estimating: 334it [04:00,  1.55it/s]Extractor Estimating: 335it [04:01,  1.56it/s]Extractor Estimating: 336it [04:02,  1.56it/s]Extractor Estimating: 337it [04:02,  1.59it/s]Extractor Estimating: 338it [04:03,  1.50it/s]Extractor Estimating: 339it [04:04,  1.48it/s]Extractor Estimating: 340it [04:04,  1.49it/s]Extractor Estimating: 341it [04:05,  1.49it/s]Extractor Estimating: 342it [04:06,  1.48it/s]Extractor Estimating: 343it [04:06,  1.46it/s]Extractor Estimating: 344it [04:07,  1.51it/s]Extractor Estimating: 345it [04:08,  1.52it/s]Extractor Estimating: 346it [04:08,  1.50it/s]Extractor Estimating: 347it [04:09,  1.47it/s]Extractor Estimating: 348it [04:10,  1.49it/s]Extractor Estimating: 349it [04:10,  1.47it/s]Extractor Estimating: 350it [04:11,  1.48it/s]Extractor Estimating: 351it [04:12,  1.47it/s]Extractor Estimating: 352it [04:13,  1.41it/s]Extractor Estimating: 353it [04:13,  1.49it/s]Extractor Estimating: 354it [04:14,  1.54it/s]Extractor Estimating: 355it [04:14,  1.48it/s]Extractor Estimating: 356it [04:15,  1.58it/s]Extractor Estimating: 357it [04:16,  1.58it/s]Extractor Estimating: 358it [04:16,  1.54it/s]Extractor Estimating: 359it [04:17,  1.53it/s]Extractor Estimating: 360it [04:18,  1.52it/s]Extractor Estimating: 361it [04:18,  1.51it/s]Extractor Estimating: 362it [04:19,  1.50it/s]Extractor Estimating: 363it [04:20,  1.49it/s]Extractor Estimating: 364it [04:20,  1.49it/s]Extractor Estimating: 365it [04:21,  1.57it/s]Extractor Estimating: 366it [04:21,  1.60it/s]Extractor Estimating: 367it [04:22,  1.67it/s]Extractor Estimating: 368it [04:23,  1.59it/s]Extractor Estimating: 369it [04:23,  1.58it/s]Extractor Estimating: 370it [04:24,  1.57it/s]Extractor Estimating: 371it [04:25,  1.42it/s]Extractor Estimating: 372it [04:26,  1.46it/s]Extractor Estimating: 373it [04:26,  1.49it/s]Extractor Estimating: 374it [04:27,  1.53it/s]Extractor Estimating: 375it [04:27,  1.48it/s]Extractor Estimating: 375it [04:27,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7388 mean pseudo reward: 0.9602549743081406
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl'}
train vocab size: 28609
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28709, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28709, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.407, loss:2871.8162
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.154, loss:2076.4100
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.095, loss:1774.9304
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 92, avg_time 1.118, loss:1629.7162
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 192, avg_time 1.105, loss:1578.2622
>> valid entity prec:0.5421, rec:0.5470, f1:0.5446
>> valid relation prec:0.4759, rec:0.0570, f1:0.1019
>> valid relation with NER prec:0.4759, rec:0.0570, f1:0.1019
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 292, avg_time 2.920, loss:1511.0415
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 84, avg_time 1.105, loss:1413.6229
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 184, avg_time 1.109, loss:1411.4527
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 284, avg_time 1.114, loss:1368.4220
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 76, avg_time 1.122, loss:1266.2399
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4806, rec:0.5632, f1:0.5186
>> valid relation prec:0.5733, rec:0.0708, f1:0.1261
>> valid relation with NER prec:0.5733, rec:0.0708, f1:0.1261
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 176, avg_time 2.909, loss:1229.4436
g_step 1200, step 276, avg_time 1.114, loss:1172.1738
g_step 1300, step 68, avg_time 1.123, loss:1187.8001
g_step 1400, step 168, avg_time 1.103, loss:1123.3613
g_step 1500, step 268, avg_time 1.113, loss:1097.9045
>> valid entity prec:0.5334, rec:0.4763, f1:0.5032
>> valid relation prec:0.4074, rec:0.0249, f1:0.0470
>> valid relation with NER prec:0.4074, rec:0.0249, f1:0.0470
g_step 1600, step 60, avg_time 2.909, loss:1062.0921
g_step 1700, step 160, avg_time 1.125, loss:1035.5948
g_step 1800, step 260, avg_time 1.108, loss:1053.8905
g_step 1900, step 52, avg_time 1.104, loss:1035.9404
g_step 2000, step 152, avg_time 1.077, loss:974.4035
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6235, rec:0.3632, f1:0.4590
>> valid relation prec:0.4658, rec:0.0743, f1:0.1282
>> valid relation with NER prec:0.4658, rec:0.0743, f1:0.1282
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 252, avg_time 2.821, loss:981.7599
g_step 2200, step 44, avg_time 1.089, loss:999.1233
g_step 2300, step 144, avg_time 1.065, loss:916.1411
g_step 2400, step 244, avg_time 1.082, loss:972.4379
g_step 2500, step 36, avg_time 1.085, loss:923.9632
>> valid entity prec:0.5013, rec:0.5303, f1:0.5154
>> valid relation prec:0.3525, rec:0.0733, f1:0.1214
>> valid relation with NER prec:0.3525, rec:0.0733, f1:0.1214
g_step 2600, step 136, avg_time 2.860, loss:890.3619
g_step 2700, step 236, avg_time 1.069, loss:923.8774
g_step 2800, step 28, avg_time 1.072, loss:894.0299
g_step 2900, step 128, avg_time 1.070, loss:868.6834
g_step 3000, step 228, avg_time 1.076, loss:862.5953
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5169, rec:0.4902, f1:0.5032
>> valid relation prec:0.3111, rec:0.0721, f1:0.1170
>> valid relation with NER prec:0.3111, rec:0.0721, f1:0.1170
g_step 3100, step 20, avg_time 2.847, loss:865.9820
g_step 3200, step 120, avg_time 1.084, loss:823.0823
g_step 3300, step 220, avg_time 1.070, loss:833.7663
g_step 3400, step 12, avg_time 1.094, loss:832.2032
g_step 3500, step 112, avg_time 1.075, loss:808.2967
>> valid entity prec:0.5236, rec:0.4276, f1:0.4707
>> valid relation prec:0.3509, rec:0.0616, f1:0.1048
>> valid relation with NER prec:0.3509, rec:0.0616, f1:0.1048
g_step 3600, step 212, avg_time 2.841, loss:800.0719
g_step 3700, step 4, avg_time 1.076, loss:821.0031
g_step 3800, step 104, avg_time 1.079, loss:778.3539
g_step 3900, step 204, avg_time 1.068, loss:766.2111
g_step 4000, step 304, avg_time 1.087, loss:791.5115
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5158, rec:0.4892, f1:0.5022
>> valid relation prec:0.2777, rec:0.0562, f1:0.0935
>> valid relation with NER prec:0.2777, rec:0.0562, f1:0.0935
g_step 4100, step 96, avg_time 2.845, loss:729.4167
g_step 4200, step 196, avg_time 1.088, loss:747.6206
g_step 4300, step 296, avg_time 1.074, loss:773.9675
g_step 4400, step 88, avg_time 1.081, loss:741.2742
g_step 4500, step 188, avg_time 1.067, loss:703.0866
>> valid entity prec:0.5396, rec:0.4126, f1:0.4677
>> valid relation prec:0.2804, rec:0.0603, f1:0.0993
>> valid relation with NER prec:0.2804, rec:0.0603, f1:0.0993
g_step 4600, step 288, avg_time 2.852, loss:735.9021
g_step 4700, step 80, avg_time 1.056, loss:697.3428
g_step 4800, step 180, avg_time 1.095, loss:723.3184
g_step 4900, step 280, avg_time 1.087, loss:690.6601
g_step 5000, step 72, avg_time 1.074, loss:665.6686
learning rate was adjusted to 0.0008
>> valid entity prec:0.5283, rec:0.4652, f1:0.4947
>> valid relation prec:0.2619, rec:0.0577, f1:0.0945
>> valid relation with NER prec:0.2619, rec:0.0577, f1:0.0945
g_step 5100, step 172, avg_time 2.851, loss:675.4182
g_step 5200, step 272, avg_time 1.072, loss:686.3337
g_step 5300, step 64, avg_time 1.080, loss:642.5244
g_step 5400, step 164, avg_time 1.074, loss:664.9265
g_step 5500, step 264, avg_time 1.077, loss:663.8434
>> valid entity prec:0.5444, rec:0.4488, f1:0.4920
>> valid relation prec:0.2617, rec:0.0735, f1:0.1148
>> valid relation with NER prec:0.2617, rec:0.0735, f1:0.1148
g_step 5600, step 56, avg_time 2.838, loss:648.0079
g_step 5700, step 156, avg_time 1.075, loss:610.9037
g_step 5800, step 256, avg_time 1.073, loss:662.0277
g_step 5900, step 48, avg_time 1.065, loss:624.1537
g_step 6000, step 148, avg_time 1.081, loss:602.0535
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5422, rec:0.4703, f1:0.5037
>> valid relation prec:0.2217, rec:0.0669, f1:0.1028
>> valid relation with NER prec:0.2217, rec:0.0669, f1:0.1028
g_step 6100, step 248, avg_time 2.843, loss:625.9309
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:47:53 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:47:53 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-47-53_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:47:54 - WARNING - datasets.builder -   Using custom data configuration default-17a4c9f12fce331d
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-17a4c9f12fce331d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:47:54,589 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:47:54,591 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:47:54,591 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:47:54,592 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:47:54,602 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:54,608 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:54,608 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:54,608 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:54,608 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:54,608 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:54,608 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:47:54,725 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:47:57,827 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:47:57,831 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-17a4c9f12fce331d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 02:47:57 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x15436cac49e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.09ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.85ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.13ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.28ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.64ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  3.93ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.10ba/s]100%|██████████| 8/8 [00:01<00:00,  4.89ba/s]100%|██████████| 8/8 [00:01<00:00,  4.24ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.96ba/s] 40%|████      | 2/5 [00:00<00:00,  4.22ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.33ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.36ba/s]100%|██████████| 5/5 [00:01<00:00,  4.61ba/s]100%|██████████| 5/5 [00:01<00:00,  4.43ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.93ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.27ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.68ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.85ba/s]100%|██████████| 8/8 [00:00<00:00, 11.28ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.08ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.33ba/s]100%|██████████| 5/5 [00:00<00:00, 11.02ba/s]100%|██████████| 5/5 [00:00<00:00, 10.76ba/s]
[INFO|trainer.py:414] 2023-08-29 02:48:02,428 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:48:02,434 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:48:02,435 >>   Num examples = 7526
[INFO|trainer.py:1149] 2023-08-29 02:48:02,435 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:48:02,435 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:48:02,435 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:48:02,435 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:48:02,435 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:56,  3.34it/s]  0%|          | 2/590 [00:00<02:51,  3.43it/s]  1%|          | 3/590 [00:00<02:49,  3.45it/s]  1%|          | 4/590 [00:01<02:48,  3.47it/s]  1%|          | 5/590 [00:01<02:48,  3.48it/s]  1%|          | 6/590 [00:01<02:47,  3.48it/s]  1%|          | 7/590 [00:02<02:47,  3.48it/s]  1%|▏         | 8/590 [00:02<02:47,  3.48it/s]  2%|▏         | 9/590 [00:02<02:46,  3.48it/s]  2%|▏         | 10/590 [00:02<02:46,  3.48it/s]  2%|▏         | 11/590 [00:03<02:46,  3.48it/s]  2%|▏         | 12/590 [00:03<02:45,  3.48it/s]  2%|▏         | 13/590 [00:03<02:45,  3.48it/s]  2%|▏         | 14/590 [00:04<02:45,  3.48it/s]  3%|▎         | 15/590 [00:04<02:45,  3.48it/s]  3%|▎         | 16/590 [00:04<02:44,  3.48it/s]  3%|▎         | 17/590 [00:04<02:44,  3.48it/s]  3%|▎         | 18/590 [00:05<02:45,  3.46it/s]  3%|▎         | 19/590 [00:05<02:44,  3.47it/s]  3%|▎         | 20/590 [00:05<02:44,  3.47it/s]  4%|▎         | 21/590 [00:06<02:43,  3.47it/s]  4%|▎         | 22/590 [00:06<02:43,  3.47it/s]  4%|▍         | 23/590 [00:06<02:43,  3.48it/s]  4%|▍         | 24/590 [00:06<02:42,  3.48it/s]  4%|▍         | 25/590 [00:07<02:42,  3.48it/s]  4%|▍         | 26/590 [00:07<02:41,  3.48it/s]  5%|▍         | 27/590 [00:07<02:41,  3.48it/s]  5%|▍         | 28/590 [00:08<02:41,  3.48it/s]  5%|▍         | 29/590 [00:08<02:47,  3.35it/s]  5%|▌         | 30/590 [00:08<02:45,  3.39it/s]  5%|▌         | 31/590 [00:08<02:43,  3.41it/s]  5%|▌         | 32/590 [00:09<02:42,  3.43it/s]  6%|▌         | 33/590 [00:09<02:41,  3.45it/s]  6%|▌         | 34/590 [00:09<02:40,  3.46it/s]  6%|▌         | 35/590 [00:10<02:40,  3.46it/s]  6%|▌         | 36/590 [00:10<02:39,  3.47it/s]  6%|▋         | 37/590 [00:10<02:39,  3.47it/s]  6%|▋         | 38/590 [00:10<02:38,  3.47it/s]  7%|▋         | 39/590 [00:11<02:38,  3.48it/s]  7%|▋         | 40/590 [00:11<02:39,  3.45it/s]  7%|▋         | 41/590 [00:11<02:38,  3.46it/s]  7%|▋         | 42/590 [00:12<02:38,  3.47it/s]  7%|▋         | 43/590 [00:12<02:37,  3.47it/s]  7%|▋         | 44/590 [00:12<02:37,  3.47it/s]  8%|▊         | 45/590 [00:12<02:36,  3.47it/s]  8%|▊         | 46/590 [00:13<02:36,  3.47it/s]  8%|▊         | 47/590 [00:13<02:36,  3.48it/s]  8%|▊         | 48/590 [00:13<02:36,  3.47it/s]  8%|▊         | 49/590 [00:14<02:35,  3.48it/s]  8%|▊         | 50/590 [00:14<02:35,  3.48it/s]  9%|▊         | 51/590 [00:14<02:35,  3.47it/s]  9%|▉         | 52/590 [00:15<02:35,  3.47it/s]  9%|▉         | 53/590 [00:15<02:34,  3.47it/s]  9%|▉         | 54/590 [00:15<02:34,  3.48it/s]  9%|▉         | 55/590 [00:15<02:34,  3.47it/s]  9%|▉         | 56/590 [00:16<02:33,  3.48it/s] 10%|▉         | 57/590 [00:16<02:33,  3.47it/s] 10%|▉         | 58/590 [00:16<02:33,  3.47it/s] 10%|█         | 59/590 [00:17<02:32,  3.47it/s] 10%|█         | 60/590 [00:17<02:32,  3.47it/s] 10%|█         | 61/590 [00:17<02:32,  3.47it/s] 11%|█         | 62/590 [00:17<02:32,  3.46it/s] 11%|█         | 63/590 [00:18<02:32,  3.46it/s] 11%|█         | 64/590 [00:18<02:31,  3.47it/s] 11%|█         | 65/590 [00:18<02:31,  3.47it/s] 11%|█         | 66/590 [00:19<02:30,  3.47it/s] 11%|█▏        | 67/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 68/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 69/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 70/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 71/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 72/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 73/590 [00:21<02:29,  3.47it/s] 13%|█▎        | 74/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 75/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 76/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 77/590 [00:22<02:27,  3.47it/s] 13%|█▎        | 78/590 [00:22<02:27,  3.47it/s] 13%|█▎        | 79/590 [00:22<02:27,  3.47it/s] 14%|█▎        | 80/590 [00:23<02:26,  3.47it/s] 14%|█▎        | 81/590 [00:23<02:26,  3.47it/s] 14%|█▍        | 82/590 [00:23<02:26,  3.47it/s] 14%|█▍        | 83/590 [00:23<02:26,  3.47it/s] 14%|█▍        | 84/590 [00:24<02:26,  3.46it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.46it/s] 15%|█▍        | 86/590 [00:24<02:25,  3.47it/s] 15%|█▍        | 87/590 [00:25<02:24,  3.47it/s] 15%|█▍        | 88/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 89/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 90/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 91/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 92/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 93/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 94/590 [00:27<02:23,  3.47it/s] 16%|█▌        | 95/590 [00:27<02:23,  3.45it/s] 16%|█▋        | 96/590 [00:27<02:22,  3.46it/s] 16%|█▋        | 97/590 [00:27<02:22,  3.46it/s] 17%|█▋        | 98/590 [00:28<02:22,  3.46it/s] 17%|█▋        | 99/590 [00:28<02:21,  3.46it/s] 17%|█▋        | 100/590 [00:28<02:21,  3.47it/s] 17%|█▋        | 101/590 [00:29<02:21,  3.46it/s] 17%|█▋        | 102/590 [00:29<02:20,  3.47it/s] 17%|█▋        | 103/590 [00:29<02:20,  3.47it/s] 18%|█▊        | 104/590 [00:29<02:20,  3.47it/s] 18%|█▊        | 105/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 106/590 [00:30<02:20,  3.46it/s] 18%|█▊        | 107/590 [00:30<02:19,  3.46it/s] 18%|█▊        | 108/590 [00:31<02:19,  3.46it/s] 18%|█▊        | 109/590 [00:31<02:18,  3.47it/s] 19%|█▊        | 110/590 [00:31<02:18,  3.47it/s] 19%|█▉        | 111/590 [00:32<02:18,  3.47it/s] 19%|█▉        | 112/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 113/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 114/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 115/590 [00:33<02:16,  3.47it/s] 20%|█▉        | 116/590 [00:33<02:16,  3.47it/s] 20%|█▉        | 117/590 [00:33<02:16,  3.47it/s] 20%|██        | 118/590 [00:33<02:02,  3.85it/s][INFO|trainer.py:2140] 2023-08-29 02:48:36,379 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:48:36,379 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 02:48:36,379 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.71it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.65it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.69it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.08it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.69it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.27it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.10it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.67it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.59it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.59it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.57it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.71it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.71it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.69it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.71it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.70it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.42it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.46it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.43it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.53it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.47it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.43it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.59it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.56it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.48it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.52it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.48it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.44it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.50it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.42it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.45it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.55it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.63it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.60it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.52it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.43it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.52it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.51it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.44it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.43it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.52it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.62it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.51it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.54it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.44it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.50it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.53it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.53it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.41it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.48it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.50it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.51it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.54it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.56it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.44it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.35it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.50it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.50it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.44it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.52it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.50it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.42it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.36it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.40it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.50it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.50it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.48it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.39it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.41it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.45it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.48it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.50it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.51it/s][A
 61%|██████▏   | 373/608 [00:07<00:05, 46.49it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.49it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.52it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.39it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.41it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.49it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.55it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.50it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.44it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.45it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.42it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.45it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.45it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.36it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.44it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.40it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.42it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.45it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.40it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.41it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.40it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.39it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.45it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.38it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.44it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.42it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.44it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.41it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.38it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.43it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.46it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.39it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.49it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.52it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.33it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.42it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.44it/s][A
 92%|█████████▏| 558/608 [00:11<00:01, 46.44it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.50it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.46it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.41it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.36it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.45it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.44it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.34it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.43it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.39it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.38it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:47<02:02,  3.85it/s]
100%|██████████| 608/608 [00:13<00:00, 46.38it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:48:49,493 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-29 02:48:49,512 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:48:52,278 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:48:52,298 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:48:52,306 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:54<50:46,  6.47s/it] 20%|██        | 120/590 [00:55<36:08,  4.61s/it] 21%|██        | 121/590 [00:55<25:55,  3.32s/it] 21%|██        | 122/590 [00:55<18:46,  2.41s/it] 21%|██        | 123/590 [00:56<13:47,  1.77s/it] 21%|██        | 124/590 [00:56<10:18,  1.33s/it] 21%|██        | 125/590 [00:56<07:52,  1.02s/it] 21%|██▏       | 126/590 [00:56<06:09,  1.25it/s] 22%|██▏       | 127/590 [00:57<04:58,  1.55it/s] 22%|██▏       | 128/590 [00:57<04:08,  1.86it/s] 22%|██▏       | 129/590 [00:57<03:33,  2.16it/s] 22%|██▏       | 130/590 [00:58<03:08,  2.44it/s] 22%|██▏       | 131/590 [00:58<02:51,  2.67it/s] 22%|██▏       | 132/590 [00:58<02:39,  2.87it/s] 23%|██▎       | 133/590 [00:58<02:31,  3.03it/s] 23%|██▎       | 134/590 [00:59<02:24,  3.15it/s] 23%|██▎       | 135/590 [00:59<02:20,  3.24it/s] 23%|██▎       | 136/590 [00:59<02:17,  3.30it/s] 23%|██▎       | 137/590 [01:00<02:15,  3.35it/s] 23%|██▎       | 138/590 [01:00<02:13,  3.39it/s] 24%|██▎       | 139/590 [01:00<02:12,  3.40it/s] 24%|██▎       | 140/590 [01:00<02:11,  3.42it/s] 24%|██▍       | 141/590 [01:01<02:10,  3.43it/s] 24%|██▍       | 142/590 [01:01<02:17,  3.26it/s] 24%|██▍       | 143/590 [01:01<02:14,  3.32it/s] 24%|██▍       | 144/590 [01:02<02:57,  2.51it/s] 25%|██▍       | 145/590 [01:02<02:42,  2.73it/s] 25%|██▍       | 146/590 [01:03<02:32,  2.91it/s] 25%|██▍       | 147/590 [01:03<02:24,  3.06it/s] 25%|██▌       | 148/590 [01:03<02:19,  3.17it/s] 25%|██▌       | 149/590 [01:03<02:15,  3.25it/s] 25%|██▌       | 150/590 [01:04<02:12,  3.31it/s] 26%|██▌       | 151/590 [01:04<02:10,  3.36it/s] 26%|██▌       | 152/590 [01:04<02:13,  3.28it/s] 26%|██▌       | 153/590 [01:05<02:10,  3.34it/s] 26%|██▌       | 154/590 [01:05<02:09,  3.38it/s] 26%|██▋       | 155/590 [01:05<02:07,  3.40it/s] 26%|██▋       | 156/590 [01:05<02:06,  3.42it/s] 27%|██▋       | 157/590 [01:06<02:06,  3.44it/s] 27%|██▋       | 158/590 [01:06<02:05,  3.45it/s] 27%|██▋       | 159/590 [01:06<02:04,  3.45it/s] 27%|██▋       | 160/590 [01:07<02:04,  3.46it/s] 27%|██▋       | 161/590 [01:07<02:03,  3.46it/s] 27%|██▋       | 162/590 [01:07<02:03,  3.47it/s] 28%|██▊       | 163/590 [01:08<02:03,  3.45it/s] 28%|██▊       | 164/590 [01:08<02:03,  3.46it/s] 28%|██▊       | 165/590 [01:08<02:02,  3.46it/s] 28%|██▊       | 166/590 [01:08<02:02,  3.46it/s] 28%|██▊       | 167/590 [01:09<02:02,  3.46it/s] 28%|██▊       | 168/590 [01:09<02:01,  3.46it/s] 29%|██▊       | 169/590 [01:09<02:01,  3.46it/s] 29%|██▉       | 170/590 [01:10<02:01,  3.47it/s] 29%|██▉       | 171/590 [01:10<02:00,  3.47it/s] 29%|██▉       | 172/590 [01:10<02:00,  3.46it/s] 29%|██▉       | 173/590 [01:10<02:00,  3.46it/s] 29%|██▉       | 174/590 [01:11<02:00,  3.46it/s] 30%|██▉       | 175/590 [01:11<02:00,  3.46it/s] 30%|██▉       | 176/590 [01:11<01:59,  3.46it/s] 30%|███       | 177/590 [01:12<01:59,  3.46it/s] 30%|███       | 178/590 [01:12<01:58,  3.46it/s] 30%|███       | 179/590 [01:12<01:58,  3.46it/s] 31%|███       | 180/590 [01:12<01:58,  3.46it/s] 31%|███       | 181/590 [01:13<01:58,  3.46it/s] 31%|███       | 182/590 [01:13<01:57,  3.46it/s] 31%|███       | 183/590 [01:13<01:57,  3.46it/s] 31%|███       | 184/590 [01:14<01:57,  3.46it/s] 31%|███▏      | 185/590 [01:14<01:56,  3.46it/s] 32%|███▏      | 186/590 [01:14<01:56,  3.46it/s] 32%|███▏      | 187/590 [01:14<01:56,  3.46it/s] 32%|███▏      | 188/590 [01:15<01:56,  3.46it/s] 32%|███▏      | 189/590 [01:15<01:55,  3.46it/s] 32%|███▏      | 190/590 [01:15<01:55,  3.46it/s] 32%|███▏      | 191/590 [01:16<01:55,  3.46it/s] 33%|███▎      | 192/590 [01:16<01:55,  3.46it/s] 33%|███▎      | 193/590 [01:16<01:54,  3.46it/s] 33%|███▎      | 194/590 [01:16<01:54,  3.45it/s] 33%|███▎      | 195/590 [01:17<01:54,  3.45it/s] 33%|███▎      | 196/590 [01:17<01:54,  3.45it/s] 33%|███▎      | 197/590 [01:17<01:53,  3.46it/s] 34%|███▎      | 198/590 [01:18<01:53,  3.46it/s] 34%|███▎      | 199/590 [01:18<01:53,  3.46it/s] 34%|███▍      | 200/590 [01:18<01:52,  3.46it/s] 34%|███▍      | 201/590 [01:18<01:52,  3.46it/s] 34%|███▍      | 202/590 [01:19<01:52,  3.46it/s] 34%|███▍      | 203/590 [01:19<01:51,  3.46it/s] 35%|███▍      | 204/590 [01:19<01:51,  3.46it/s] 35%|███▍      | 205/590 [01:20<01:51,  3.45it/s] 35%|███▍      | 206/590 [01:20<01:51,  3.46it/s] 35%|███▌      | 207/590 [01:20<01:50,  3.46it/s] 35%|███▌      | 208/590 [01:21<01:50,  3.46it/s] 35%|███▌      | 209/590 [01:21<01:50,  3.46it/s] 36%|███▌      | 210/590 [01:21<01:49,  3.46it/s] 36%|███▌      | 211/590 [01:21<01:49,  3.46it/s] 36%|███▌      | 212/590 [01:22<01:49,  3.46it/s] 36%|███▌      | 213/590 [01:22<01:48,  3.46it/s] 36%|███▋      | 214/590 [01:22<01:48,  3.46it/s] 36%|███▋      | 215/590 [01:23<01:48,  3.46it/s] 37%|███▋      | 216/590 [01:23<01:48,  3.44it/s] 37%|███▋      | 217/590 [01:23<01:48,  3.45it/s] 37%|███▋      | 218/590 [01:23<01:47,  3.45it/s] 37%|███▋      | 219/590 [01:24<01:47,  3.45it/s] 37%|███▋      | 220/590 [01:24<01:47,  3.45it/s] 37%|███▋      | 221/590 [01:24<01:46,  3.46it/s] 38%|███▊      | 222/590 [01:25<01:46,  3.45it/s] 38%|███▊      | 223/590 [01:25<01:46,  3.46it/s] 38%|███▊      | 224/590 [01:25<01:45,  3.46it/s] 38%|███▊      | 225/590 [01:25<01:45,  3.46it/s] 38%|███▊      | 226/590 [01:26<01:45,  3.45it/s] 38%|███▊      | 227/590 [01:26<01:45,  3.45it/s] 39%|███▊      | 228/590 [01:26<01:44,  3.45it/s] 39%|███▉      | 229/590 [01:27<01:44,  3.46it/s] 39%|███▉      | 230/590 [01:27<01:44,  3.46it/s] 39%|███▉      | 231/590 [01:27<01:43,  3.46it/s] 39%|███▉      | 232/590 [01:27<01:43,  3.46it/s] 39%|███▉      | 233/590 [01:28<01:43,  3.46it/s] 40%|███▉      | 234/590 [01:28<01:42,  3.46it/s] 40%|███▉      | 235/590 [01:28<01:42,  3.46it/s] 40%|████      | 236/590 [01:29<01:32,  3.83it/s][INFO|trainer.py:2140] 2023-08-29 02:49:31,463 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:49:31,463 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 02:49:31,464 >>   Batch size = 8
{'eval_loss': 0.9448865056037903, 'eval_runtime': 13.0743, 'eval_samples_per_second': 371.723, 'eval_steps_per_second': 46.504, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.27it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.48it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.55it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.82it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.41it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.18it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.84it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.45it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.34it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.34it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.34it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.47it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.52it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.54it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.54it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.47it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.22it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.22it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.34it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.39it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.34it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.43it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.43it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.44it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.42it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.29it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.28it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.31it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.39it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.27it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.36it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.45it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.48it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.44it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.39it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.31it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.21it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.25it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.40it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.42it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.38it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.48it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.42it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.44it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.33it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.30it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.31it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.37it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.44it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.38it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.49it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.49it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.35it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.35it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.25it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.26it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.21it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.38it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.38it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.46it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.44it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.42it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.32it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.26it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.24it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.19it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.24it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.38it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.41it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.40it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.38it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.36it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.18it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.23it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.23it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.22it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.31it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.42it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.37it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.45it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.32it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.31it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.21it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 46.20it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.29it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.24it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.32it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.40it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.43it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.44it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.30it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.24it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.23it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.25it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.24it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.29it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.40it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.40it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.40it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.36it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.26it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.16it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.25it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.22it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.29it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.22it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.34it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.39it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.40it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.27it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.25it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.29it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.32it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.33it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.29it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.31it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.37it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.37it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.37it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.24it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.27it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:42<01:32,  3.83it/s]
100%|██████████| 608/608 [00:13<00:00, 46.27it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:49:44,600 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-29 02:49:44,624 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:49:47,170 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:49:47,187 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:49:47,197 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:50<38:25,  6.53s/it] 40%|████      | 238/590 [01:50<27:20,  4.66s/it] 41%|████      | 239/590 [01:50<19:35,  3.35s/it] 41%|████      | 240/590 [01:51<14:10,  2.43s/it] 41%|████      | 241/590 [01:51<10:24,  1.79s/it] 41%|████      | 242/590 [01:51<07:45,  1.34s/it] 41%|████      | 243/590 [01:51<05:55,  1.02s/it] 41%|████▏     | 244/590 [01:52<04:37,  1.25it/s] 42%|████▏     | 245/590 [01:52<03:43,  1.54it/s] 42%|████▏     | 246/590 [01:52<03:05,  1.85it/s] 42%|████▏     | 247/590 [01:53<02:39,  2.15it/s] 42%|████▏     | 248/590 [01:53<02:20,  2.43it/s] 42%|████▏     | 249/590 [01:53<02:07,  2.67it/s] 42%|████▏     | 250/590 [01:53<01:58,  2.87it/s] 43%|████▎     | 251/590 [01:54<01:52,  3.02it/s] 43%|████▎     | 252/590 [01:54<01:47,  3.14it/s] 43%|████▎     | 253/590 [01:54<01:44,  3.23it/s] 43%|████▎     | 254/590 [01:55<01:41,  3.30it/s] 43%|████▎     | 255/590 [01:55<01:40,  3.35it/s] 43%|████▎     | 256/590 [01:55<01:38,  3.38it/s] 44%|████▎     | 257/590 [01:55<01:37,  3.41it/s] 44%|████▎     | 258/590 [01:56<01:36,  3.43it/s] 44%|████▍     | 259/590 [01:56<01:36,  3.44it/s] 44%|████▍     | 260/590 [01:56<01:35,  3.44it/s] 44%|████▍     | 261/590 [01:57<01:35,  3.45it/s] 44%|████▍     | 262/590 [01:57<01:34,  3.45it/s] 45%|████▍     | 263/590 [01:57<01:34,  3.46it/s] 45%|████▍     | 264/590 [01:57<01:34,  3.46it/s] 45%|████▍     | 265/590 [01:58<01:33,  3.46it/s] 45%|████▌     | 266/590 [01:58<01:33,  3.46it/s] 45%|████▌     | 267/590 [01:58<01:33,  3.46it/s] 45%|████▌     | 268/590 [01:59<01:33,  3.46it/s] 46%|████▌     | 269/590 [01:59<01:32,  3.47it/s] 46%|████▌     | 270/590 [01:59<01:32,  3.46it/s] 46%|████▌     | 271/590 [02:00<01:34,  3.36it/s] 46%|████▌     | 272/590 [02:00<01:33,  3.39it/s] 46%|████▋     | 273/590 [02:00<01:32,  3.41it/s] 46%|████▋     | 274/590 [02:00<01:32,  3.43it/s] 47%|████▋     | 275/590 [02:01<01:31,  3.44it/s] 47%|████▋     | 276/590 [02:01<01:31,  3.45it/s] 47%|████▋     | 277/590 [02:01<01:30,  3.45it/s] 47%|████▋     | 278/590 [02:02<01:30,  3.45it/s] 47%|████▋     | 279/590 [02:02<02:05,  2.48it/s] 47%|████▋     | 280/590 [02:03<01:58,  2.61it/s] 48%|████▊     | 281/590 [02:03<01:49,  2.82it/s] 48%|████▊     | 282/590 [02:03<01:43,  2.99it/s] 48%|████▊     | 283/590 [02:03<01:38,  3.11it/s] 48%|████▊     | 284/590 [02:04<01:35,  3.21it/s] 48%|████▊     | 285/590 [02:04<01:32,  3.28it/s] 48%|████▊     | 286/590 [02:04<01:31,  3.33it/s] 49%|████▊     | 287/590 [02:05<01:29,  3.37it/s] 49%|████▉     | 288/590 [02:05<01:28,  3.40it/s] 49%|████▉     | 289/590 [02:05<01:28,  3.42it/s] 49%|████▉     | 290/590 [02:05<01:27,  3.43it/s] 49%|████▉     | 291/590 [02:06<01:27,  3.43it/s] 49%|████▉     | 292/590 [02:06<01:26,  3.44it/s] 50%|████▉     | 293/590 [02:06<01:26,  3.44it/s] 50%|████▉     | 294/590 [02:07<01:25,  3.45it/s] 50%|█████     | 295/590 [02:07<01:25,  3.45it/s] 50%|█████     | 296/590 [02:07<01:25,  3.45it/s] 50%|█████     | 297/590 [02:07<01:24,  3.45it/s] 51%|█████     | 298/590 [02:08<01:24,  3.46it/s] 51%|█████     | 299/590 [02:08<01:24,  3.46it/s] 51%|█████     | 300/590 [02:08<01:23,  3.46it/s] 51%|█████     | 301/590 [02:09<01:23,  3.46it/s] 51%|█████     | 302/590 [02:09<01:23,  3.45it/s] 51%|█████▏    | 303/590 [02:09<01:23,  3.45it/s] 52%|█████▏    | 304/590 [02:10<01:22,  3.45it/s] 52%|█████▏    | 305/590 [02:10<01:22,  3.45it/s] 52%|█████▏    | 306/590 [02:10<01:22,  3.46it/s] 52%|█████▏    | 307/590 [02:10<01:21,  3.46it/s] 52%|█████▏    | 308/590 [02:11<01:21,  3.46it/s] 52%|█████▏    | 309/590 [02:11<01:21,  3.46it/s] 53%|█████▎    | 310/590 [02:11<01:20,  3.46it/s] 53%|█████▎    | 311/590 [02:12<01:20,  3.46it/s] 53%|█████▎    | 312/590 [02:12<01:20,  3.46it/s] 53%|█████▎    | 313/590 [02:12<01:20,  3.45it/s] 53%|█████▎    | 314/590 [02:12<01:19,  3.45it/s] 53%|█████▎    | 315/590 [02:13<01:19,  3.46it/s] 54%|█████▎    | 316/590 [02:13<01:19,  3.46it/s] 54%|█████▎    | 317/590 [02:13<01:18,  3.46it/s] 54%|█████▍    | 318/590 [02:14<01:18,  3.46it/s] 54%|█████▍    | 319/590 [02:14<01:18,  3.46it/s] 54%|█████▍    | 320/590 [02:14<01:18,  3.46it/s] 54%|█████▍    | 321/590 [02:14<01:17,  3.46it/s] 55%|█████▍    | 322/590 [02:15<01:17,  3.46it/s] 55%|█████▍    | 323/590 [02:15<01:17,  3.46it/s] 55%|█████▍    | 324/590 [02:15<01:16,  3.46it/s] 55%|█████▌    | 325/590 [02:16<01:16,  3.46it/s] 55%|█████▌    | 326/590 [02:16<01:16,  3.46it/s] 55%|█████▌    | 327/590 [02:16<01:16,  3.46it/s] 56%|█████▌    | 328/590 [02:16<01:15,  3.46it/s] 56%|█████▌    | 329/590 [02:17<01:15,  3.46it/s] 56%|█████▌    | 330/590 [02:17<01:15,  3.46it/s] 56%|█████▌    | 331/590 [02:17<01:14,  3.46it/s] 56%|█████▋    | 332/590 [02:18<01:14,  3.46it/s] 56%|█████▋    | 333/590 [02:18<01:14,  3.45it/s] 57%|█████▋    | 334/590 [02:18<01:14,  3.45it/s] 57%|█████▋    | 335/590 [02:18<01:13,  3.45it/s] 57%|█████▋    | 336/590 [02:19<01:13,  3.45it/s] 57%|█████▋    | 337/590 [02:19<01:13,  3.45it/s] 57%|█████▋    | 338/590 [02:19<01:12,  3.46it/s] 57%|█████▋    | 339/590 [02:20<01:12,  3.45it/s] 58%|█████▊    | 340/590 [02:20<01:12,  3.46it/s] 58%|█████▊    | 341/590 [02:20<01:12,  3.46it/s] 58%|█████▊    | 342/590 [02:20<01:11,  3.46it/s] 58%|█████▊    | 343/590 [02:21<01:11,  3.45it/s] 58%|█████▊    | 344/590 [02:21<01:11,  3.44it/s] 58%|█████▊    | 345/590 [02:21<01:11,  3.45it/s] 59%|█████▊    | 346/590 [02:22<01:10,  3.45it/s] 59%|█████▉    | 347/590 [02:22<01:10,  3.45it/s] 59%|█████▉    | 348/590 [02:22<01:10,  3.45it/s] 59%|█████▉    | 349/590 [02:23<01:09,  3.46it/s] 59%|█████▉    | 350/590 [02:23<01:09,  3.45it/s] 59%|█████▉    | 351/590 [02:23<01:09,  3.46it/s] 60%|█████▉    | 352/590 [02:23<01:08,  3.46it/s] 60%|█████▉    | 353/590 [02:24<01:08,  3.46it/s] 60%|██████    | 354/590 [02:24<01:01,  3.84it/s][INFO|trainer.py:2140] 2023-08-29 02:50:26,820 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:50:26,820 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 02:50:26,820 >>   Batch size = 8
{'eval_loss': 0.9390559792518616, 'eval_runtime': 13.1132, 'eval_samples_per_second': 370.618, 'eval_steps_per_second': 46.365, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.74it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.15it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.48it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.76it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.42it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.06it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.87it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.35it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.31it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.39it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.39it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.46it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.48it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.42it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.45it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.33it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.19it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.07it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.16it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.21it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.32it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.36it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.39it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.46it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.23it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.13it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.04it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.07it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.19it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.23it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.33it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.37it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.38it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.40it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.26it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.10it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.10it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.14it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.19it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.29it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.31it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.37it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.42it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.37it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.14it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.14it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 46.20it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.27it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.26it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.28it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.36it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.38it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.37it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.27it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.10it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.20it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.22it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.26it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.35it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.35it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.33it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.00it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.06it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.01it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.08it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.17it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.24it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.23it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.29it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.32it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.30it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.27it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.12it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.15it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.22it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.33it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.23it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.28it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.34it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.32it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.32it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.23it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.17it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 46.21it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.21it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.31it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.91it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.37it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.39it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.34it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.30it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.28it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.24it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.15it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.29it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.20it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.19it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.19it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.21it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.24it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.24it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.28it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.17it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.16it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.28it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.19it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.34it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.23it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.22it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.29it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.27it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.28it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.17it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.15it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.20it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.20it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.23it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.30it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.28it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.25it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.25it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:37<01:01,  3.84it/s]
100%|██████████| 608/608 [00:13<00:00, 46.25it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:50:39,969 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-29 02:50:39,986 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:50:42,508 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:50:42,521 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:50:42,535 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:45<24:59,  6.38s/it] 60%|██████    | 356/590 [02:45<17:45,  4.55s/it] 61%|██████    | 357/590 [02:45<12:43,  3.27s/it] 61%|██████    | 358/590 [02:45<09:11,  2.38s/it] 61%|██████    | 359/590 [02:46<06:44,  1.75s/it] 61%|██████    | 360/590 [02:46<05:01,  1.31s/it] 61%|██████    | 361/590 [02:46<03:50,  1.01s/it] 61%|██████▏   | 362/590 [02:47<03:00,  1.27it/s] 62%|██████▏   | 363/590 [02:47<02:25,  1.56it/s] 62%|██████▏   | 364/590 [02:47<02:00,  1.87it/s] 62%|██████▏   | 365/590 [02:47<01:43,  2.17it/s] 62%|██████▏   | 366/590 [02:48<01:31,  2.45it/s] 62%|██████▏   | 367/590 [02:48<01:23,  2.68it/s] 62%|██████▏   | 368/590 [02:48<01:17,  2.88it/s] 63%|██████▎   | 369/590 [02:49<01:12,  3.03it/s] 63%|██████▎   | 370/590 [02:49<01:09,  3.15it/s] 63%|██████▎   | 371/590 [02:49<01:07,  3.23it/s] 63%|██████▎   | 372/590 [02:49<01:06,  3.30it/s] 63%|██████▎   | 373/590 [02:50<01:04,  3.35it/s] 63%|██████▎   | 374/590 [02:50<01:03,  3.38it/s] 64%|██████▎   | 375/590 [02:50<01:03,  3.41it/s] 64%|██████▎   | 376/590 [02:51<01:02,  3.42it/s] 64%|██████▍   | 377/590 [02:51<01:01,  3.44it/s] 64%|██████▍   | 378/590 [02:51<01:02,  3.41it/s] 64%|██████▍   | 379/590 [02:51<01:01,  3.42it/s] 64%|██████▍   | 380/590 [02:52<01:01,  3.44it/s] 65%|██████▍   | 381/590 [02:52<01:00,  3.44it/s] 65%|██████▍   | 382/590 [02:52<01:00,  3.45it/s] 65%|██████▍   | 383/590 [02:53<00:59,  3.45it/s] 65%|██████▌   | 384/590 [02:53<00:59,  3.45it/s] 65%|██████▌   | 385/590 [02:53<00:59,  3.46it/s] 65%|██████▌   | 386/590 [02:54<00:58,  3.46it/s] 66%|██████▌   | 387/590 [02:54<00:58,  3.46it/s] 66%|██████▌   | 388/590 [02:54<00:58,  3.46it/s] 66%|██████▌   | 389/590 [02:54<00:58,  3.45it/s] 66%|██████▌   | 390/590 [02:55<00:57,  3.45it/s] 66%|██████▋   | 391/590 [02:55<00:57,  3.45it/s] 66%|██████▋   | 392/590 [02:55<00:57,  3.45it/s] 67%|██████▋   | 393/590 [02:56<00:56,  3.46it/s] 67%|██████▋   | 394/590 [02:56<00:56,  3.46it/s] 67%|██████▋   | 395/590 [02:56<00:56,  3.46it/s] 67%|██████▋   | 396/590 [02:56<00:56,  3.46it/s] 67%|██████▋   | 397/590 [02:57<00:55,  3.46it/s] 67%|██████▋   | 398/590 [02:57<00:55,  3.45it/s] 68%|██████▊   | 399/590 [02:57<00:55,  3.46it/s] 68%|██████▊   | 400/590 [02:58<00:57,  3.29it/s] 68%|██████▊   | 401/590 [02:58<00:56,  3.34it/s] 68%|██████▊   | 402/590 [02:58<00:55,  3.38it/s] 68%|██████▊   | 403/590 [02:58<00:54,  3.40it/s] 68%|██████▊   | 404/590 [02:59<00:54,  3.42it/s] 69%|██████▊   | 405/590 [02:59<00:53,  3.43it/s] 69%|██████▉   | 406/590 [02:59<00:53,  3.44it/s] 69%|██████▉   | 407/590 [03:00<00:53,  3.44it/s] 69%|██████▉   | 408/590 [03:00<00:52,  3.44it/s] 69%|██████▉   | 409/590 [03:00<00:52,  3.44it/s] 69%|██████▉   | 410/590 [03:01<00:54,  3.33it/s] 70%|██████▉   | 411/590 [03:01<00:55,  3.23it/s] 70%|██████▉   | 412/590 [03:01<00:54,  3.30it/s] 70%|███████   | 413/590 [03:01<00:53,  3.34it/s] 70%|███████   | 414/590 [03:02<00:52,  3.37it/s] 70%|███████   | 415/590 [03:02<01:09,  2.53it/s] 71%|███████   | 416/590 [03:03<01:03,  2.75it/s] 71%|███████   | 417/590 [03:03<00:59,  2.93it/s] 71%|███████   | 418/590 [03:03<00:56,  3.07it/s] 71%|███████   | 419/590 [03:04<00:53,  3.18it/s] 71%|███████   | 420/590 [03:04<00:52,  3.26it/s] 71%|███████▏  | 421/590 [03:04<00:51,  3.30it/s] 72%|███████▏  | 422/590 [03:04<00:50,  3.35it/s] 72%|███████▏  | 423/590 [03:05<00:49,  3.38it/s] 72%|███████▏  | 424/590 [03:05<00:48,  3.40it/s] 72%|███████▏  | 425/590 [03:05<00:48,  3.42it/s] 72%|███████▏  | 426/590 [03:06<00:47,  3.43it/s] 72%|███████▏  | 427/590 [03:06<00:47,  3.44it/s] 73%|███████▎  | 428/590 [03:06<00:47,  3.44it/s] 73%|███████▎  | 429/590 [03:06<00:46,  3.45it/s] 73%|███████▎  | 430/590 [03:07<00:46,  3.45it/s] 73%|███████▎  | 431/590 [03:07<00:46,  3.45it/s] 73%|███████▎  | 432/590 [03:07<00:45,  3.45it/s] 73%|███████▎  | 433/590 [03:08<00:45,  3.46it/s] 74%|███████▎  | 434/590 [03:08<00:45,  3.45it/s] 74%|███████▎  | 435/590 [03:08<00:44,  3.46it/s] 74%|███████▍  | 436/590 [03:08<00:44,  3.46it/s] 74%|███████▍  | 437/590 [03:09<00:44,  3.46it/s] 74%|███████▍  | 438/590 [03:09<00:44,  3.45it/s] 74%|███████▍  | 439/590 [03:09<00:43,  3.45it/s] 75%|███████▍  | 440/590 [03:10<00:43,  3.45it/s] 75%|███████▍  | 441/590 [03:10<00:43,  3.45it/s] 75%|███████▍  | 442/590 [03:10<00:42,  3.45it/s] 75%|███████▌  | 443/590 [03:10<00:42,  3.46it/s] 75%|███████▌  | 444/590 [03:11<00:42,  3.46it/s] 75%|███████▌  | 445/590 [03:11<00:41,  3.46it/s] 76%|███████▌  | 446/590 [03:11<00:41,  3.46it/s] 76%|███████▌  | 447/590 [03:12<00:41,  3.45it/s] 76%|███████▌  | 448/590 [03:12<00:41,  3.46it/s] 76%|███████▌  | 449/590 [03:12<00:40,  3.45it/s] 76%|███████▋  | 450/590 [03:12<00:40,  3.45it/s] 76%|███████▋  | 451/590 [03:13<00:40,  3.45it/s] 77%|███████▋  | 452/590 [03:13<00:39,  3.45it/s] 77%|███████▋  | 453/590 [03:13<00:39,  3.45it/s] 77%|███████▋  | 454/590 [03:14<00:39,  3.45it/s] 77%|███████▋  | 455/590 [03:14<00:39,  3.45it/s] 77%|███████▋  | 456/590 [03:14<00:38,  3.46it/s] 77%|███████▋  | 457/590 [03:15<00:38,  3.46it/s] 78%|███████▊  | 458/590 [03:15<00:38,  3.46it/s] 78%|███████▊  | 459/590 [03:15<00:37,  3.46it/s] 78%|███████▊  | 460/590 [03:15<00:37,  3.45it/s] 78%|███████▊  | 461/590 [03:16<00:37,  3.45it/s] 78%|███████▊  | 462/590 [03:16<00:37,  3.45it/s] 78%|███████▊  | 463/590 [03:16<00:36,  3.45it/s] 79%|███████▊  | 464/590 [03:17<00:36,  3.45it/s] 79%|███████▉  | 465/590 [03:17<00:36,  3.45it/s] 79%|███████▉  | 466/590 [03:17<00:35,  3.45it/s] 79%|███████▉  | 467/590 [03:17<00:35,  3.45it/s] 79%|███████▉  | 468/590 [03:18<00:35,  3.46it/s] 79%|███████▉  | 469/590 [03:18<00:34,  3.46it/s] 80%|███████▉  | 470/590 [03:18<00:34,  3.46it/s] 80%|███████▉  | 471/590 [03:19<00:34,  3.44it/s] 80%|████████  | 472/590 [03:19<00:30,  3.81it/s][INFO|trainer.py:2140] 2023-08-29 02:51:21,706 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:51:21,706 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 02:51:21,706 >>   Batch size = 8
{'eval_loss': 0.9433580636978149, 'eval_runtime': 13.1379, 'eval_samples_per_second': 369.922, 'eval_steps_per_second': 46.278, 'epoch': 3.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.87it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.37it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.60it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.75it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.40it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.15it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.89it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.36it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.30it/s][A
  9%|▊         | 53/608 [00:01<00:12, 46.18it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.27it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.39it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.36it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.42it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.39it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.42it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.37it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.18it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.20it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.13it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.22it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.37it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.39it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.44it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.38it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.34it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.33it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.23it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.13it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.15it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.20it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.25it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.33it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.34it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.34it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.38it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.19it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.25it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.19it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.18it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.24it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.26it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.24it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.28it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.22it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.31it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.28it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.34it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.23it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.25it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.22it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.27it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.34it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.32it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.23it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.28it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.29it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.28it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.30it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.16it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.28it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.28it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.31it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.37it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.25it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.34it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.25it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.23it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.18it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.12it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.25it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.29it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.29it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.27it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.27it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.33it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.24it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.21it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.17it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.16it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.25it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.30it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.24it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 46.24it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.35it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.27it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.26it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.26it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.24it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.31it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.30it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.28it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.25it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.20it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.27it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.21it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.22it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.16it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.20it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.22it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.28it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.32it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.23it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.21it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.27it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.19it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.19it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.24it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.23it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.28it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.18it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.21it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.28it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.26it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.25it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.26it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.22it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.30it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.30it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.12it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.19it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:32<00:30,  3.81it/s]
100%|██████████| 608/608 [00:13<00:00, 46.19it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:51:34,855 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-29 02:51:34,876 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:51:37,152 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:51:37,172 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:51:37,182 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:39<12:09,  6.23s/it] 80%|████████  | 474/590 [03:39<08:36,  4.45s/it] 81%|████████  | 475/590 [03:40<06:08,  3.20s/it] 81%|████████  | 476/590 [03:40<04:25,  2.33s/it] 81%|████████  | 477/590 [03:40<03:13,  1.72s/it] 81%|████████  | 478/590 [03:40<02:24,  1.29s/it] 81%|████████  | 479/590 [03:41<01:49,  1.01it/s] 81%|████████▏ | 480/590 [03:41<01:25,  1.28it/s] 82%|████████▏ | 481/590 [03:41<01:08,  1.58it/s] 82%|████████▏ | 482/590 [03:42<00:57,  1.89it/s] 82%|████████▏ | 483/590 [03:42<00:48,  2.19it/s] 82%|████████▏ | 484/590 [03:42<00:43,  2.46it/s] 82%|████████▏ | 485/590 [03:42<00:38,  2.70it/s] 82%|████████▏ | 486/590 [03:43<00:35,  2.89it/s] 83%|████████▎ | 487/590 [03:43<00:33,  3.04it/s] 83%|████████▎ | 488/590 [03:43<00:32,  3.15it/s] 83%|████████▎ | 489/590 [03:44<00:31,  3.24it/s] 83%|████████▎ | 490/590 [03:44<00:30,  3.31it/s] 83%|████████▎ | 491/590 [03:44<00:29,  3.36it/s] 83%|████████▎ | 492/590 [03:44<00:28,  3.39it/s] 84%|████████▎ | 493/590 [03:45<00:28,  3.41it/s] 84%|████████▎ | 494/590 [03:45<00:28,  3.43it/s] 84%|████████▍ | 495/590 [03:45<00:27,  3.44it/s] 84%|████████▍ | 496/590 [03:46<00:27,  3.45it/s] 84%|████████▍ | 497/590 [03:46<00:26,  3.45it/s] 84%|████████▍ | 498/590 [03:46<00:26,  3.45it/s] 85%|████████▍ | 499/590 [03:46<00:26,  3.45it/s] 85%|████████▍ | 500/590 [03:47<00:26,  3.45it/s]                                                  85%|████████▍ | 500/590 [03:47<00:26,  3.45it/s] 85%|████████▍ | 501/590 [03:47<00:25,  3.45it/s] 85%|████████▌ | 502/590 [03:47<00:25,  3.45it/s] 85%|████████▌ | 503/590 [03:48<00:25,  3.45it/s] 85%|████████▌ | 504/590 [03:48<00:24,  3.46it/s] 86%|████████▌ | 505/590 [03:48<00:24,  3.46it/s] 86%|████████▌ | 506/590 [03:48<00:24,  3.46it/s] 86%|████████▌ | 507/590 [03:49<00:24,  3.46it/s] 86%|████████▌ | 508/590 [03:49<00:23,  3.46it/s] 86%|████████▋ | 509/590 [03:49<00:23,  3.45it/s] 86%|████████▋ | 510/590 [03:50<00:23,  3.45it/s] 87%|████████▋ | 511/590 [03:50<00:22,  3.46it/s] 87%|████████▋ | 512/590 [03:50<00:22,  3.45it/s] 87%|████████▋ | 513/590 [03:50<00:22,  3.46it/s] 87%|████████▋ | 514/590 [03:51<00:21,  3.46it/s] 87%|████████▋ | 515/590 [03:51<00:21,  3.46it/s] 87%|████████▋ | 516/590 [03:51<00:21,  3.46it/s] 88%|████████▊ | 517/590 [03:52<00:21,  3.46it/s] 88%|████████▊ | 518/590 [03:52<00:20,  3.46it/s] 88%|████████▊ | 519/590 [03:52<00:20,  3.46it/s] 88%|████████▊ | 520/590 [03:53<00:20,  3.42it/s] 88%|████████▊ | 521/590 [03:53<00:20,  3.43it/s] 88%|████████▊ | 522/590 [03:53<00:19,  3.44it/s] 89%|████████▊ | 523/590 [03:53<00:19,  3.44it/s] 89%|████████▉ | 524/590 [03:54<00:19,  3.45it/s] 89%|████████▉ | 525/590 [03:54<00:18,  3.45it/s] 89%|████████▉ | 526/590 [03:54<00:18,  3.45it/s] 89%|████████▉ | 527/590 [03:55<00:18,  3.45it/s] 89%|████████▉ | 528/590 [03:55<00:17,  3.45it/s] 90%|████████▉ | 529/590 [03:55<00:17,  3.45it/s] 90%|████████▉ | 530/590 [03:55<00:17,  3.45it/s] 90%|█████████ | 531/590 [03:56<00:17,  3.32it/s] 90%|█████████ | 532/590 [03:56<00:17,  3.36it/s] 90%|█████████ | 533/590 [03:56<00:16,  3.39it/s] 91%|█████████ | 534/590 [03:57<00:16,  3.41it/s] 91%|█████████ | 535/590 [03:57<00:16,  3.42it/s] 91%|█████████ | 536/590 [03:57<00:15,  3.43it/s] 91%|█████████ | 537/590 [03:57<00:15,  3.43it/s] 91%|█████████ | 538/590 [03:58<00:15,  3.44it/s] 91%|█████████▏| 539/590 [03:58<00:14,  3.44it/s] 92%|█████████▏| 540/590 [03:58<00:14,  3.44it/s] 92%|█████████▏| 541/590 [03:59<00:14,  3.45it/s] 92%|█████████▏| 542/590 [03:59<00:14,  3.30it/s] 92%|█████████▏| 543/590 [03:59<00:14,  3.35it/s] 92%|█████████▏| 544/590 [04:00<00:13,  3.38it/s] 92%|█████████▏| 545/590 [04:00<00:13,  3.40it/s] 93%|█████████▎| 546/590 [04:00<00:12,  3.42it/s] 93%|█████████▎| 547/590 [04:00<00:12,  3.43it/s] 93%|█████████▎| 548/590 [04:01<00:12,  3.30it/s] 93%|█████████▎| 549/590 [04:01<00:12,  3.34it/s] 93%|█████████▎| 550/590 [04:01<00:11,  3.37it/s] 93%|█████████▎| 551/590 [04:02<00:11,  3.40it/s] 94%|█████████▎| 552/590 [04:02<00:11,  3.41it/s] 94%|█████████▎| 553/590 [04:02<00:11,  3.26it/s] 94%|█████████▍| 554/590 [04:03<00:11,  3.26it/s] 94%|█████████▍| 555/590 [04:03<00:10,  3.31it/s] 94%|█████████▍| 556/590 [04:03<00:10,  3.35it/s] 94%|█████████▍| 557/590 [04:03<00:09,  3.38it/s] 95%|█████████▍| 558/590 [04:04<00:09,  3.40it/s] 95%|█████████▍| 559/590 [04:04<00:09,  3.41it/s] 95%|█████████▍| 560/590 [04:04<00:08,  3.42it/s] 95%|█████████▌| 561/590 [04:05<00:08,  3.43it/s] 95%|█████████▌| 562/590 [04:05<00:08,  3.44it/s] 95%|█████████▌| 563/590 [04:05<00:07,  3.44it/s] 96%|█████████▌| 564/590 [04:05<00:07,  3.34it/s] 96%|█████████▌| 565/590 [04:06<00:07,  3.37it/s] 96%|█████████▌| 566/590 [04:06<00:07,  3.40it/s] 96%|█████████▌| 567/590 [04:06<00:06,  3.41it/s] 96%|█████████▋| 568/590 [04:07<00:06,  3.43it/s] 96%|█████████▋| 569/590 [04:07<00:06,  3.44it/s] 97%|█████████▋| 570/590 [04:07<00:05,  3.44it/s] 97%|█████████▋| 571/590 [04:08<00:05,  3.45it/s] 97%|█████████▋| 572/590 [04:08<00:05,  3.45it/s] 97%|█████████▋| 573/590 [04:08<00:04,  3.45it/s] 97%|█████████▋| 574/590 [04:08<00:04,  3.45it/s] 97%|█████████▋| 575/590 [04:09<00:04,  3.45it/s] 98%|█████████▊| 576/590 [04:09<00:04,  3.45it/s] 98%|█████████▊| 577/590 [04:09<00:03,  3.45it/s] 98%|█████████▊| 578/590 [04:10<00:03,  3.45it/s] 98%|█████████▊| 579/590 [04:10<00:03,  3.45it/s] 98%|█████████▊| 580/590 [04:10<00:02,  3.46it/s] 98%|█████████▊| 581/590 [04:10<00:02,  3.44it/s] 99%|█████████▊| 582/590 [04:11<00:02,  3.45it/s] 99%|█████████▉| 583/590 [04:11<00:02,  3.45it/s] 99%|█████████▉| 584/590 [04:11<00:01,  3.45it/s] 99%|█████████▉| 585/590 [04:12<00:01,  3.45it/s] 99%|█████████▉| 586/590 [04:12<00:01,  3.45it/s] 99%|█████████▉| 587/590 [04:12<00:00,  3.45it/s]100%|█████████▉| 588/590 [04:12<00:00,  3.45it/s]100%|█████████▉| 589/590 [04:13<00:00,  3.45it/s]100%|██████████| 590/590 [04:13<00:00,  3.83it/s][INFO|trainer.py:2140] 2023-08-29 02:52:15,867 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:52:15,868 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 02:52:15,868 >>   Batch size = 8
{'eval_loss': 0.9490073919296265, 'eval_runtime': 13.1347, 'eval_samples_per_second': 370.012, 'eval_steps_per_second': 46.29, 'epoch': 4.0}
{'loss': 0.8335, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.39it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.17it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.44it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.68it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.25it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.99it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.79it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.33it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.27it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.26it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.28it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.34it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.37it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.45it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.42it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.45it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.30it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.11it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.26it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.25it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.29it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.37it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.31it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.38it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.44it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.36it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.22it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.26it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.29it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.20it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.32it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.32it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.34it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.33it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.31it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.26it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.21it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.22it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.24it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.16it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.26it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.30it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.32it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.41it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.28it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.25it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.27it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.25it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.27it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.11it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.33it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.30it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.34it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.34it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.17it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.22it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.26it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.21it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.31it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.24it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.32it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.36it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.32it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.27it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.19it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.19it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.25it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.30it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.33it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.33it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.36it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.26it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.32it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.32it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.24it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.15it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.23it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.28it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.35it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.31it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.34it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.23it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.27it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.26it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.25it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.20it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.20it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.25it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.29it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.34it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.31it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.19it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.28it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.27it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.22it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.23it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.21it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.24it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.30it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.33it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.18it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.16it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.15it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.20it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.26it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.25it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.12it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.21it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.19it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.24it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.14it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.08it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.19it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.21it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.25it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.24it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.25it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.22it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.14it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.20it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.19it/s][A                                                 
                                                 [A100%|██████████| 590/590 [04:26<00:00,  3.83it/s]
100%|██████████| 608/608 [00:13<00:00, 46.19it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:52:29,021 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-29 02:52:29,044 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:52:31,347 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:52:31,376 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:52:31,386 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:52:36,049 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:52:36,054 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-236 (score: 0.9390559792518616).
                                                 100%|██████████| 590/590 [04:35<00:00,  3.83it/s]100%|██████████| 590/590 [04:35<00:00,  2.14it/s]
[INFO|trainer.py:1894] 2023-08-29 02:52:37,760 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 02:52:37,780 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:52:40,186 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:52:40,209 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:52:40,222 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:52:40,445 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:40,445 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:40,445 >>   train_loss               =     0.8275
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:40,445 >>   train_runtime            = 0:04:35.31
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:40,445 >>   train_samples            =       7526
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:40,446 >>   train_samples_per_second =    136.677
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:40,446 >>   train_steps_per_second   =      2.143
{'eval_loss': 0.9509726166725159, 'eval_runtime': 13.1356, 'eval_samples_per_second': 369.986, 'eval_steps_per_second': 46.286, 'epoch': 5.0}
{'train_runtime': 275.3199, 'train_samples_per_second': 136.677, 'train_steps_per_second': 2.143, 'train_loss': 0.8274916955980204, 'epoch': 5.0}
08/29/2023 02:52:40 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:52:40,480 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:52:40,480 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 02:52:40,480 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 58.32it/s]  2%|▏         | 12/608 [00:00<00:11, 51.03it/s]  3%|▎         | 18/608 [00:00<00:12, 48.91it/s]  4%|▍         | 23/608 [00:00<00:12, 48.11it/s]  5%|▍         | 28/608 [00:00<00:12, 47.69it/s]  5%|▌         | 33/608 [00:00<00:12, 47.37it/s]  6%|▋         | 38/608 [00:00<00:12, 47.22it/s]  7%|▋         | 43/608 [00:00<00:12, 47.05it/s]  8%|▊         | 48/608 [00:01<00:11, 46.67it/s]  9%|▊         | 53/608 [00:01<00:11, 46.49it/s] 10%|▉         | 58/608 [00:01<00:11, 46.37it/s] 10%|█         | 63/608 [00:01<00:11, 46.50it/s] 11%|█         | 68/608 [00:01<00:11, 46.57it/s] 12%|█▏        | 73/608 [00:01<00:11, 46.61it/s] 13%|█▎        | 78/608 [00:01<00:11, 46.71it/s] 14%|█▎        | 83/608 [00:01<00:11, 46.58it/s] 14%|█▍        | 88/608 [00:01<00:11, 46.71it/s] 15%|█▌        | 93/608 [00:01<00:11, 46.64it/s] 16%|█▌        | 98/608 [00:02<00:10, 46.49it/s] 17%|█▋        | 103/608 [00:02<00:10, 46.56it/s] 18%|█▊        | 108/608 [00:02<00:10, 46.59it/s] 19%|█▊        | 113/608 [00:02<00:10, 46.54it/s] 19%|█▉        | 118/608 [00:02<00:10, 46.62it/s] 20%|██        | 123/608 [00:02<00:10, 46.59it/s] 21%|██        | 128/608 [00:02<00:10, 46.52it/s] 22%|██▏       | 133/608 [00:02<00:10, 46.60it/s] 23%|██▎       | 138/608 [00:02<00:10, 46.59it/s] 24%|██▎       | 143/608 [00:03<00:09, 46.60it/s] 24%|██▍       | 148/608 [00:03<00:09, 46.59it/s] 25%|██▌       | 153/608 [00:03<00:09, 46.58it/s] 26%|██▌       | 158/608 [00:03<00:09, 46.62it/s] 27%|██▋       | 163/608 [00:03<00:09, 46.59it/s] 28%|██▊       | 168/608 [00:03<00:09, 46.60it/s] 28%|██▊       | 173/608 [00:03<00:09, 46.60it/s] 29%|██▉       | 178/608 [00:03<00:09, 46.60it/s] 30%|███       | 183/608 [00:03<00:09, 46.50it/s] 31%|███       | 188/608 [00:04<00:09, 46.48it/s] 32%|███▏      | 193/608 [00:04<00:08, 46.54it/s] 33%|███▎      | 198/608 [00:04<00:08, 46.55it/s] 33%|███▎      | 203/608 [00:04<00:08, 46.59it/s] 34%|███▍      | 208/608 [00:04<00:08, 46.55it/s] 35%|███▌      | 213/608 [00:04<00:08, 46.47it/s] 36%|███▌      | 218/608 [00:04<00:08, 46.55it/s] 37%|███▋      | 223/608 [00:04<00:08, 46.61it/s] 38%|███▊      | 228/608 [00:04<00:08, 46.61it/s] 38%|███▊      | 233/608 [00:04<00:08, 46.50it/s] 39%|███▉      | 238/608 [00:05<00:07, 46.47it/s] 40%|███▉      | 243/608 [00:05<00:07, 46.43it/s] 41%|████      | 248/608 [00:05<00:07, 46.59it/s] 42%|████▏     | 253/608 [00:05<00:07, 46.55it/s] 42%|████▏     | 258/608 [00:05<00:07, 46.56it/s] 43%|████▎     | 263/608 [00:05<00:07, 46.54it/s] 44%|████▍     | 268/608 [00:05<00:07, 46.61it/s] 45%|████▍     | 273/608 [00:05<00:07, 46.59it/s] 46%|████▌     | 278/608 [00:05<00:07, 46.49it/s] 47%|████▋     | 283/608 [00:06<00:06, 46.52it/s] 47%|████▋     | 288/608 [00:06<00:06, 46.51it/s] 48%|████▊     | 293/608 [00:06<00:06, 46.54it/s] 49%|████▉     | 298/608 [00:06<00:06, 46.52it/s] 50%|████▉     | 303/608 [00:06<00:06, 46.54it/s] 51%|█████     | 308/608 [00:06<00:06, 46.53it/s] 51%|█████▏    | 313/608 [00:06<00:06, 46.47it/s] 52%|█████▏    | 318/608 [00:06<00:06, 46.60it/s] 53%|█████▎    | 323/608 [00:06<00:06, 46.62it/s] 54%|█████▍    | 328/608 [00:07<00:06, 46.54it/s] 55%|█████▍    | 333/608 [00:07<00:05, 46.56it/s] 56%|█████▌    | 338/608 [00:07<00:05, 46.48it/s] 56%|█████▋    | 343/608 [00:07<00:05, 46.52it/s] 57%|█████▋    | 348/608 [00:07<00:05, 46.58it/s] 58%|█████▊    | 353/608 [00:07<00:05, 46.54it/s] 59%|█████▉    | 358/608 [00:07<00:05, 46.56it/s] 60%|█████▉    | 363/608 [00:07<00:05, 46.52it/s] 61%|██████    | 368/608 [00:07<00:05, 46.51it/s] 61%|██████▏   | 373/608 [00:07<00:05, 46.53it/s] 62%|██████▏   | 378/608 [00:08<00:04, 46.52it/s] 63%|██████▎   | 383/608 [00:08<00:04, 46.46it/s] 64%|██████▍   | 388/608 [00:08<00:04, 46.53it/s] 65%|██████▍   | 393/608 [00:08<00:04, 46.56it/s] 65%|██████▌   | 398/608 [00:08<00:04, 46.48it/s] 66%|██████▋   | 403/608 [00:08<00:04, 46.47it/s] 67%|██████▋   | 408/608 [00:08<00:04, 46.43it/s] 68%|██████▊   | 413/608 [00:08<00:04, 46.51it/s] 69%|██████▉   | 418/608 [00:08<00:04, 46.57it/s] 70%|██████▉   | 423/608 [00:09<00:03, 46.55it/s] 70%|███████   | 428/608 [00:09<00:03, 46.43it/s] 71%|███████   | 433/608 [00:09<00:03, 46.36it/s] 72%|███████▏  | 438/608 [00:09<00:03, 46.42it/s] 73%|███████▎  | 443/608 [00:09<00:03, 46.47it/s] 74%|███████▎  | 448/608 [00:09<00:03, 46.54it/s] 75%|███████▍  | 453/608 [00:09<00:03, 46.57it/s] 75%|███████▌  | 458/608 [00:09<00:03, 46.47it/s] 76%|███████▌  | 463/608 [00:09<00:03, 46.51it/s] 77%|███████▋  | 468/608 [00:10<00:03, 46.51it/s] 78%|███████▊  | 473/608 [00:10<00:02, 46.52it/s] 79%|███████▊  | 478/608 [00:10<00:02, 46.57it/s] 79%|███████▉  | 483/608 [00:10<00:02, 46.51it/s] 80%|████████  | 488/608 [00:10<00:02, 46.49it/s] 81%|████████  | 493/608 [00:10<00:02, 46.47it/s] 82%|████████▏ | 498/608 [00:10<00:02, 46.46it/s] 83%|████████▎ | 503/608 [00:10<00:02, 46.55it/s] 84%|████████▎ | 508/608 [00:10<00:02, 46.54it/s] 84%|████████▍ | 513/608 [00:10<00:02, 46.54it/s] 85%|████████▌ | 518/608 [00:11<00:01, 46.60it/s] 86%|████████▌ | 523/608 [00:11<00:01, 46.46it/s] 87%|████████▋ | 528/608 [00:11<00:01, 46.49it/s] 88%|████████▊ | 533/608 [00:11<00:01, 46.49it/s] 88%|████████▊ | 538/608 [00:11<00:01, 46.54it/s] 89%|████████▉ | 543/608 [00:11<00:01, 46.55it/s] 90%|█████████ | 548/608 [00:11<00:01, 46.55it/s] 91%|█████████ | 553/608 [00:11<00:01, 46.42it/s] 92%|█████████▏| 558/608 [00:11<00:01, 46.47it/s] 93%|█████████▎| 563/608 [00:12<00:00, 46.44it/s] 93%|█████████▎| 568/608 [00:12<00:00, 46.46it/s] 94%|█████████▍| 573/608 [00:12<00:00, 46.43it/s] 95%|█████████▌| 578/608 [00:12<00:00, 46.49it/s] 96%|█████████▌| 583/608 [00:12<00:00, 46.38it/s] 97%|█████████▋| 588/608 [00:12<00:00, 46.42it/s] 98%|█████████▊| 593/608 [00:12<00:00, 46.50it/s] 98%|█████████▊| 598/608 [00:12<00:00, 46.46it/s] 99%|█████████▉| 603/608 [00:12<00:00, 46.48it/s]100%|██████████| 608/608 [00:13<00:00, 46.56it/s]100%|██████████| 608/608 [00:13<00:00, 46.62it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:52:53,547 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:53,547 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:53,547 >>   eval_loss               =     0.9391
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:53,547 >>   eval_runtime            = 0:00:13.06
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:53,547 >>   eval_samples            =       4860
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:53,547 >>   eval_samples_per_second =    371.944
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:53,547 >>   eval_steps_per_second   =     46.531
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:53,547 >>   perplexity              =     2.5576
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:00,356 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:00,360 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:00,360 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:00,360 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:00,360 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:53:00,984 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:53:00,985 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:53:01,560 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:53:02,597 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:53:02,597 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:05,429 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:05,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:05,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:05,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:53:05,433 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:53:06,061 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:53:06,062 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:53:06,641 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:53:06,798 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:53:06,798 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-236
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-354
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'labels': ['headquarters location', 'licensed to broadcast to', 'member of political party', 'narrative location', 'notable work'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14287
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14387, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.61it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:23,  1.59it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:28,  1.54it/s]Extractor Predicting: 47it [00:29,  1.53it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:30,  1.54it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:32,  1.49it/s]Extractor Predicting: 53it [00:33,  1.50it/s]Extractor Predicting: 54it [00:34,  1.52it/s]Extractor Predicting: 55it [00:34,  1.52it/s]Extractor Predicting: 56it [00:35,  1.52it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:36,  1.48it/s]Extractor Predicting: 59it [00:37,  1.50it/s]Extractor Predicting: 60it [00:38,  1.49it/s]Extractor Predicting: 61it [00:38,  1.54it/s]Extractor Predicting: 62it [00:39,  1.54it/s]Extractor Predicting: 63it [00:40,  1.52it/s]Extractor Predicting: 64it [00:40,  1.53it/s]Extractor Predicting: 65it [00:41,  1.54it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:42,  1.49it/s]Extractor Predicting: 68it [00:43,  1.54it/s]Extractor Predicting: 69it [00:44,  1.53it/s]Extractor Predicting: 70it [00:44,  1.53it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:45,  1.56it/s]Extractor Predicting: 73it [00:46,  1.59it/s]Extractor Predicting: 74it [00:47,  1.59it/s]Extractor Predicting: 75it [00:47,  1.59it/s]Extractor Predicting: 76it [00:48,  1.62it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:49,  1.60it/s]Extractor Predicting: 79it [00:50,  1.58it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.45it/s]Extractor Predicting: 82it [00:52,  1.49it/s]Extractor Predicting: 83it [00:52,  1.53it/s]Extractor Predicting: 84it [00:53,  1.55it/s]Extractor Predicting: 85it [00:54,  1.56it/s]Extractor Predicting: 86it [00:54,  1.59it/s]Extractor Predicting: 87it [00:55,  1.59it/s]Extractor Predicting: 88it [00:56,  1.58it/s]Extractor Predicting: 89it [00:56,  1.58it/s]Extractor Predicting: 90it [00:57,  1.58it/s]Extractor Predicting: 91it [00:58,  1.58it/s]Extractor Predicting: 92it [00:58,  1.56it/s]Extractor Predicting: 93it [00:59,  1.59it/s]Extractor Predicting: 94it [00:59,  1.59it/s]Extractor Predicting: 95it [01:00,  1.57it/s]Extractor Predicting: 96it [01:01,  1.57it/s]Extractor Predicting: 97it [01:01,  1.58it/s]Extractor Predicting: 98it [01:02,  1.62it/s]Extractor Predicting: 99it [01:03,  1.57it/s]Extractor Predicting: 100it [01:03,  1.57it/s]Extractor Predicting: 101it [01:04,  1.60it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:05,  1.55it/s]Extractor Predicting: 104it [01:06,  1.56it/s]Extractor Predicting: 105it [01:06,  1.57it/s]Extractor Predicting: 106it [01:07,  1.59it/s]Extractor Predicting: 107it [01:08,  1.61it/s]Extractor Predicting: 108it [01:08,  1.57it/s]Extractor Predicting: 109it [01:09,  1.57it/s]Extractor Predicting: 110it [01:10,  1.58it/s]Extractor Predicting: 111it [01:10,  1.61it/s]Extractor Predicting: 112it [01:11,  1.61it/s]Extractor Predicting: 113it [01:11,  1.62it/s]Extractor Predicting: 114it [01:12,  1.61it/s]Extractor Predicting: 115it [01:13,  1.61it/s]Extractor Predicting: 116it [01:13,  1.60it/s]Extractor Predicting: 117it [01:14,  1.62it/s]Extractor Predicting: 118it [01:14,  1.62it/s]Extractor Predicting: 119it [01:15,  1.63it/s]Extractor Predicting: 120it [01:16,  1.61it/s]Extractor Predicting: 121it [01:16,  1.58it/s]Extractor Predicting: 122it [01:17,  1.59it/s]Extractor Predicting: 123it [01:18,  1.59it/s]Extractor Predicting: 124it [01:18,  1.58it/s]Extractor Predicting: 125it [01:19,  1.52it/s]Extractor Predicting: 126it [01:20,  1.52it/s]Extractor Predicting: 127it [01:20,  1.54it/s]Extractor Predicting: 128it [01:21,  1.54it/s]Extractor Predicting: 129it [01:22,  1.54it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.56it/s]Extractor Predicting: 132it [01:23,  1.57it/s]Extractor Predicting: 133it [01:24,  1.52it/s]Extractor Predicting: 134it [01:25,  1.54it/s]Extractor Predicting: 135it [01:25,  1.53it/s]Extractor Predicting: 136it [01:26,  1.54it/s]Extractor Predicting: 137it [01:27,  1.53it/s]Extractor Predicting: 138it [01:27,  1.52it/s]Extractor Predicting: 139it [01:28,  1.53it/s]Extractor Predicting: 140it [01:29,  1.53it/s]Extractor Predicting: 141it [01:29,  1.52it/s]Extractor Predicting: 142it [01:30,  1.55it/s]Extractor Predicting: 143it [01:31,  1.54it/s]Extractor Predicting: 144it [01:31,  1.54it/s]Extractor Predicting: 145it [01:32,  1.55it/s]Extractor Predicting: 146it [01:33,  1.58it/s]Extractor Predicting: 147it [01:33,  1.52it/s]Extractor Predicting: 148it [01:34,  1.51it/s]Extractor Predicting: 149it [01:35,  1.52it/s]Extractor Predicting: 150it [01:35,  1.54it/s]Extractor Predicting: 151it [01:36,  1.57it/s]Extractor Predicting: 152it [01:36,  1.58it/s]Extractor Predicting: 153it [01:37,  1.63it/s]Extractor Predicting: 154it [01:38,  1.61it/s]Extractor Predicting: 155it [01:38,  1.57it/s]Extractor Predicting: 156it [01:39,  1.58it/s]Extractor Predicting: 157it [01:40,  1.59it/s]Extractor Predicting: 158it [01:40,  1.58it/s]Extractor Predicting: 159it [01:41,  1.55it/s]Extractor Predicting: 160it [01:41,  1.58it/s]Extractor Predicting: 161it [01:42,  1.55it/s]Extractor Predicting: 162it [01:43,  1.55it/s]Extractor Predicting: 163it [01:43,  1.54it/s]Extractor Predicting: 164it [01:44,  1.56it/s]Extractor Predicting: 165it [01:45,  1.55it/s]Extractor Predicting: 166it [01:45,  1.55it/s]Extractor Predicting: 167it [01:46,  1.53it/s]Extractor Predicting: 168it [01:47,  1.56it/s]Extractor Predicting: 169it [01:47,  1.54it/s]Extractor Predicting: 170it [01:48,  1.56it/s]Extractor Predicting: 171it [01:49,  1.57it/s]Extractor Predicting: 172it [01:49,  1.56it/s]Extractor Predicting: 173it [01:50,  1.53it/s]Extractor Predicting: 174it [01:51,  1.57it/s]Extractor Predicting: 175it [01:51,  1.55it/s]Extractor Predicting: 176it [01:52,  1.55it/s]Extractor Predicting: 177it [01:53,  1.50it/s]Extractor Predicting: 178it [01:53,  1.48it/s]Extractor Predicting: 179it [01:54,  1.38it/s]Extractor Predicting: 180it [01:55,  1.40it/s]Extractor Predicting: 181it [01:55,  1.42it/s]Extractor Predicting: 182it [01:56,  1.48it/s]Extractor Predicting: 183it [01:57,  1.46it/s]Extractor Predicting: 184it [01:57,  1.45it/s]Extractor Predicting: 185it [01:58,  1.44it/s]Extractor Predicting: 186it [01:59,  1.45it/s]Extractor Predicting: 187it [02:00,  1.45it/s]Extractor Predicting: 188it [02:00,  1.46it/s]Extractor Predicting: 189it [02:01,  1.48it/s]Extractor Predicting: 190it [02:02,  1.47it/s]Extractor Predicting: 191it [02:02,  1.48it/s]Extractor Predicting: 192it [02:03,  1.50it/s]Extractor Predicting: 193it [02:03,  1.54it/s]Extractor Predicting: 194it [02:04,  1.54it/s]Extractor Predicting: 195it [02:04,  1.96it/s]Extractor Predicting: 195it [02:04,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:20,636 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:20,642 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:20,642 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:20,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:20,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:55:21,252 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:55:21,253 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:55:21,836 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:55:22,869 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:55:22,869 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:25,802 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:25,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:25,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:25,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:25,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:55:26,448 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:55:26,449 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:55:27,024 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:55:27,179 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:55:27,179 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.46864686468646866,
  "recall": 0.05843621399176955,
  "score": 0.10391511159897548,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21244
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21344, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.59it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:21,  1.44it/s]Extractor Predicting: 34it [00:21,  1.47it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:24,  1.54it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.60it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:30,  1.59it/s]Extractor Predicting: 48it [00:30,  1.58it/s]Extractor Predicting: 49it [00:31,  1.62it/s]Extractor Predicting: 50it [00:31,  1.61it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:33,  1.67it/s]Extractor Predicting: 54it [00:34,  1.68it/s]Extractor Predicting: 55it [00:34,  1.67it/s]Extractor Predicting: 56it [00:35,  1.64it/s]Extractor Predicting: 57it [00:36,  1.64it/s]Extractor Predicting: 58it [00:36,  1.69it/s]Extractor Predicting: 59it [00:37,  1.68it/s]Extractor Predicting: 60it [00:37,  1.67it/s]Extractor Predicting: 61it [00:38,  1.62it/s]Extractor Predicting: 62it [00:39,  1.66it/s]Extractor Predicting: 63it [00:39,  1.66it/s]Extractor Predicting: 64it [00:40,  1.61it/s]Extractor Predicting: 65it [00:40,  1.65it/s]Extractor Predicting: 66it [00:41,  1.64it/s]Extractor Predicting: 67it [00:42,  1.67it/s]Extractor Predicting: 68it [00:42,  1.67it/s]Extractor Predicting: 69it [00:43,  1.69it/s]Extractor Predicting: 70it [00:43,  1.68it/s]Extractor Predicting: 71it [00:44,  1.64it/s]Extractor Predicting: 72it [00:45,  1.63it/s]Extractor Predicting: 73it [00:45,  1.63it/s]Extractor Predicting: 74it [00:46,  1.61it/s]Extractor Predicting: 75it [00:47,  1.60it/s]Extractor Predicting: 76it [00:47,  1.58it/s]Extractor Predicting: 77it [00:48,  1.59it/s]Extractor Predicting: 78it [00:48,  1.59it/s]Extractor Predicting: 79it [00:49,  1.64it/s]Extractor Predicting: 80it [00:50,  1.66it/s]Extractor Predicting: 81it [00:50,  1.69it/s]Extractor Predicting: 82it [00:51,  1.67it/s]Extractor Predicting: 83it [00:51,  1.68it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:53,  1.63it/s]Extractor Predicting: 86it [00:53,  1.67it/s]Extractor Predicting: 87it [00:54,  1.68it/s]Extractor Predicting: 88it [00:54,  1.74it/s]Extractor Predicting: 89it [00:55,  1.72it/s]Extractor Predicting: 90it [00:56,  1.73it/s]Extractor Predicting: 91it [00:56,  1.81it/s]Extractor Predicting: 92it [00:57,  1.76it/s]Extractor Predicting: 93it [00:57,  1.76it/s]Extractor Predicting: 94it [00:58,  1.76it/s]Extractor Predicting: 95it [00:58,  1.75it/s]Extractor Predicting: 96it [00:59,  1.71it/s]Extractor Predicting: 97it [01:00,  1.67it/s]Extractor Predicting: 98it [01:00,  1.72it/s]Extractor Predicting: 99it [01:01,  1.72it/s]Extractor Predicting: 100it [01:01,  1.72it/s]Extractor Predicting: 101it [01:02,  1.73it/s]Extractor Predicting: 102it [01:02,  1.72it/s]Extractor Predicting: 103it [01:03,  1.73it/s]Extractor Predicting: 104it [01:04,  1.71it/s]Extractor Predicting: 105it [01:04,  1.76it/s]Extractor Predicting: 106it [01:05,  1.74it/s]Extractor Predicting: 107it [01:05,  1.77it/s]Extractor Predicting: 108it [01:06,  1.74it/s]Extractor Predicting: 109it [01:06,  1.78it/s]Extractor Predicting: 110it [01:07,  1.73it/s]Extractor Predicting: 111it [01:08,  1.75it/s]Extractor Predicting: 112it [01:08,  1.70it/s]Extractor Predicting: 113it [01:09,  1.69it/s]Extractor Predicting: 114it [01:09,  1.70it/s]Extractor Predicting: 115it [01:10,  1.67it/s]Extractor Predicting: 116it [01:11,  1.68it/s]Extractor Predicting: 117it [01:11,  1.67it/s]Extractor Predicting: 118it [01:12,  1.64it/s]Extractor Predicting: 119it [01:12,  1.64it/s]Extractor Predicting: 120it [01:13,  1.65it/s]Extractor Predicting: 121it [01:14,  1.69it/s]Extractor Predicting: 122it [01:14,  1.66it/s]Extractor Predicting: 123it [01:15,  1.52it/s]Extractor Predicting: 124it [01:16,  1.53it/s]Extractor Predicting: 125it [01:16,  1.54it/s]Extractor Predicting: 126it [01:17,  1.58it/s]Extractor Predicting: 127it [01:17,  1.62it/s]Extractor Predicting: 128it [01:18,  1.61it/s]Extractor Predicting: 129it [01:19,  1.62it/s]Extractor Predicting: 130it [01:19,  1.61it/s]Extractor Predicting: 131it [01:20,  1.58it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:21,  1.59it/s]Extractor Predicting: 134it [01:22,  1.60it/s]Extractor Predicting: 135it [01:22,  1.64it/s]Extractor Predicting: 136it [01:23,  1.61it/s]Extractor Predicting: 137it [01:24,  1.65it/s]Extractor Predicting: 138it [01:24,  1.63it/s]Extractor Predicting: 139it [01:25,  1.66it/s]Extractor Predicting: 140it [01:25,  1.67it/s]Extractor Predicting: 141it [01:26,  1.63it/s]Extractor Predicting: 142it [01:27,  1.66it/s]Extractor Predicting: 143it [01:27,  1.64it/s]Extractor Predicting: 144it [01:28,  1.60it/s]Extractor Predicting: 145it [01:29,  1.57it/s]Extractor Predicting: 146it [01:29,  1.58it/s]Extractor Predicting: 147it [01:30,  1.62it/s]Extractor Predicting: 148it [01:31,  1.58it/s]Extractor Predicting: 149it [01:31,  1.58it/s]Extractor Predicting: 150it [01:32,  1.62it/s]Extractor Predicting: 151it [01:32,  1.62it/s]Extractor Predicting: 152it [01:33,  1.61it/s]Extractor Predicting: 153it [01:34,  1.64it/s]Extractor Predicting: 154it [01:34,  1.65it/s]Extractor Predicting: 155it [01:35,  1.61it/s]Extractor Predicting: 156it [01:35,  1.63it/s]Extractor Predicting: 157it [01:36,  1.61it/s]Extractor Predicting: 158it [01:37,  1.45it/s]Extractor Predicting: 159it [01:38,  1.47it/s]Extractor Predicting: 160it [01:38,  1.52it/s]Extractor Predicting: 161it [01:39,  1.53it/s]Extractor Predicting: 162it [01:39,  1.56it/s]Extractor Predicting: 163it [01:40,  1.58it/s]Extractor Predicting: 164it [01:41,  1.58it/s]Extractor Predicting: 165it [01:41,  1.57it/s]Extractor Predicting: 166it [01:42,  1.55it/s]Extractor Predicting: 167it [01:43,  1.56it/s]Extractor Predicting: 168it [01:43,  1.60it/s]Extractor Predicting: 169it [01:44,  1.60it/s]Extractor Predicting: 170it [01:44,  1.57it/s]Extractor Predicting: 171it [01:45,  1.55it/s]Extractor Predicting: 172it [01:46,  1.60it/s]Extractor Predicting: 173it [01:46,  1.59it/s]Extractor Predicting: 174it [01:47,  1.57it/s]Extractor Predicting: 175it [01:48,  1.57it/s]Extractor Predicting: 176it [01:48,  1.61it/s]Extractor Predicting: 177it [01:49,  1.62it/s]Extractor Predicting: 178it [01:49,  1.65it/s]Extractor Predicting: 179it [01:50,  1.68it/s]Extractor Predicting: 180it [01:51,  1.65it/s]Extractor Predicting: 181it [01:51,  1.57it/s]Extractor Predicting: 182it [01:52,  1.57it/s]Extractor Predicting: 183it [01:53,  1.59it/s]Extractor Predicting: 184it [01:53,  1.57it/s]Extractor Predicting: 185it [01:54,  1.56it/s]Extractor Predicting: 186it [01:55,  1.57it/s]Extractor Predicting: 187it [01:55,  1.59it/s]Extractor Predicting: 188it [01:56,  1.63it/s]Extractor Predicting: 189it [01:56,  1.65it/s]Extractor Predicting: 190it [01:57,  1.63it/s]Extractor Predicting: 191it [01:58,  1.63it/s]Extractor Predicting: 192it [01:58,  1.62it/s]Extractor Predicting: 193it [01:59,  1.65it/s]Extractor Predicting: 194it [01:59,  1.70it/s]Extractor Predicting: 195it [02:00,  1.73it/s]Extractor Predicting: 196it [02:00,  1.66it/s]Extractor Predicting: 197it [02:01,  1.65it/s]Extractor Predicting: 198it [02:02,  1.62it/s]Extractor Predicting: 199it [02:02,  1.69it/s]Extractor Predicting: 200it [02:03,  1.68it/s]Extractor Predicting: 201it [02:04,  1.66it/s]Extractor Predicting: 202it [02:04,  1.69it/s]Extractor Predicting: 203it [02:05,  1.69it/s]Extractor Predicting: 204it [02:05,  1.63it/s]Extractor Predicting: 205it [02:06,  1.64it/s]Extractor Predicting: 206it [02:07,  1.63it/s]Extractor Predicting: 207it [02:07,  1.62it/s]Extractor Predicting: 208it [02:08,  1.62it/s]Extractor Predicting: 209it [02:08,  1.63it/s]Extractor Predicting: 210it [02:09,  1.60it/s]Extractor Predicting: 211it [02:10,  1.59it/s]Extractor Predicting: 212it [02:10,  1.60it/s]Extractor Predicting: 213it [02:11,  1.64it/s]Extractor Predicting: 214it [02:12,  1.63it/s]Extractor Predicting: 215it [02:12,  1.65it/s]Extractor Predicting: 216it [02:13,  1.66it/s]Extractor Predicting: 217it [02:13,  1.67it/s]Extractor Predicting: 218it [02:14,  1.61it/s]Extractor Predicting: 219it [02:15,  1.60it/s]Extractor Predicting: 220it [02:15,  1.58it/s]Extractor Predicting: 221it [02:16,  1.52it/s]Extractor Predicting: 222it [02:17,  1.50it/s]Extractor Predicting: 223it [02:17,  1.56it/s]Extractor Predicting: 224it [02:18,  1.55it/s]Extractor Predicting: 225it [02:19,  1.55it/s]Extractor Predicting: 226it [02:19,  1.54it/s]Extractor Predicting: 227it [02:20,  1.56it/s]Extractor Predicting: 228it [02:20,  1.57it/s]Extractor Predicting: 229it [02:21,  1.59it/s]Extractor Predicting: 230it [02:22,  1.58it/s]Extractor Predicting: 231it [02:22,  1.56it/s]Extractor Predicting: 232it [02:23,  1.58it/s]Extractor Predicting: 233it [02:24,  1.59it/s]Extractor Predicting: 234it [02:24,  1.63it/s]Extractor Predicting: 235it [02:25,  1.64it/s]Extractor Predicting: 236it [02:25,  1.70it/s]Extractor Predicting: 237it [02:26,  1.71it/s]Extractor Predicting: 238it [02:26,  1.70it/s]Extractor Predicting: 239it [02:27,  1.69it/s]Extractor Predicting: 240it [02:28,  1.66it/s]Extractor Predicting: 241it [02:28,  1.63it/s]Extractor Predicting: 242it [02:29,  1.61it/s]Extractor Predicting: 243it [02:30,  1.65it/s]Extractor Predicting: 244it [02:30,  1.66it/s]Extractor Predicting: 245it [02:31,  1.66it/s]Extractor Predicting: 246it [02:31,  1.59it/s]Extractor Predicting: 247it [02:32,  1.60it/s]Extractor Predicting: 248it [02:33,  1.57it/s]Extractor Predicting: 249it [02:33,  1.55it/s]Extractor Predicting: 250it [02:34,  1.53it/s]Extractor Predicting: 251it [02:35,  1.53it/s]Extractor Predicting: 252it [02:35,  1.53it/s]Extractor Predicting: 253it [02:36,  1.54it/s]Extractor Predicting: 254it [02:37,  1.56it/s]Extractor Predicting: 255it [02:37,  1.59it/s]Extractor Predicting: 256it [02:38,  1.53it/s]Extractor Predicting: 257it [02:39,  1.53it/s]Extractor Predicting: 258it [02:39,  1.52it/s]Extractor Predicting: 259it [02:40,  1.54it/s]Extractor Predicting: 260it [02:41,  1.53it/s]Extractor Predicting: 261it [02:41,  1.50it/s]Extractor Predicting: 262it [02:42,  1.52it/s]Extractor Predicting: 263it [02:43,  1.53it/s]Extractor Predicting: 264it [02:43,  1.54it/s]Extractor Predicting: 265it [02:44,  1.53it/s]Extractor Predicting: 266it [02:44,  1.52it/s]Extractor Predicting: 267it [02:45,  1.52it/s]Extractor Predicting: 268it [02:46,  1.38it/s]Extractor Predicting: 269it [02:47,  1.43it/s]Extractor Predicting: 270it [02:47,  1.48it/s]Extractor Predicting: 271it [02:48,  1.51it/s]Extractor Predicting: 272it [02:49,  1.52it/s]Extractor Predicting: 273it [02:49,  1.52it/s]Extractor Predicting: 274it [02:50,  1.51it/s]Extractor Predicting: 275it [02:51,  1.51it/s]Extractor Predicting: 276it [02:51,  1.53it/s]Extractor Predicting: 277it [02:52,  1.55it/s]Extractor Predicting: 278it [02:52,  1.54it/s]Extractor Predicting: 279it [02:53,  1.54it/s]Extractor Predicting: 280it [02:54,  1.59it/s]Extractor Predicting: 281it [02:54,  1.61it/s]Extractor Predicting: 282it [02:55,  1.64it/s]Extractor Predicting: 283it [02:56,  1.61it/s]Extractor Predicting: 284it [02:56,  1.59it/s]Extractor Predicting: 285it [02:57,  1.59it/s]Extractor Predicting: 286it [02:57,  1.65it/s]Extractor Predicting: 287it [02:58,  1.67it/s]Extractor Predicting: 288it [02:59,  1.61it/s]Extractor Predicting: 289it [02:59,  1.60it/s]Extractor Predicting: 290it [03:00,  1.61it/s]Extractor Predicting: 291it [03:00,  1.60it/s]Extractor Predicting: 292it [03:01,  1.66it/s]Extractor Predicting: 293it [03:02,  1.66it/s]Extractor Predicting: 294it [03:02,  1.68it/s]Extractor Predicting: 295it [03:03,  1.73it/s]Extractor Predicting: 296it [03:03,  1.73it/s]Extractor Predicting: 297it [03:04,  1.72it/s]Extractor Predicting: 298it [03:05,  1.64it/s]Extractor Predicting: 299it [03:05,  1.54it/s]Extractor Predicting: 300it [03:06,  1.52it/s]Extractor Predicting: 301it [03:07,  1.52it/s]Extractor Predicting: 302it [03:07,  1.55it/s]Extractor Predicting: 303it [03:08,  1.58it/s]Extractor Predicting: 304it [03:09,  1.58it/s]Extractor Predicting: 305it [03:09,  1.55it/s]Extractor Predicting: 306it [03:10,  1.54it/s]Extractor Predicting: 307it [03:11,  1.52it/s]Extractor Predicting: 308it [03:11,  1.52it/s]Extractor Predicting: 309it [03:12,  1.49it/s]Extractor Predicting: 310it [03:13,  1.49it/s]Extractor Predicting: 311it [03:13,  1.49it/s]Extractor Predicting: 312it [03:14,  1.51it/s]Extractor Predicting: 313it [03:15,  1.52it/s]Extractor Predicting: 314it [03:15,  1.53it/s]Extractor Predicting: 315it [03:16,  1.58it/s]Extractor Predicting: 316it [03:16,  1.54it/s]Extractor Predicting: 317it [03:17,  1.52it/s]Extractor Predicting: 318it [03:18,  1.47it/s]Extractor Predicting: 319it [03:19,  1.48it/s]Extractor Predicting: 320it [03:19,  1.49it/s]Extractor Predicting: 321it [03:20,  1.48it/s]Extractor Predicting: 322it [03:21,  1.47it/s]Extractor Predicting: 323it [03:21,  1.47it/s]Extractor Predicting: 324it [03:22,  1.47it/s]Extractor Predicting: 325it [03:23,  1.47it/s]Extractor Predicting: 326it [03:23,  1.46it/s]Extractor Predicting: 327it [03:24,  1.45it/s]Extractor Predicting: 328it [03:25,  1.46it/s]Extractor Predicting: 329it [03:25,  1.46it/s]Extractor Predicting: 330it [03:26,  1.46it/s]Extractor Predicting: 331it [03:27,  1.45it/s]Extractor Predicting: 332it [03:27,  1.45it/s]Extractor Predicting: 333it [03:28,  1.44it/s]Extractor Predicting: 334it [03:29,  1.46it/s]Extractor Predicting: 335it [03:30,  1.45it/s]Extractor Predicting: 336it [03:30,  1.42it/s]Extractor Predicting: 337it [03:31,  1.43it/s]Extractor Predicting: 338it [03:32,  1.43it/s]Extractor Predicting: 339it [03:32,  1.44it/s]Extractor Predicting: 340it [03:33,  1.45it/s]Extractor Predicting: 341it [03:34,  1.47it/s]Extractor Predicting: 342it [03:34,  1.48it/s]Extractor Predicting: 343it [03:35,  1.50it/s]Extractor Predicting: 344it [03:36,  1.53it/s]Extractor Predicting: 345it [03:36,  1.53it/s]Extractor Predicting: 346it [03:37,  1.65it/s]Extractor Predicting: 346it [03:37,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:12,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:12,591 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:12,591 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:12,591 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:12,591 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:59:13,188 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:59:13,193 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:59:13,769 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:59:14,818 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:59:14,818 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:17,692 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:17,701 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:17,701 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:17,701 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:59:17,701 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:59:18,340 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:59:18,341 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:59:18,608 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:59:18,745 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:59:18,745 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3502824858757062,
  "recall": 0.014950566674704605,
  "score": 0.028677150786308968,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6093
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6193, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.55it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.48it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:12,  1.43it/s]Extractor Predicting: 21it [00:13,  1.45it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.41it/s]Extractor Predicting: 24it [00:15,  1.41it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:16,  1.49it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:19,  1.68it/s]Extractor Predicting: 31it [00:19,  1.77it/s]Extractor Predicting: 32it [00:20,  1.82it/s]Extractor Predicting: 33it [00:20,  1.86it/s]Extractor Predicting: 34it [00:21,  1.86it/s]Extractor Predicting: 35it [00:21,  1.89it/s]Extractor Predicting: 36it [00:22,  1.87it/s]Extractor Predicting: 37it [00:23,  1.84it/s]Extractor Predicting: 38it [00:23,  1.87it/s]Extractor Predicting: 39it [00:24,  1.85it/s]Extractor Predicting: 40it [00:24,  1.85it/s]Extractor Predicting: 41it [00:25,  1.84it/s]Extractor Predicting: 42it [00:25,  1.83it/s]Extractor Predicting: 43it [00:26,  1.83it/s]Extractor Predicting: 44it [00:26,  1.85it/s]Extractor Predicting: 45it [00:27,  1.90it/s]Extractor Predicting: 46it [00:27,  1.78it/s]Extractor Predicting: 47it [00:28,  1.80it/s]Extractor Predicting: 48it [00:29,  1.81it/s]Extractor Predicting: 49it [00:29,  1.88it/s]Extractor Predicting: 50it [00:30,  1.83it/s]Extractor Predicting: 51it [00:30,  1.79it/s]Extractor Predicting: 52it [00:31,  1.84it/s]Extractor Predicting: 53it [00:31,  1.84it/s]Extractor Predicting: 54it [00:32,  1.85it/s]Extractor Predicting: 55it [00:32,  1.87it/s]Extractor Predicting: 56it [00:33,  1.86it/s]Extractor Predicting: 57it [00:33,  1.89it/s]Extractor Predicting: 58it [00:34,  1.77it/s]Extractor Predicting: 59it [00:35,  1.65it/s]Extractor Predicting: 60it [00:35,  1.56it/s]Extractor Predicting: 61it [00:36,  1.51it/s]Extractor Predicting: 62it [00:37,  1.50it/s]Extractor Predicting: 63it [00:37,  1.48it/s]Extractor Predicting: 64it [00:38,  1.48it/s]Extractor Predicting: 65it [00:39,  1.46it/s]Extractor Predicting: 66it [00:40,  1.46it/s]Extractor Predicting: 67it [00:40,  1.46it/s]Extractor Predicting: 68it [00:41,  1.45it/s]Extractor Predicting: 69it [00:42,  1.44it/s]Extractor Predicting: 70it [00:42,  1.46it/s]Extractor Predicting: 71it [00:43,  1.44it/s]Extractor Predicting: 72it [00:44,  1.45it/s]Extractor Predicting: 73it [00:44,  1.46it/s]Extractor Predicting: 74it [00:45,  1.48it/s]Extractor Predicting: 75it [00:46,  1.50it/s]Extractor Predicting: 76it [00:46,  1.50it/s]Extractor Predicting: 77it [00:47,  1.85it/s]Extractor Predicting: 77it [00:47,  1.64it/s]
[INFO|configuration_utils.py:515] 2023-08-29 03:00:06,879 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:00:06,881 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:00:06,887 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:00:06,888 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 03:00:06,890 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:00:10,023 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 03:00:10,028 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 03:00:10,047 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:00:10,048 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:00:10,056 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:00:10,061 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:00:10,061 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:00:10,061 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:00:10,061 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:00:10,061 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:00:10,061 >> loading file outputs/wrapper/wiki/unseen_10_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3333333333333333,
  "recall": 0.001492908683752177,
  "score": 0.0029725043349021546,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 03:00:10,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:10,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:11,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:12,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:13,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:13,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:14,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:15,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:15,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:16,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:17,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:18,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:18,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:19,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:20,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:20,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:21,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:22,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:22,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:23,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:24,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:24,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:25,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:47, 16.22s/it][WARNING|generation_utils.py:914] 2023-08-29 03:00:26,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:27,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:27,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:28,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:29,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:29,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:30,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:31,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:31,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:32,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:33,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:33,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:34,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:35,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:35,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:36,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:37,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:37,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:38,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:39,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:39,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:40,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:41,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:23, 15.64s/it][WARNING|generation_utils.py:914] 2023-08-29 03:00:41,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:42,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:43,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:44,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:44,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:45,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:46,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:46,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:47,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:48,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:48,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:49,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:50,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:50,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:51,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:52,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:53,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:54,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:55,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:55,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:56,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:57,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:57,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:48<03:14, 16.19s/it][WARNING|generation_utils.py:914] 2023-08-29 03:00:58,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:59,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:00,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:01,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:01,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:02,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:03,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:04,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:04,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:05,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:06,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:07,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:08,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:08,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:09,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:10,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:11,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:12,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:12,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:13,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:14,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:15,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:16,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:16,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:07<03:11, 17.42s/it][WARNING|generation_utils.py:914] 2023-08-29 03:01:17,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:18,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:19,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:20,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:21,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:22,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:23,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:24,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:24,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:25,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:26,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:27,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:27,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:28,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:29,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:30,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:31,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:32,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:32,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:33,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:34,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:35,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:35,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:26<02:58, 17.88s/it][WARNING|generation_utils.py:914] 2023-08-29 03:01:36,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:37,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:38,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:38,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:39,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:40,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:40,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:41,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:42,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:43,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:43,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:44,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:45,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:45,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:46,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:47,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:48,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:49,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:50,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:50,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:51,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:52,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:53,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:53,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:54,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:45<02:43, 18.18s/it][WARNING|generation_utils.py:914] 2023-08-29 03:01:55,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:56,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:57,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:58,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:58,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:59,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:00,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:01,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:02,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:03,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:03,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:04,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:06,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:07,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:08,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:08,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:09,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:10,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:11,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:12,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:12,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:13,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:14,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:15,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:05<02:32, 19.02s/it][WARNING|generation_utils.py:914] 2023-08-29 03:02:16,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:17,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:18,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:18,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:19,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:20,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:21,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:21,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:22,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:23,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:24,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:25,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:25,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:26,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:27,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:28,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:28,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:29,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:30,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:31,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:31,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:32,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:33,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:33,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:34,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:35,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:36,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:36,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:27<02:18, 19.77s/it][WARNING|generation_utils.py:914] 2023-08-29 03:02:37,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:38,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:38,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:39,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:40,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:41,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:42,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:42,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:43,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:44,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:45,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:46,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:46,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:47,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:48,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:49,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:49,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:50,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:51,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:51,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:52,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:53,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:54,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:54,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:45<01:55, 19.28s/it][WARNING|generation_utils.py:914] 2023-08-29 03:02:55,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:56,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:57,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:58,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:58,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:59,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:00,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:00,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:01,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:02,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:03,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:03,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:04,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:05,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:06,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:06,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:07,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:08,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:09,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:09,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:10,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:11,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:01<01:31, 18.37s/it][WARNING|generation_utils.py:914] 2023-08-29 03:03:12,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:12,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:13,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:14,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:14,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:15,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:16,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:16,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:17,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:18,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:18,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:19,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:20,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:21,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:21,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:22,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:23,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:23,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:24,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:25,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:26,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:26,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:27,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:17<01:10, 17.66s/it][WARNING|generation_utils.py:914] 2023-08-29 03:03:28,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:28,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:29,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:30,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:31,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:31,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:32,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:33,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:33,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:34,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:35,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:36,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:36,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:37,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:38,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:38,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:39,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:40,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:41,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:41,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:42,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:43,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:33<00:51, 17.15s/it][WARNING|generation_utils.py:914] 2023-08-29 03:03:44,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:44,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:45,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:46,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:47,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:48,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:49,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:50,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:51,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:52,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:53,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:53,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:55,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:55,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:56,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:57,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:58,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:58,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:03:59,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:00,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:01,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:51<00:34, 17.32s/it][WARNING|generation_utils.py:914] 2023-08-29 03:04:01,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:02,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:03,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:03,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:04,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:05,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:06,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:06,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:07,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:08,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:09,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:10,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:10,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:11,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:12,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:13,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:13,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:14,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:15,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:16,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:16,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:17,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:18,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:19,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:19,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:10<00:17, 17.80s/it][WARNING|generation_utils.py:914] 2023-08-29 03:04:20,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:21,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:22,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:22,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:23,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:24,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:25,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:26,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:26,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:27,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:28,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:29,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:29,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:30,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:31,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:32,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:32,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:33,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:34,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:35,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:36,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:37,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:37,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:04:38,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:28<00:00, 18.00s/it]Generating: 100%|██████████| 15/15 [04:28<00:00, 17.92s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:44,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:44,926 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:44,926 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:44,926 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:44,926 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:04:45,589 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:04:45,590 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:04:46,152 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:04:47,224 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:04:47,224 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:50,075 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:50,081 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:50,081 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:50,081 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:04:50,081 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:04:50,716 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:04:50,717 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:04:51,323 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:04:51,489 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:04:51,489 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8206521739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.845108695652174, 'errors': {''}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : the Prime Minister .\n']
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : the Prime Minister .\n', 'Relation : member of political party . Context : After the death of Prime Minister Jean Paul VandenBerg , the Conservative Party began a wave of rightwing politics after the 1993 election . Head Entity : Jean Paul VandenBerg , Tail Entity : Canadian Conservative Party .\n']
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : the Prime Minister .\n', 'Relation : member of political party . Context : After the death of Prime Minister Jean Paul VandenBerg , the Conservative Party began a wave of rightwing politics after the 1993 election . Head Entity : Jean Paul VandenBerg , Tail Entity : Canadian Conservative Party .\n', 'Relation : member of political party . Context : This was the first time that one political party had its president replaced by a third party. Head Entity : Robert S. McNamara , Tail Entity : United States President .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : narrative location .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : notable work .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 496, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 568, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : applies to jurisdiction .', 'success_rate': 0.7725, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : family name .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 190, 'raw': 288}
{'target': 600, 'success': 209, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 277, 'raw': 416}
{'target': 600, 'success': 300, 'raw': 448}
{'target': 600, 'success': 320, 'raw': 480}
{'target': 600, 'success': 343, 'raw': 512}
{'target': 600, 'success': 362, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 512, 'raw': 736}
{'target': 600, 'success': 540, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 597, 'raw': 864}
{'target': 600, 'success': 616, 'raw': 896}
{'prompt': 'Relation : genre .', 'success_rate': 0.6875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.796875, 'errors': {'', "('members', 'is a list of', '', 'Its members include .')", "('directory', 'is a list of', '', 'It is a directory structure for directory s , such as .')", "('1976', 'is a list of', '', 'It was first published in 1976 and was made into a movie .')", "('compilation', 'is a list of', '', 'It is a compilation of .')", 'not enough values to unpack (expected 2, got 1)', "('language', 'is a list of', '', 'It is a list of a language in .')"}}
['Relation : located in the administrative territorial entity . Context : The city of Marchec is a municipality of a municipality of the French administrative district of the Réunion , in the French administrative canton of the region of Réunion . Head Entity : Le Réunion , Tail Entity : Réunion .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8301630434782609, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 584, 'raw': 768}
{'target': 600, 'success': 607, 'raw': 800}
{'prompt': 'Relation : member of .', 'success_rate': 0.75875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : use .', 'success_rate': 0.7890625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('American Revolution', 'use', '', 'In 1841 , he played an influential role on and in the American Revolution of 1776 .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 13270
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13370, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.56it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.50it/s]Extractor Estimating: 4it [00:02,  1.55it/s]Extractor Estimating: 5it [00:03,  1.54it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.52it/s]Extractor Estimating: 8it [00:05,  1.56it/s]Extractor Estimating: 9it [00:05,  1.55it/s]Extractor Estimating: 10it [00:06,  1.61it/s]Extractor Estimating: 11it [00:07,  1.61it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.58it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:10,  1.63it/s]Extractor Estimating: 18it [00:11,  1.64it/s]Extractor Estimating: 19it [00:12,  1.60it/s]Extractor Estimating: 20it [00:12,  1.56it/s]Extractor Estimating: 21it [00:13,  1.56it/s]Extractor Estimating: 22it [00:14,  1.51it/s]Extractor Estimating: 23it [00:14,  1.56it/s]Extractor Estimating: 24it [00:15,  1.52it/s]Extractor Estimating: 25it [00:16,  1.53it/s]Extractor Estimating: 26it [00:16,  1.55it/s]Extractor Estimating: 27it [00:17,  1.58it/s]Extractor Estimating: 28it [00:17,  1.58it/s]Extractor Estimating: 29it [00:18,  1.60it/s]Extractor Estimating: 30it [00:19,  1.54it/s]Extractor Estimating: 31it [00:19,  1.49it/s]Extractor Estimating: 32it [00:20,  1.55it/s]Extractor Estimating: 33it [00:21,  1.61it/s]Extractor Estimating: 34it [00:21,  1.67it/s]Extractor Estimating: 35it [00:22,  1.66it/s]Extractor Estimating: 36it [00:22,  1.67it/s]Extractor Estimating: 37it [00:23,  1.70it/s]Extractor Estimating: 38it [00:23,  1.67it/s]Extractor Estimating: 39it [00:24,  1.55it/s]Extractor Estimating: 40it [00:25,  1.56it/s]Extractor Estimating: 41it [00:25,  1.58it/s]Extractor Estimating: 42it [00:26,  1.59it/s]Extractor Estimating: 43it [00:27,  1.65it/s]Extractor Estimating: 44it [00:27,  1.67it/s]Extractor Estimating: 45it [00:28,  1.62it/s]Extractor Estimating: 46it [00:29,  1.63it/s]Extractor Estimating: 47it [00:29,  1.66it/s]Extractor Estimating: 48it [00:30,  1.62it/s]Extractor Estimating: 49it [00:30,  1.59it/s]Extractor Estimating: 50it [00:31,  1.58it/s]Extractor Estimating: 51it [00:32,  1.62it/s]Extractor Estimating: 52it [00:32,  1.53it/s]Extractor Estimating: 53it [00:33,  1.53it/s]Extractor Estimating: 54it [00:34,  1.55it/s]Extractor Estimating: 55it [00:34,  1.61it/s]Extractor Estimating: 56it [00:35,  1.59it/s]Extractor Estimating: 57it [00:35,  1.62it/s]Extractor Estimating: 58it [00:36,  1.64it/s]Extractor Estimating: 59it [00:37,  1.63it/s]Extractor Estimating: 60it [00:37,  1.65it/s]Extractor Estimating: 61it [00:38,  1.64it/s]Extractor Estimating: 62it [00:38,  1.62it/s]Extractor Estimating: 63it [00:39,  1.60it/s]Extractor Estimating: 64it [00:40,  1.65it/s]Extractor Estimating: 65it [00:40,  1.55it/s]Extractor Estimating: 66it [00:41,  1.59it/s]Extractor Estimating: 67it [00:42,  1.59it/s]Extractor Estimating: 68it [00:42,  1.57it/s]Extractor Estimating: 69it [00:43,  1.55it/s]Extractor Estimating: 70it [00:44,  1.60it/s]Extractor Estimating: 71it [00:44,  1.58it/s]Extractor Estimating: 72it [00:45,  1.62it/s]Extractor Estimating: 73it [00:45,  1.61it/s]Extractor Estimating: 74it [00:46,  1.60it/s]Extractor Estimating: 75it [00:47,  1.59it/s]Extractor Estimating: 76it [00:47,  1.58it/s]Extractor Estimating: 77it [00:48,  1.47it/s]Extractor Estimating: 78it [00:49,  1.46it/s]Extractor Estimating: 79it [00:49,  1.48it/s]Extractor Estimating: 80it [00:50,  1.47it/s]Extractor Estimating: 81it [00:51,  1.47it/s]Extractor Estimating: 82it [00:51,  1.50it/s]Extractor Estimating: 83it [00:52,  1.51it/s]Extractor Estimating: 84it [00:53,  1.49it/s]Extractor Estimating: 85it [00:53,  1.50it/s]Extractor Estimating: 86it [00:54,  1.49it/s]Extractor Estimating: 87it [00:55,  1.46it/s]Extractor Estimating: 88it [00:56,  1.44it/s]Extractor Estimating: 89it [00:56,  1.44it/s]Extractor Estimating: 90it [00:57,  1.49it/s]Extractor Estimating: 91it [00:58,  1.45it/s]Extractor Estimating: 92it [00:58,  1.48it/s]Extractor Estimating: 93it [00:59,  1.49it/s]Extractor Estimating: 94it [01:00,  1.49it/s]Extractor Estimating: 95it [01:00,  1.46it/s]Extractor Estimating: 96it [01:01,  1.46it/s]Extractor Estimating: 97it [01:02,  1.46it/s]Extractor Estimating: 98it [01:02,  1.44it/s]Extractor Estimating: 99it [01:03,  1.48it/s]Extractor Estimating: 100it [01:04,  1.38it/s]Extractor Estimating: 101it [01:05,  1.39it/s]Extractor Estimating: 102it [01:05,  1.39it/s]Extractor Estimating: 103it [01:06,  1.46it/s]Extractor Estimating: 104it [01:07,  1.50it/s]Extractor Estimating: 105it [01:07,  1.46it/s]Extractor Estimating: 106it [01:08,  1.50it/s]Extractor Estimating: 107it [01:09,  1.47it/s]Extractor Estimating: 108it [01:09,  1.50it/s]Extractor Estimating: 109it [01:10,  1.48it/s]Extractor Estimating: 110it [01:11,  1.52it/s]Extractor Estimating: 111it [01:11,  1.50it/s]Extractor Estimating: 112it [01:12,  1.49it/s]Extractor Estimating: 113it [01:13,  1.48it/s]Extractor Estimating: 114it [01:13,  1.49it/s]Extractor Estimating: 115it [01:14,  1.46it/s]Extractor Estimating: 116it [01:15,  1.39it/s]Extractor Estimating: 117it [01:16,  1.31it/s]Extractor Estimating: 118it [01:16,  1.38it/s]Extractor Estimating: 119it [01:17,  1.44it/s]Extractor Estimating: 120it [01:18,  1.46it/s]Extractor Estimating: 121it [01:18,  1.48it/s]Extractor Estimating: 122it [01:19,  1.51it/s]Extractor Estimating: 123it [01:19,  1.52it/s]Extractor Estimating: 124it [01:20,  1.48it/s]Extractor Estimating: 125it [01:21,  1.50it/s]Extractor Estimating: 126it [01:22,  1.49it/s]Extractor Estimating: 127it [01:22,  1.44it/s]Extractor Estimating: 128it [01:23,  1.45it/s]Extractor Estimating: 129it [01:24,  1.48it/s]Extractor Estimating: 130it [01:24,  1.58it/s]Extractor Estimating: 131it [01:25,  1.56it/s]Extractor Estimating: 132it [01:25,  1.62it/s]Extractor Estimating: 133it [01:26,  1.60it/s]Extractor Estimating: 134it [01:27,  1.56it/s]Extractor Estimating: 135it [01:27,  1.55it/s]Extractor Estimating: 136it [01:28,  1.59it/s]Extractor Estimating: 137it [01:29,  1.57it/s]Extractor Estimating: 138it [01:29,  1.56it/s]Extractor Estimating: 139it [01:30,  1.56it/s]Extractor Estimating: 140it [01:30,  1.59it/s]Extractor Estimating: 141it [01:31,  1.48it/s]Extractor Estimating: 142it [01:32,  1.54it/s]Extractor Estimating: 143it [01:32,  1.56it/s]Extractor Estimating: 144it [01:33,  1.50it/s]Extractor Estimating: 145it [01:34,  1.50it/s]Extractor Estimating: 146it [01:35,  1.51it/s]Extractor Estimating: 147it [01:35,  1.55it/s]Extractor Estimating: 148it [01:36,  1.51it/s]Extractor Estimating: 149it [01:36,  1.54it/s]Extractor Estimating: 150it [01:37,  1.52it/s]Extractor Estimating: 151it [01:38,  1.49it/s]Extractor Estimating: 152it [01:38,  1.54it/s]Extractor Estimating: 153it [01:39,  1.46it/s]Extractor Estimating: 154it [01:40,  1.44it/s]Extractor Estimating: 155it [01:41,  1.42it/s]Extractor Estimating: 156it [01:41,  1.50it/s]Extractor Estimating: 157it [01:42,  1.50it/s]Extractor Estimating: 158it [01:43,  1.51it/s]Extractor Estimating: 159it [01:43,  1.48it/s]Extractor Estimating: 160it [01:44,  1.50it/s]Extractor Estimating: 161it [01:45,  1.47it/s]Extractor Estimating: 162it [01:45,  1.47it/s]Extractor Estimating: 163it [01:46,  1.41it/s]Extractor Estimating: 164it [01:47,  1.39it/s]Extractor Estimating: 165it [01:47,  1.41it/s]Extractor Estimating: 166it [01:48,  1.44it/s]Extractor Estimating: 167it [01:49,  1.48it/s]Extractor Estimating: 168it [01:49,  1.51it/s]Extractor Estimating: 169it [01:50,  1.47it/s]Extractor Estimating: 170it [01:51,  1.47it/s]Extractor Estimating: 171it [01:52,  1.45it/s]Extractor Estimating: 172it [01:52,  1.52it/s]Extractor Estimating: 173it [01:53,  1.55it/s]Extractor Estimating: 174it [01:53,  1.50it/s]Extractor Estimating: 175it [01:54,  1.52it/s]Extractor Estimating: 176it [01:55,  1.50it/s]Extractor Estimating: 177it [01:55,  1.52it/s]Extractor Estimating: 178it [01:56,  1.49it/s]Extractor Estimating: 179it [01:57,  1.43it/s]Extractor Estimating: 180it [01:58,  1.42it/s]Extractor Estimating: 181it [01:58,  1.43it/s]Extractor Estimating: 182it [01:59,  1.40it/s]Extractor Estimating: 183it [02:00,  1.44it/s]Extractor Estimating: 184it [02:00,  1.41it/s]Extractor Estimating: 185it [02:01,  1.44it/s]Extractor Estimating: 186it [02:02,  1.43it/s]Extractor Estimating: 187it [02:02,  1.53it/s]Extractor Estimating: 188it [02:03,  1.55it/s]Extractor Estimating: 189it [02:04,  1.54it/s]Extractor Estimating: 190it [02:04,  1.49it/s]Extractor Estimating: 191it [02:05,  1.48it/s]Extractor Estimating: 192it [02:06,  1.47it/s]Extractor Estimating: 193it [02:07,  1.33it/s]Extractor Estimating: 194it [02:07,  1.39it/s]Extractor Estimating: 195it [02:08,  1.40it/s]Extractor Estimating: 196it [02:09,  1.44it/s]Extractor Estimating: 197it [02:09,  1.47it/s]Extractor Estimating: 198it [02:10,  1.48it/s]Extractor Estimating: 199it [02:11,  1.50it/s]Extractor Estimating: 200it [02:11,  1.53it/s]Extractor Estimating: 201it [02:12,  1.61it/s]Extractor Estimating: 202it [02:12,  1.59it/s]Extractor Estimating: 203it [02:13,  1.51it/s]Extractor Estimating: 204it [02:14,  1.53it/s]Extractor Estimating: 205it [02:14,  1.60it/s]Extractor Estimating: 206it [02:15,  1.50it/s]Extractor Estimating: 207it [02:16,  1.51it/s]Extractor Estimating: 208it [02:16,  1.52it/s]Extractor Estimating: 209it [02:17,  1.58it/s]Extractor Estimating: 210it [02:18,  1.63it/s]Extractor Estimating: 211it [02:18,  1.67it/s]Extractor Estimating: 212it [02:19,  1.74it/s]Extractor Estimating: 213it [02:19,  1.65it/s]Extractor Estimating: 214it [02:20,  1.62it/s]Extractor Estimating: 215it [02:21,  1.64it/s]Extractor Estimating: 216it [02:21,  1.67it/s]Extractor Estimating: 217it [02:22,  1.68it/s]Extractor Estimating: 218it [02:22,  1.72it/s]Extractor Estimating: 219it [02:23,  1.63it/s]Extractor Estimating: 220it [02:23,  1.67it/s]Extractor Estimating: 221it [02:24,  1.66it/s]Extractor Estimating: 222it [02:25,  1.70it/s]Extractor Estimating: 223it [02:25,  1.72it/s]Extractor Estimating: 224it [02:26,  1.68it/s]Extractor Estimating: 225it [02:27,  1.59it/s]Extractor Estimating: 226it [02:27,  1.65it/s]Extractor Estimating: 227it [02:28,  1.60it/s]Extractor Estimating: 228it [02:28,  1.64it/s]Extractor Estimating: 229it [02:29,  1.64it/s]Extractor Estimating: 230it [02:30,  1.69it/s]Extractor Estimating: 231it [02:30,  1.67it/s]Extractor Estimating: 232it [02:31,  1.67it/s]Extractor Estimating: 233it [02:31,  1.70it/s]Extractor Estimating: 234it [02:32,  1.75it/s]Extractor Estimating: 235it [02:32,  1.75it/s]Extractor Estimating: 236it [02:33,  1.78it/s]Extractor Estimating: 237it [02:33,  1.77it/s]Extractor Estimating: 238it [02:34,  1.77it/s]Extractor Estimating: 239it [02:35,  1.73it/s]Extractor Estimating: 240it [02:35,  1.75it/s]Extractor Estimating: 241it [02:36,  1.75it/s]Extractor Estimating: 242it [02:36,  1.67it/s]Extractor Estimating: 243it [02:37,  1.65it/s]Extractor Estimating: 244it [02:38,  1.65it/s]Extractor Estimating: 245it [02:38,  1.63it/s]Extractor Estimating: 246it [02:39,  1.67it/s]Extractor Estimating: 247it [02:39,  1.74it/s]Extractor Estimating: 248it [02:40,  1.75it/s]Extractor Estimating: 249it [02:41,  1.76it/s]Extractor Estimating: 250it [02:41,  1.65it/s]Extractor Estimating: 251it [02:42,  1.66it/s]Extractor Estimating: 252it [02:42,  1.67it/s]Extractor Estimating: 253it [02:43,  1.69it/s]Extractor Estimating: 254it [02:43,  1.79it/s]Extractor Estimating: 255it [02:44,  1.75it/s]Extractor Estimating: 256it [02:45,  1.74it/s]Extractor Estimating: 257it [02:45,  1.81it/s]Extractor Estimating: 258it [02:46,  1.79it/s]Extractor Estimating: 259it [02:46,  1.73it/s]Extractor Estimating: 260it [02:47,  1.76it/s]Extractor Estimating: 261it [02:48,  1.72it/s]Extractor Estimating: 262it [02:48,  1.67it/s]Extractor Estimating: 263it [02:49,  1.69it/s]Extractor Estimating: 264it [02:49,  1.72it/s]Extractor Estimating: 265it [02:50,  1.72it/s]Extractor Estimating: 266it [02:50,  1.76it/s]Extractor Estimating: 267it [02:51,  1.70it/s]Extractor Estimating: 268it [02:52,  1.70it/s]Extractor Estimating: 269it [02:52,  1.71it/s]Extractor Estimating: 270it [02:53,  1.78it/s]Extractor Estimating: 271it [02:53,  1.72it/s]Extractor Estimating: 272it [02:54,  1.75it/s]Extractor Estimating: 273it [02:54,  1.80it/s]Extractor Estimating: 274it [02:55,  1.74it/s]Extractor Estimating: 275it [02:56,  1.75it/s]Extractor Estimating: 276it [02:56,  1.67it/s]Extractor Estimating: 277it [02:57,  1.58it/s]Extractor Estimating: 278it [02:58,  1.51it/s]Extractor Estimating: 279it [02:58,  1.54it/s]Extractor Estimating: 280it [02:59,  1.51it/s]Extractor Estimating: 281it [03:00,  1.53it/s]Extractor Estimating: 282it [03:00,  1.42it/s]Extractor Estimating: 283it [03:01,  1.42it/s]Extractor Estimating: 284it [03:02,  1.43it/s]Extractor Estimating: 285it [03:03,  1.44it/s]Extractor Estimating: 286it [03:03,  1.45it/s]Extractor Estimating: 287it [03:04,  1.48it/s]Extractor Estimating: 288it [03:05,  1.46it/s]Extractor Estimating: 289it [03:05,  1.46it/s]Extractor Estimating: 290it [03:06,  1.52it/s]Extractor Estimating: 291it [03:07,  1.49it/s]Extractor Estimating: 292it [03:07,  1.48it/s]Extractor Estimating: 293it [03:08,  1.49it/s]Extractor Estimating: 294it [03:09,  1.46it/s]Extractor Estimating: 295it [03:09,  1.48it/s]Extractor Estimating: 296it [03:10,  1.49it/s]Extractor Estimating: 297it [03:11,  1.52it/s]Extractor Estimating: 298it [03:11,  1.54it/s]Extractor Estimating: 299it [03:12,  1.50it/s]Extractor Estimating: 300it [03:13,  1.47it/s]Extractor Estimating: 301it [03:13,  1.59it/s]Extractor Estimating: 302it [03:14,  1.58it/s]Extractor Estimating: 303it [03:14,  1.61it/s]Extractor Estimating: 304it [03:15,  1.54it/s]Extractor Estimating: 305it [03:16,  1.55it/s]Extractor Estimating: 306it [03:16,  1.54it/s]Extractor Estimating: 307it [03:17,  1.59it/s]Extractor Estimating: 308it [03:18,  1.57it/s]Extractor Estimating: 309it [03:18,  1.57it/s]Extractor Estimating: 310it [03:19,  1.62it/s]Extractor Estimating: 311it [03:19,  1.65it/s]Extractor Estimating: 312it [03:20,  1.60it/s]Extractor Estimating: 313it [03:21,  1.67it/s]Extractor Estimating: 314it [03:21,  1.69it/s]Extractor Estimating: 315it [03:22,  1.67it/s]Extractor Estimating: 316it [03:22,  1.64it/s]Extractor Estimating: 317it [03:23,  1.71it/s]Extractor Estimating: 318it [03:24,  1.67it/s]Extractor Estimating: 319it [03:24,  1.72it/s]Extractor Estimating: 320it [03:25,  1.70it/s]Extractor Estimating: 321it [03:25,  1.67it/s]Extractor Estimating: 322it [03:26,  1.66it/s]Extractor Estimating: 323it [03:27,  1.65it/s]Extractor Estimating: 324it [03:27,  1.53it/s]Extractor Estimating: 325it [03:28,  1.57it/s]Extractor Estimating: 326it [03:29,  1.57it/s]Extractor Estimating: 327it [03:29,  1.56it/s]Extractor Estimating: 328it [03:30,  1.61it/s]Extractor Estimating: 329it [03:30,  1.59it/s]Extractor Estimating: 330it [03:31,  1.60it/s]Extractor Estimating: 331it [03:32,  1.61it/s]Extractor Estimating: 332it [03:32,  1.56it/s]Extractor Estimating: 333it [03:33,  1.58it/s]Extractor Estimating: 334it [03:34,  1.55it/s]Extractor Estimating: 335it [03:34,  1.56it/s]Extractor Estimating: 336it [03:35,  1.54it/s]Extractor Estimating: 337it [03:36,  1.49it/s]Extractor Estimating: 338it [03:36,  1.50it/s]Extractor Estimating: 339it [03:37,  1.47it/s]Extractor Estimating: 340it [03:38,  1.51it/s]Extractor Estimating: 341it [03:38,  1.49it/s]Extractor Estimating: 342it [03:39,  1.49it/s]Extractor Estimating: 343it [03:40,  1.47it/s]Extractor Estimating: 344it [03:40,  1.49it/s]Extractor Estimating: 345it [03:41,  1.50it/s]Extractor Estimating: 346it [03:42,  1.55it/s]Extractor Estimating: 347it [03:42,  1.50it/s]Extractor Estimating: 348it [03:43,  1.53it/s]Extractor Estimating: 349it [03:44,  1.53it/s]Extractor Estimating: 350it [03:44,  1.50it/s]Extractor Estimating: 351it [03:45,  1.45it/s]Extractor Estimating: 352it [03:46,  1.48it/s]Extractor Estimating: 353it [03:46,  1.49it/s]Extractor Estimating: 354it [03:47,  1.48it/s]Extractor Estimating: 355it [03:48,  1.54it/s]Extractor Estimating: 356it [03:48,  1.53it/s]Extractor Estimating: 357it [03:49,  1.58it/s]Extractor Estimating: 358it [03:49,  1.60it/s]Extractor Estimating: 359it [03:50,  1.56it/s]Extractor Estimating: 360it [03:51,  1.57it/s]Extractor Estimating: 361it [03:51,  1.60it/s]Extractor Estimating: 362it [03:52,  1.57it/s]Extractor Estimating: 363it [03:53,  1.55it/s]Extractor Estimating: 364it [03:53,  1.51it/s]Extractor Estimating: 365it [03:54,  1.50it/s]Extractor Estimating: 366it [03:55,  1.37it/s]Extractor Estimating: 367it [03:56,  1.39it/s]Extractor Estimating: 368it [03:56,  1.44it/s]Extractor Estimating: 369it [03:57,  1.47it/s]Extractor Estimating: 370it [03:58,  1.47it/s]Extractor Estimating: 371it [03:58,  1.53it/s]Extractor Estimating: 372it [03:59,  1.54it/s]Extractor Estimating: 373it [04:00,  1.53it/s]Extractor Estimating: 374it [04:00,  1.57it/s]Extractor Estimating: 375it [04:01,  1.62it/s]Extractor Estimating: 375it [04:01,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:12,971 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:12,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:12,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:12,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:12,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:09:13,612 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:09:13,613 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:09:14,186 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:09:15,261 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:09:15,262 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:18,138 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:18,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:18,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:18,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:18,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:09:18,798 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:09:18,799 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:09:19,372 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:09:19,548 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:09:19,548 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 05:37:58,686 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 05:37:58,708 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7466 mean pseudo reward: 0.9006500684372076
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl'}
train vocab size: 26383
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26483, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26483, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.089, loss:1289.0205
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.095, loss:1291.5073
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.092, loss:1310.0199
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.104, loss:1224.3892
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.084, loss:1253.4032
>> valid entity prec:0.4970, rec:0.5620, f1:0.5275
>> valid relation prec:0.5061, rec:0.0857, f1:0.1465
>> valid relation with NER prec:0.5061, rec:0.0857, f1:0.1465
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.887, loss:1181.1026
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.098, loss:1170.3076
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.098, loss:1082.7655
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.098, loss:1126.6984
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.081, loss:1052.4252
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4935, rec:0.5394, f1:0.5155
>> valid relation prec:0.5079, rec:0.0729, f1:0.1275
>> valid relation with NER prec:0.5079, rec:0.0729, f1:0.1275
g_step 1100, step 164, avg_time 2.878, loss:1024.9134
g_step 1200, step 264, avg_time 1.100, loss:1002.0827
g_step 1300, step 52, avg_time 1.085, loss:966.9149
g_step 1400, step 152, avg_time 1.093, loss:960.5483
g_step 1500, step 252, avg_time 1.101, loss:961.4354
>> valid entity prec:0.5151, rec:0.5071, f1:0.5111
>> valid relation prec:0.5507, rec:0.0626, f1:0.1124
>> valid relation with NER prec:0.5507, rec:0.0626, f1:0.1124
g_step 1600, step 40, avg_time 2.862, loss:909.9606
g_step 1700, step 140, avg_time 1.109, loss:913.4255
g_step 1800, step 240, avg_time 1.090, loss:891.6342
g_step 1900, step 28, avg_time 1.079, loss:913.5014
g_step 2000, step 128, avg_time 1.081, loss:827.2080
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4886, rec:0.4742, f1:0.4813
>> valid relation prec:0.4289, rec:0.0739, f1:0.1261
>> valid relation with NER prec:0.4289, rec:0.0739, f1:0.1261
g_step 2100, step 228, avg_time 2.873, loss:863.7227
g_step 2200, step 16, avg_time 1.087, loss:839.5393
g_step 2300, step 116, avg_time 1.091, loss:807.0992
g_step 2400, step 216, avg_time 1.114, loss:818.9252
g_step 2500, step 4, avg_time 1.069, loss:820.6347
>> valid entity prec:0.5295, rec:0.4805, f1:0.5038
>> valid relation prec:0.4987, rec:0.0820, f1:0.1408
>> valid relation with NER prec:0.4987, rec:0.0820, f1:0.1408
g_step 2600, step 104, avg_time 2.878, loss:761.1503
g_step 2700, step 204, avg_time 1.100, loss:788.5089
g_step 2800, step 304, avg_time 1.081, loss:793.9295
g_step 2900, step 92, avg_time 1.077, loss:766.6656
g_step 3000, step 192, avg_time 1.091, loss:743.1445
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5126, rec:0.4175, f1:0.4602
>> valid relation prec:0.4059, rec:0.0653, f1:0.1125
>> valid relation with NER prec:0.4059, rec:0.0653, f1:0.1125
g_step 3100, step 292, avg_time 2.856, loss:748.3367
g_step 3200, step 80, avg_time 1.082, loss:712.1282
g_step 3300, step 180, avg_time 1.089, loss:730.9758
g_step 3400, step 280, avg_time 1.087, loss:730.4643
g_step 3500, step 68, avg_time 1.089, loss:683.7615
>> valid entity prec:0.5706, rec:0.3998, f1:0.4701
>> valid relation prec:0.4258, rec:0.0762, f1:0.1293
>> valid relation with NER prec:0.4258, rec:0.0762, f1:0.1293
g_step 3600, step 168, avg_time 2.837, loss:704.9458
g_step 3700, step 268, avg_time 1.089, loss:700.3489
g_step 3800, step 56, avg_time 1.077, loss:650.7805
g_step 3900, step 156, avg_time 1.093, loss:674.0458
g_step 4000, step 256, avg_time 1.095, loss:677.5531
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5124, rec:0.4593, f1:0.4844
>> valid relation prec:0.2965, rec:0.0636, f1:0.1048
>> valid relation with NER prec:0.2965, rec:0.0636, f1:0.1048
g_step 4100, step 44, avg_time 2.851, loss:653.6319
g_step 4200, step 144, avg_time 1.081, loss:621.6960
g_step 4300, step 244, avg_time 1.095, loss:637.5797
g_step 4400, step 32, avg_time 1.071, loss:644.4628
g_step 4500, step 132, avg_time 1.089, loss:619.2734
>> valid entity prec:0.5226, rec:0.5006, f1:0.5114
>> valid relation prec:0.3538, rec:0.0937, f1:0.1482
>> valid relation with NER prec:0.3538, rec:0.0937, f1:0.1482
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 232, avg_time 2.848, loss:623.9874
g_step 4700, step 20, avg_time 1.083, loss:619.7990
g_step 4800, step 120, avg_time 1.102, loss:596.7141
g_step 4900, step 220, avg_time 1.081, loss:591.7931
g_step 5000, step 8, avg_time 1.078, loss:601.0828
learning rate was adjusted to 0.0008
>> valid entity prec:0.4661, rec:0.4475, f1:0.4566
>> valid relation prec:0.3041, rec:0.0665, f1:0.1092
>> valid relation with NER prec:0.3041, rec:0.0665, f1:0.1092
g_step 5100, step 108, avg_time 2.854, loss:565.9107
g_step 5200, step 208, avg_time 1.075, loss:575.7169
g_step 5300, step 308, avg_time 1.085, loss:594.9296
g_step 5400, step 96, avg_time 1.089, loss:530.5527
g_step 5500, step 196, avg_time 1.085, loss:548.3915
>> valid entity prec:0.5244, rec:0.4530, f1:0.4861
>> valid relation prec:0.3220, rec:0.0859, f1:0.1356
>> valid relation with NER prec:0.3220, rec:0.0859, f1:0.1356
g_step 5600, step 296, avg_time 2.848, loss:591.5298
g_step 5700, step 84, avg_time 1.075, loss:529.3982
g_step 5800, step 184, avg_time 1.083, loss:531.6896
g_step 5900, step 284, avg_time 1.092, loss:566.7886
g_step 6000, step 72, avg_time 1.078, loss:514.1158
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5292, rec:0.4454, f1:0.4837
>> valid relation prec:0.3034, rec:0.0863, f1:0.1344
>> valid relation with NER prec:0.3034, rec:0.0863, f1:0.1344
g_step 6100, step 172, avg_time 2.838, loss:501.8562
g_step 6200, step 272, avg_time 1.085, loss:531.4068
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 05:37:58 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 05:37:58 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_05-37-58_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 05:37:59 - WARNING - datasets.builder -   Using custom data configuration default-451c08b5e7e4e73a
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-451c08b5e7e4e73a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 05:38:00,000 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:38:00,001 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:38:00,001 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:38:00,002 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:38:00,010 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:38:00,015 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:38:00,015 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:38:00,015 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:38:00,015 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:38:00,015 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:38:00,015 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 05:38:00,168 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:38:03,251 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 05:38:03,254 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-451c08b5e7e4e73a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.20ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.22ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.75ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.05ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.24ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.38ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.45ba/s]100%|██████████| 8/8 [00:01<00:00,  5.22ba/s]100%|██████████| 8/8 [00:01<00:00,  4.29ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.06ba/s] 40%|████      | 2/5 [00:00<00:00,  4.29ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.38ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.39ba/s]100%|██████████| 5/5 [00:01<00:00,  4.64ba/s]100%|██████████| 5/5 [00:01<00:00,  4.48ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.81ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.48ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.55ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.73ba/s]100%|██████████| 8/8 [00:00<00:00, 11.20ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.29ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.09ba/s]100%|██████████| 5/5 [00:00<00:00, 10.89ba/s]100%|██████████| 5/5 [00:00<00:00, 10.55ba/s]
[INFO|trainer.py:414] 2023-08-29 05:38:07,826 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 05:38:07,845 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 05:38:07,845 >>   Num examples = 7544
[INFO|trainer.py:1149] 2023-08-29 05:38:07,845 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 05:38:07,845 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 05:38:07,845 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 05:38:07,845 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 05:38:07,845 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:59,  3.29it/s]  0%|          | 2/590 [00:00<02:52,  3.41it/s]  1%|          | 3/590 [00:00<02:50,  3.45it/s]  1%|          | 4/590 [00:01<02:49,  3.46it/s]  1%|          | 5/590 [00:01<02:48,  3.47it/s]  1%|          | 6/590 [00:01<02:48,  3.47it/s]  1%|          | 7/590 [00:02<02:47,  3.48it/s]  1%|▏         | 8/590 [00:02<02:47,  3.48it/s]  2%|▏         | 9/590 [00:02<02:46,  3.49it/s]  2%|▏         | 10/590 [00:02<02:46,  3.49it/s]  2%|▏         | 11/590 [00:03<02:45,  3.49it/s]  2%|▏         | 12/590 [00:03<02:45,  3.49it/s]  2%|▏         | 13/590 [00:03<02:45,  3.49it/s]  2%|▏         | 14/590 [00:04<02:45,  3.49it/s]  3%|▎         | 15/590 [00:04<02:44,  3.49it/s]  3%|▎         | 16/590 [00:04<02:44,  3.49it/s]  3%|▎         | 17/590 [00:04<02:44,  3.49it/s]  3%|▎         | 18/590 [00:05<02:44,  3.49it/s]  3%|▎         | 19/590 [00:05<02:43,  3.49it/s]  3%|▎         | 20/590 [00:05<02:43,  3.49it/s]  4%|▎         | 21/590 [00:06<02:43,  3.49it/s]  4%|▎         | 22/590 [00:06<02:42,  3.49it/s]  4%|▍         | 23/590 [00:06<02:42,  3.49it/s]  4%|▍         | 24/590 [00:06<02:42,  3.49it/s]  4%|▍         | 25/590 [00:07<02:42,  3.49it/s]  4%|▍         | 26/590 [00:07<02:41,  3.49it/s]  5%|▍         | 27/590 [00:07<02:41,  3.49it/s]  5%|▍         | 28/590 [00:08<02:41,  3.49it/s]  5%|▍         | 29/590 [00:08<02:40,  3.49it/s]  5%|▌         | 30/590 [00:08<02:40,  3.49it/s]  5%|▌         | 31/590 [00:08<02:40,  3.49it/s]  5%|▌         | 32/590 [00:09<02:39,  3.49it/s]  6%|▌         | 33/590 [00:09<02:39,  3.48it/s]  6%|▌         | 34/590 [00:09<02:39,  3.49it/s]  6%|▌         | 35/590 [00:10<02:39,  3.49it/s]  6%|▌         | 36/590 [00:10<02:38,  3.49it/s]  6%|▋         | 37/590 [00:10<02:38,  3.49it/s]  6%|▋         | 38/590 [00:10<02:38,  3.49it/s]  7%|▋         | 39/590 [00:11<02:37,  3.49it/s]  7%|▋         | 40/590 [00:11<02:37,  3.49it/s]  7%|▋         | 41/590 [00:11<02:37,  3.49it/s]  7%|▋         | 42/590 [00:12<02:37,  3.49it/s]  7%|▋         | 43/590 [00:12<02:36,  3.49it/s]  7%|▋         | 44/590 [00:12<02:36,  3.48it/s]  8%|▊         | 45/590 [00:12<02:36,  3.48it/s]  8%|▊         | 46/590 [00:13<02:36,  3.48it/s]  8%|▊         | 47/590 [00:13<02:35,  3.48it/s]  8%|▊         | 48/590 [00:13<02:35,  3.48it/s]  8%|▊         | 49/590 [00:14<02:35,  3.49it/s]  8%|▊         | 50/590 [00:14<02:34,  3.49it/s]  9%|▊         | 51/590 [00:14<02:36,  3.45it/s]  9%|▉         | 52/590 [00:14<02:35,  3.46it/s]  9%|▉         | 53/590 [00:15<02:34,  3.47it/s]  9%|▉         | 54/590 [00:15<02:34,  3.48it/s]  9%|▉         | 55/590 [00:15<02:33,  3.48it/s]  9%|▉         | 56/590 [00:16<02:33,  3.48it/s] 10%|▉         | 57/590 [00:16<02:33,  3.48it/s] 10%|▉         | 58/590 [00:16<02:32,  3.49it/s] 10%|█         | 59/590 [00:16<02:32,  3.49it/s] 10%|█         | 60/590 [00:17<02:32,  3.48it/s] 10%|█         | 61/590 [00:17<02:31,  3.49it/s] 11%|█         | 62/590 [00:17<02:31,  3.49it/s] 11%|█         | 63/590 [00:18<02:31,  3.49it/s] 11%|█         | 64/590 [00:18<02:30,  3.49it/s] 11%|█         | 65/590 [00:18<02:30,  3.49it/s] 11%|█         | 66/590 [00:18<02:30,  3.48it/s] 11%|█▏        | 67/590 [00:19<02:30,  3.49it/s] 12%|█▏        | 68/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 69/590 [00:19<02:29,  3.48it/s] 12%|█▏        | 70/590 [00:20<02:29,  3.48it/s] 12%|█▏        | 71/590 [00:20<02:29,  3.48it/s] 12%|█▏        | 72/590 [00:20<02:28,  3.48it/s] 12%|█▏        | 73/590 [00:20<02:28,  3.48it/s] 13%|█▎        | 74/590 [00:21<02:28,  3.48it/s] 13%|█▎        | 75/590 [00:21<02:27,  3.48it/s] 13%|█▎        | 76/590 [00:21<02:27,  3.48it/s] 13%|█▎        | 77/590 [00:22<02:27,  3.48it/s] 13%|█▎        | 78/590 [00:22<02:26,  3.48it/s] 13%|█▎        | 79/590 [00:22<02:26,  3.48it/s] 14%|█▎        | 80/590 [00:22<02:26,  3.48it/s] 14%|█▎        | 81/590 [00:23<02:26,  3.48it/s] 14%|█▍        | 82/590 [00:23<02:25,  3.48it/s] 14%|█▍        | 83/590 [00:23<02:25,  3.48it/s] 14%|█▍        | 84/590 [00:24<02:25,  3.48it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.48it/s] 15%|█▍        | 86/590 [00:24<02:25,  3.47it/s] 15%|█▍        | 87/590 [00:24<02:24,  3.47it/s] 15%|█▍        | 88/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 89/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 90/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 91/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 92/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 93/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 94/590 [00:26<02:22,  3.47it/s] 16%|█▌        | 95/590 [00:27<02:22,  3.48it/s] 16%|█▋        | 96/590 [00:27<02:22,  3.48it/s] 16%|█▋        | 97/590 [00:27<02:21,  3.48it/s] 17%|█▋        | 98/590 [00:28<02:21,  3.48it/s] 17%|█▋        | 99/590 [00:28<02:21,  3.48it/s] 17%|█▋        | 100/590 [00:28<02:20,  3.48it/s] 17%|█▋        | 101/590 [00:29<02:20,  3.48it/s] 17%|█▋        | 102/590 [00:29<02:20,  3.48it/s] 17%|█▋        | 103/590 [00:29<02:20,  3.48it/s] 18%|█▊        | 104/590 [00:29<02:20,  3.46it/s] 18%|█▊        | 105/590 [00:30<02:20,  3.46it/s] 18%|█▊        | 106/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 107/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 108/590 [00:31<02:18,  3.47it/s] 18%|█▊        | 109/590 [00:31<02:18,  3.47it/s] 19%|█▊        | 110/590 [00:31<02:18,  3.47it/s] 19%|█▉        | 111/590 [00:31<02:17,  3.47it/s] 19%|█▉        | 112/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 113/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 114/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 115/590 [00:33<02:16,  3.47it/s] 20%|█▉        | 116/590 [00:33<02:16,  3.47it/s] 20%|█▉        | 117/590 [00:33<02:16,  3.47it/s] 20%|██        | 118/590 [00:33<02:11,  3.60it/s][INFO|trainer.py:2140] 2023-08-29 05:38:41,726 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:38:41,726 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 05:38:41,727 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.50it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.81it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.98it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.25it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.76it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.28it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.94it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.62it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.70it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.85it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.84it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.87it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.85it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.91it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.67it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.62it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.48it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.50it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.62it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.73it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.83it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.84it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.92it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.75it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.68it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.51it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.48it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.56it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.71it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.79it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.82it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.87it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.86it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.66it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.58it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.46it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.55it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.65it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.71it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.84it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.80it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.76it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.71it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.63it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.57it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.58it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.55it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.67it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.70it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.76it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.72it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.70it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.60it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.57it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.53it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.60it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.59it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.67it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.67it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.75it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.76it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.62it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.62it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.50it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.58it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.64it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.62it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.73it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.68it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.74it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.68it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.64it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.57it/s][A
 61%|██████▏   | 373/608 [00:07<00:05, 46.59it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.52it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.65it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.69it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.66it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.73it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.74it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.64it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.63it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.54it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.56it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.61it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.67it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.61it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.72it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.64it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.56it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.58it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.64it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.55it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.57it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.60it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.59it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.66it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.61it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.63it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.59it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.63it/s][A
 84%|████████▍ | 513/608 [00:10<00:02, 46.65it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.57it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.55it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.66it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.72it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.66it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.58it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.61it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.58it/s][A
 92%|█████████▏| 558/608 [00:11<00:01, 46.59it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.68it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.54it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.61it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.60it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.68it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.62it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.56it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.49it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.57it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.56it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:46<02:11,  3.60it/s]
100%|██████████| 608/608 [00:13<00:00, 46.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:38:54,771 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-29 05:38:54,790 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:38:57,047 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:38:57,062 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:38:57,081 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:54<48:59,  6.24s/it] 20%|██        | 120/590 [00:54<34:54,  4.46s/it] 21%|██        | 121/590 [00:54<25:03,  3.21s/it] 21%|██        | 122/590 [00:54<18:10,  2.33s/it] 21%|██        | 123/590 [00:55<13:21,  1.72s/it] 21%|██        | 124/590 [00:55<10:00,  1.29s/it] 21%|██        | 125/590 [00:55<07:39,  1.01it/s] 21%|██▏       | 126/590 [00:56<06:00,  1.29it/s] 22%|██▏       | 127/590 [00:56<04:51,  1.59it/s] 22%|██▏       | 128/590 [00:56<04:03,  1.90it/s] 22%|██▏       | 129/590 [00:56<03:29,  2.20it/s] 22%|██▏       | 130/590 [00:57<03:06,  2.47it/s] 22%|██▏       | 131/590 [00:57<02:50,  2.70it/s] 22%|██▏       | 132/590 [00:57<02:38,  2.89it/s] 23%|██▎       | 133/590 [00:58<02:29,  3.05it/s] 23%|██▎       | 134/590 [00:58<02:23,  3.17it/s] 23%|██▎       | 135/590 [00:58<02:19,  3.26it/s] 23%|██▎       | 136/590 [00:58<02:16,  3.32it/s] 23%|██▎       | 137/590 [00:59<02:14,  3.37it/s] 23%|██▎       | 138/590 [00:59<02:12,  3.40it/s] 24%|██▎       | 139/590 [00:59<02:11,  3.43it/s] 24%|██▎       | 140/590 [01:00<02:10,  3.44it/s] 24%|██▍       | 141/590 [01:00<02:10,  3.45it/s] 24%|██▍       | 142/590 [01:00<02:09,  3.45it/s] 24%|██▍       | 143/590 [01:00<02:09,  3.46it/s] 24%|██▍       | 144/590 [01:01<02:08,  3.46it/s] 25%|██▍       | 145/590 [01:01<02:08,  3.47it/s] 25%|██▍       | 146/590 [01:01<02:07,  3.47it/s] 25%|██▍       | 147/590 [01:02<02:07,  3.47it/s] 25%|██▌       | 148/590 [01:02<02:07,  3.47it/s] 25%|██▌       | 149/590 [01:02<02:06,  3.48it/s] 25%|██▌       | 150/590 [01:02<02:06,  3.47it/s] 26%|██▌       | 151/590 [01:03<02:06,  3.48it/s] 26%|██▌       | 152/590 [01:03<02:06,  3.48it/s] 26%|██▌       | 153/590 [01:03<02:06,  3.46it/s] 26%|██▌       | 154/590 [01:04<02:05,  3.47it/s] 26%|██▋       | 155/590 [01:04<02:05,  3.47it/s] 26%|██▋       | 156/590 [01:04<02:05,  3.47it/s] 27%|██▋       | 157/590 [01:04<02:04,  3.47it/s] 27%|██▋       | 158/590 [01:05<02:04,  3.47it/s] 27%|██▋       | 159/590 [01:05<02:04,  3.47it/s] 27%|██▋       | 160/590 [01:05<02:03,  3.47it/s] 27%|██▋       | 161/590 [01:06<02:03,  3.48it/s] 27%|██▋       | 162/590 [01:06<02:03,  3.47it/s] 28%|██▊       | 163/590 [01:06<02:02,  3.47it/s] 28%|██▊       | 164/590 [01:06<02:03,  3.46it/s] 28%|██▊       | 165/590 [01:07<02:02,  3.46it/s] 28%|██▊       | 166/590 [01:07<02:02,  3.47it/s] 28%|██▊       | 167/590 [01:07<02:01,  3.47it/s] 28%|██▊       | 168/590 [01:08<02:01,  3.47it/s] 29%|██▊       | 169/590 [01:08<02:01,  3.47it/s] 29%|██▉       | 170/590 [01:08<02:00,  3.47it/s] 29%|██▉       | 171/590 [01:08<02:00,  3.47it/s] 29%|██▉       | 172/590 [01:09<02:00,  3.47it/s] 29%|██▉       | 173/590 [01:09<02:00,  3.47it/s] 29%|██▉       | 174/590 [01:09<01:59,  3.47it/s] 30%|██▉       | 175/590 [01:10<01:59,  3.47it/s] 30%|██▉       | 176/590 [01:10<01:59,  3.47it/s] 30%|███       | 177/590 [01:10<01:59,  3.47it/s] 30%|███       | 178/590 [01:11<01:58,  3.47it/s] 30%|███       | 179/590 [01:11<01:58,  3.47it/s] 31%|███       | 180/590 [01:11<01:58,  3.47it/s] 31%|███       | 181/590 [01:11<01:57,  3.47it/s] 31%|███       | 182/590 [01:12<01:57,  3.47it/s] 31%|███       | 183/590 [01:12<01:57,  3.47it/s] 31%|███       | 184/590 [01:12<01:56,  3.47it/s] 31%|███▏      | 185/590 [01:13<01:56,  3.47it/s] 32%|███▏      | 186/590 [01:13<01:56,  3.46it/s] 32%|███▏      | 187/590 [01:13<01:56,  3.47it/s] 32%|███▏      | 188/590 [01:13<01:55,  3.47it/s] 32%|███▏      | 189/590 [01:14<01:55,  3.47it/s] 32%|███▏      | 190/590 [01:14<01:55,  3.47it/s] 32%|███▏      | 191/590 [01:14<01:54,  3.47it/s] 33%|███▎      | 192/590 [01:15<01:54,  3.47it/s] 33%|███▎      | 193/590 [01:15<01:54,  3.47it/s] 33%|███▎      | 194/590 [01:15<01:54,  3.47it/s] 33%|███▎      | 195/590 [01:15<01:53,  3.47it/s] 33%|███▎      | 196/590 [01:16<01:53,  3.47it/s] 33%|███▎      | 197/590 [01:16<01:53,  3.46it/s] 34%|███▎      | 198/590 [01:16<01:53,  3.47it/s] 34%|███▎      | 199/590 [01:17<01:52,  3.47it/s] 34%|███▍      | 200/590 [01:17<01:52,  3.47it/s] 34%|███▍      | 201/590 [01:17<01:52,  3.47it/s] 34%|███▍      | 202/590 [01:17<01:52,  3.46it/s] 34%|███▍      | 203/590 [01:18<01:51,  3.46it/s] 35%|███▍      | 204/590 [01:18<01:51,  3.47it/s] 35%|███▍      | 205/590 [01:18<01:50,  3.47it/s] 35%|███▍      | 206/590 [01:19<01:50,  3.47it/s] 35%|███▌      | 207/590 [01:19<01:50,  3.47it/s] 35%|███▌      | 208/590 [01:19<01:50,  3.47it/s] 35%|███▌      | 209/590 [01:19<01:49,  3.47it/s] 36%|███▌      | 210/590 [01:20<01:49,  3.47it/s] 36%|███▌      | 211/590 [01:20<01:49,  3.47it/s] 36%|███▌      | 212/590 [01:20<01:48,  3.47it/s] 36%|███▌      | 213/590 [01:21<01:48,  3.46it/s] 36%|███▋      | 214/590 [01:21<01:48,  3.47it/s] 36%|███▋      | 215/590 [01:21<01:48,  3.47it/s] 37%|███▋      | 216/590 [01:21<01:47,  3.47it/s] 37%|███▋      | 217/590 [01:22<01:47,  3.47it/s] 37%|███▋      | 218/590 [01:22<01:47,  3.47it/s] 37%|███▋      | 219/590 [01:22<01:46,  3.47it/s] 37%|███▋      | 220/590 [01:23<01:46,  3.47it/s] 37%|███▋      | 221/590 [01:23<01:46,  3.47it/s] 38%|███▊      | 222/590 [01:23<01:46,  3.47it/s] 38%|███▊      | 223/590 [01:23<01:45,  3.47it/s] 38%|███▊      | 224/590 [01:24<01:45,  3.46it/s] 38%|███▊      | 225/590 [01:24<01:45,  3.46it/s] 38%|███▊      | 226/590 [01:24<01:45,  3.46it/s] 38%|███▊      | 227/590 [01:25<01:44,  3.46it/s] 39%|███▊      | 228/590 [01:25<01:44,  3.47it/s] 39%|███▉      | 229/590 [01:25<01:44,  3.47it/s] 39%|███▉      | 230/590 [01:26<01:43,  3.47it/s] 39%|███▉      | 231/590 [01:26<01:43,  3.47it/s] 39%|███▉      | 232/590 [01:26<01:43,  3.47it/s] 39%|███▉      | 233/590 [01:26<01:42,  3.47it/s] 40%|███▉      | 234/590 [01:27<01:42,  3.47it/s] 40%|███▉      | 235/590 [01:27<01:42,  3.46it/s] 40%|████      | 236/590 [01:27<01:38,  3.59it/s][INFO|trainer.py:2140] 2023-08-29 05:39:35,556 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:39:35,556 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 05:39:35,556 >>   Batch size = 8
{'eval_loss': 0.9505587816238403, 'eval_runtime': 13.0265, 'eval_samples_per_second': 373.086, 'eval_steps_per_second': 46.674, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.43it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.45it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.61it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.02it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.60it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.27it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.90it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.62it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.65it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.71it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.77it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.80it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.68it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.78it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.77it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.64it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.54it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.37it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.48it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.61it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.62it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.72it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.72it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.67it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.63it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.58it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.50it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.46it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.49it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.61it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.59it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.74it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.68it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.66it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.59it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.52it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.51it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.56it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.56it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.58it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.70it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.71it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.63it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.58it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.59it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.53it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.55it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.53it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.58it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.57it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.58it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.65it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.54it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.54it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.57it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.50it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.58it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.49it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.53it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.59it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.67it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.66it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.50it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.49it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.54it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.54it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.58it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.58it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.50it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.63it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.61it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.63it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.56it/s][A
 61%|██████▏   | 373/608 [00:07<00:05, 46.45it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.51it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.49it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.58it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.58it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.56it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.60it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.55it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.50it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.50it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.61it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.55it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.61it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.69it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.54it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.51it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.56it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.51it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.56it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.57it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.51it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.59it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.67it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.55it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.62it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.53it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.51it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.54it/s][A
 84%|████████▍ | 513/608 [00:10<00:02, 46.56it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.56it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.58it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.58it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.52it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.50it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.62it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.50it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.53it/s][A
 92%|█████████▏| 558/608 [00:11<00:01, 46.55it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.58it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.53it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.61it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.56it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.59it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.52it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.46it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.52it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.62it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.59it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:40<01:38,  3.59it/s]
100%|██████████| 608/608 [00:13<00:00, 46.59it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:39:48,613 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-29 05:39:48,635 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:39:51,040 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:39:51,055 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:39:51,079 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:48<38:06,  6.48s/it] 40%|████      | 238/590 [01:48<27:06,  4.62s/it] 41%|████      | 239/590 [01:49<19:25,  3.32s/it] 41%|████      | 240/590 [01:49<14:03,  2.41s/it] 41%|████      | 241/590 [01:49<10:18,  1.77s/it] 41%|████      | 242/590 [01:50<07:41,  1.33s/it] 41%|████      | 243/590 [01:50<05:52,  1.02s/it] 41%|████▏     | 244/590 [01:50<04:35,  1.25it/s] 42%|████▏     | 245/590 [01:50<03:42,  1.55it/s] 42%|████▏     | 246/590 [01:51<03:04,  1.86it/s] 42%|████▏     | 247/590 [01:51<02:38,  2.16it/s] 42%|████▏     | 248/590 [01:51<02:20,  2.44it/s] 42%|████▏     | 249/590 [01:52<02:07,  2.68it/s] 42%|████▏     | 250/590 [01:52<01:58,  2.87it/s] 43%|████▎     | 251/590 [01:52<01:51,  3.03it/s] 43%|████▎     | 252/590 [01:52<01:47,  3.15it/s] 43%|████▎     | 253/590 [01:53<01:43,  3.24it/s] 43%|████▎     | 254/590 [01:53<01:41,  3.31it/s] 43%|████▎     | 255/590 [01:53<01:41,  3.30it/s] 43%|████▎     | 256/590 [01:54<01:39,  3.34it/s] 44%|████▎     | 257/590 [01:54<01:38,  3.38it/s] 44%|████▎     | 258/590 [01:54<01:37,  3.41it/s] 44%|████▍     | 259/590 [01:54<01:36,  3.42it/s] 44%|████▍     | 260/590 [01:55<01:36,  3.43it/s] 44%|████▍     | 261/590 [01:55<01:35,  3.44it/s] 44%|████▍     | 262/590 [01:55<01:35,  3.45it/s] 45%|████▍     | 263/590 [01:56<01:34,  3.46it/s] 45%|████▍     | 264/590 [01:56<01:34,  3.46it/s] 45%|████▍     | 265/590 [01:56<01:33,  3.46it/s] 45%|████▌     | 266/590 [01:57<01:33,  3.47it/s] 45%|████▌     | 267/590 [01:57<01:33,  3.47it/s] 45%|████▌     | 268/590 [01:57<01:32,  3.47it/s] 46%|████▌     | 269/590 [01:57<01:32,  3.47it/s] 46%|████▌     | 270/590 [01:58<01:32,  3.46it/s] 46%|████▌     | 271/590 [01:58<01:32,  3.46it/s] 46%|████▌     | 272/590 [01:58<01:31,  3.46it/s] 46%|████▋     | 273/590 [01:59<01:31,  3.46it/s] 46%|████▋     | 274/590 [01:59<01:31,  3.47it/s] 47%|████▋     | 275/590 [01:59<01:30,  3.47it/s] 47%|████▋     | 276/590 [01:59<01:30,  3.47it/s] 47%|████▋     | 277/590 [02:00<01:30,  3.47it/s] 47%|████▋     | 278/590 [02:00<01:29,  3.47it/s] 47%|████▋     | 279/590 [02:00<01:29,  3.47it/s] 47%|████▋     | 280/590 [02:01<01:29,  3.47it/s] 48%|████▊     | 281/590 [02:01<01:29,  3.47it/s] 48%|████▊     | 282/590 [02:01<01:28,  3.47it/s] 48%|████▊     | 283/590 [02:01<01:28,  3.47it/s] 48%|████▊     | 284/590 [02:02<01:28,  3.47it/s] 48%|████▊     | 285/590 [02:02<01:27,  3.47it/s] 48%|████▊     | 286/590 [02:02<01:27,  3.47it/s] 49%|████▊     | 287/590 [02:03<01:27,  3.46it/s] 49%|████▉     | 288/590 [02:03<01:27,  3.47it/s] 49%|████▉     | 289/590 [02:03<01:26,  3.47it/s] 49%|████▉     | 290/590 [02:03<01:26,  3.47it/s] 49%|████▉     | 291/590 [02:04<01:26,  3.47it/s] 49%|████▉     | 292/590 [02:04<01:26,  3.46it/s] 50%|████▉     | 293/590 [02:04<01:25,  3.46it/s] 50%|████▉     | 294/590 [02:05<01:25,  3.46it/s] 50%|█████     | 295/590 [02:05<01:25,  3.47it/s] 50%|█████     | 296/590 [02:05<01:24,  3.47it/s] 50%|█████     | 297/590 [02:05<01:24,  3.47it/s] 51%|█████     | 298/590 [02:06<01:24,  3.47it/s] 51%|█████     | 299/590 [02:06<01:23,  3.47it/s] 51%|█████     | 300/590 [02:06<01:23,  3.47it/s] 51%|█████     | 301/590 [02:07<01:23,  3.47it/s] 51%|█████     | 302/590 [02:07<01:23,  3.47it/s] 51%|█████▏    | 303/590 [02:07<01:22,  3.46it/s] 52%|█████▏    | 304/590 [02:07<01:22,  3.47it/s] 52%|█████▏    | 305/590 [02:08<01:22,  3.47it/s] 52%|█████▏    | 306/590 [02:08<01:21,  3.47it/s] 52%|█████▏    | 307/590 [02:08<01:21,  3.47it/s] 52%|█████▏    | 308/590 [02:09<01:21,  3.47it/s] 52%|█████▏    | 309/590 [02:09<01:20,  3.47it/s] 53%|█████▎    | 310/590 [02:09<01:20,  3.47it/s] 53%|█████▎    | 311/590 [02:09<01:20,  3.47it/s] 53%|█████▎    | 312/590 [02:10<01:20,  3.47it/s] 53%|█████▎    | 313/590 [02:10<01:19,  3.47it/s] 53%|█████▎    | 314/590 [02:10<01:19,  3.46it/s] 53%|█████▎    | 315/590 [02:11<01:19,  3.46it/s] 54%|█████▎    | 316/590 [02:11<01:19,  3.47it/s] 54%|█████▎    | 317/590 [02:11<01:18,  3.47it/s] 54%|█████▍    | 318/590 [02:12<01:18,  3.47it/s] 54%|█████▍    | 319/590 [02:12<01:18,  3.47it/s] 54%|█████▍    | 320/590 [02:12<01:17,  3.47it/s] 54%|█████▍    | 321/590 [02:12<01:17,  3.47it/s] 55%|█████▍    | 322/590 [02:13<01:17,  3.47it/s] 55%|█████▍    | 323/590 [02:13<01:16,  3.47it/s] 55%|█████▍    | 324/590 [02:13<01:16,  3.47it/s] 55%|█████▌    | 325/590 [02:14<01:16,  3.46it/s] 55%|█████▌    | 326/590 [02:14<01:16,  3.46it/s] 55%|█████▌    | 327/590 [02:14<01:15,  3.47it/s] 56%|█████▌    | 328/590 [02:14<01:15,  3.47it/s] 56%|█████▌    | 329/590 [02:15<01:15,  3.47it/s] 56%|█████▌    | 330/590 [02:15<01:15,  3.47it/s] 56%|█████▌    | 331/590 [02:15<01:14,  3.47it/s] 56%|█████▋    | 332/590 [02:16<01:14,  3.47it/s] 56%|█████▋    | 333/590 [02:16<01:14,  3.47it/s] 57%|█████▋    | 334/590 [02:16<01:13,  3.47it/s] 57%|█████▋    | 335/590 [02:16<01:13,  3.47it/s] 57%|█████▋    | 336/590 [02:17<01:13,  3.46it/s] 57%|█████▋    | 337/590 [02:17<01:13,  3.47it/s] 57%|█████▋    | 338/590 [02:17<01:12,  3.47it/s] 57%|█████▋    | 339/590 [02:18<01:12,  3.47it/s] 58%|█████▊    | 340/590 [02:18<01:12,  3.47it/s] 58%|█████▊    | 341/590 [02:18<01:11,  3.47it/s] 58%|█████▊    | 342/590 [02:18<01:11,  3.47it/s] 58%|█████▊    | 343/590 [02:19<01:11,  3.47it/s] 58%|█████▊    | 344/590 [02:19<01:10,  3.47it/s] 58%|█████▊    | 345/590 [02:19<01:10,  3.47it/s] 59%|█████▊    | 346/590 [02:20<01:10,  3.47it/s] 59%|█████▉    | 347/590 [02:20<01:10,  3.47it/s] 59%|█████▉    | 348/590 [02:20<01:09,  3.47it/s] 59%|█████▉    | 349/590 [02:20<01:09,  3.47it/s] 59%|█████▉    | 350/590 [02:21<01:09,  3.47it/s] 59%|█████▉    | 351/590 [02:21<01:08,  3.47it/s] 60%|█████▉    | 352/590 [02:21<01:08,  3.47it/s] 60%|█████▉    | 353/590 [02:22<01:08,  3.47it/s] 60%|██████    | 354/590 [02:22<01:05,  3.58it/s][INFO|trainer.py:2140] 2023-08-29 05:40:30,214 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:40:30,214 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 05:40:30,214 >>   Batch size = 8
{'eval_loss': 0.9639475345611572, 'eval_runtime': 13.0471, 'eval_samples_per_second': 372.498, 'eval_steps_per_second': 46.601, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.19it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.38it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.49it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.80it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.45it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.18it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.92it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.87it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.73it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.73it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.71it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.74it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.62it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.65it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.60it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.60it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.56it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.57it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.62it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.68it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.56it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.62it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.58it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.57it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.57it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.67it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.57it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.62it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.61it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.65it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.67it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.59it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.61it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.49it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.54it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.53it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.49it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.61it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.59it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.60it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.64it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.62it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.58it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.58it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.64it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.52it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.49it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.51it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.52it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.48it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.46it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.36it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.56it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.57it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.64it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.53it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.51it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.59it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.49it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.59it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.61it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.57it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.56it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.66it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.59it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.58it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.56it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.58it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.59it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.61it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.51it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.62it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.55it/s][A
 61%|██████▏   | 373/608 [00:07<00:05, 46.54it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.53it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.62it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.58it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.55it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.60it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.55it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.54it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.62it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.61it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.52it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.52it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.48it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.48it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.54it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.52it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.53it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.56it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.60it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.60it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.57it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.55it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.55it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.51it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.55it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.56it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.58it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.59it/s][A
 84%|████████▍ | 513/608 [00:10<00:02, 46.62it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.62it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.61it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.50it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.53it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.54it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.53it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.55it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.55it/s][A
 92%|█████████▏| 558/608 [00:11<00:01, 46.52it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.60it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.55it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.55it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.45it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.48it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.51it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.52it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.58it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.60it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.55it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:35<01:05,  3.58it/s]
100%|██████████| 608/608 [00:13<00:00, 46.55it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:40:43,288 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-29 05:40:43,303 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:40:45,511 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:40:45,524 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:40:45,529 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:42<24:19,  6.21s/it] 60%|██████    | 356/590 [02:42<17:18,  4.44s/it] 61%|██████    | 357/590 [02:43<12:23,  3.19s/it] 61%|██████    | 358/590 [02:43<08:58,  2.32s/it] 61%|██████    | 359/590 [02:43<06:35,  1.71s/it] 61%|██████    | 360/590 [02:43<04:55,  1.28s/it] 61%|██████    | 361/590 [02:44<03:45,  1.02it/s] 61%|██████▏   | 362/590 [02:44<02:56,  1.29it/s] 62%|██████▏   | 363/590 [02:44<02:22,  1.59it/s] 62%|██████▏   | 364/590 [02:45<01:59,  1.90it/s] 62%|██████▏   | 365/590 [02:45<01:42,  2.20it/s] 62%|██████▏   | 366/590 [02:45<01:30,  2.47it/s] 62%|██████▏   | 367/590 [02:45<01:22,  2.69it/s] 62%|██████▏   | 368/590 [02:46<01:17,  2.88it/s] 63%|██████▎   | 369/590 [02:46<01:12,  3.04it/s] 63%|██████▎   | 370/590 [02:46<01:09,  3.16it/s] 63%|██████▎   | 371/590 [02:47<01:07,  3.25it/s] 63%|██████▎   | 372/590 [02:47<01:05,  3.31it/s] 63%|██████▎   | 373/590 [02:47<01:04,  3.36it/s] 63%|██████▎   | 374/590 [02:47<01:03,  3.39it/s] 64%|██████▎   | 375/590 [02:48<01:02,  3.42it/s] 64%|██████▎   | 376/590 [02:48<01:02,  3.43it/s] 64%|██████▍   | 377/590 [02:48<01:01,  3.44it/s] 64%|██████▍   | 378/590 [02:49<01:01,  3.44it/s] 64%|██████▍   | 379/590 [02:49<01:01,  3.45it/s] 64%|██████▍   | 380/590 [02:49<01:00,  3.46it/s] 65%|██████▍   | 381/590 [02:49<01:00,  3.46it/s] 65%|██████▍   | 382/590 [02:50<00:59,  3.47it/s] 65%|██████▍   | 383/590 [02:50<00:59,  3.47it/s] 65%|██████▌   | 384/590 [02:50<00:59,  3.47it/s] 65%|██████▌   | 385/590 [02:51<00:59,  3.47it/s] 65%|██████▌   | 386/590 [02:51<00:58,  3.47it/s] 66%|██████▌   | 387/590 [02:51<00:58,  3.47it/s] 66%|██████▌   | 388/590 [02:51<00:58,  3.47it/s] 66%|██████▌   | 389/590 [02:52<00:57,  3.47it/s] 66%|██████▌   | 390/590 [02:52<00:57,  3.47it/s] 66%|██████▋   | 391/590 [02:52<00:57,  3.47it/s] 66%|██████▋   | 392/590 [02:53<00:57,  3.46it/s] 67%|██████▋   | 393/590 [02:53<00:56,  3.46it/s] 67%|██████▋   | 394/590 [02:53<00:56,  3.47it/s] 67%|██████▋   | 395/590 [02:54<00:59,  3.30it/s] 67%|██████▋   | 396/590 [02:54<00:58,  3.34it/s] 67%|██████▋   | 397/590 [02:54<00:57,  3.38it/s] 67%|██████▋   | 398/590 [02:54<00:56,  3.40it/s] 68%|██████▊   | 399/590 [02:55<00:55,  3.42it/s] 68%|██████▊   | 400/590 [02:55<00:55,  3.44it/s] 68%|██████▊   | 401/590 [02:55<00:54,  3.45it/s] 68%|██████▊   | 402/590 [02:56<00:54,  3.45it/s] 68%|██████▊   | 403/590 [02:56<00:54,  3.45it/s] 68%|██████▊   | 404/590 [02:56<00:53,  3.45it/s] 69%|██████▊   | 405/590 [02:56<00:53,  3.46it/s] 69%|██████▉   | 406/590 [02:57<00:53,  3.46it/s] 69%|██████▉   | 407/590 [02:57<00:52,  3.46it/s] 69%|██████▉   | 408/590 [02:57<00:52,  3.46it/s] 69%|██████▉   | 409/590 [02:58<00:52,  3.46it/s] 69%|██████▉   | 410/590 [02:58<00:51,  3.46it/s] 70%|██████▉   | 411/590 [02:58<00:51,  3.47it/s] 70%|██████▉   | 412/590 [02:58<00:51,  3.46it/s] 70%|███████   | 413/590 [02:59<00:51,  3.47it/s] 70%|███████   | 414/590 [02:59<00:50,  3.46it/s] 70%|███████   | 415/590 [02:59<00:50,  3.46it/s] 71%|███████   | 416/590 [03:00<00:50,  3.46it/s] 71%|███████   | 417/590 [03:00<00:49,  3.46it/s] 71%|███████   | 418/590 [03:00<00:49,  3.46it/s] 71%|███████   | 419/590 [03:00<00:49,  3.46it/s] 71%|███████   | 420/590 [03:01<00:49,  3.46it/s] 71%|███████▏  | 421/590 [03:01<00:48,  3.46it/s] 72%|███████▏  | 422/590 [03:01<00:48,  3.46it/s] 72%|███████▏  | 423/590 [03:02<00:48,  3.46it/s] 72%|███████▏  | 424/590 [03:02<00:47,  3.47it/s] 72%|███████▏  | 425/590 [03:02<00:47,  3.46it/s] 72%|███████▏  | 426/590 [03:02<00:47,  3.46it/s] 72%|███████▏  | 427/590 [03:03<00:47,  3.46it/s] 73%|███████▎  | 428/590 [03:03<00:46,  3.47it/s] 73%|███████▎  | 429/590 [03:03<00:46,  3.47it/s] 73%|███████▎  | 430/590 [03:04<00:46,  3.47it/s] 73%|███████▎  | 431/590 [03:04<00:45,  3.47it/s] 73%|███████▎  | 432/590 [03:04<00:45,  3.47it/s] 73%|███████▎  | 433/590 [03:04<00:45,  3.46it/s] 74%|███████▎  | 434/590 [03:05<00:45,  3.46it/s] 74%|███████▎  | 435/590 [03:05<00:44,  3.47it/s] 74%|███████▍  | 436/590 [03:05<00:44,  3.46it/s] 74%|███████▍  | 437/590 [03:06<00:44,  3.46it/s] 74%|███████▍  | 438/590 [03:06<00:43,  3.47it/s] 74%|███████▍  | 439/590 [03:06<00:43,  3.47it/s] 75%|███████▍  | 440/590 [03:06<00:43,  3.47it/s] 75%|███████▍  | 441/590 [03:07<00:42,  3.47it/s] 75%|███████▍  | 442/590 [03:07<00:42,  3.47it/s] 75%|███████▌  | 443/590 [03:07<00:42,  3.47it/s] 75%|███████▌  | 444/590 [03:08<00:42,  3.47it/s] 75%|███████▌  | 445/590 [03:08<00:41,  3.47it/s] 76%|███████▌  | 446/590 [03:08<00:41,  3.47it/s] 76%|███████▌  | 447/590 [03:09<00:41,  3.46it/s] 76%|███████▌  | 448/590 [03:09<00:41,  3.46it/s] 76%|███████▌  | 449/590 [03:09<00:40,  3.46it/s] 76%|███████▋  | 450/590 [03:09<00:40,  3.47it/s] 76%|███████▋  | 451/590 [03:10<00:40,  3.47it/s] 77%|███████▋  | 452/590 [03:10<00:39,  3.47it/s] 77%|███████▋  | 453/590 [03:10<00:39,  3.47it/s] 77%|███████▋  | 454/590 [03:11<00:39,  3.47it/s] 77%|███████▋  | 455/590 [03:11<00:38,  3.47it/s] 77%|███████▋  | 456/590 [03:11<00:38,  3.47it/s] 77%|███████▋  | 457/590 [03:11<00:38,  3.47it/s] 78%|███████▊  | 458/590 [03:12<00:38,  3.46it/s] 78%|███████▊  | 459/590 [03:12<00:37,  3.46it/s] 78%|███████▊  | 460/590 [03:12<00:37,  3.46it/s] 78%|███████▊  | 461/590 [03:13<00:37,  3.47it/s] 78%|███████▊  | 462/590 [03:13<00:36,  3.47it/s] 78%|███████▊  | 463/590 [03:13<00:36,  3.47it/s] 79%|███████▊  | 464/590 [03:13<00:36,  3.47it/s] 79%|███████▉  | 465/590 [03:14<00:36,  3.47it/s] 79%|███████▉  | 466/590 [03:14<00:35,  3.47it/s] 79%|███████▉  | 467/590 [03:14<00:35,  3.47it/s] 79%|███████▉  | 468/590 [03:15<00:35,  3.47it/s] 79%|███████▉  | 469/590 [03:15<00:34,  3.46it/s] 80%|███████▉  | 470/590 [03:15<00:34,  3.46it/s] 80%|███████▉  | 471/590 [03:15<00:34,  3.47it/s] 80%|████████  | 472/590 [03:16<00:32,  3.59it/s][INFO|trainer.py:2140] 2023-08-29 05:41:24,048 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:41:24,048 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 05:41:24,048 >>   Batch size = 8
{'eval_loss': 0.970319926738739, 'eval_runtime': 13.0503, 'eval_samples_per_second': 372.407, 'eval_steps_per_second': 46.589, 'epoch': 3.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.02it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.46it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.61it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.86it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.43it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.24it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.10it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.74it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.67it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.76it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.68it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.72it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.76it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.66it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.65it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.70it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.66it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.56it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.65it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.60it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.65it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.65it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.67it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.60it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.65it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.62it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.62it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.66it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.61it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.56it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.60it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.59it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.64it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.64it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.63it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.60it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.56it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.64it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.58it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.61it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.59it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.58it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.63it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.61it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.67it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.62it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.63it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.57it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.60it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.59it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.62it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.56it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.63it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.66it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.64it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.51it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.58it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.62it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.63it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.65it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.56it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.63it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.64it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.62it/s][A
 54%|█████▍    | 328/608 [00:07<00:05, 46.70it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.64it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.57it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.63it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.60it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.59it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.64it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.60it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.64it/s][A
 61%|██████▏   | 373/608 [00:07<00:05, 46.58it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.58it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.54it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.58it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.58it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.61it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.63it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.58it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.65it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.61it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.67it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.68it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.55it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.61it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.57it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.63it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.56it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.55it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.59it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.60it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.58it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.68it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.63it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.62it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.64it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.55it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.55it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.63it/s][A
 84%|████████▍ | 513/608 [00:10<00:02, 46.60it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.63it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.65it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.54it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.61it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.67it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.58it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.60it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.61it/s][A
 92%|█████████▏| 558/608 [00:11<00:01, 46.56it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.62it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.65it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.54it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.59it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.64it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.58it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.55it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.54it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.51it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.61it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:29<00:32,  3.59it/s]
100%|██████████| 608/608 [00:13<00:00, 46.61it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:41:37,096 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-29 05:41:37,112 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:41:40,906 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:41:40,923 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:41:40,932 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:38<13:14,  6.79s/it] 80%|████████  | 474/590 [03:38<09:21,  4.84s/it] 81%|████████  | 475/590 [03:38<06:39,  3.48s/it] 81%|████████  | 476/590 [03:39<04:47,  2.52s/it] 81%|████████  | 477/590 [03:39<03:28,  1.85s/it] 81%|████████  | 478/590 [03:39<02:34,  1.38s/it] 81%|████████  | 479/590 [03:39<01:56,  1.05s/it] 81%|████████▏ | 480/590 [03:40<01:30,  1.22it/s] 82%|████████▏ | 481/590 [03:40<01:12,  1.51it/s] 82%|████████▏ | 482/590 [03:40<00:59,  1.82it/s] 82%|████████▏ | 483/590 [03:41<00:50,  2.12it/s] 82%|████████▏ | 484/590 [03:41<00:44,  2.41it/s] 82%|████████▏ | 485/590 [03:41<00:39,  2.65it/s] 82%|████████▏ | 486/590 [03:41<00:36,  2.85it/s] 83%|████████▎ | 487/590 [03:42<00:34,  3.02it/s] 83%|████████▎ | 488/590 [03:42<00:32,  3.14it/s] 83%|████████▎ | 489/590 [03:42<00:31,  3.24it/s] 83%|████████▎ | 490/590 [03:43<00:30,  3.31it/s] 83%|████████▎ | 491/590 [03:43<00:29,  3.35it/s] 83%|████████▎ | 492/590 [03:43<00:28,  3.39it/s] 84%|████████▎ | 493/590 [03:43<00:28,  3.41it/s] 84%|████████▎ | 494/590 [03:44<00:27,  3.43it/s] 84%|████████▍ | 495/590 [03:44<00:27,  3.44it/s] 84%|████████▍ | 496/590 [03:44<00:27,  3.44it/s] 84%|████████▍ | 497/590 [03:45<00:26,  3.45it/s] 84%|████████▍ | 498/590 [03:45<00:26,  3.46it/s] 85%|████████▍ | 499/590 [03:45<00:26,  3.47it/s] 85%|████████▍ | 500/590 [03:45<00:25,  3.47it/s]                                                  85%|████████▍ | 500/590 [03:45<00:25,  3.47it/s] 85%|████████▍ | 501/590 [03:46<00:25,  3.47it/s] 85%|████████▌ | 502/590 [03:46<00:25,  3.47it/s] 85%|████████▌ | 503/590 [03:46<00:25,  3.47it/s] 85%|████████▌ | 504/590 [03:47<00:24,  3.47it/s] 86%|████████▌ | 505/590 [03:47<00:24,  3.47it/s] 86%|████████▌ | 506/590 [03:47<00:24,  3.47it/s] 86%|████████▌ | 507/590 [03:47<00:23,  3.46it/s] 86%|████████▌ | 508/590 [03:48<00:23,  3.47it/s] 86%|████████▋ | 509/590 [03:48<00:23,  3.47it/s] 86%|████████▋ | 510/590 [03:48<00:23,  3.47it/s] 87%|████████▋ | 511/590 [03:49<00:22,  3.47it/s] 87%|████████▋ | 512/590 [03:49<00:22,  3.47it/s] 87%|████████▋ | 513/590 [03:49<00:22,  3.47it/s] 87%|████████▋ | 514/590 [03:49<00:21,  3.47it/s] 87%|████████▋ | 515/590 [03:50<00:21,  3.47it/s] 87%|████████▋ | 516/590 [03:50<00:21,  3.47it/s] 88%|████████▊ | 517/590 [03:50<00:21,  3.47it/s] 88%|████████▊ | 518/590 [03:51<00:20,  3.46it/s] 88%|████████▊ | 519/590 [03:51<00:20,  3.47it/s] 88%|████████▊ | 520/590 [03:51<00:20,  3.47it/s] 88%|████████▊ | 521/590 [03:52<00:19,  3.47it/s] 88%|████████▊ | 522/590 [03:52<00:19,  3.47it/s] 89%|████████▊ | 523/590 [03:52<00:19,  3.48it/s] 89%|████████▉ | 524/590 [03:52<00:18,  3.48it/s] 89%|████████▉ | 525/590 [03:53<00:18,  3.47it/s] 89%|████████▉ | 526/590 [03:53<00:18,  3.47it/s] 89%|████████▉ | 527/590 [03:53<00:18,  3.47it/s] 89%|████████▉ | 528/590 [03:54<00:17,  3.47it/s] 90%|████████▉ | 529/590 [03:54<00:17,  3.47it/s] 90%|████████▉ | 530/590 [03:54<00:17,  3.47it/s] 90%|█████████ | 531/590 [03:54<00:16,  3.47it/s] 90%|█████████ | 532/590 [03:55<00:16,  3.47it/s] 90%|█████████ | 533/590 [03:55<00:16,  3.47it/s] 91%|█████████ | 534/590 [03:55<00:16,  3.47it/s] 91%|█████████ | 535/590 [03:56<00:15,  3.47it/s] 91%|█████████ | 536/590 [03:56<00:15,  3.47it/s] 91%|█████████ | 537/590 [03:56<00:15,  3.47it/s] 91%|█████████ | 538/590 [03:56<00:14,  3.47it/s] 91%|█████████▏| 539/590 [03:57<00:14,  3.47it/s] 92%|█████████▏| 540/590 [03:57<00:14,  3.47it/s] 92%|█████████▏| 541/590 [03:57<00:14,  3.33it/s] 92%|█████████▏| 542/590 [03:58<00:14,  3.37it/s] 92%|█████████▏| 543/590 [03:58<00:13,  3.40it/s] 92%|█████████▏| 544/590 [03:58<00:13,  3.42it/s] 92%|█████████▏| 545/590 [03:58<00:13,  3.44it/s] 93%|█████████▎| 546/590 [03:59<00:12,  3.44it/s] 93%|█████████▎| 547/590 [03:59<00:12,  3.45it/s] 93%|█████████▎| 548/590 [03:59<00:12,  3.45it/s] 93%|█████████▎| 549/590 [04:00<00:11,  3.45it/s] 93%|█████████▎| 550/590 [04:00<00:11,  3.46it/s] 93%|█████████▎| 551/590 [04:00<00:11,  3.46it/s] 94%|█████████▎| 552/590 [04:01<00:11,  3.34it/s] 94%|█████████▎| 553/590 [04:01<00:10,  3.37it/s] 94%|█████████▍| 554/590 [04:01<00:10,  3.40it/s] 94%|█████████▍| 555/590 [04:01<00:10,  3.42it/s] 94%|█████████▍| 556/590 [04:02<00:09,  3.43it/s] 94%|█████████▍| 557/590 [04:02<00:09,  3.44it/s] 95%|█████████▍| 558/590 [04:02<00:09,  3.45it/s] 95%|█████████▍| 559/590 [04:03<00:08,  3.46it/s] 95%|█████████▍| 560/590 [04:03<00:08,  3.46it/s] 95%|█████████▌| 561/590 [04:03<00:08,  3.47it/s] 95%|█████████▌| 562/590 [04:03<00:08,  3.47it/s] 95%|█████████▌| 563/590 [04:04<00:07,  3.46it/s] 96%|█████████▌| 564/590 [04:04<00:07,  3.46it/s] 96%|█████████▌| 565/590 [04:04<00:07,  3.46it/s] 96%|█████████▌| 566/590 [04:05<00:06,  3.47it/s] 96%|█████████▌| 567/590 [04:05<00:06,  3.47it/s] 96%|█████████▋| 568/590 [04:05<00:06,  3.47it/s] 96%|█████████▋| 569/590 [04:05<00:06,  3.47it/s] 97%|█████████▋| 570/590 [04:06<00:05,  3.47it/s] 97%|█████████▋| 571/590 [04:06<00:05,  3.47it/s] 97%|█████████▋| 572/590 [04:06<00:05,  3.47it/s] 97%|█████████▋| 573/590 [04:07<00:04,  3.47it/s] 97%|█████████▋| 574/590 [04:07<00:04,  3.46it/s] 97%|█████████▋| 575/590 [04:07<00:04,  3.46it/s] 98%|█████████▊| 576/590 [04:07<00:04,  3.47it/s] 98%|█████████▊| 577/590 [04:08<00:03,  3.47it/s] 98%|█████████▊| 578/590 [04:08<00:03,  3.47it/s] 98%|█████████▊| 579/590 [04:08<00:03,  3.47it/s] 98%|█████████▊| 580/590 [04:09<00:02,  3.47it/s] 98%|█████████▊| 581/590 [04:09<00:02,  3.47it/s] 99%|█████████▊| 582/590 [04:09<00:02,  3.47it/s] 99%|█████████▉| 583/590 [04:09<00:02,  3.47it/s] 99%|█████████▉| 584/590 [04:10<00:01,  3.47it/s] 99%|█████████▉| 585/590 [04:10<00:01,  3.46it/s] 99%|█████████▉| 586/590 [04:10<00:01,  3.46it/s] 99%|█████████▉| 587/590 [04:11<00:00,  3.46it/s]100%|█████████▉| 588/590 [04:11<00:00,  3.46it/s]100%|█████████▉| 589/590 [04:11<00:00,  3.46it/s]100%|██████████| 590/590 [04:11<00:00,  3.58it/s][INFO|trainer.py:2140] 2023-08-29 05:42:19,800 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:42:19,801 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 05:42:19,801 >>   Batch size = 8
{'eval_loss': 0.983846127986908, 'eval_runtime': 13.0382, 'eval_samples_per_second': 372.752, 'eval_steps_per_second': 46.632, 'epoch': 4.0}
{'loss': 0.7284, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.18it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.51it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.66it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.90it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.50it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.23it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.99it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.81it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.77it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.75it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.78it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.69it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.66it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.70it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.76it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.69it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.67it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.59it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.54it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.70it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.68it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.68it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.64it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.58it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.63it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.68it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.66it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.65it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.64it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.67it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.67it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.66it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.55it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.63it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.60it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.61it/s][A
 31%|███       | 188/608 [00:04<00:08, 46.70it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.67it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.64it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.67it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.65it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.69it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.55it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.51it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.60it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.63it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.62it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.65it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.56it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.66it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.69it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.59it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.58it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.51it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.55it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.60it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.58it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.64it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.68it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.63it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.65it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.64it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.59it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.59it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.64it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.61it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.64it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.65it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.60it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.66it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.66it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.58it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.64it/s][A
 61%|██████▏   | 373/608 [00:07<00:05, 46.48it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.59it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.65it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.59it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.63it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.66it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.62it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.66it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.62it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.63it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.63it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.56it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.54it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.58it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.61it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.63it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.66it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.61it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.64it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.56it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.64it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.52it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.59it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.62it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.54it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.59it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.66it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.58it/s][A
 84%|████████▍ | 513/608 [00:10<00:02, 46.59it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.56it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.54it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.61it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.62it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.57it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.64it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.65it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.57it/s][A
 92%|█████████▏| 558/608 [00:11<00:01, 46.62it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.60it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.59it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.55it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.50it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.58it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.58it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.55it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.63it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.58it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.51it/s][A                                                 
                                                 [A100%|██████████| 590/590 [04:24<00:00,  3.58it/s]
100%|██████████| 608/608 [00:13<00:00, 46.51it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:42:32,854 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-29 05:42:32,869 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:42:35,359 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:42:35,384 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:42:35,397 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 05:42:39,976 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 05:42:39,978 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118 (score: 0.9505587816238403).
                                                 100%|██████████| 590/590 [04:33<00:00,  3.58it/s]100%|██████████| 590/590 [04:33<00:00,  2.16it/s]
[INFO|trainer.py:1894] 2023-08-29 05:42:41,602 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 05:42:41,619 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:42:43,835 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:42:43,850 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:42:43,860 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:42:44,025 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:44,026 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:44,026 >>   train_loss               =     0.7238
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:44,026 >>   train_runtime            = 0:04:33.75
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:44,026 >>   train_samples            =       7544
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:44,026 >>   train_samples_per_second =    137.788
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:44,026 >>   train_steps_per_second   =      2.155
{'eval_loss': 0.9871525764465332, 'eval_runtime': 13.0386, 'eval_samples_per_second': 372.741, 'eval_steps_per_second': 46.631, 'epoch': 5.0}
{'train_runtime': 273.7534, 'train_samples_per_second': 137.788, 'train_steps_per_second': 2.155, 'train_loss': 0.7238481101343187, 'epoch': 5.0}
08/29/2023 05:42:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 05:42:44,066 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:42:44,067 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 05:42:44,067 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 58.41it/s]  2%|▏         | 12/608 [00:00<00:11, 51.34it/s]  3%|▎         | 18/608 [00:00<00:11, 49.27it/s]  4%|▍         | 23/608 [00:00<00:12, 48.53it/s]  5%|▍         | 28/608 [00:00<00:12, 48.13it/s]  5%|▌         | 33/608 [00:00<00:12, 47.75it/s]  6%|▋         | 38/608 [00:00<00:11, 47.59it/s]  7%|▋         | 43/608 [00:00<00:11, 47.43it/s]  8%|▊         | 48/608 [00:00<00:11, 46.98it/s]  9%|▊         | 53/608 [00:01<00:11, 46.97it/s] 10%|▉         | 58/608 [00:01<00:11, 46.96it/s] 10%|█         | 63/608 [00:01<00:11, 46.92it/s] 11%|█         | 68/608 [00:01<00:11, 46.99it/s] 12%|█▏        | 73/608 [00:01<00:11, 46.98it/s] 13%|█▎        | 78/608 [00:01<00:11, 46.97it/s] 14%|█▎        | 83/608 [00:01<00:11, 46.75it/s] 14%|█▍        | 88/608 [00:01<00:11, 47.03it/s] 15%|█▌        | 93/608 [00:01<00:10, 47.03it/s] 16%|█▌        | 98/608 [00:02<00:10, 47.00it/s] 17%|█▋        | 103/608 [00:02<00:10, 47.02it/s] 18%|█▊        | 108/608 [00:02<00:10, 46.89it/s] 19%|█▊        | 113/608 [00:02<00:10, 46.96it/s] 19%|█▉        | 118/608 [00:02<00:10, 46.97it/s] 20%|██        | 123/608 [00:02<00:10, 46.92it/s] 21%|██        | 128/608 [00:02<00:10, 46.93it/s] 22%|██▏       | 133/608 [00:02<00:10, 46.96it/s] 23%|██▎       | 138/608 [00:02<00:10, 46.93it/s] 24%|██▎       | 143/608 [00:03<00:09, 46.95it/s] 24%|██▍       | 148/608 [00:03<00:09, 46.96it/s] 25%|██▌       | 153/608 [00:03<00:09, 46.99it/s] 26%|██▌       | 158/608 [00:03<00:09, 46.92it/s] 27%|██▋       | 163/608 [00:03<00:09, 46.92it/s] 28%|██▊       | 168/608 [00:03<00:09, 46.93it/s] 28%|██▊       | 173/608 [00:03<00:09, 46.86it/s] 29%|██▉       | 178/608 [00:03<00:09, 46.94it/s] 30%|███       | 183/608 [00:03<00:09, 46.91it/s] 31%|███       | 188/608 [00:03<00:08, 46.93it/s] 32%|███▏      | 193/608 [00:04<00:08, 46.96it/s] 33%|███▎      | 198/608 [00:04<00:08, 46.99it/s] 33%|███▎      | 203/608 [00:04<00:08, 46.89it/s] 34%|███▍      | 208/608 [00:04<00:08, 46.92it/s] 35%|███▌      | 213/608 [00:04<00:08, 46.92it/s] 36%|███▌      | 218/608 [00:04<00:08, 46.91it/s] 37%|███▋      | 223/608 [00:04<00:08, 46.93it/s] 38%|███▊      | 228/608 [00:04<00:08, 46.86it/s] 38%|███▊      | 233/608 [00:04<00:08, 46.87it/s] 39%|███▉      | 238/608 [00:05<00:07, 46.92it/s] 40%|███▉      | 243/608 [00:05<00:07, 46.95it/s] 41%|████      | 248/608 [00:05<00:07, 46.94it/s] 42%|████▏     | 253/608 [00:05<00:07, 46.93it/s] 42%|████▏     | 258/608 [00:05<00:07, 46.85it/s] 43%|████▎     | 263/608 [00:05<00:07, 46.87it/s] 44%|████▍     | 268/608 [00:05<00:07, 46.94it/s] 45%|████▍     | 273/608 [00:05<00:07, 46.89it/s] 46%|████▌     | 278/608 [00:05<00:07, 46.88it/s] 47%|████▋     | 283/608 [00:06<00:06, 46.94it/s] 47%|████▋     | 288/608 [00:06<00:06, 46.90it/s] 48%|████▊     | 293/608 [00:06<00:06, 46.89it/s] 49%|████▉     | 298/608 [00:06<00:06, 46.95it/s] 50%|████▉     | 303/608 [00:06<00:06, 46.89it/s] 51%|█████     | 308/608 [00:06<00:06, 46.85it/s] 51%|█████▏    | 313/608 [00:06<00:06, 46.94it/s] 52%|█████▏    | 318/608 [00:06<00:06, 46.86it/s] 53%|█████▎    | 323/608 [00:06<00:06, 46.89it/s] 54%|█████▍    | 328/608 [00:06<00:05, 46.90it/s] 55%|█████▍    | 333/608 [00:07<00:05, 46.92it/s] 56%|█████▌    | 338/608 [00:07<00:05, 46.82it/s] 56%|█████▋    | 343/608 [00:07<00:05, 46.93it/s] 57%|█████▋    | 348/608 [00:07<00:05, 46.92it/s] 58%|█████▊    | 353/608 [00:07<00:05, 46.89it/s] 59%|█████▉    | 358/608 [00:07<00:05, 46.89it/s] 60%|█████▉    | 363/608 [00:07<00:05, 46.85it/s] 61%|██████    | 368/608 [00:07<00:05, 46.87it/s] 61%|██████▏   | 373/608 [00:07<00:05, 46.93it/s] 62%|██████▏   | 378/608 [00:08<00:04, 46.94it/s] 63%|██████▎   | 383/608 [00:08<00:04, 46.79it/s] 64%|██████▍   | 388/608 [00:08<00:04, 46.73it/s] 65%|██████▍   | 393/608 [00:08<00:04, 46.73it/s] 65%|██████▌   | 398/608 [00:08<00:04, 46.84it/s] 66%|██████▋   | 403/608 [00:08<00:04, 46.89it/s] 67%|██████▋   | 408/608 [00:08<00:04, 46.85it/s] 68%|██████▊   | 413/608 [00:08<00:04, 46.88it/s] 69%|██████▉   | 418/608 [00:08<00:04, 46.87it/s] 70%|██████▉   | 423/608 [00:08<00:03, 46.95it/s] 70%|███████   | 428/608 [00:09<00:03, 46.96it/s] 71%|███████   | 433/608 [00:09<00:03, 46.84it/s] 72%|███████▏  | 438/608 [00:09<00:03, 46.78it/s] 73%|███████▎  | 443/608 [00:09<00:03, 46.85it/s] 74%|███████▎  | 448/608 [00:09<00:03, 46.93it/s] 75%|███████▍  | 453/608 [00:09<00:03, 46.86it/s] 75%|███████▌  | 458/608 [00:09<00:03, 46.86it/s] 76%|███████▌  | 463/608 [00:09<00:03, 46.90it/s] 77%|███████▋  | 468/608 [00:09<00:02, 46.89it/s] 78%|███████▊  | 473/608 [00:10<00:02, 46.90it/s] 79%|███████▊  | 478/608 [00:10<00:02, 46.85it/s] 79%|███████▉  | 483/608 [00:10<00:02, 46.73it/s] 80%|████████  | 488/608 [00:10<00:02, 46.86it/s] 81%|████████  | 493/608 [00:10<00:02, 46.86it/s] 82%|████████▏ | 498/608 [00:10<00:02, 46.86it/s] 83%|████████▎ | 503/608 [00:10<00:02, 46.79it/s] 84%|████████▎ | 508/608 [00:10<00:02, 46.87it/s] 84%|████████▍ | 513/608 [00:10<00:02, 46.86it/s] 85%|████████▌ | 518/608 [00:11<00:01, 46.88it/s] 86%|████████▌ | 523/608 [00:11<00:01, 46.86it/s] 87%|████████▋ | 528/608 [00:11<00:01, 46.74it/s] 88%|████████▊ | 533/608 [00:11<00:01, 46.77it/s] 88%|████████▊ | 538/608 [00:11<00:01, 46.85it/s] 89%|████████▉ | 543/608 [00:11<00:01, 46.72it/s] 90%|█████████ | 548/608 [00:11<00:01, 46.79it/s] 91%|█████████ | 553/608 [00:11<00:01, 46.78it/s] 92%|█████████▏| 558/608 [00:11<00:01, 46.78it/s] 93%|█████████▎| 563/608 [00:11<00:00, 46.77it/s] 93%|█████████▎| 568/608 [00:12<00:00, 46.79it/s] 94%|█████████▍| 573/608 [00:12<00:00, 46.78it/s] 95%|█████████▌| 578/608 [00:12<00:00, 46.78it/s] 96%|█████████▌| 583/608 [00:12<00:00, 46.74it/s] 97%|█████████▋| 588/608 [00:12<00:00, 46.78it/s] 98%|█████████▊| 593/608 [00:12<00:00, 46.70it/s] 98%|█████████▊| 598/608 [00:12<00:00, 46.71it/s] 99%|█████████▉| 603/608 [00:12<00:00, 46.67it/s]100%|██████████| 608/608 [00:12<00:00, 46.79it/s]100%|██████████| 608/608 [00:12<00:00, 46.97it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:42:57,035 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:57,035 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:57,035 >>   eval_loss               =     0.9506
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:57,035 >>   eval_runtime            = 0:00:12.96
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:57,035 >>   eval_samples            =       4860
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:57,035 >>   eval_samples_per_second =    374.753
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:57,035 >>   eval_steps_per_second   =     46.883
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:42:57,035 >>   perplexity              =     2.5872
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:03,064 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:03,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:03,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:03,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:03,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:43:03,943 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:43:03,977 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:43:04,690 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:43:05,753 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:43:05,753 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:08,065 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:08,102 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:08,102 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:08,102 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:43:08,102 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:43:08,624 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:43:08,625 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:43:09,337 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:43:09,498 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:43:09,498 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-354
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-236
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'labels': ['headquarters location', 'licensed to broadcast to', 'member of political party', 'narrative location', 'notable work'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14287
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14387, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.60it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.62it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:21,  1.62it/s]Extractor Predicting: 35it [00:22,  1.62it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:23,  1.59it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:26,  1.62it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:29,  1.54it/s]Extractor Predicting: 47it [00:29,  1.53it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:30,  1.54it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.49it/s]Extractor Predicting: 53it [00:33,  1.51it/s]Extractor Predicting: 54it [00:34,  1.52it/s]Extractor Predicting: 55it [00:34,  1.53it/s]Extractor Predicting: 56it [00:35,  1.52it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:36,  1.48it/s]Extractor Predicting: 59it [00:37,  1.50it/s]Extractor Predicting: 60it [00:38,  1.49it/s]Extractor Predicting: 61it [00:38,  1.54it/s]Extractor Predicting: 62it [00:39,  1.54it/s]Extractor Predicting: 63it [00:40,  1.52it/s]Extractor Predicting: 64it [00:40,  1.53it/s]Extractor Predicting: 65it [00:41,  1.54it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:42,  1.49it/s]Extractor Predicting: 68it [00:43,  1.54it/s]Extractor Predicting: 69it [00:44,  1.53it/s]Extractor Predicting: 70it [00:44,  1.53it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:46,  1.56it/s]Extractor Predicting: 73it [00:46,  1.60it/s]Extractor Predicting: 74it [00:47,  1.60it/s]Extractor Predicting: 75it [00:47,  1.59it/s]Extractor Predicting: 76it [00:48,  1.62it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:49,  1.60it/s]Extractor Predicting: 79it [00:50,  1.58it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:51,  1.57it/s]Extractor Predicting: 82it [00:52,  1.58it/s]Extractor Predicting: 83it [00:52,  1.59it/s]Extractor Predicting: 84it [00:53,  1.58it/s]Extractor Predicting: 85it [00:54,  1.59it/s]Extractor Predicting: 86it [00:54,  1.59it/s]Extractor Predicting: 87it [00:55,  1.59it/s]Extractor Predicting: 88it [00:56,  1.57it/s]Extractor Predicting: 89it [00:56,  1.58it/s]Extractor Predicting: 90it [00:57,  1.58it/s]Extractor Predicting: 91it [00:58,  1.59it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [00:59,  1.59it/s]Extractor Predicting: 95it [01:00,  1.58it/s]Extractor Predicting: 96it [01:01,  1.43it/s]Extractor Predicting: 97it [01:02,  1.47it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:03,  1.53it/s]Extractor Predicting: 101it [01:04,  1.57it/s]Extractor Predicting: 102it [01:05,  1.55it/s]Extractor Predicting: 103it [01:05,  1.53it/s]Extractor Predicting: 104it [01:06,  1.55it/s]Extractor Predicting: 105it [01:07,  1.56it/s]Extractor Predicting: 106it [01:07,  1.58it/s]Extractor Predicting: 107it [01:08,  1.60it/s]Extractor Predicting: 108it [01:09,  1.57it/s]Extractor Predicting: 109it [01:09,  1.57it/s]Extractor Predicting: 110it [01:10,  1.58it/s]Extractor Predicting: 111it [01:10,  1.61it/s]Extractor Predicting: 112it [01:11,  1.61it/s]Extractor Predicting: 113it [01:12,  1.62it/s]Extractor Predicting: 114it [01:12,  1.61it/s]Extractor Predicting: 115it [01:13,  1.61it/s]Extractor Predicting: 116it [01:14,  1.60it/s]Extractor Predicting: 117it [01:14,  1.61it/s]Extractor Predicting: 118it [01:15,  1.62it/s]Extractor Predicting: 119it [01:15,  1.62it/s]Extractor Predicting: 120it [01:16,  1.61it/s]Extractor Predicting: 121it [01:17,  1.58it/s]Extractor Predicting: 122it [01:17,  1.59it/s]Extractor Predicting: 123it [01:18,  1.60it/s]Extractor Predicting: 124it [01:19,  1.58it/s]Extractor Predicting: 125it [01:19,  1.53it/s]Extractor Predicting: 126it [01:20,  1.53it/s]Extractor Predicting: 127it [01:21,  1.54it/s]Extractor Predicting: 128it [01:21,  1.54it/s]Extractor Predicting: 129it [01:22,  1.55it/s]Extractor Predicting: 130it [01:22,  1.59it/s]Extractor Predicting: 131it [01:23,  1.57it/s]Extractor Predicting: 132it [01:24,  1.58it/s]Extractor Predicting: 133it [01:24,  1.52it/s]Extractor Predicting: 134it [01:25,  1.54it/s]Extractor Predicting: 135it [01:26,  1.53it/s]Extractor Predicting: 136it [01:26,  1.54it/s]Extractor Predicting: 137it [01:27,  1.54it/s]Extractor Predicting: 138it [01:28,  1.53it/s]Extractor Predicting: 139it [01:28,  1.53it/s]Extractor Predicting: 140it [01:29,  1.54it/s]Extractor Predicting: 141it [01:30,  1.52it/s]Extractor Predicting: 142it [01:30,  1.55it/s]Extractor Predicting: 143it [01:31,  1.54it/s]Extractor Predicting: 144it [01:32,  1.54it/s]Extractor Predicting: 145it [01:32,  1.55it/s]Extractor Predicting: 146it [01:33,  1.58it/s]Extractor Predicting: 147it [01:33,  1.52it/s]Extractor Predicting: 148it [01:34,  1.52it/s]Extractor Predicting: 149it [01:35,  1.52it/s]Extractor Predicting: 150it [01:35,  1.55it/s]Extractor Predicting: 151it [01:36,  1.57it/s]Extractor Predicting: 152it [01:37,  1.59it/s]Extractor Predicting: 153it [01:37,  1.64it/s]Extractor Predicting: 154it [01:38,  1.61it/s]Extractor Predicting: 155it [01:39,  1.57it/s]Extractor Predicting: 156it [01:39,  1.58it/s]Extractor Predicting: 157it [01:40,  1.59it/s]Extractor Predicting: 158it [01:40,  1.58it/s]Extractor Predicting: 159it [01:41,  1.55it/s]Extractor Predicting: 160it [01:42,  1.58it/s]Extractor Predicting: 161it [01:42,  1.56it/s]Extractor Predicting: 162it [01:43,  1.55it/s]Extractor Predicting: 163it [01:44,  1.54it/s]Extractor Predicting: 164it [01:44,  1.56it/s]Extractor Predicting: 165it [01:45,  1.55it/s]Extractor Predicting: 166it [01:46,  1.54it/s]Extractor Predicting: 167it [01:46,  1.53it/s]Extractor Predicting: 168it [01:47,  1.56it/s]Extractor Predicting: 169it [01:48,  1.53it/s]Extractor Predicting: 170it [01:48,  1.54it/s]Extractor Predicting: 171it [01:49,  1.56it/s]Extractor Predicting: 172it [01:49,  1.55it/s]Extractor Predicting: 173it [01:50,  1.52it/s]Extractor Predicting: 174it [01:51,  1.57it/s]Extractor Predicting: 175it [01:51,  1.53it/s]Extractor Predicting: 176it [01:52,  1.53it/s]Extractor Predicting: 177it [01:53,  1.49it/s]Extractor Predicting: 178it [01:53,  1.47it/s]Extractor Predicting: 179it [01:54,  1.48it/s]Extractor Predicting: 180it [01:55,  1.47it/s]Extractor Predicting: 181it [01:56,  1.47it/s]Extractor Predicting: 182it [01:56,  1.50it/s]Extractor Predicting: 183it [01:57,  1.48it/s]Extractor Predicting: 184it [01:58,  1.47it/s]Extractor Predicting: 185it [01:58,  1.45it/s]Extractor Predicting: 186it [01:59,  1.34it/s]Extractor Predicting: 187it [02:00,  1.37it/s]Extractor Predicting: 188it [02:01,  1.39it/s]Extractor Predicting: 189it [02:01,  1.42it/s]Extractor Predicting: 190it [02:02,  1.42it/s]Extractor Predicting: 191it [02:03,  1.44it/s]Extractor Predicting: 192it [02:03,  1.47it/s]Extractor Predicting: 193it [02:04,  1.51it/s]Extractor Predicting: 194it [02:04,  1.52it/s]Extractor Predicting: 195it [02:05,  1.94it/s]Extractor Predicting: 195it [02:05,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:22,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:22,790 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:22,790 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:22,790 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:22,790 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:45:23,099 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:45:23,100 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:45:23,782 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:45:24,801 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:45:24,801 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:28,278 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:28,282 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:28,282 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:28,282 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:45:28,282 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:45:28,925 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:45:28,927 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:45:29,502 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:45:29,658 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:45:29,659 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5141800246609125,
  "recall": 0.08580246913580247,
  "score": 0.14706400987480164,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21244
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21344, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.48it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:24,  1.55it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.60it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:30,  1.59it/s]Extractor Predicting: 48it [00:30,  1.58it/s]Extractor Predicting: 49it [00:31,  1.62it/s]Extractor Predicting: 50it [00:31,  1.62it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:33,  1.67it/s]Extractor Predicting: 54it [00:34,  1.69it/s]Extractor Predicting: 55it [00:34,  1.68it/s]Extractor Predicting: 56it [00:35,  1.64it/s]Extractor Predicting: 57it [00:36,  1.64it/s]Extractor Predicting: 58it [00:36,  1.69it/s]Extractor Predicting: 59it [00:37,  1.69it/s]Extractor Predicting: 60it [00:37,  1.67it/s]Extractor Predicting: 61it [00:38,  1.61it/s]Extractor Predicting: 62it [00:39,  1.64it/s]Extractor Predicting: 63it [00:39,  1.52it/s]Extractor Predicting: 64it [00:40,  1.52it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:41,  1.58it/s]Extractor Predicting: 67it [00:42,  1.63it/s]Extractor Predicting: 68it [00:42,  1.65it/s]Extractor Predicting: 69it [00:43,  1.68it/s]Extractor Predicting: 70it [00:44,  1.66it/s]Extractor Predicting: 71it [00:44,  1.62it/s]Extractor Predicting: 72it [00:45,  1.61it/s]Extractor Predicting: 73it [00:45,  1.62it/s]Extractor Predicting: 74it [00:46,  1.60it/s]Extractor Predicting: 75it [00:47,  1.60it/s]Extractor Predicting: 76it [00:47,  1.57it/s]Extractor Predicting: 77it [00:48,  1.59it/s]Extractor Predicting: 78it [00:49,  1.57it/s]Extractor Predicting: 79it [00:49,  1.63it/s]Extractor Predicting: 80it [00:50,  1.65it/s]Extractor Predicting: 81it [00:50,  1.68it/s]Extractor Predicting: 82it [00:51,  1.67it/s]Extractor Predicting: 83it [00:52,  1.67it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:53,  1.62it/s]Extractor Predicting: 86it [00:53,  1.67it/s]Extractor Predicting: 87it [00:54,  1.68it/s]Extractor Predicting: 88it [00:55,  1.74it/s]Extractor Predicting: 89it [00:55,  1.72it/s]Extractor Predicting: 90it [00:56,  1.72it/s]Extractor Predicting: 91it [00:56,  1.80it/s]Extractor Predicting: 92it [00:57,  1.75it/s]Extractor Predicting: 93it [00:57,  1.76it/s]Extractor Predicting: 94it [00:58,  1.76it/s]Extractor Predicting: 95it [00:59,  1.75it/s]Extractor Predicting: 96it [00:59,  1.70it/s]Extractor Predicting: 97it [01:00,  1.67it/s]Extractor Predicting: 98it [01:00,  1.71it/s]Extractor Predicting: 99it [01:01,  1.72it/s]Extractor Predicting: 100it [01:01,  1.72it/s]Extractor Predicting: 101it [01:02,  1.72it/s]Extractor Predicting: 102it [01:03,  1.71it/s]Extractor Predicting: 103it [01:03,  1.72it/s]Extractor Predicting: 104it [01:04,  1.70it/s]Extractor Predicting: 105it [01:04,  1.75it/s]Extractor Predicting: 106it [01:05,  1.74it/s]Extractor Predicting: 107it [01:06,  1.77it/s]Extractor Predicting: 108it [01:06,  1.73it/s]Extractor Predicting: 109it [01:07,  1.78it/s]Extractor Predicting: 110it [01:07,  1.73it/s]Extractor Predicting: 111it [01:08,  1.75it/s]Extractor Predicting: 112it [01:08,  1.70it/s]Extractor Predicting: 113it [01:09,  1.68it/s]Extractor Predicting: 114it [01:10,  1.69it/s]Extractor Predicting: 115it [01:10,  1.66it/s]Extractor Predicting: 116it [01:11,  1.66it/s]Extractor Predicting: 117it [01:11,  1.65it/s]Extractor Predicting: 118it [01:12,  1.64it/s]Extractor Predicting: 119it [01:13,  1.63it/s]Extractor Predicting: 120it [01:13,  1.65it/s]Extractor Predicting: 121it [01:14,  1.67it/s]Extractor Predicting: 122it [01:15,  1.65it/s]Extractor Predicting: 123it [01:15,  1.51it/s]Extractor Predicting: 124it [01:16,  1.52it/s]Extractor Predicting: 125it [01:17,  1.54it/s]Extractor Predicting: 126it [01:17,  1.58it/s]Extractor Predicting: 127it [01:18,  1.61it/s]Extractor Predicting: 128it [01:18,  1.60it/s]Extractor Predicting: 129it [01:19,  1.62it/s]Extractor Predicting: 130it [01:20,  1.61it/s]Extractor Predicting: 131it [01:20,  1.58it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.60it/s]Extractor Predicting: 134it [01:22,  1.60it/s]Extractor Predicting: 135it [01:23,  1.64it/s]Extractor Predicting: 136it [01:23,  1.62it/s]Extractor Predicting: 137it [01:24,  1.66it/s]Extractor Predicting: 138it [01:25,  1.64it/s]Extractor Predicting: 139it [01:25,  1.67it/s]Extractor Predicting: 140it [01:26,  1.67it/s]Extractor Predicting: 141it [01:26,  1.64it/s]Extractor Predicting: 142it [01:27,  1.67it/s]Extractor Predicting: 143it [01:28,  1.65it/s]Extractor Predicting: 144it [01:28,  1.61it/s]Extractor Predicting: 145it [01:29,  1.57it/s]Extractor Predicting: 146it [01:30,  1.59it/s]Extractor Predicting: 147it [01:30,  1.61it/s]Extractor Predicting: 148it [01:31,  1.58it/s]Extractor Predicting: 149it [01:31,  1.58it/s]Extractor Predicting: 150it [01:32,  1.62it/s]Extractor Predicting: 151it [01:33,  1.62it/s]Extractor Predicting: 152it [01:33,  1.60it/s]Extractor Predicting: 153it [01:34,  1.63it/s]Extractor Predicting: 154it [01:34,  1.65it/s]Extractor Predicting: 155it [01:35,  1.61it/s]Extractor Predicting: 156it [01:36,  1.63it/s]Extractor Predicting: 157it [01:36,  1.60it/s]Extractor Predicting: 158it [01:37,  1.61it/s]Extractor Predicting: 159it [01:38,  1.59it/s]Extractor Predicting: 160it [01:38,  1.61it/s]Extractor Predicting: 161it [01:39,  1.58it/s]Extractor Predicting: 162it [01:39,  1.60it/s]Extractor Predicting: 163it [01:40,  1.62it/s]Extractor Predicting: 164it [01:41,  1.61it/s]Extractor Predicting: 165it [01:41,  1.58it/s]Extractor Predicting: 166it [01:42,  1.55it/s]Extractor Predicting: 167it [01:43,  1.57it/s]Extractor Predicting: 168it [01:43,  1.61it/s]Extractor Predicting: 169it [01:44,  1.59it/s]Extractor Predicting: 170it [01:45,  1.57it/s]Extractor Predicting: 171it [01:45,  1.55it/s]Extractor Predicting: 172it [01:46,  1.60it/s]Extractor Predicting: 173it [01:46,  1.59it/s]Extractor Predicting: 174it [01:47,  1.57it/s]Extractor Predicting: 175it [01:48,  1.40it/s]Extractor Predicting: 176it [01:49,  1.48it/s]Extractor Predicting: 177it [01:49,  1.52it/s]Extractor Predicting: 178it [01:50,  1.57it/s]Extractor Predicting: 179it [01:50,  1.62it/s]Extractor Predicting: 180it [01:51,  1.61it/s]Extractor Predicting: 181it [01:52,  1.53it/s]Extractor Predicting: 182it [01:52,  1.54it/s]Extractor Predicting: 183it [01:53,  1.57it/s]Extractor Predicting: 184it [01:54,  1.55it/s]Extractor Predicting: 185it [01:54,  1.54it/s]Extractor Predicting: 186it [01:55,  1.55it/s]Extractor Predicting: 187it [01:55,  1.58it/s]Extractor Predicting: 188it [01:56,  1.62it/s]Extractor Predicting: 189it [01:57,  1.64it/s]Extractor Predicting: 190it [01:57,  1.61it/s]Extractor Predicting: 191it [01:58,  1.61it/s]Extractor Predicting: 192it [01:59,  1.60it/s]Extractor Predicting: 193it [01:59,  1.64it/s]Extractor Predicting: 194it [02:00,  1.70it/s]Extractor Predicting: 195it [02:00,  1.72it/s]Extractor Predicting: 196it [02:01,  1.66it/s]Extractor Predicting: 197it [02:02,  1.64it/s]Extractor Predicting: 198it [02:02,  1.62it/s]Extractor Predicting: 199it [02:03,  1.69it/s]Extractor Predicting: 200it [02:03,  1.68it/s]Extractor Predicting: 201it [02:04,  1.66it/s]Extractor Predicting: 202it [02:05,  1.66it/s]Extractor Predicting: 203it [02:05,  1.66it/s]Extractor Predicting: 204it [02:06,  1.62it/s]Extractor Predicting: 205it [02:06,  1.63it/s]Extractor Predicting: 206it [02:07,  1.62it/s]Extractor Predicting: 207it [02:08,  1.61it/s]Extractor Predicting: 208it [02:08,  1.62it/s]Extractor Predicting: 209it [02:09,  1.62it/s]Extractor Predicting: 210it [02:10,  1.60it/s]Extractor Predicting: 211it [02:10,  1.59it/s]Extractor Predicting: 212it [02:11,  1.60it/s]Extractor Predicting: 213it [02:11,  1.63it/s]Extractor Predicting: 214it [02:12,  1.63it/s]Extractor Predicting: 215it [02:13,  1.64it/s]Extractor Predicting: 216it [02:13,  1.65it/s]Extractor Predicting: 217it [02:14,  1.67it/s]Extractor Predicting: 218it [02:14,  1.61it/s]Extractor Predicting: 219it [02:15,  1.60it/s]Extractor Predicting: 220it [02:16,  1.58it/s]Extractor Predicting: 221it [02:16,  1.53it/s]Extractor Predicting: 222it [02:17,  1.51it/s]Extractor Predicting: 223it [02:18,  1.57it/s]Extractor Predicting: 224it [02:18,  1.56it/s]Extractor Predicting: 225it [02:19,  1.56it/s]Extractor Predicting: 226it [02:20,  1.54it/s]Extractor Predicting: 227it [02:20,  1.56it/s]Extractor Predicting: 228it [02:21,  1.56it/s]Extractor Predicting: 229it [02:21,  1.60it/s]Extractor Predicting: 230it [02:22,  1.60it/s]Extractor Predicting: 231it [02:23,  1.56it/s]Extractor Predicting: 232it [02:23,  1.58it/s]Extractor Predicting: 233it [02:24,  1.60it/s]Extractor Predicting: 234it [02:25,  1.64it/s]Extractor Predicting: 235it [02:25,  1.64it/s]Extractor Predicting: 236it [02:26,  1.70it/s]Extractor Predicting: 237it [02:26,  1.72it/s]Extractor Predicting: 238it [02:27,  1.71it/s]Extractor Predicting: 239it [02:27,  1.71it/s]Extractor Predicting: 240it [02:28,  1.68it/s]Extractor Predicting: 241it [02:29,  1.63it/s]Extractor Predicting: 242it [02:29,  1.61it/s]Extractor Predicting: 243it [02:30,  1.65it/s]Extractor Predicting: 244it [02:31,  1.66it/s]Extractor Predicting: 245it [02:31,  1.65it/s]Extractor Predicting: 246it [02:32,  1.59it/s]Extractor Predicting: 247it [02:32,  1.59it/s]Extractor Predicting: 248it [02:33,  1.57it/s]Extractor Predicting: 249it [02:34,  1.54it/s]Extractor Predicting: 250it [02:34,  1.53it/s]Extractor Predicting: 251it [02:35,  1.53it/s]Extractor Predicting: 252it [02:36,  1.52it/s]Extractor Predicting: 253it [02:36,  1.53it/s]Extractor Predicting: 254it [02:37,  1.54it/s]Extractor Predicting: 255it [02:38,  1.57it/s]Extractor Predicting: 256it [02:38,  1.52it/s]Extractor Predicting: 257it [02:39,  1.52it/s]Extractor Predicting: 258it [02:40,  1.50it/s]Extractor Predicting: 259it [02:40,  1.53it/s]Extractor Predicting: 260it [02:41,  1.52it/s]Extractor Predicting: 261it [02:42,  1.49it/s]Extractor Predicting: 262it [02:42,  1.50it/s]Extractor Predicting: 263it [02:43,  1.52it/s]Extractor Predicting: 264it [02:44,  1.53it/s]Extractor Predicting: 265it [02:44,  1.51it/s]Extractor Predicting: 266it [02:45,  1.51it/s]Extractor Predicting: 267it [02:46,  1.51it/s]Extractor Predicting: 268it [02:46,  1.52it/s]Extractor Predicting: 269it [02:47,  1.53it/s]Extractor Predicting: 270it [02:48,  1.55it/s]Extractor Predicting: 271it [02:48,  1.56it/s]Extractor Predicting: 272it [02:49,  1.54it/s]Extractor Predicting: 273it [02:50,  1.54it/s]Extractor Predicting: 274it [02:50,  1.51it/s]Extractor Predicting: 275it [02:51,  1.51it/s]Extractor Predicting: 276it [02:52,  1.53it/s]Extractor Predicting: 277it [02:52,  1.56it/s]Extractor Predicting: 278it [02:53,  1.54it/s]Extractor Predicting: 279it [02:53,  1.53it/s]Extractor Predicting: 280it [02:54,  1.59it/s]Extractor Predicting: 281it [02:55,  1.61it/s]Extractor Predicting: 282it [02:55,  1.46it/s]Extractor Predicting: 283it [02:56,  1.48it/s]Extractor Predicting: 284it [02:57,  1.49it/s]Extractor Predicting: 285it [02:57,  1.53it/s]Extractor Predicting: 286it [02:58,  1.60it/s]Extractor Predicting: 287it [02:59,  1.63it/s]Extractor Predicting: 288it [02:59,  1.58it/s]Extractor Predicting: 289it [03:00,  1.58it/s]Extractor Predicting: 290it [03:00,  1.59it/s]Extractor Predicting: 291it [03:01,  1.59it/s]Extractor Predicting: 292it [03:02,  1.64it/s]Extractor Predicting: 293it [03:02,  1.64it/s]Extractor Predicting: 294it [03:03,  1.67it/s]Extractor Predicting: 295it [03:03,  1.72it/s]Extractor Predicting: 296it [03:04,  1.72it/s]Extractor Predicting: 297it [03:05,  1.72it/s]Extractor Predicting: 298it [03:05,  1.63it/s]Extractor Predicting: 299it [03:06,  1.53it/s]Extractor Predicting: 300it [03:07,  1.51it/s]Extractor Predicting: 301it [03:07,  1.51it/s]Extractor Predicting: 302it [03:08,  1.54it/s]Extractor Predicting: 303it [03:09,  1.56it/s]Extractor Predicting: 304it [03:09,  1.57it/s]Extractor Predicting: 305it [03:10,  1.53it/s]Extractor Predicting: 306it [03:11,  1.52it/s]Extractor Predicting: 307it [03:11,  1.51it/s]Extractor Predicting: 308it [03:12,  1.51it/s]Extractor Predicting: 309it [03:13,  1.48it/s]Extractor Predicting: 310it [03:13,  1.48it/s]Extractor Predicting: 311it [03:14,  1.49it/s]Extractor Predicting: 312it [03:15,  1.51it/s]Extractor Predicting: 313it [03:15,  1.52it/s]Extractor Predicting: 314it [03:16,  1.53it/s]Extractor Predicting: 315it [03:16,  1.58it/s]Extractor Predicting: 316it [03:17,  1.54it/s]Extractor Predicting: 317it [03:18,  1.52it/s]Extractor Predicting: 318it [03:19,  1.46it/s]Extractor Predicting: 319it [03:19,  1.48it/s]Extractor Predicting: 320it [03:20,  1.49it/s]Extractor Predicting: 321it [03:21,  1.48it/s]Extractor Predicting: 322it [03:21,  1.46it/s]Extractor Predicting: 323it [03:22,  1.46it/s]Extractor Predicting: 324it [03:23,  1.46it/s]Extractor Predicting: 325it [03:23,  1.46it/s]Extractor Predicting: 326it [03:24,  1.45it/s]Extractor Predicting: 327it [03:25,  1.45it/s]Extractor Predicting: 328it [03:25,  1.45it/s]Extractor Predicting: 329it [03:26,  1.44it/s]Extractor Predicting: 330it [03:27,  1.45it/s]Extractor Predicting: 331it [03:27,  1.43it/s]Extractor Predicting: 332it [03:28,  1.44it/s]Extractor Predicting: 333it [03:29,  1.42it/s]Extractor Predicting: 334it [03:30,  1.44it/s]Extractor Predicting: 335it [03:30,  1.44it/s]Extractor Predicting: 336it [03:31,  1.40it/s]Extractor Predicting: 337it [03:32,  1.42it/s]Extractor Predicting: 338it [03:32,  1.42it/s]Extractor Predicting: 339it [03:33,  1.43it/s]Extractor Predicting: 340it [03:34,  1.44it/s]Extractor Predicting: 341it [03:34,  1.46it/s]Extractor Predicting: 342it [03:35,  1.46it/s]Extractor Predicting: 343it [03:36,  1.49it/s]Extractor Predicting: 344it [03:36,  1.52it/s]Extractor Predicting: 345it [03:37,  1.52it/s]Extractor Predicting: 346it [03:38,  1.64it/s]Extractor Predicting: 346it [03:38,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:16,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:16,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:16,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:16,581 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:16,581 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:49:17,186 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:49:17,187 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:49:17,758 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:49:18,803 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:49:18,803 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:21,658 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:21,662 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:21,662 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:21,662 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:21,662 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:49:22,268 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:49:22,269 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:49:22,829 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:49:22,981 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:49:22,981 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35,
  "recall": 0.01856763925729443,
  "score": 0.03526448362720403,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6093
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6193, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.63it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.53it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.46it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.42it/s]Extractor Predicting: 21it [00:13,  1.45it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.40it/s]Extractor Predicting: 24it [00:15,  1.40it/s]Extractor Predicting: 25it [00:16,  1.45it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:19,  1.69it/s]Extractor Predicting: 31it [00:19,  1.78it/s]Extractor Predicting: 32it [00:20,  1.83it/s]Extractor Predicting: 33it [00:20,  1.87it/s]Extractor Predicting: 34it [00:21,  1.87it/s]Extractor Predicting: 35it [00:21,  1.91it/s]Extractor Predicting: 36it [00:22,  1.89it/s]Extractor Predicting: 37it [00:23,  1.85it/s]Extractor Predicting: 38it [00:23,  1.88it/s]Extractor Predicting: 39it [00:24,  1.86it/s]Extractor Predicting: 40it [00:24,  1.87it/s]Extractor Predicting: 41it [00:25,  1.86it/s]Extractor Predicting: 42it [00:25,  1.84it/s]Extractor Predicting: 43it [00:26,  1.83it/s]Extractor Predicting: 44it [00:26,  1.85it/s]Extractor Predicting: 45it [00:27,  1.90it/s]Extractor Predicting: 46it [00:27,  1.92it/s]Extractor Predicting: 47it [00:28,  1.90it/s]Extractor Predicting: 48it [00:28,  1.88it/s]Extractor Predicting: 49it [00:29,  1.93it/s]Extractor Predicting: 50it [00:30,  1.86it/s]Extractor Predicting: 51it [00:30,  1.82it/s]Extractor Predicting: 52it [00:31,  1.85it/s]Extractor Predicting: 53it [00:31,  1.85it/s]Extractor Predicting: 54it [00:32,  1.86it/s]Extractor Predicting: 55it [00:32,  1.88it/s]Extractor Predicting: 56it [00:33,  1.87it/s]Extractor Predicting: 57it [00:33,  1.90it/s]Extractor Predicting: 58it [00:34,  1.77it/s]Extractor Predicting: 59it [00:35,  1.64it/s]Extractor Predicting: 60it [00:35,  1.55it/s]Extractor Predicting: 61it [00:36,  1.50it/s]Extractor Predicting: 62it [00:37,  1.48it/s]Extractor Predicting: 63it [00:37,  1.46it/s]Extractor Predicting: 64it [00:38,  1.46it/s]Extractor Predicting: 65it [00:39,  1.45it/s]Extractor Predicting: 66it [00:40,  1.44it/s]Extractor Predicting: 67it [00:40,  1.44it/s]Extractor Predicting: 68it [00:41,  1.43it/s]Extractor Predicting: 69it [00:42,  1.42it/s]Extractor Predicting: 70it [00:42,  1.44it/s]Extractor Predicting: 71it [00:43,  1.42it/s]Extractor Predicting: 72it [00:44,  1.42it/s]Extractor Predicting: 73it [00:44,  1.44it/s]Extractor Predicting: 74it [00:45,  1.46it/s]Extractor Predicting: 75it [00:46,  1.48it/s]Extractor Predicting: 76it [00:46,  1.49it/s]Extractor Predicting: 77it [00:47,  1.83it/s]Extractor Predicting: 77it [00:47,  1.63it/s]
[INFO|configuration_utils.py:515] 2023-08-29 05:50:11,432 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:50:11,433 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:50:11,444 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:50:11,445 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 05:50:11,447 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:50:14,390 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 05:50:14,392 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 05:50:14,402 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:50:14,403 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:50:14,407 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:50:14,410 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:50:14,410 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:50:14,410 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:50:14,410 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:50:14,410 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:50:14,410 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5804195804195804,
  "recall": 0.020651903458571784,
  "score": 0.03988467083133109,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 05:50:14,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:15,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:16,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:16,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:17,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:18,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:18,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:19,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:20,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:20,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:21,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:22,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:22,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:23,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:24,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:24,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:25,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:26,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:26,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:27,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:28,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:28,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:29, 14.94s/it][WARNING|generation_utils.py:914] 2023-08-29 05:50:29,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:30,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:30,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:31,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:32,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:32,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:33,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:34,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:34,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:35,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:35,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:36,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:37,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:37,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:38,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:39,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:39,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:40,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:41,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:41,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:42,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:42,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:43,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:12, 14.82s/it][WARNING|generation_utils.py:914] 2023-08-29 05:50:44,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:45,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:45,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:46,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:47,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:47,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:48,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:49,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:49,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:50,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:51,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:51,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:52,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:53,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:53,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:54,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:55,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:56,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:56,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:57,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:58,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:58,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:50:59,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:45<03:03, 15.25s/it][WARNING|generation_utils.py:914] 2023-08-29 05:51:00,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:00,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:01,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:02,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:02,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:03,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:04,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:05,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:06,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:06,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:07,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:08,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:09,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:09,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:10,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:11,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:12,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:12,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:13,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:14,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:15,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:15,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:16,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:02<02:55, 15.97s/it][WARNING|generation_utils.py:914] 2023-08-29 05:51:17,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:17,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:18,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:19,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:20,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:21,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:21,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:22,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:23,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:24,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:24,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:25,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:26,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:27,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:28,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:29,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:29,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:30,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:31,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:32,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:32,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:34,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:34,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:21<02:49, 17.00s/it][WARNING|generation_utils.py:914] 2023-08-29 05:51:35,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:36,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:37,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:38,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:38,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:39,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:40,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:41,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:41,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:42,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:43,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:43,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:44,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:45,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:46,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:46,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:47,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:48,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:48,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:49,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:50,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:50,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:52,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:52,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:38<02:34, 17.18s/it][WARNING|generation_utils.py:914] 2023-08-29 05:51:53,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:54,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:55,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:55,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:56,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:57,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:58,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:51:59,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:00,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:01,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:02,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:03,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:04,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:04,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:05,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:06,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:07,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:08,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:09,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:09,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:10,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:11,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:12,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:12,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:13,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:59<02:27, 18.45s/it][WARNING|generation_utils.py:914] 2023-08-29 05:52:14,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:15,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:16,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:16,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:17,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:18,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:18,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:19,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:20,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:21,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:21,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:22,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:23,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:23,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:24,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:25,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:26,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:26,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:27,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:28,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:28,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:29,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:30,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:31,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:31,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:32,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:33,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:19<02:11, 18.81s/it][WARNING|generation_utils.py:914] 2023-08-29 05:52:34,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:34,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:35,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:36,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:37,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:38,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:39,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:39,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:40,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:41,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:42,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:43,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:43,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:44,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:45,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:45,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:46,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:47,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:47,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:48,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:49,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:49,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:50,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:51,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:37<01:51, 18.53s/it][WARNING|generation_utils.py:914] 2023-08-29 05:52:52,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:52,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:53,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:54,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:55,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:55,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:56,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:57,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:57,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:58,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:59,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:59,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:00,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:01,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:02,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:02,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:03,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:04,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:05,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:05,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:06,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:07,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:53<01:28, 17.69s/it][WARNING|generation_utils.py:914] 2023-08-29 05:53:07,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:08,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:09,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:09,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:10,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:11,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:11,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:12,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:13,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:14,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:14,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:15,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:15,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:16,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:17,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:17,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:18,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:19,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:19,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:20,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:21,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:21,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:22,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:08<01:07, 16.92s/it][WARNING|generation_utils.py:914] 2023-08-29 05:53:23,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:23,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:24,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:25,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:25,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:26,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:27,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:28,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:28,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:29,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:30,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:31,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:31,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:32,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:33,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:33,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:34,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:35,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:35,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:36,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:37,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:23<00:49, 16.38s/it][WARNING|generation_utils.py:914] 2023-08-29 05:53:38,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:39,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:39,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:40,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:41,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:42,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:43,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:43,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:44,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:45,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:45,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:47,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:47,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:48,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:49,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:50,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:51,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:52,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:53,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:54,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:55,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:41<00:33, 17.00s/it][WARNING|generation_utils.py:914] 2023-08-29 05:53:56,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:57,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:58,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:58,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:59,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:00,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:01,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:01,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:02,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:02,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:03,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:04,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:04,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:05,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:06,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:07,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:08,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:08,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:09,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:10,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:10,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:11,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:12,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:58<00:16, 16.91s/it][WARNING|generation_utils.py:914] 2023-08-29 05:54:13,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:14,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:14,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:15,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:16,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:17,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:18,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:18,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:19,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:20,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:20,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:21,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:22,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:23,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:23,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:24,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:25,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:26,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:26,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:27,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:28,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:29,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:29,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:30,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:16<00:00, 17.30s/it]Generating: 100%|██████████| 15/15 [04:16<00:00, 17.13s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:37,980 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:37,984 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:37,984 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:37,984 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:37,984 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:54:38,567 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:54:38,568 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:54:39,153 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:54:40,203 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:54:40,203 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:43,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:43,059 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:43,060 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:43,060 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:43,060 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:54:43,693 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:54:43,694 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:54:44,277 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:54:44,430 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:54:44,430 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8536931818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8532608695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('The Supernatural', 'licensed to broadcast to', '', 'In 2010 , the CW premiered the pilot episode , titled , The Supernatural , following a fictional character called Tom Sawyer in the series , The Shield .')"}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : the Prime Minister .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.84375, 'errors': {''}}
['Relation : narrative location . Context : Later in Life , he played the title role of a young hero in a new science fiction thriller series directed by Mike Moorcock . Head Entity : The title , Tail Entity : Life .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : narrative location .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : notable work .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('National Institute of Art', 'notable work', '', 'In 2000 , he founded the National Institute of Art ( nIAM ) on the west coast of the United States , where he designed the first major American art installation .')", "('Esther', 'notable work', '', 'In 1787 , he married a Catholic nun named Esther .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 581, 'raw': 736}
{'target': 600, 'success': 609, 'raw': 768}
{'prompt': 'Relation : applies to jurisdiction .', 'success_rate': 0.79296875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 520, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 625, 'raw': 800}
{'prompt': 'Relation : family name .', 'success_rate': 0.78125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 198, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 244, 'raw': 352}
{'target': 600, 'success': 265, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 341, 'raw': 480}
{'target': 600, 'success': 366, 'raw': 512}
{'target': 600, 'success': 387, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 450, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 495, 'raw': 704}
{'target': 600, 'success': 521, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 565, 'raw': 800}
{'target': 600, 'success': 591, 'raw': 832}
{'target': 600, 'success': 611, 'raw': 864}
{'prompt': 'Relation : genre .', 'success_rate': 0.7071759259259259, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.7916666666666666, 'errors': {'', "('the', 'is a list of', '', 'It is a web site that contains detailed information about the .')", "('the largest', 'is a list of', '', 'It is the largest directory of directories of .')", "('3 . 2 million', 'is a list of', '', 'It contains approximately 3 . 2 million .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.8678977272727273, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8410326086956522, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8315217391304348, 'errors': {''}}
['Relation : use . Context : Later in Life , he came close to becoming involved in the revival of French philosophy at the end of the 17th century , when he first sought to publish two works , Le Bicêtre et dans leur fois . Head Entity : Le Bicêtre et dans leur fois , Tail Entity : Le Bicêtre et dans la philosophy .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : use .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('IBM PC', 'use', '', 'The program consisted of a series of software , programs which were written to simulate various operating systems and hardware such as the IBM PC , Commodore 64 , Power , VZIO , ZX Spectrum , HP , and Commodore 64 Classic .')", "('printing', 'use', '', 'The main function of the library is for printing and writing to the file system by writing to a file called a .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 11986
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12086, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.57it/s]Extractor Estimating: 2it [00:01,  1.61it/s]Extractor Estimating: 3it [00:01,  1.48it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.63it/s]Extractor Estimating: 8it [00:05,  1.61it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.66it/s]Extractor Estimating: 11it [00:06,  1.63it/s]Extractor Estimating: 12it [00:07,  1.65it/s]Extractor Estimating: 13it [00:08,  1.61it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.59it/s]Extractor Estimating: 16it [00:09,  1.64it/s]Extractor Estimating: 17it [00:10,  1.67it/s]Extractor Estimating: 18it [00:11,  1.68it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:12,  1.67it/s]Extractor Estimating: 21it [00:12,  1.67it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:14,  1.68it/s]Extractor Estimating: 25it [00:15,  1.67it/s]Extractor Estimating: 26it [00:15,  1.68it/s]Extractor Estimating: 27it [00:16,  1.55it/s]Extractor Estimating: 28it [00:17,  1.56it/s]Extractor Estimating: 29it [00:17,  1.61it/s]Extractor Estimating: 30it [00:18,  1.63it/s]Extractor Estimating: 31it [00:19,  1.64it/s]Extractor Estimating: 32it [00:19,  1.68it/s]Extractor Estimating: 33it [00:20,  1.70it/s]Extractor Estimating: 34it [00:20,  1.67it/s]Extractor Estimating: 35it [00:21,  1.67it/s]Extractor Estimating: 36it [00:22,  1.68it/s]Extractor Estimating: 37it [00:22,  1.65it/s]Extractor Estimating: 38it [00:23,  1.68it/s]Extractor Estimating: 39it [00:23,  1.67it/s]Extractor Estimating: 40it [00:24,  1.68it/s]Extractor Estimating: 41it [00:25,  1.67it/s]Extractor Estimating: 42it [00:25,  1.67it/s]Extractor Estimating: 43it [00:26,  1.66it/s]Extractor Estimating: 44it [00:26,  1.68it/s]Extractor Estimating: 45it [00:27,  1.68it/s]Extractor Estimating: 46it [00:28,  1.67it/s]Extractor Estimating: 47it [00:28,  1.66it/s]Extractor Estimating: 48it [00:29,  1.68it/s]Extractor Estimating: 49it [00:29,  1.68it/s]Extractor Estimating: 50it [00:30,  1.65it/s]Extractor Estimating: 51it [00:31,  1.59it/s]Extractor Estimating: 52it [00:31,  1.65it/s]Extractor Estimating: 53it [00:32,  1.63it/s]Extractor Estimating: 54it [00:32,  1.59it/s]Extractor Estimating: 55it [00:33,  1.64it/s]Extractor Estimating: 56it [00:34,  1.68it/s]Extractor Estimating: 57it [00:34,  1.73it/s]Extractor Estimating: 58it [00:35,  1.67it/s]Extractor Estimating: 59it [00:35,  1.73it/s]Extractor Estimating: 60it [00:36,  1.65it/s]Extractor Estimating: 61it [00:37,  1.69it/s]Extractor Estimating: 62it [00:37,  1.71it/s]Extractor Estimating: 63it [00:38,  1.73it/s]Extractor Estimating: 64it [00:38,  1.70it/s]Extractor Estimating: 65it [00:39,  1.69it/s]Extractor Estimating: 66it [00:39,  1.72it/s]Extractor Estimating: 67it [00:40,  1.64it/s]Extractor Estimating: 68it [00:41,  1.62it/s]Extractor Estimating: 69it [00:41,  1.63it/s]Extractor Estimating: 70it [00:42,  1.63it/s]Extractor Estimating: 71it [00:43,  1.63it/s]Extractor Estimating: 72it [00:43,  1.66it/s]Extractor Estimating: 73it [00:44,  1.68it/s]Extractor Estimating: 74it [00:44,  1.69it/s]Extractor Estimating: 75it [00:45,  1.69it/s]Extractor Estimating: 76it [00:46,  1.63it/s]Extractor Estimating: 77it [00:46,  1.63it/s]Extractor Estimating: 78it [00:47,  1.60it/s]Extractor Estimating: 79it [00:47,  1.61it/s]Extractor Estimating: 80it [00:48,  1.57it/s]Extractor Estimating: 81it [00:49,  1.54it/s]Extractor Estimating: 82it [00:49,  1.53it/s]Extractor Estimating: 83it [00:50,  1.52it/s]Extractor Estimating: 84it [00:51,  1.55it/s]Extractor Estimating: 85it [00:51,  1.52it/s]Extractor Estimating: 86it [00:52,  1.52it/s]Extractor Estimating: 87it [00:53,  1.50it/s]Extractor Estimating: 88it [00:53,  1.49it/s]Extractor Estimating: 89it [00:54,  1.54it/s]Extractor Estimating: 90it [00:55,  1.51it/s]Extractor Estimating: 91it [00:56,  1.39it/s]Extractor Estimating: 92it [00:56,  1.44it/s]Extractor Estimating: 93it [00:57,  1.41it/s]Extractor Estimating: 94it [00:58,  1.46it/s]Extractor Estimating: 95it [00:58,  1.46it/s]Extractor Estimating: 96it [00:59,  1.51it/s]Extractor Estimating: 97it [01:00,  1.47it/s]Extractor Estimating: 98it [01:00,  1.39it/s]Extractor Estimating: 99it [01:01,  1.41it/s]Extractor Estimating: 100it [01:02,  1.45it/s]Extractor Estimating: 101it [01:02,  1.48it/s]Extractor Estimating: 102it [01:03,  1.48it/s]Extractor Estimating: 103it [01:04,  1.49it/s]Extractor Estimating: 104it [01:04,  1.50it/s]Extractor Estimating: 105it [01:05,  1.43it/s]Extractor Estimating: 106it [01:06,  1.47it/s]Extractor Estimating: 107it [01:06,  1.50it/s]Extractor Estimating: 108it [01:07,  1.50it/s]Extractor Estimating: 109it [01:08,  1.50it/s]Extractor Estimating: 110it [01:08,  1.52it/s]Extractor Estimating: 111it [01:09,  1.52it/s]Extractor Estimating: 112it [01:10,  1.43it/s]Extractor Estimating: 113it [01:11,  1.43it/s]Extractor Estimating: 114it [01:11,  1.39it/s]Extractor Estimating: 115it [01:12,  1.46it/s]Extractor Estimating: 116it [01:13,  1.48it/s]Extractor Estimating: 117it [01:13,  1.48it/s]Extractor Estimating: 118it [01:14,  1.52it/s]Extractor Estimating: 119it [01:15,  1.54it/s]Extractor Estimating: 120it [01:15,  1.48it/s]Extractor Estimating: 121it [01:16,  1.51it/s]Extractor Estimating: 122it [01:17,  1.46it/s]Extractor Estimating: 123it [01:17,  1.51it/s]Extractor Estimating: 124it [01:18,  1.52it/s]Extractor Estimating: 125it [01:19,  1.47it/s]Extractor Estimating: 126it [01:19,  1.49it/s]Extractor Estimating: 127it [01:20,  1.48it/s]Extractor Estimating: 128it [01:21,  1.51it/s]Extractor Estimating: 129it [01:21,  1.46it/s]Extractor Estimating: 130it [01:22,  1.45it/s]Extractor Estimating: 131it [01:23,  1.49it/s]Extractor Estimating: 132it [01:23,  1.51it/s]Extractor Estimating: 133it [01:24,  1.55it/s]Extractor Estimating: 134it [01:25,  1.54it/s]Extractor Estimating: 135it [01:25,  1.55it/s]Extractor Estimating: 136it [01:26,  1.56it/s]Extractor Estimating: 137it [01:26,  1.56it/s]Extractor Estimating: 138it [01:27,  1.53it/s]Extractor Estimating: 139it [01:28,  1.55it/s]Extractor Estimating: 140it [01:28,  1.53it/s]Extractor Estimating: 141it [01:29,  1.53it/s]Extractor Estimating: 142it [01:30,  1.49it/s]Extractor Estimating: 143it [01:30,  1.50it/s]Extractor Estimating: 144it [01:31,  1.54it/s]Extractor Estimating: 145it [01:32,  1.58it/s]Extractor Estimating: 146it [01:32,  1.60it/s]Extractor Estimating: 147it [01:33,  1.58it/s]Extractor Estimating: 148it [01:34,  1.50it/s]Extractor Estimating: 149it [01:34,  1.52it/s]Extractor Estimating: 150it [01:35,  1.52it/s]Extractor Estimating: 151it [01:36,  1.46it/s]Extractor Estimating: 152it [01:36,  1.48it/s]Extractor Estimating: 153it [01:37,  1.52it/s]Extractor Estimating: 154it [01:38,  1.57it/s]Extractor Estimating: 155it [01:38,  1.53it/s]Extractor Estimating: 156it [01:39,  1.53it/s]Extractor Estimating: 157it [01:40,  1.53it/s]Extractor Estimating: 158it [01:40,  1.55it/s]Extractor Estimating: 159it [01:41,  1.55it/s]Extractor Estimating: 160it [01:42,  1.55it/s]Extractor Estimating: 161it [01:42,  1.48it/s]Extractor Estimating: 162it [01:43,  1.47it/s]Extractor Estimating: 163it [01:44,  1.46it/s]Extractor Estimating: 164it [01:44,  1.51it/s]Extractor Estimating: 165it [01:45,  1.36it/s]Extractor Estimating: 166it [01:46,  1.39it/s]Extractor Estimating: 167it [01:47,  1.40it/s]Extractor Estimating: 168it [01:47,  1.41it/s]Extractor Estimating: 169it [01:48,  1.45it/s]Extractor Estimating: 170it [01:49,  1.45it/s]Extractor Estimating: 171it [01:49,  1.45it/s]Extractor Estimating: 172it [01:50,  1.49it/s]Extractor Estimating: 173it [01:51,  1.46it/s]Extractor Estimating: 174it [01:51,  1.48it/s]Extractor Estimating: 175it [01:52,  1.47it/s]Extractor Estimating: 176it [01:53,  1.49it/s]Extractor Estimating: 177it [01:53,  1.50it/s]Extractor Estimating: 178it [01:54,  1.48it/s]Extractor Estimating: 179it [01:55,  1.54it/s]Extractor Estimating: 180it [01:55,  1.57it/s]Extractor Estimating: 181it [01:56,  1.55it/s]Extractor Estimating: 182it [01:56,  1.54it/s]Extractor Estimating: 183it [01:57,  1.55it/s]Extractor Estimating: 184it [01:58,  1.55it/s]Extractor Estimating: 185it [01:58,  1.51it/s]Extractor Estimating: 186it [01:59,  1.51it/s]Extractor Estimating: 187it [02:00,  1.48it/s]Extractor Estimating: 188it [02:01,  1.47it/s]Extractor Estimating: 189it [02:01,  1.47it/s]Extractor Estimating: 190it [02:02,  1.52it/s]Extractor Estimating: 191it [02:02,  1.52it/s]Extractor Estimating: 192it [02:03,  1.49it/s]Extractor Estimating: 193it [02:04,  1.48it/s]Extractor Estimating: 194it [02:04,  1.55it/s]Extractor Estimating: 195it [02:05,  1.52it/s]Extractor Estimating: 196it [02:06,  1.53it/s]Extractor Estimating: 197it [02:06,  1.52it/s]Extractor Estimating: 198it [02:07,  1.49it/s]Extractor Estimating: 199it [02:08,  1.52it/s]Extractor Estimating: 200it [02:08,  1.52it/s]Extractor Estimating: 201it [02:09,  1.57it/s]Extractor Estimating: 202it [02:10,  1.54it/s]Extractor Estimating: 203it [02:10,  1.59it/s]Extractor Estimating: 204it [02:11,  1.57it/s]Extractor Estimating: 205it [02:11,  1.65it/s]Extractor Estimating: 206it [02:12,  1.67it/s]Extractor Estimating: 207it [02:13,  1.66it/s]Extractor Estimating: 208it [02:13,  1.69it/s]Extractor Estimating: 209it [02:14,  1.72it/s]Extractor Estimating: 210it [02:14,  1.72it/s]Extractor Estimating: 211it [02:15,  1.71it/s]Extractor Estimating: 212it [02:16,  1.72it/s]Extractor Estimating: 213it [02:16,  1.76it/s]Extractor Estimating: 214it [02:17,  1.75it/s]Extractor Estimating: 215it [02:17,  1.82it/s]Extractor Estimating: 216it [02:18,  1.86it/s]Extractor Estimating: 217it [02:18,  1.79it/s]Extractor Estimating: 218it [02:19,  1.75it/s]Extractor Estimating: 219it [02:19,  1.74it/s]Extractor Estimating: 220it [02:20,  1.71it/s]Extractor Estimating: 221it [02:21,  1.72it/s]Extractor Estimating: 222it [02:21,  1.75it/s]Extractor Estimating: 223it [02:22,  1.74it/s]Extractor Estimating: 224it [02:22,  1.74it/s]Extractor Estimating: 225it [02:23,  1.76it/s]Extractor Estimating: 226it [02:23,  1.74it/s]Extractor Estimating: 227it [02:24,  1.70it/s]Extractor Estimating: 228it [02:25,  1.69it/s]Extractor Estimating: 229it [02:25,  1.69it/s]Extractor Estimating: 230it [02:26,  1.70it/s]Extractor Estimating: 231it [02:26,  1.72it/s]Extractor Estimating: 232it [02:27,  1.72it/s]Extractor Estimating: 233it [02:28,  1.76it/s]Extractor Estimating: 234it [02:28,  1.74it/s]Extractor Estimating: 235it [02:29,  1.79it/s]Extractor Estimating: 236it [02:29,  1.84it/s]Extractor Estimating: 237it [02:30,  1.77it/s]Extractor Estimating: 238it [02:30,  1.72it/s]Extractor Estimating: 239it [02:31,  1.69it/s]Extractor Estimating: 240it [02:32,  1.69it/s]Extractor Estimating: 241it [02:32,  1.68it/s]Extractor Estimating: 242it [02:33,  1.66it/s]Extractor Estimating: 243it [02:33,  1.68it/s]Extractor Estimating: 244it [02:34,  1.69it/s]Extractor Estimating: 245it [02:35,  1.70it/s]Extractor Estimating: 246it [02:35,  1.71it/s]Extractor Estimating: 247it [02:36,  1.71it/s]Extractor Estimating: 248it [02:36,  1.71it/s]Extractor Estimating: 249it [02:37,  1.72it/s]Extractor Estimating: 250it [02:37,  1.71it/s]Extractor Estimating: 251it [02:38,  1.69it/s]Extractor Estimating: 252it [02:39,  1.75it/s]Extractor Estimating: 253it [02:39,  1.77it/s]Extractor Estimating: 254it [02:40,  1.77it/s]Extractor Estimating: 255it [02:40,  1.77it/s]Extractor Estimating: 256it [02:41,  1.78it/s]Extractor Estimating: 257it [02:41,  1.75it/s]Extractor Estimating: 258it [02:42,  1.61it/s]Extractor Estimating: 259it [02:43,  1.68it/s]Extractor Estimating: 260it [02:43,  1.69it/s]Extractor Estimating: 261it [02:44,  1.74it/s]Extractor Estimating: 262it [02:44,  1.76it/s]Extractor Estimating: 263it [02:45,  1.81it/s]Extractor Estimating: 264it [02:45,  1.79it/s]Extractor Estimating: 265it [02:46,  1.78it/s]Extractor Estimating: 266it [02:47,  1.77it/s]Extractor Estimating: 267it [02:47,  1.72it/s]Extractor Estimating: 268it [02:48,  1.72it/s]Extractor Estimating: 269it [02:48,  1.74it/s]Extractor Estimating: 270it [02:49,  1.74it/s]Extractor Estimating: 271it [02:49,  1.78it/s]Extractor Estimating: 272it [02:50,  1.75it/s]Extractor Estimating: 273it [02:51,  1.77it/s]Extractor Estimating: 274it [02:51,  1.77it/s]Extractor Estimating: 275it [02:52,  1.65it/s]Extractor Estimating: 276it [02:53,  1.59it/s]Extractor Estimating: 277it [02:53,  1.57it/s]Extractor Estimating: 278it [02:54,  1.57it/s]Extractor Estimating: 279it [02:55,  1.52it/s]Extractor Estimating: 280it [02:55,  1.50it/s]Extractor Estimating: 281it [02:56,  1.44it/s]Extractor Estimating: 282it [02:57,  1.45it/s]Extractor Estimating: 283it [02:57,  1.45it/s]Extractor Estimating: 284it [02:58,  1.43it/s]Extractor Estimating: 285it [02:59,  1.41it/s]Extractor Estimating: 286it [02:59,  1.45it/s]Extractor Estimating: 287it [03:00,  1.48it/s]Extractor Estimating: 288it [03:01,  1.53it/s]Extractor Estimating: 289it [03:01,  1.55it/s]Extractor Estimating: 290it [03:02,  1.54it/s]Extractor Estimating: 291it [03:03,  1.54it/s]Extractor Estimating: 292it [03:03,  1.55it/s]Extractor Estimating: 293it [03:04,  1.53it/s]Extractor Estimating: 294it [03:05,  1.54it/s]Extractor Estimating: 295it [03:05,  1.56it/s]Extractor Estimating: 296it [03:06,  1.50it/s]Extractor Estimating: 297it [03:07,  1.48it/s]Extractor Estimating: 298it [03:07,  1.45it/s]Extractor Estimating: 299it [03:08,  1.42it/s]Extractor Estimating: 300it [03:09,  1.37it/s]Extractor Estimating: 301it [03:09,  1.49it/s]Extractor Estimating: 302it [03:10,  1.47it/s]Extractor Estimating: 303it [03:11,  1.44it/s]Extractor Estimating: 304it [03:11,  1.52it/s]Extractor Estimating: 305it [03:12,  1.48it/s]Extractor Estimating: 306it [03:13,  1.51it/s]Extractor Estimating: 307it [03:13,  1.63it/s]Extractor Estimating: 308it [03:14,  1.65it/s]Extractor Estimating: 309it [03:14,  1.69it/s]Extractor Estimating: 310it [03:15,  1.67it/s]Extractor Estimating: 311it [03:16,  1.66it/s]Extractor Estimating: 312it [03:16,  1.63it/s]Extractor Estimating: 313it [03:17,  1.46it/s]Extractor Estimating: 314it [03:18,  1.50it/s]Extractor Estimating: 315it [03:18,  1.51it/s]Extractor Estimating: 316it [03:19,  1.57it/s]Extractor Estimating: 317it [03:20,  1.64it/s]Extractor Estimating: 318it [03:20,  1.71it/s]Extractor Estimating: 319it [03:21,  1.74it/s]Extractor Estimating: 320it [03:21,  1.74it/s]Extractor Estimating: 321it [03:22,  1.59it/s]Extractor Estimating: 322it [03:23,  1.54it/s]Extractor Estimating: 323it [03:23,  1.44it/s]Extractor Estimating: 324it [03:24,  1.45it/s]Extractor Estimating: 325it [03:25,  1.50it/s]Extractor Estimating: 326it [03:25,  1.58it/s]Extractor Estimating: 327it [03:26,  1.55it/s]Extractor Estimating: 328it [03:27,  1.52it/s]Extractor Estimating: 329it [03:27,  1.62it/s]Extractor Estimating: 330it [03:28,  1.61it/s]Extractor Estimating: 331it [03:28,  1.61it/s]Extractor Estimating: 332it [03:29,  1.59it/s]Extractor Estimating: 333it [03:30,  1.61it/s]Extractor Estimating: 334it [03:30,  1.63it/s]Extractor Estimating: 335it [03:31,  1.67it/s]Extractor Estimating: 336it [03:31,  1.68it/s]Extractor Estimating: 337it [03:32,  1.49it/s]Extractor Estimating: 338it [03:33,  1.57it/s]Extractor Estimating: 339it [03:34,  1.51it/s]Extractor Estimating: 340it [03:34,  1.47it/s]Extractor Estimating: 341it [03:35,  1.53it/s]Extractor Estimating: 342it [03:35,  1.57it/s]Extractor Estimating: 343it [03:36,  1.60it/s]Extractor Estimating: 344it [03:37,  1.60it/s]Extractor Estimating: 345it [03:37,  1.58it/s]Extractor Estimating: 346it [03:38,  1.52it/s]Extractor Estimating: 347it [03:39,  1.54it/s]Extractor Estimating: 348it [03:39,  1.50it/s]Extractor Estimating: 349it [03:40,  1.50it/s]Extractor Estimating: 350it [03:41,  1.51it/s]Extractor Estimating: 351it [03:41,  1.55it/s]Extractor Estimating: 352it [03:42,  1.53it/s]Extractor Estimating: 353it [03:43,  1.49it/s]Extractor Estimating: 354it [03:43,  1.52it/s]Extractor Estimating: 355it [03:44,  1.55it/s]Extractor Estimating: 356it [03:45,  1.56it/s]Extractor Estimating: 357it [03:45,  1.59it/s]Extractor Estimating: 358it [03:46,  1.61it/s]Extractor Estimating: 359it [03:46,  1.61it/s]Extractor Estimating: 360it [03:47,  1.56it/s]Extractor Estimating: 361it [03:48,  1.58it/s]Extractor Estimating: 362it [03:48,  1.58it/s]Extractor Estimating: 363it [03:49,  1.61it/s]Extractor Estimating: 364it [03:50,  1.58it/s]Extractor Estimating: 365it [03:50,  1.59it/s]Extractor Estimating: 366it [03:51,  1.50it/s]Extractor Estimating: 367it [03:52,  1.53it/s]Extractor Estimating: 368it [03:52,  1.57it/s]Extractor Estimating: 369it [03:53,  1.59it/s]Extractor Estimating: 370it [03:53,  1.62it/s]Extractor Estimating: 371it [03:54,  1.53it/s]Extractor Estimating: 372it [03:55,  1.50it/s]Extractor Estimating: 373it [03:56,  1.44it/s]Extractor Estimating: 374it [03:56,  1.50it/s]Extractor Estimating: 374it [03:56,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:02,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:02,120 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:02,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:02,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:02,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:59:02,725 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:59:02,726 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:59:03,292 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:59:04,353 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:59:04,353 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:07,189 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:07,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:07,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:07,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:07,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:59:07,845 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:59:07,846 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:59:08,426 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:59:08,586 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:59:08,587 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 08:28:59,803 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 08:28:59,832 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7511 mean pseudo reward: 0.9085696943256726
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl'}
train vocab size: 24063
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24163, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24163, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.124, loss:1027.2540
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.101, loss:974.9925
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.085, loss:1006.9151
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.093, loss:920.9416
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.078, loss:995.1819
>> valid entity prec:0.5126, rec:0.5492, f1:0.5303
>> valid relation prec:0.4687, rec:0.0956, f1:0.1587
>> valid relation with NER prec:0.4687, rec:0.0956, f1:0.1587
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.876, loss:935.3307
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.072, loss:919.7084
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.100, loss:886.9490
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.098, loss:934.7032
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.096, loss:894.2138
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5653, rec:0.5454, f1:0.5551
>> valid relation prec:0.4397, rec:0.0908, f1:0.1505
>> valid relation with NER prec:0.4397, rec:0.0908, f1:0.1505
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.871, loss:863.3399
g_step 1200, step 261, avg_time 1.083, loss:862.9844
g_step 1300, step 48, avg_time 1.101, loss:850.5994
g_step 1400, step 148, avg_time 1.103, loss:796.3091
g_step 1500, step 248, avg_time 1.087, loss:830.4300
>> valid entity prec:0.4385, rec:0.5880, f1:0.5024
>> valid relation prec:0.4359, rec:0.0560, f1:0.0993
>> valid relation with NER prec:0.4359, rec:0.0560, f1:0.0993
g_step 1600, step 35, avg_time 2.871, loss:769.3502
g_step 1700, step 135, avg_time 1.096, loss:773.7537
g_step 1800, step 235, avg_time 1.097, loss:760.4075
g_step 1900, step 22, avg_time 1.090, loss:790.6626
g_step 2000, step 122, avg_time 1.091, loss:718.0122
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5241, rec:0.5426, f1:0.5332
>> valid relation prec:0.4103, rec:0.0904, f1:0.1482
>> valid relation with NER prec:0.4103, rec:0.0904, f1:0.1482
g_step 2100, step 222, avg_time 2.883, loss:730.4502
g_step 2200, step 9, avg_time 1.090, loss:731.9351
g_step 2300, step 109, avg_time 1.084, loss:682.8252
g_step 2400, step 209, avg_time 1.084, loss:720.3101
g_step 2500, step 309, avg_time 1.112, loss:710.4686
>> valid entity prec:0.5230, rec:0.5381, f1:0.5304
>> valid relation prec:0.4337, rec:0.1085, f1:0.1736
>> valid relation with NER prec:0.4337, rec:0.1085, f1:0.1736
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.858, loss:661.1612
g_step 2700, step 196, avg_time 1.099, loss:656.2879
g_step 2800, step 296, avg_time 1.103, loss:683.3330
g_step 2900, step 83, avg_time 1.086, loss:650.2923
g_step 3000, step 183, avg_time 1.093, loss:666.0754
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5025, rec:0.5234, f1:0.5127
>> valid relation prec:0.3974, rec:0.1013, f1:0.1615
>> valid relation with NER prec:0.3974, rec:0.1013, f1:0.1615
g_step 3100, step 283, avg_time 2.872, loss:629.6482
g_step 3200, step 70, avg_time 1.097, loss:603.6871
g_step 3300, step 170, avg_time 1.080, loss:605.5848
g_step 3400, step 270, avg_time 1.122, loss:633.5497
g_step 3500, step 57, avg_time 1.077, loss:600.9480
>> valid entity prec:0.5492, rec:0.4900, f1:0.5179
>> valid relation prec:0.4449, rec:0.0881, f1:0.1471
>> valid relation with NER prec:0.4449, rec:0.0881, f1:0.1471
g_step 3600, step 157, avg_time 2.875, loss:576.6190
g_step 3700, step 257, avg_time 1.094, loss:608.2772
g_step 3800, step 44, avg_time 1.095, loss:586.3199
g_step 3900, step 144, avg_time 1.071, loss:576.0014
g_step 4000, step 244, avg_time 1.108, loss:570.2226
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5837, rec:0.4489, f1:0.5075
>> valid relation prec:0.3871, rec:0.1003, f1:0.1593
>> valid relation with NER prec:0.3871, rec:0.1003, f1:0.1593
g_step 4100, step 31, avg_time 2.854, loss:581.9653
g_step 4200, step 131, avg_time 1.104, loss:534.4020
g_step 4300, step 231, avg_time 1.097, loss:547.6183
g_step 4400, step 18, avg_time 1.081, loss:571.2431
g_step 4500, step 118, avg_time 1.086, loss:513.4113
>> valid entity prec:0.5274, rec:0.4587, f1:0.4907
>> valid relation prec:0.4105, rec:0.0803, f1:0.1343
>> valid relation with NER prec:0.4105, rec:0.0803, f1:0.1343
g_step 4600, step 218, avg_time 2.878, loss:547.3416
g_step 4700, step 5, avg_time 1.083, loss:533.5714
g_step 4800, step 105, avg_time 1.096, loss:504.2043
g_step 4900, step 205, avg_time 1.078, loss:539.3435
g_step 5000, step 305, avg_time 1.119, loss:514.9552
learning rate was adjusted to 0.0008
>> valid entity prec:0.5456, rec:0.4712, f1:0.5056
>> valid relation prec:0.4053, rec:0.0978, f1:0.1576
>> valid relation with NER prec:0.4053, rec:0.0978, f1:0.1576
g_step 5100, step 92, avg_time 2.860, loss:481.6046
g_step 5200, step 192, avg_time 1.102, loss:484.0227
g_step 5300, step 292, avg_time 1.104, loss:511.4465
g_step 5400, step 79, avg_time 1.100, loss:477.3298
g_step 5500, step 179, avg_time 1.096, loss:480.0280
>> valid entity prec:0.5712, rec:0.4777, f1:0.5203
>> valid relation prec:0.3764, rec:0.0978, f1:0.1553
>> valid relation with NER prec:0.3764, rec:0.0978, f1:0.1553
g_step 5600, step 279, avg_time 2.879, loss:489.0966
g_step 5700, step 66, avg_time 1.112, loss:446.9888
g_step 5800, step 166, avg_time 1.111, loss:459.6749
g_step 5900, step 266, avg_time 1.099, loss:459.9559
g_step 6000, step 53, avg_time 1.108, loss:453.8404
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5369, rec:0.5192, f1:0.5279
>> valid relation prec:0.3255, rec:0.0797, f1:0.1280
>> valid relation with NER prec:0.3255, rec:0.0797, f1:0.1280
g_step 6100, step 153, avg_time 2.871, loss:437.7137
g_step 6200, step 253, avg_time 1.096, loss:469.2246
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:28:59 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:28:59 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-28-59_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:29:00 - WARNING - datasets.builder -   Using custom data configuration default-44afc526a97a4a3c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-44afc526a97a4a3c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:29:01,041 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:29:01,042 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:29:01,042 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:29:01,043 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:29:01,052 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:29:01,056 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:29:01,057 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:29:01,057 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:29:01,057 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:29:01,057 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:29:01,057 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:29:01,197 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:29:04,282 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:29:04,285 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-44afc526a97a4a3c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.25ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.02ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.32ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.48ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.56ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.61ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.65ba/s]100%|██████████| 8/8 [00:01<00:00,  5.42ba/s]100%|██████████| 8/8 [00:01<00:00,  4.73ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.67ba/s] 40%|████      | 2/5 [00:00<00:00,  3.49ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.87ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.08ba/s]100%|██████████| 5/5 [00:01<00:00,  4.42ba/s]100%|██████████| 5/5 [00:01<00:00,  4.02ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.60ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.35ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.59ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.58ba/s]100%|██████████| 8/8 [00:00<00:00, 11.06ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.78ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.52ba/s]100%|██████████| 5/5 [00:00<00:00, 11.00ba/s]100%|██████████| 5/5 [00:00<00:00, 10.75ba/s]
[INFO|trainer.py:414] 2023-08-29 08:29:08,817 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:29:08,835 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:29:08,835 >>   Num examples = 7544
[INFO|trainer.py:1149] 2023-08-29 08:29:08,835 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:29:08,835 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:29:08,835 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:29:08,835 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:29:08,835 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:58,  3.30it/s]  0%|          | 2/590 [00:00<02:52,  3.41it/s]  1%|          | 3/590 [00:00<02:50,  3.44it/s]  1%|          | 4/590 [00:01<02:49,  3.45it/s]  1%|          | 5/590 [00:01<02:48,  3.46it/s]  1%|          | 6/590 [00:01<02:49,  3.45it/s]  1%|          | 7/590 [00:02<02:48,  3.46it/s]  1%|▏         | 8/590 [00:02<02:47,  3.47it/s]  2%|▏         | 9/590 [00:02<02:47,  3.47it/s]  2%|▏         | 10/590 [00:02<02:46,  3.47it/s]  2%|▏         | 11/590 [00:03<02:46,  3.47it/s]  2%|▏         | 12/590 [00:03<02:46,  3.48it/s]  2%|▏         | 13/590 [00:03<02:45,  3.48it/s]  2%|▏         | 14/590 [00:04<02:45,  3.48it/s]  3%|▎         | 15/590 [00:04<02:45,  3.47it/s]  3%|▎         | 16/590 [00:04<02:45,  3.48it/s]  3%|▎         | 17/590 [00:04<02:45,  3.47it/s]  3%|▎         | 18/590 [00:05<02:44,  3.47it/s]  3%|▎         | 19/590 [00:05<02:44,  3.47it/s]  3%|▎         | 20/590 [00:05<02:44,  3.47it/s]  4%|▎         | 21/590 [00:06<02:43,  3.48it/s]  4%|▎         | 22/590 [00:06<02:43,  3.48it/s]  4%|▍         | 23/590 [00:06<02:43,  3.48it/s]  4%|▍         | 24/590 [00:06<02:42,  3.48it/s]  4%|▍         | 25/590 [00:07<02:42,  3.48it/s]  4%|▍         | 26/590 [00:07<02:42,  3.48it/s]  5%|▍         | 27/590 [00:07<02:41,  3.48it/s]  5%|▍         | 28/590 [00:08<02:42,  3.46it/s]  5%|▍         | 29/590 [00:08<02:41,  3.46it/s]  5%|▌         | 30/590 [00:08<02:41,  3.47it/s]  5%|▌         | 31/590 [00:08<02:40,  3.47it/s]  5%|▌         | 32/590 [00:09<02:40,  3.47it/s]  6%|▌         | 33/590 [00:09<02:40,  3.47it/s]  6%|▌         | 34/590 [00:09<02:40,  3.47it/s]  6%|▌         | 35/590 [00:10<02:39,  3.47it/s]  6%|▌         | 36/590 [00:10<02:39,  3.47it/s]  6%|▋         | 37/590 [00:10<02:39,  3.47it/s]  6%|▋         | 38/590 [00:10<02:38,  3.47it/s]  7%|▋         | 39/590 [00:11<02:38,  3.47it/s]  7%|▋         | 40/590 [00:11<02:38,  3.47it/s]  7%|▋         | 41/590 [00:11<02:38,  3.47it/s]  7%|▋         | 42/590 [00:12<02:37,  3.47it/s]  7%|▋         | 43/590 [00:12<02:37,  3.48it/s]  7%|▋         | 44/590 [00:12<02:37,  3.47it/s]  8%|▊         | 45/590 [00:12<02:37,  3.47it/s]  8%|▊         | 46/590 [00:13<02:36,  3.47it/s]  8%|▊         | 47/590 [00:13<02:36,  3.47it/s]  8%|▊         | 48/590 [00:13<02:36,  3.47it/s]  8%|▊         | 49/590 [00:14<02:35,  3.47it/s]  8%|▊         | 50/590 [00:14<02:36,  3.46it/s]  9%|▊         | 51/590 [00:14<02:35,  3.46it/s]  9%|▉         | 52/590 [00:14<02:35,  3.46it/s]  9%|▉         | 53/590 [00:15<02:34,  3.47it/s]  9%|▉         | 54/590 [00:15<02:34,  3.47it/s]  9%|▉         | 55/590 [00:15<02:34,  3.47it/s]  9%|▉         | 56/590 [00:16<02:33,  3.47it/s] 10%|▉         | 57/590 [00:16<02:33,  3.47it/s] 10%|▉         | 58/590 [00:16<02:33,  3.47it/s] 10%|█         | 59/590 [00:17<02:32,  3.47it/s] 10%|█         | 60/590 [00:17<02:32,  3.47it/s] 10%|█         | 61/590 [00:17<02:32,  3.46it/s] 11%|█         | 62/590 [00:17<02:32,  3.46it/s] 11%|█         | 63/590 [00:18<02:32,  3.46it/s] 11%|█         | 64/590 [00:18<02:31,  3.46it/s] 11%|█         | 65/590 [00:18<02:31,  3.47it/s] 11%|█         | 66/590 [00:19<02:31,  3.47it/s] 11%|█▏        | 67/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 68/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 69/590 [00:19<02:30,  3.46it/s] 12%|█▏        | 70/590 [00:20<02:30,  3.46it/s] 12%|█▏        | 71/590 [00:20<02:29,  3.46it/s] 12%|█▏        | 72/590 [00:20<02:30,  3.45it/s] 12%|█▏        | 73/590 [00:21<02:29,  3.45it/s] 13%|█▎        | 74/590 [00:21<02:29,  3.46it/s] 13%|█▎        | 75/590 [00:21<02:28,  3.46it/s] 13%|█▎        | 76/590 [00:21<02:28,  3.46it/s] 13%|█▎        | 77/590 [00:22<02:28,  3.46it/s] 13%|█▎        | 78/590 [00:22<02:27,  3.46it/s] 13%|█▎        | 79/590 [00:22<02:27,  3.46it/s] 14%|█▎        | 80/590 [00:23<02:27,  3.46it/s] 14%|█▎        | 81/590 [00:23<02:26,  3.46it/s] 14%|█▍        | 82/590 [00:23<02:26,  3.46it/s] 14%|█▍        | 83/590 [00:23<02:26,  3.46it/s] 14%|█▍        | 84/590 [00:24<02:26,  3.46it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.46it/s] 15%|█▍        | 86/590 [00:24<02:25,  3.46it/s] 15%|█▍        | 87/590 [00:25<02:25,  3.45it/s] 15%|█▍        | 88/590 [00:25<02:25,  3.46it/s] 15%|█▌        | 89/590 [00:25<02:24,  3.46it/s] 15%|█▌        | 90/590 [00:25<02:24,  3.46it/s] 15%|█▌        | 91/590 [00:26<02:24,  3.46it/s] 16%|█▌        | 92/590 [00:26<02:23,  3.46it/s] 16%|█▌        | 93/590 [00:26<02:23,  3.46it/s] 16%|█▌        | 94/590 [00:27<02:23,  3.46it/s] 16%|█▌        | 95/590 [00:27<02:23,  3.46it/s] 16%|█▋        | 96/590 [00:27<02:22,  3.46it/s] 16%|█▋        | 97/590 [00:27<02:22,  3.46it/s] 17%|█▋        | 98/590 [00:28<02:22,  3.45it/s] 17%|█▋        | 99/590 [00:28<02:22,  3.46it/s] 17%|█▋        | 100/590 [00:28<02:21,  3.46it/s] 17%|█▋        | 101/590 [00:29<02:21,  3.46it/s] 17%|█▋        | 102/590 [00:29<02:21,  3.46it/s] 17%|█▋        | 103/590 [00:29<02:20,  3.46it/s] 18%|█▊        | 104/590 [00:30<02:20,  3.46it/s] 18%|█▊        | 105/590 [00:30<02:20,  3.46it/s] 18%|█▊        | 106/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 107/590 [00:30<02:19,  3.46it/s] 18%|█▊        | 108/590 [00:31<02:19,  3.46it/s] 18%|█▊        | 109/590 [00:31<02:19,  3.46it/s] 19%|█▊        | 110/590 [00:31<02:18,  3.46it/s] 19%|█▉        | 111/590 [00:32<02:18,  3.46it/s] 19%|█▉        | 112/590 [00:32<02:18,  3.46it/s] 19%|█▉        | 113/590 [00:32<02:17,  3.46it/s] 19%|█▉        | 114/590 [00:32<02:17,  3.46it/s] 19%|█▉        | 115/590 [00:33<02:17,  3.46it/s] 20%|█▉        | 116/590 [00:33<02:16,  3.46it/s] 20%|█▉        | 117/590 [00:33<02:16,  3.46it/s] 20%|██        | 118/590 [00:34<02:11,  3.58it/s][INFO|trainer.py:2140] 2023-08-29 08:29:42,864 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:29:42,864 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 08:29:42,864 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.51it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.19it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.56it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.85it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.60it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.11it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.87it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.63it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.52it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.57it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.65it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.63it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.59it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.62it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.56it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.56it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.42it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.40it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.41it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.46it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.55it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.56it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.54it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.51it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.44it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.34it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.34it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.37it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.39it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.44it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.54it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.49it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.52it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.42it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.51it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.42it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.48it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.37it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.38it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.44it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.53it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.54it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.52it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.51it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.34it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.43it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.45it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.36it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.47it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.49it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.45it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.58it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.45it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.43it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.39it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.29it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.41it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.43it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.39it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.49it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.49it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.47it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.30it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.32it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.38it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.40it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.40it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.42it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.43it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.49it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.45it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.44it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.41it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.35it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.35it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.44it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.41it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.42it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.46it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.44it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.48it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.43it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.28it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.32it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.42it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.40it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.43it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.40it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.42it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.42it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.44it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.35it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.40it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.39it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.42it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.35it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.33it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.39it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.51it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.34it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.34it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.31it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.38it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.42it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.47it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.43it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.49it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.47it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.25it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.31it/s][A
 92%|█████████▏| 558/608 [00:11<00:01, 46.47it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.37it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.42it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.48it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.35it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.38it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.44it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.34it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.32it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.37it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.40it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.40it/s][A 20%|██        | 118/590 [00:47<02:11,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:29:55,964 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-29 08:29:55,983 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:29:58,296 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:29:58,314 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:29:58,328 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:54<50:10,  6.39s/it] 20%|██        | 120/590 [00:54<35:43,  4.56s/it] 21%|██        | 121/590 [00:55<25:37,  3.28s/it] 21%|██        | 122/590 [00:55<18:34,  2.38s/it] 21%|██        | 123/590 [00:55<13:39,  1.75s/it] 21%|██        | 124/590 [00:56<10:12,  1.32s/it] 21%|██        | 125/590 [00:56<07:48,  1.01s/it] 21%|██▏       | 126/590 [00:56<06:07,  1.26it/s] 22%|██▏       | 127/590 [00:56<04:56,  1.56it/s] 22%|██▏       | 128/590 [00:57<04:07,  1.87it/s] 22%|██▏       | 129/590 [00:57<03:32,  2.17it/s] 22%|██▏       | 130/590 [00:57<03:08,  2.44it/s] 22%|██▏       | 131/590 [00:58<02:51,  2.68it/s] 22%|██▏       | 132/590 [00:58<02:39,  2.87it/s] 23%|██▎       | 133/590 [00:58<02:31,  3.03it/s] 23%|██▎       | 134/590 [00:59<02:25,  3.14it/s] 23%|██▎       | 135/590 [00:59<02:20,  3.23it/s] 23%|██▎       | 136/590 [00:59<02:17,  3.29it/s] 23%|██▎       | 137/590 [00:59<02:15,  3.34it/s] 23%|██▎       | 138/590 [01:00<02:13,  3.38it/s] 24%|██▎       | 139/590 [01:00<02:12,  3.40it/s] 24%|██▎       | 140/590 [01:00<02:11,  3.42it/s] 24%|██▍       | 141/590 [01:01<02:10,  3.43it/s] 24%|██▍       | 142/590 [01:01<02:10,  3.44it/s] 24%|██▍       | 143/590 [01:01<02:09,  3.45it/s] 24%|██▍       | 144/590 [01:01<02:09,  3.45it/s] 25%|██▍       | 145/590 [01:02<02:09,  3.45it/s] 25%|██▍       | 146/590 [01:02<02:08,  3.45it/s] 25%|██▍       | 147/590 [01:02<02:08,  3.46it/s] 25%|██▌       | 148/590 [01:03<02:07,  3.46it/s] 25%|██▌       | 149/590 [01:03<02:07,  3.46it/s] 25%|██▌       | 150/590 [01:03<02:07,  3.46it/s] 26%|██▌       | 151/590 [01:03<02:06,  3.46it/s] 26%|██▌       | 152/590 [01:04<02:06,  3.46it/s] 26%|██▌       | 153/590 [01:04<02:06,  3.46it/s] 26%|██▌       | 154/590 [01:04<02:05,  3.46it/s] 26%|██▋       | 155/590 [01:05<02:05,  3.46it/s] 26%|██▋       | 156/590 [01:05<02:05,  3.46it/s] 27%|██▋       | 157/590 [01:05<02:05,  3.45it/s] 27%|██▋       | 158/590 [01:05<02:04,  3.46it/s] 27%|██▋       | 159/590 [01:06<02:04,  3.46it/s] 27%|██▋       | 160/590 [01:06<02:04,  3.46it/s] 27%|██▋       | 161/590 [01:06<02:04,  3.46it/s] 27%|██▋       | 162/590 [01:07<02:03,  3.46it/s] 28%|██▊       | 163/590 [01:07<02:03,  3.46it/s] 28%|██▊       | 164/590 [01:07<02:03,  3.46it/s] 28%|██▊       | 165/590 [01:07<02:02,  3.46it/s] 28%|██▊       | 166/590 [01:08<02:02,  3.46it/s] 28%|██▊       | 167/590 [01:08<02:03,  3.44it/s] 28%|██▊       | 168/590 [01:08<02:02,  3.44it/s] 29%|██▊       | 169/590 [01:09<02:02,  3.45it/s] 29%|██▉       | 170/590 [01:09<02:01,  3.45it/s] 29%|██▉       | 171/590 [01:09<02:01,  3.45it/s] 29%|██▉       | 172/590 [01:10<02:01,  3.45it/s] 29%|██▉       | 173/590 [01:10<02:00,  3.45it/s] 29%|██▉       | 174/590 [01:10<02:00,  3.46it/s] 30%|██▉       | 175/590 [01:10<02:00,  3.46it/s] 30%|██▉       | 176/590 [01:11<01:59,  3.45it/s] 30%|███       | 177/590 [01:11<01:59,  3.46it/s] 30%|███       | 178/590 [01:11<01:59,  3.44it/s] 30%|███       | 179/590 [01:12<01:59,  3.45it/s] 31%|███       | 180/590 [01:12<01:58,  3.45it/s] 31%|███       | 181/590 [01:12<01:58,  3.45it/s] 31%|███       | 182/590 [01:12<01:58,  3.45it/s] 31%|███       | 183/590 [01:13<01:57,  3.46it/s] 31%|███       | 184/590 [01:13<01:57,  3.46it/s] 31%|███▏      | 185/590 [01:13<01:57,  3.46it/s] 32%|███▏      | 186/590 [01:14<01:56,  3.46it/s] 32%|███▏      | 187/590 [01:14<01:56,  3.46it/s] 32%|███▏      | 188/590 [01:14<01:56,  3.46it/s] 32%|███▏      | 189/590 [01:14<01:56,  3.45it/s] 32%|███▏      | 190/590 [01:15<01:55,  3.45it/s] 32%|███▏      | 191/590 [01:15<01:55,  3.46it/s] 33%|███▎      | 192/590 [01:15<01:55,  3.46it/s] 33%|███▎      | 193/590 [01:16<01:54,  3.46it/s] 33%|███▎      | 194/590 [01:16<01:54,  3.46it/s] 33%|███▎      | 195/590 [01:16<01:54,  3.45it/s] 33%|███▎      | 196/590 [01:16<01:54,  3.46it/s] 33%|███▎      | 197/590 [01:17<01:53,  3.45it/s] 34%|███▎      | 198/590 [01:17<01:53,  3.45it/s] 34%|███▎      | 199/590 [01:17<01:53,  3.45it/s] 34%|███▍      | 200/590 [01:18<01:53,  3.45it/s] 34%|███▍      | 201/590 [01:18<01:52,  3.45it/s] 34%|███▍      | 202/590 [01:18<01:52,  3.45it/s] 34%|███▍      | 203/590 [01:18<01:52,  3.45it/s] 35%|███▍      | 204/590 [01:19<01:51,  3.46it/s] 35%|███▍      | 205/590 [01:19<01:51,  3.46it/s] 35%|███▍      | 206/590 [01:19<01:51,  3.46it/s] 35%|███▌      | 207/590 [01:20<01:50,  3.45it/s] 35%|███▌      | 208/590 [01:20<01:50,  3.46it/s] 35%|███▌      | 209/590 [01:20<01:50,  3.46it/s] 36%|███▌      | 210/590 [01:21<01:49,  3.46it/s] 36%|███▌      | 211/590 [01:21<01:50,  3.44it/s] 36%|███▌      | 212/590 [01:21<01:49,  3.45it/s] 36%|███▌      | 213/590 [01:21<01:49,  3.45it/s] 36%|███▋      | 214/590 [01:22<01:48,  3.45it/s] 36%|███▋      | 215/590 [01:22<01:48,  3.45it/s] 37%|███▋      | 216/590 [01:22<01:48,  3.45it/s] 37%|███▋      | 217/590 [01:23<01:47,  3.45it/s] 37%|███▋      | 218/590 [01:23<01:47,  3.46it/s] 37%|███▋      | 219/590 [01:23<01:47,  3.45it/s] 37%|███▋      | 220/590 [01:23<01:47,  3.46it/s] 37%|███▋      | 221/590 [01:24<01:46,  3.45it/s] 38%|███▊      | 222/590 [01:24<01:46,  3.45it/s] 38%|███▊      | 223/590 [01:24<01:46,  3.45it/s] 38%|███▊      | 224/590 [01:25<01:45,  3.45it/s] 38%|███▊      | 225/590 [01:25<01:45,  3.46it/s] 38%|███▊      | 226/590 [01:25<01:45,  3.45it/s] 38%|███▊      | 227/590 [01:25<01:45,  3.45it/s] 39%|███▊      | 228/590 [01:26<01:44,  3.45it/s] 39%|███▉      | 229/590 [01:26<01:45,  3.44it/s] 39%|███▉      | 230/590 [01:26<01:44,  3.44it/s] 39%|███▉      | 231/590 [01:27<01:44,  3.45it/s] 39%|███▉      | 232/590 [01:27<01:43,  3.45it/s] 39%|███▉      | 233/590 [01:27<01:43,  3.45it/s] 40%|███▉      | 234/590 [01:27<01:43,  3.45it/s] 40%|███▉      | 235/590 [01:28<01:42,  3.45it/s] 40%|████      | 236/590 [01:28<01:38,  3.58it/s][INFO|trainer.py:2140] 2023-08-29 08:30:37,350 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:30:37,350 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 08:30:37,350 >>   Batch size = 8
{'eval_loss': 0.9655474424362183, 'eval_runtime': 13.089, 'eval_samples_per_second': 371.304, 'eval_steps_per_second': 46.451, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.42it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.28it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.55it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.85it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.43it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.99it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.77it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.18it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.22it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.33it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.42it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.50it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.54it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.56it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.54it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.44it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.27it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.26it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.32it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.38it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.45it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.50it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.53it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.50it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.44it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.26it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.26it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.23it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.31it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.43it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.46it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.49it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.54it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.45it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.42it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.32it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.32it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.23it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.29it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.42it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.46it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.47it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.42it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.32it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.31it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.27it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.26it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.25it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.27it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.44it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.49it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.44it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.35it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.34it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.36it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.32it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.19it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.28it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.35it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.47it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.48it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.41it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.42it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.38it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.32it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.23it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.32it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.34it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.36it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.42it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.43it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.39it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.41it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.31it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.29it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.30it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.27it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.34it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.18it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.41it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.48it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.44it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.38it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.33it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.32it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.30it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.26it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.36it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.41it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.42it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.43it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.34it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.30it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.32it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.35it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.32it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.27it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.30it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.37it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.41it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.47it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.36it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.28it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.30it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.28it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.27it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.37it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.29it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.34it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.40it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.44it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.30it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.33it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.28it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.30it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.37it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.36it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.31it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.31it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.40it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.37it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.37it/s][A 40%|████      | 236/590 [01:41<01:38,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:30:50,478 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-29 08:30:50,510 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:30:52,866 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:30:52,881 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:30:52,891 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:49<37:29,  6.37s/it] 40%|████      | 238/590 [01:49<26:41,  4.55s/it] 41%|████      | 239/590 [01:49<19:08,  3.27s/it] 41%|████      | 240/590 [01:49<13:52,  2.38s/it] 41%|████      | 241/590 [01:50<10:11,  1.75s/it] 41%|████      | 242/590 [01:50<07:36,  1.31s/it] 41%|████      | 243/590 [01:50<05:48,  1.01s/it] 41%|████▏     | 244/590 [01:51<04:33,  1.27it/s] 42%|████▏     | 245/590 [01:51<03:40,  1.56it/s] 42%|████▏     | 246/590 [01:51<03:03,  1.87it/s] 42%|████▏     | 247/590 [01:51<02:38,  2.17it/s] 42%|████▏     | 248/590 [01:52<02:19,  2.44it/s] 42%|████▏     | 249/590 [01:52<02:07,  2.67it/s] 42%|████▏     | 250/590 [01:52<01:58,  2.87it/s] 43%|████▎     | 251/590 [01:53<01:52,  3.02it/s] 43%|████▎     | 252/590 [01:53<01:47,  3.14it/s] 43%|████▎     | 253/590 [01:53<01:44,  3.23it/s] 43%|████▎     | 254/590 [01:54<01:42,  3.27it/s] 43%|████▎     | 255/590 [01:54<01:41,  3.29it/s] 43%|████▎     | 256/590 [01:54<01:40,  3.31it/s] 44%|████▎     | 257/590 [01:54<01:39,  3.36it/s] 44%|████▎     | 258/590 [01:55<01:38,  3.39it/s] 44%|████▍     | 259/590 [01:55<01:37,  3.41it/s] 44%|████▍     | 260/590 [01:55<01:36,  3.42it/s] 44%|████▍     | 261/590 [01:56<01:35,  3.43it/s] 44%|████▍     | 262/590 [01:56<01:35,  3.44it/s] 45%|████▍     | 263/590 [01:56<01:34,  3.45it/s] 45%|████▍     | 264/590 [01:56<01:34,  3.45it/s] 45%|████▍     | 265/590 [01:57<01:34,  3.45it/s] 45%|████▌     | 266/590 [01:57<01:33,  3.45it/s] 45%|████▌     | 267/590 [01:57<01:33,  3.45it/s] 45%|████▌     | 268/590 [01:58<01:33,  3.46it/s] 46%|████▌     | 269/590 [01:58<01:32,  3.46it/s] 46%|████▌     | 270/590 [01:58<01:32,  3.46it/s] 46%|████▌     | 271/590 [01:58<01:32,  3.46it/s] 46%|████▌     | 272/590 [01:59<01:31,  3.46it/s] 46%|████▋     | 273/590 [01:59<01:31,  3.46it/s] 46%|████▋     | 274/590 [01:59<01:31,  3.46it/s] 47%|████▋     | 275/590 [02:00<01:31,  3.46it/s] 47%|████▋     | 276/590 [02:00<01:31,  3.45it/s] 47%|████▋     | 277/590 [02:00<01:30,  3.45it/s] 47%|████▋     | 278/590 [02:00<01:30,  3.45it/s] 47%|████▋     | 279/590 [02:01<01:30,  3.45it/s] 47%|████▋     | 280/590 [02:01<01:29,  3.45it/s] 48%|████▊     | 281/590 [02:01<01:29,  3.45it/s] 48%|████▊     | 282/590 [02:02<01:29,  3.45it/s] 48%|████▊     | 283/590 [02:02<01:28,  3.46it/s] 48%|████▊     | 284/590 [02:02<01:28,  3.45it/s] 48%|████▊     | 285/590 [02:03<01:28,  3.46it/s] 48%|████▊     | 286/590 [02:03<01:28,  3.45it/s] 49%|████▊     | 287/590 [02:03<01:27,  3.45it/s] 49%|████▉     | 288/590 [02:03<01:27,  3.45it/s] 49%|████▉     | 289/590 [02:04<01:27,  3.45it/s] 49%|████▉     | 290/590 [02:04<01:26,  3.45it/s] 49%|████▉     | 291/590 [02:04<01:26,  3.45it/s] 49%|████▉     | 292/590 [02:05<01:26,  3.45it/s] 50%|████▉     | 293/590 [02:05<01:25,  3.46it/s] 50%|████▉     | 294/590 [02:05<01:25,  3.46it/s] 50%|█████     | 295/590 [02:05<01:25,  3.46it/s] 50%|█████     | 296/590 [02:06<01:25,  3.46it/s] 50%|█████     | 297/590 [02:06<01:24,  3.46it/s] 51%|█████     | 298/590 [02:06<01:25,  3.43it/s] 51%|█████     | 299/590 [02:07<01:24,  3.44it/s] 51%|█████     | 300/590 [02:07<01:24,  3.44it/s] 51%|█████     | 301/590 [02:07<01:23,  3.45it/s] 51%|█████     | 302/590 [02:07<01:23,  3.45it/s] 51%|█████▏    | 303/590 [02:08<01:23,  3.45it/s] 52%|█████▏    | 304/590 [02:08<01:22,  3.45it/s] 52%|█████▏    | 305/590 [02:08<01:22,  3.45it/s] 52%|█████▏    | 306/590 [02:09<01:22,  3.45it/s] 52%|█████▏    | 307/590 [02:09<01:21,  3.46it/s] 52%|█████▏    | 308/590 [02:09<01:21,  3.46it/s] 52%|█████▏    | 309/590 [02:09<01:21,  3.45it/s] 53%|█████▎    | 310/590 [02:10<01:21,  3.45it/s] 53%|█████▎    | 311/590 [02:10<01:20,  3.46it/s] 53%|█████▎    | 312/590 [02:10<01:20,  3.46it/s] 53%|█████▎    | 313/590 [02:11<01:20,  3.46it/s] 53%|█████▎    | 314/590 [02:11<01:19,  3.46it/s] 53%|█████▎    | 315/590 [02:11<01:19,  3.46it/s] 54%|█████▎    | 316/590 [02:11<01:19,  3.46it/s] 54%|█████▎    | 317/590 [02:12<01:18,  3.46it/s] 54%|█████▍    | 318/590 [02:12<01:18,  3.46it/s] 54%|█████▍    | 319/590 [02:12<01:18,  3.46it/s] 54%|█████▍    | 320/590 [02:13<01:18,  3.45it/s] 54%|█████▍    | 321/590 [02:13<01:17,  3.45it/s] 55%|█████▍    | 322/590 [02:13<01:17,  3.45it/s] 55%|█████▍    | 323/590 [02:14<01:17,  3.45it/s] 55%|█████▍    | 324/590 [02:14<01:17,  3.45it/s] 55%|█████▌    | 325/590 [02:14<01:16,  3.45it/s] 55%|█████▌    | 326/590 [02:14<01:16,  3.46it/s] 55%|█████▌    | 327/590 [02:15<01:16,  3.45it/s] 56%|█████▌    | 328/590 [02:15<01:15,  3.46it/s] 56%|█████▌    | 329/590 [02:15<01:15,  3.46it/s] 56%|█████▌    | 330/590 [02:16<01:15,  3.46it/s] 56%|█████▌    | 331/590 [02:16<01:15,  3.45it/s] 56%|█████▋    | 332/590 [02:16<01:14,  3.45it/s] 56%|█████▋    | 333/590 [02:16<01:14,  3.45it/s] 57%|█████▋    | 334/590 [02:17<01:14,  3.46it/s] 57%|█████▋    | 335/590 [02:17<01:13,  3.46it/s] 57%|█████▋    | 336/590 [02:17<01:13,  3.46it/s] 57%|█████▋    | 337/590 [02:18<01:13,  3.45it/s] 57%|█████▋    | 338/590 [02:18<01:12,  3.46it/s] 57%|█████▋    | 339/590 [02:18<01:12,  3.45it/s] 58%|█████▊    | 340/590 [02:18<01:12,  3.45it/s] 58%|█████▊    | 341/590 [02:19<01:12,  3.45it/s] 58%|█████▊    | 342/590 [02:19<01:11,  3.45it/s] 58%|█████▊    | 343/590 [02:19<01:11,  3.45it/s] 58%|█████▊    | 344/590 [02:20<01:11,  3.45it/s] 58%|█████▊    | 345/590 [02:20<01:10,  3.45it/s] 59%|█████▊    | 346/590 [02:20<01:10,  3.45it/s] 59%|█████▉    | 347/590 [02:20<01:10,  3.45it/s] 59%|█████▉    | 348/590 [02:21<01:10,  3.45it/s] 59%|█████▉    | 349/590 [02:21<01:09,  3.45it/s] 59%|█████▉    | 350/590 [02:21<01:09,  3.45it/s] 59%|█████▉    | 351/590 [02:22<01:09,  3.45it/s] 60%|█████▉    | 352/590 [02:22<01:08,  3.45it/s] 60%|█████▉    | 353/590 [02:22<01:08,  3.45it/s] 60%|██████    | 354/590 [02:22<01:06,  3.57it/s][INFO|trainer.py:2140] 2023-08-29 08:31:31,812 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:31:31,812 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 08:31:31,812 >>   Batch size = 8
{'eval_loss': 0.9917697310447693, 'eval_runtime': 13.1083, 'eval_samples_per_second': 370.756, 'eval_steps_per_second': 46.383, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.54it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.23it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.51it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.90it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.45it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.94it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.67it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.36it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.41it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.54it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.52it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.62it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.62it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.48it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.42it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.21it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.17it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.15it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.21it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.36it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.47it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.48it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.53it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.41it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.29it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.18it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.09it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.10it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.32it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.45it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.38it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.45it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.43it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.27it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.20it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.24it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.31it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.27it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.35it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.42it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.48it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.37it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.33it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.28it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.23it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.23it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.36it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.38it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.36it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.41it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.88it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.42it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.39it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.28it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.24it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.35it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.37it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.38it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.43it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.37it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.37it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.39it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.24it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.29it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.29it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.36it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.43it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.43it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.41it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.38it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.32it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.28it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.28it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.32it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.34it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.26it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.41it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.45it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.28it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.31it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.34it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.32it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.28it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.32it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.32it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.41it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.48it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.30it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.33it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.25it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.32it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.29it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.35it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.33it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.40it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.41it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.41it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.30it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.33it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.35it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.26it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.29it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.36it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.34it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.42it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.41it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.34it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.30it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.33it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.30it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.36it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.42it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.34it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.33it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.41it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.39it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.39it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.32it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.26it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.26it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.28it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.28it/s][A 60%|██████    | 354/590 [02:36<01:06,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:31:44,945 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-29 08:31:44,964 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:31:47,312 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:31:47,333 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:31:47,342 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:43<24:51,  6.34s/it] 60%|██████    | 356/590 [02:43<17:39,  4.53s/it] 61%|██████    | 357/590 [02:44<12:38,  3.26s/it] 61%|██████    | 358/590 [02:44<09:09,  2.37s/it] 61%|██████    | 359/590 [02:44<06:42,  1.74s/it] 61%|██████    | 360/590 [02:44<05:00,  1.31s/it] 61%|██████    | 361/590 [02:45<03:49,  1.00s/it] 61%|██████▏   | 362/590 [02:45<02:59,  1.27it/s] 62%|██████▏   | 363/590 [02:45<02:24,  1.57it/s] 62%|██████▏   | 364/590 [02:46<02:00,  1.87it/s] 62%|██████▏   | 365/590 [02:46<01:43,  2.17it/s] 62%|██████▏   | 366/590 [02:46<01:31,  2.45it/s] 62%|██████▏   | 367/590 [02:46<01:23,  2.67it/s] 62%|██████▏   | 368/590 [02:47<01:17,  2.87it/s] 63%|██████▎   | 369/590 [02:47<01:13,  3.02it/s] 63%|██████▎   | 370/590 [02:47<01:09,  3.14it/s] 63%|██████▎   | 371/590 [02:48<01:07,  3.23it/s] 63%|██████▎   | 372/590 [02:48<01:06,  3.29it/s] 63%|██████▎   | 373/590 [02:48<01:04,  3.34it/s] 63%|██████▎   | 374/590 [02:48<01:03,  3.38it/s] 64%|██████▎   | 375/590 [02:49<01:03,  3.40it/s] 64%|██████▎   | 376/590 [02:49<01:02,  3.42it/s] 64%|██████▍   | 377/590 [02:49<01:02,  3.43it/s] 64%|██████▍   | 378/590 [02:50<01:01,  3.43it/s] 64%|██████▍   | 379/590 [02:50<01:01,  3.44it/s] 64%|██████▍   | 380/590 [02:50<01:00,  3.44it/s] 65%|██████▍   | 381/590 [02:50<01:00,  3.45it/s] 65%|██████▍   | 382/590 [02:51<01:00,  3.45it/s] 65%|██████▍   | 383/590 [02:51<00:59,  3.45it/s] 65%|██████▌   | 384/590 [02:51<00:59,  3.46it/s] 65%|██████▌   | 385/590 [02:52<00:59,  3.46it/s] 65%|██████▌   | 386/590 [02:52<00:58,  3.46it/s] 66%|██████▌   | 387/590 [02:52<00:58,  3.46it/s] 66%|██████▌   | 388/590 [02:53<00:58,  3.46it/s] 66%|██████▌   | 389/590 [02:53<00:58,  3.45it/s] 66%|██████▌   | 390/590 [02:53<00:57,  3.45it/s] 66%|██████▋   | 391/590 [02:53<00:57,  3.46it/s] 66%|██████▋   | 392/590 [02:54<00:57,  3.46it/s] 67%|██████▋   | 393/590 [02:54<00:57,  3.40it/s] 67%|██████▋   | 394/590 [02:54<00:58,  3.34it/s] 67%|██████▋   | 395/590 [02:55<00:57,  3.37it/s] 67%|██████▋   | 396/590 [02:55<00:57,  3.40it/s] 67%|██████▋   | 397/590 [02:55<00:56,  3.42it/s] 67%|██████▋   | 398/590 [02:55<00:56,  3.43it/s] 68%|██████▊   | 399/590 [02:56<00:55,  3.44it/s] 68%|██████▊   | 400/590 [02:56<00:55,  3.45it/s] 68%|██████▊   | 401/590 [02:56<00:54,  3.45it/s] 68%|██████▊   | 402/590 [02:57<00:54,  3.45it/s] 68%|██████▊   | 403/590 [02:57<00:54,  3.45it/s] 68%|██████▊   | 404/590 [02:57<00:53,  3.45it/s] 69%|██████▊   | 405/590 [02:57<00:53,  3.45it/s] 69%|██████▉   | 406/590 [02:58<00:53,  3.46it/s] 69%|██████▉   | 407/590 [02:58<00:53,  3.45it/s] 69%|██████▉   | 408/590 [02:58<00:52,  3.45it/s] 69%|██████▉   | 409/590 [02:59<00:52,  3.45it/s] 69%|██████▉   | 410/590 [02:59<00:52,  3.45it/s] 70%|██████▉   | 411/590 [02:59<00:51,  3.45it/s] 70%|██████▉   | 412/590 [03:00<00:51,  3.46it/s] 70%|███████   | 413/590 [03:00<00:51,  3.46it/s] 70%|███████   | 414/590 [03:00<00:50,  3.46it/s] 70%|███████   | 415/590 [03:00<00:50,  3.46it/s] 71%|███████   | 416/590 [03:01<00:50,  3.46it/s] 71%|███████   | 417/590 [03:01<00:50,  3.46it/s] 71%|███████   | 418/590 [03:01<00:49,  3.45it/s] 71%|███████   | 419/590 [03:02<00:49,  3.45it/s] 71%|███████   | 420/590 [03:02<00:49,  3.45it/s] 71%|███████▏  | 421/590 [03:02<00:48,  3.45it/s] 72%|███████▏  | 422/590 [03:02<00:48,  3.46it/s] 72%|███████▏  | 423/590 [03:03<00:48,  3.45it/s] 72%|███████▏  | 424/590 [03:03<00:48,  3.46it/s] 72%|███████▏  | 425/590 [03:03<00:47,  3.46it/s] 72%|███████▏  | 426/590 [03:04<00:47,  3.46it/s] 72%|███████▏  | 427/590 [03:04<00:47,  3.46it/s] 73%|███████▎  | 428/590 [03:04<00:46,  3.46it/s] 73%|███████▎  | 429/590 [03:04<00:46,  3.44it/s] 73%|███████▎  | 430/590 [03:05<00:46,  3.44it/s] 73%|███████▎  | 431/590 [03:05<00:46,  3.45it/s] 73%|███████▎  | 432/590 [03:05<00:45,  3.45it/s] 73%|███████▎  | 433/590 [03:06<00:45,  3.45it/s] 74%|███████▎  | 434/590 [03:06<00:45,  3.45it/s] 74%|███████▎  | 435/590 [03:06<00:44,  3.45it/s] 74%|███████▍  | 436/590 [03:06<00:44,  3.46it/s] 74%|███████▍  | 437/590 [03:07<00:44,  3.46it/s] 74%|███████▍  | 438/590 [03:07<00:43,  3.46it/s] 74%|███████▍  | 439/590 [03:07<00:43,  3.46it/s] 75%|███████▍  | 440/590 [03:08<00:43,  3.45it/s] 75%|███████▍  | 441/590 [03:08<00:43,  3.45it/s] 75%|███████▍  | 442/590 [03:08<00:42,  3.45it/s] 75%|███████▌  | 443/590 [03:08<00:42,  3.45it/s] 75%|███████▌  | 444/590 [03:09<00:42,  3.45it/s] 75%|███████▌  | 445/590 [03:09<00:41,  3.45it/s] 76%|███████▌  | 446/590 [03:09<00:41,  3.45it/s] 76%|███████▌  | 447/590 [03:10<00:41,  3.45it/s] 76%|███████▌  | 448/590 [03:10<00:41,  3.45it/s] 76%|███████▌  | 449/590 [03:10<00:40,  3.46it/s] 76%|███████▋  | 450/590 [03:11<00:40,  3.46it/s] 76%|███████▋  | 451/590 [03:11<00:40,  3.46it/s] 77%|███████▋  | 452/590 [03:11<00:39,  3.46it/s] 77%|███████▋  | 453/590 [03:11<00:39,  3.46it/s] 77%|███████▋  | 454/590 [03:12<00:39,  3.45it/s] 77%|███████▋  | 455/590 [03:12<00:39,  3.45it/s] 77%|███████▋  | 456/590 [03:12<00:38,  3.45it/s] 77%|███████▋  | 457/590 [03:13<00:38,  3.45it/s] 78%|███████▊  | 458/590 [03:13<00:38,  3.45it/s] 78%|███████▊  | 459/590 [03:13<00:37,  3.45it/s] 78%|███████▊  | 460/590 [03:13<00:37,  3.45it/s] 78%|███████▊  | 461/590 [03:14<00:37,  3.45it/s] 78%|███████▊  | 462/590 [03:14<00:37,  3.45it/s] 78%|███████▊  | 463/590 [03:14<00:36,  3.45it/s] 79%|███████▊  | 464/590 [03:15<00:36,  3.45it/s] 79%|███████▉  | 465/590 [03:15<00:36,  3.45it/s] 79%|███████▉  | 466/590 [03:15<00:35,  3.45it/s] 79%|███████▉  | 467/590 [03:15<00:35,  3.46it/s] 79%|███████▉  | 468/590 [03:16<00:35,  3.45it/s] 79%|███████▉  | 469/590 [03:16<00:35,  3.45it/s] 80%|███████▉  | 470/590 [03:16<00:34,  3.45it/s] 80%|███████▉  | 471/590 [03:17<00:34,  3.46it/s] 80%|████████  | 472/590 [03:17<00:32,  3.58it/s][INFO|trainer.py:2140] 2023-08-29 08:32:26,184 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:32:26,184 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 08:32:26,184 >>   Batch size = 8
{'eval_loss': 0.996068000793457, 'eval_runtime': 13.1138, 'eval_samples_per_second': 370.602, 'eval_steps_per_second': 46.363, 'epoch': 3.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.04it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.31it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.53it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.74it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.35it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.03it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.77it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.55it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.47it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.42it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.38it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.49it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.56it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.48it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.42it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.36it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.24it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.29it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.34it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.37it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.40it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.51it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.55it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.45it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.30it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.25it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.20it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.18it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.27it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.35it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.38it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.49it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.50it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.33it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.24it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.26it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.19it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.34it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.38it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.43it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.48it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.49it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.42it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.32it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.24it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.27it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.26it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.25it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.37it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.35it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.43it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.44it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.33it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.38it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.26it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.29it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.32it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.40it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.45it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.40it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.41it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.37it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.28it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.36it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.33it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.27it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.31it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.36it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.38it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.48it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.44it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.26it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.27it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.35it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.31it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.37it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.36it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.41it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.38it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.42it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.40it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.32it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.22it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.35it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.33it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.36it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.41it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.43it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.43it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.36it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.30it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.27it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.28it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.34it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.35it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.38it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.38it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.42it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.42it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.35it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.29it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.31it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.37it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.31it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.33it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.32it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.39it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.30it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.30it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.29it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.28it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.36it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.33it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.31it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.34it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.36it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.40it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.32it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.36it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.31it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.30it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.30it/s][A 80%|████████  | 472/590 [03:30<00:32,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:32:39,307 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-29 08:32:39,328 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:32:41,477 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:32:41,492 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:32:41,498 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:37<12:07,  6.22s/it] 80%|████████  | 474/590 [03:37<08:35,  4.44s/it] 81%|████████  | 475/590 [03:38<06:07,  3.20s/it] 81%|████████  | 476/590 [03:38<04:24,  2.32s/it] 81%|████████  | 477/590 [03:38<03:13,  1.71s/it] 81%|████████  | 478/590 [03:38<02:24,  1.29s/it] 81%|████████  | 479/590 [03:39<01:49,  1.01it/s] 81%|████████▏ | 480/590 [03:39<01:25,  1.29it/s] 82%|████████▏ | 481/590 [03:39<01:08,  1.59it/s] 82%|████████▏ | 482/590 [03:40<00:57,  1.89it/s] 82%|████████▏ | 483/590 [03:40<00:48,  2.19it/s] 82%|████████▏ | 484/590 [03:40<00:43,  2.46it/s] 82%|████████▏ | 485/590 [03:40<00:39,  2.69it/s] 82%|████████▏ | 486/590 [03:41<00:36,  2.88it/s] 83%|████████▎ | 487/590 [03:41<00:33,  3.03it/s] 83%|████████▎ | 488/590 [03:41<00:32,  3.15it/s] 83%|████████▎ | 489/590 [03:42<00:31,  3.24it/s] 83%|████████▎ | 490/590 [03:42<00:30,  3.30it/s] 83%|████████▎ | 491/590 [03:42<00:29,  3.35it/s] 83%|████████▎ | 492/590 [03:42<00:28,  3.38it/s] 84%|████████▎ | 493/590 [03:43<00:28,  3.40it/s] 84%|████████▎ | 494/590 [03:43<00:28,  3.42it/s] 84%|████████▍ | 495/590 [03:43<00:27,  3.43it/s] 84%|████████▍ | 496/590 [03:44<00:27,  3.43it/s] 84%|████████▍ | 497/590 [03:44<00:27,  3.44it/s] 84%|████████▍ | 498/590 [03:44<00:26,  3.44it/s] 85%|████████▍ | 499/590 [03:44<00:26,  3.45it/s] 85%|████████▍ | 500/590 [03:45<00:26,  3.45it/s]                                                  85%|████████▍ | 500/590 [03:45<00:26,  3.45it/s] 85%|████████▍ | 501/590 [03:45<00:25,  3.45it/s] 85%|████████▌ | 502/590 [03:45<00:25,  3.46it/s] 85%|████████▌ | 503/590 [03:46<00:25,  3.46it/s] 85%|████████▌ | 504/590 [03:46<00:24,  3.46it/s] 86%|████████▌ | 505/590 [03:46<00:24,  3.46it/s] 86%|████████▌ | 506/590 [03:46<00:24,  3.46it/s] 86%|████████▌ | 507/590 [03:47<00:24,  3.44it/s] 86%|████████▌ | 508/590 [03:47<00:23,  3.44it/s] 86%|████████▋ | 509/590 [03:47<00:23,  3.45it/s] 86%|████████▋ | 510/590 [03:48<00:23,  3.45it/s] 87%|████████▋ | 511/590 [03:48<00:22,  3.45it/s] 87%|████████▋ | 512/590 [03:48<00:22,  3.46it/s] 87%|████████▋ | 513/590 [03:49<00:22,  3.46it/s] 87%|████████▋ | 514/590 [03:49<00:21,  3.46it/s] 87%|████████▋ | 515/590 [03:49<00:21,  3.46it/s] 87%|████████▋ | 516/590 [03:49<00:21,  3.46it/s] 88%|████████▊ | 517/590 [03:50<00:21,  3.46it/s] 88%|████████▊ | 518/590 [03:50<00:21,  3.42it/s] 88%|████████▊ | 519/590 [03:50<00:20,  3.43it/s] 88%|████████▊ | 520/590 [03:51<00:20,  3.44it/s] 88%|████████▊ | 521/590 [03:51<00:20,  3.44it/s] 88%|████████▊ | 522/590 [03:51<00:19,  3.45it/s] 89%|████████▊ | 523/590 [03:51<00:19,  3.45it/s] 89%|████████▉ | 524/590 [03:52<00:19,  3.45it/s] 89%|████████▉ | 525/590 [03:52<00:18,  3.45it/s] 89%|████████▉ | 526/590 [03:52<00:18,  3.46it/s] 89%|████████▉ | 527/590 [03:53<00:18,  3.46it/s] 89%|████████▉ | 528/590 [03:53<00:17,  3.46it/s] 90%|████████▉ | 529/590 [03:53<00:17,  3.45it/s] 90%|████████▉ | 530/590 [03:53<00:17,  3.45it/s] 90%|█████████ | 531/590 [03:54<00:17,  3.45it/s] 90%|█████████ | 532/590 [03:54<00:17,  3.40it/s] 90%|█████████ | 533/590 [03:54<00:16,  3.37it/s] 91%|█████████ | 534/590 [03:55<00:16,  3.39it/s] 91%|█████████ | 535/590 [03:55<00:16,  3.41it/s] 91%|█████████ | 536/590 [03:55<00:15,  3.42it/s] 91%|█████████ | 537/590 [03:55<00:15,  3.43it/s] 91%|█████████ | 538/590 [03:56<00:15,  3.44it/s] 91%|█████████▏| 539/590 [03:56<00:14,  3.44it/s] 92%|█████████▏| 540/590 [03:56<00:14,  3.43it/s] 92%|█████████▏| 541/590 [03:57<00:14,  3.44it/s] 92%|█████████▏| 542/590 [03:57<00:13,  3.44it/s] 92%|█████████▏| 543/590 [03:57<00:13,  3.45it/s] 92%|█████████▏| 544/590 [03:58<00:13,  3.45it/s] 92%|█████████▏| 545/590 [03:58<00:13,  3.45it/s] 93%|█████████▎| 546/590 [03:58<00:12,  3.45it/s] 93%|█████████▎| 547/590 [03:58<00:12,  3.45it/s] 93%|█████████▎| 548/590 [03:59<00:12,  3.45it/s] 93%|█████████▎| 549/590 [03:59<00:11,  3.46it/s] 93%|█████████▎| 550/590 [03:59<00:11,  3.45it/s] 93%|█████████▎| 551/590 [04:00<00:11,  3.45it/s] 94%|█████████▎| 552/590 [04:00<00:11,  3.45it/s] 94%|█████████▎| 553/590 [04:00<00:10,  3.45it/s] 94%|█████████▍| 554/590 [04:00<00:10,  3.45it/s] 94%|█████████▍| 555/590 [04:01<00:10,  3.45it/s] 94%|█████████▍| 556/590 [04:01<00:09,  3.46it/s] 94%|█████████▍| 557/590 [04:01<00:09,  3.46it/s] 95%|█████████▍| 558/590 [04:02<00:09,  3.46it/s] 95%|█████████▍| 559/590 [04:02<00:08,  3.46it/s] 95%|█████████▍| 560/590 [04:02<00:08,  3.46it/s] 95%|█████████▌| 561/590 [04:02<00:08,  3.46it/s] 95%|█████████▌| 562/590 [04:03<00:08,  3.46it/s] 95%|█████████▌| 563/590 [04:03<00:07,  3.46it/s] 96%|█████████▌| 564/590 [04:03<00:07,  3.46it/s] 96%|█████████▌| 565/590 [04:04<00:07,  3.46it/s] 96%|█████████▌| 566/590 [04:04<00:06,  3.46it/s] 96%|█████████▌| 567/590 [04:04<00:06,  3.46it/s] 96%|█████████▋| 568/590 [04:04<00:06,  3.46it/s] 96%|█████████▋| 569/590 [04:05<00:06,  3.45it/s] 97%|█████████▋| 570/590 [04:05<00:05,  3.45it/s] 97%|█████████▋| 571/590 [04:05<00:05,  3.45it/s] 97%|█████████▋| 572/590 [04:06<00:05,  3.46it/s] 97%|█████████▋| 573/590 [04:06<00:04,  3.45it/s] 97%|█████████▋| 574/590 [04:06<00:04,  3.46it/s] 97%|█████████▋| 575/590 [04:06<00:04,  3.45it/s] 98%|█████████▊| 576/590 [04:07<00:04,  3.45it/s] 98%|█████████▊| 577/590 [04:07<00:03,  3.45it/s] 98%|█████████▊| 578/590 [04:07<00:03,  3.46it/s] 98%|█████████▊| 579/590 [04:08<00:03,  3.46it/s] 98%|█████████▊| 580/590 [04:08<00:02,  3.45it/s] 98%|█████████▊| 581/590 [04:08<00:02,  3.45it/s] 99%|█████████▊| 582/590 [04:09<00:02,  3.45it/s] 99%|█████████▉| 583/590 [04:09<00:02,  3.45it/s] 99%|█████████▉| 584/590 [04:09<00:01,  3.45it/s] 99%|█████████▉| 585/590 [04:09<00:01,  3.45it/s] 99%|█████████▉| 586/590 [04:10<00:01,  3.45it/s] 99%|█████████▉| 587/590 [04:10<00:00,  3.45it/s]100%|█████████▉| 588/590 [04:10<00:00,  3.46it/s]100%|█████████▉| 589/590 [04:11<00:00,  3.46it/s]100%|██████████| 590/590 [04:11<00:00,  3.58it/s][INFO|trainer.py:2140] 2023-08-29 08:33:20,141 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:33:20,141 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 08:33:20,141 >>   Batch size = 8
{'eval_loss': 1.0053728818893433, 'eval_runtime': 13.1099, 'eval_samples_per_second': 370.712, 'eval_steps_per_second': 46.377, 'epoch': 4.0}
{'loss': 0.6337, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.34it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.28it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.50it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.79it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.35it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.01it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.76it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.50it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.38it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.45it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.48it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.53it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.65it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.60it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.55it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.40it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.19it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.24it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 46.35it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.33it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.44it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.47it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.44it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.49it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.34it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.35it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.30it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.28it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.65it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 45.95it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.15it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.37it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.29it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.30it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.24it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.26it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.26it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.21it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.27it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.44it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.47it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.45it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.46it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.41it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.30it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.33it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.29it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.25it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.28it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.36it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.39it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.49it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.43it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.37it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.30it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.31it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.34it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.26it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.34it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.38it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.41it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.41it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.37it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.33it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.36it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.30it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.35it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.41it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.38it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.35it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.42it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.47it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.36it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.35it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.33it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.24it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.35it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.43it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.41it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.42it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.40it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.36it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.29it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.27it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.33it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.37it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.36it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.36it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.40it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.34it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.37it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.34it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.32it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.23it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.38it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.36it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.18it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.31it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.28it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.34it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.27it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.28it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.20it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.25it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.33it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.37it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.41it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.33it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.32it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.34it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.40it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.30it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.24it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.23it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.32it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.41it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.37it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.40it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.36it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.36it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.40it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.40it/s][A100%|██████████| 590/590 [04:24<00:00,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:33:33,267 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-29 08:33:33,287 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:33:35,621 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:33:35,636 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:33:35,649 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:33:40,508 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:33:40,510 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-118 (score: 0.9655474424362183).
                                                 100%|██████████| 590/590 [04:33<00:00,  3.58it/s]100%|██████████| 590/590 [04:33<00:00,  2.16it/s]
[INFO|trainer.py:1894] 2023-08-29 08:33:42,129 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 08:33:42,141 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:33:44,490 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:33:44,511 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:33:44,522 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:33:44,702 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:44,703 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:44,703 >>   train_loss               =     0.6296
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:44,703 >>   train_runtime            = 0:04:33.29
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:44,703 >>   train_samples            =       7544
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:44,703 >>   train_samples_per_second =    138.021
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:44,703 >>   train_steps_per_second   =      2.159
{'eval_loss': 1.011130452156067, 'eval_runtime': 13.1129, 'eval_samples_per_second': 370.629, 'eval_steps_per_second': 46.367, 'epoch': 5.0}
{'train_runtime': 273.2913, 'train_samples_per_second': 138.021, 'train_steps_per_second': 2.159, 'train_loss': 0.6296169798252946, 'epoch': 5.0}
08/29/2023 08:33:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:33:44,745 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:33:44,745 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 08:33:44,745 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 58.14it/s]  2%|▏         | 12/608 [00:00<00:11, 50.98it/s]  3%|▎         | 18/608 [00:00<00:12, 48.97it/s]  4%|▍         | 23/608 [00:00<00:12, 48.19it/s]  5%|▍         | 28/608 [00:00<00:12, 47.83it/s]  5%|▌         | 33/608 [00:00<00:12, 47.50it/s]  6%|▋         | 38/608 [00:00<00:12, 47.26it/s]  7%|▋         | 43/608 [00:00<00:12, 47.02it/s]  8%|▊         | 48/608 [00:01<00:11, 46.84it/s]  9%|▊         | 53/608 [00:01<00:11, 46.86it/s] 10%|▉         | 58/608 [00:01<00:11, 46.88it/s] 10%|█         | 63/608 [00:01<00:11, 46.90it/s] 11%|█         | 68/608 [00:01<00:11, 46.86it/s] 12%|█▏        | 73/608 [00:01<00:11, 46.96it/s] 13%|█▎        | 78/608 [00:01<00:11, 46.89it/s] 14%|█▎        | 83/608 [00:01<00:11, 46.79it/s] 14%|█▍        | 88/608 [00:01<00:11, 46.77it/s] 15%|█▌        | 93/608 [00:01<00:11, 46.66it/s] 16%|█▌        | 98/608 [00:02<00:10, 46.72it/s] 17%|█▋        | 103/608 [00:02<00:10, 46.68it/s] 18%|█▊        | 108/608 [00:02<00:10, 46.58it/s] 19%|█▊        | 113/608 [00:02<00:10, 46.57it/s] 19%|█▉        | 118/608 [00:02<00:10, 46.59it/s] 20%|██        | 123/608 [00:02<00:10, 46.62it/s] 21%|██        | 128/608 [00:02<00:10, 46.60it/s] 22%|██▏       | 133/608 [00:02<00:10, 46.62it/s] 23%|██▎       | 138/608 [00:02<00:10, 46.59it/s] 24%|██▎       | 143/608 [00:03<00:09, 46.66it/s] 24%|██▍       | 148/608 [00:03<00:09, 46.63it/s] 25%|██▌       | 153/608 [00:03<00:09, 46.58it/s] 26%|██▌       | 158/608 [00:03<00:09, 46.54it/s] 27%|██▋       | 163/608 [00:03<00:09, 46.60it/s] 28%|██▊       | 168/608 [00:03<00:09, 46.60it/s] 28%|██▊       | 173/608 [00:03<00:09, 46.53it/s] 29%|██▉       | 178/608 [00:03<00:09, 46.61it/s] 30%|███       | 183/608 [00:03<00:09, 46.67it/s] 31%|███       | 188/608 [00:04<00:09, 46.62it/s] 32%|███▏      | 193/608 [00:04<00:08, 46.70it/s] 33%|███▎      | 198/608 [00:04<00:08, 46.62it/s] 33%|███▎      | 203/608 [00:04<00:08, 46.59it/s] 34%|███▍      | 208/608 [00:04<00:08, 46.59it/s] 35%|███▌      | 213/608 [00:04<00:08, 46.63it/s] 36%|███▌      | 218/608 [00:04<00:08, 46.60it/s] 37%|███▋      | 223/608 [00:04<00:08, 46.57it/s] 38%|███▊      | 228/608 [00:04<00:08, 46.61it/s] 38%|███▊      | 233/608 [00:04<00:08, 46.59it/s] 39%|███▉      | 238/608 [00:05<00:07, 46.65it/s] 40%|███▉      | 243/608 [00:05<00:07, 46.59it/s] 41%|████      | 248/608 [00:05<00:07, 46.59it/s] 42%|████▏     | 253/608 [00:05<00:07, 46.60it/s] 42%|████▏     | 258/608 [00:05<00:07, 46.54it/s] 43%|████▎     | 263/608 [00:05<00:07, 46.59it/s] 44%|████▍     | 268/608 [00:05<00:07, 46.56it/s] 45%|████▍     | 273/608 [00:05<00:07, 46.58it/s] 46%|████▌     | 278/608 [00:05<00:07, 46.60it/s] 47%|████▋     | 283/608 [00:06<00:06, 46.60it/s] 47%|████▋     | 288/608 [00:06<00:06, 46.64it/s] 48%|████▊     | 293/608 [00:06<00:06, 46.63it/s] 49%|████▉     | 298/608 [00:06<00:06, 46.58it/s] 50%|████▉     | 303/608 [00:06<00:06, 46.60it/s] 51%|█████     | 308/608 [00:06<00:06, 46.56it/s] 51%|█████▏    | 313/608 [00:06<00:06, 46.62it/s] 52%|█████▏    | 318/608 [00:06<00:06, 46.61it/s] 53%|█████▎    | 323/608 [00:06<00:06, 46.61it/s] 54%|█████▍    | 328/608 [00:07<00:06, 46.54it/s] 55%|█████▍    | 333/608 [00:07<00:05, 46.59it/s] 56%|█████▌    | 338/608 [00:07<00:05, 46.61it/s] 56%|█████▋    | 343/608 [00:07<00:05, 46.43it/s] 57%|█████▋    | 348/608 [00:07<00:05, 46.51it/s] 58%|█████▊    | 353/608 [00:07<00:05, 46.51it/s] 59%|█████▉    | 358/608 [00:07<00:05, 46.50it/s] 60%|█████▉    | 363/608 [00:07<00:05, 46.60it/s] 61%|██████    | 368/608 [00:07<00:05, 46.55it/s] 61%|██████▏   | 373/608 [00:07<00:05, 46.50it/s] 62%|██████▏   | 378/608 [00:08<00:04, 46.57it/s] 63%|██████▎   | 383/608 [00:08<00:04, 46.56it/s] 64%|██████▍   | 388/608 [00:08<00:04, 46.63it/s] 65%|██████▍   | 393/608 [00:08<00:04, 46.54it/s] 65%|██████▌   | 398/608 [00:08<00:04, 46.58it/s] 66%|██████▋   | 403/608 [00:08<00:04, 46.55it/s] 67%|██████▋   | 408/608 [00:08<00:04, 46.55it/s] 68%|██████▊   | 413/608 [00:08<00:04, 46.62it/s] 69%|██████▉   | 418/608 [00:08<00:04, 46.55it/s] 70%|██████▉   | 423/608 [00:09<00:03, 46.53it/s] 70%|███████   | 428/608 [00:09<00:03, 46.55it/s] 71%|███████   | 433/608 [00:09<00:03, 46.56it/s] 72%|███████▏  | 438/608 [00:09<00:03, 46.62it/s] 73%|███████▎  | 443/608 [00:09<00:03, 46.60it/s] 74%|███████▎  | 448/608 [00:09<00:03, 46.58it/s] 75%|███████▍  | 453/608 [00:09<00:03, 46.51it/s] 75%|███████▌  | 458/608 [00:09<00:03, 46.55it/s] 76%|███████▌  | 463/608 [00:09<00:03, 46.60it/s] 77%|███████▋  | 468/608 [00:10<00:03, 46.57it/s] 78%|███████▊  | 473/608 [00:10<00:02, 46.49it/s] 79%|███████▊  | 478/608 [00:10<00:02, 46.57it/s] 79%|███████▉  | 483/608 [00:10<00:02, 46.60it/s] 80%|████████  | 488/608 [00:10<00:02, 46.56it/s] 81%|████████  | 493/608 [00:10<00:02, 46.60it/s] 82%|████████▏ | 498/608 [00:10<00:02, 46.48it/s] 83%|████████▎ | 503/608 [00:10<00:02, 46.57it/s] 84%|████████▎ | 508/608 [00:10<00:02, 46.64it/s] 84%|████████▍ | 513/608 [00:10<00:02, 46.59it/s] 85%|████████▌ | 518/608 [00:11<00:01, 46.55it/s] 86%|████████▌ | 523/608 [00:11<00:01, 46.47it/s] 87%|████████▋ | 528/608 [00:11<00:01, 46.49it/s] 88%|████████▊ | 533/608 [00:11<00:01, 46.46it/s] 88%|████████▊ | 538/608 [00:11<00:01, 46.45it/s] 89%|████████▉ | 543/608 [00:11<00:01, 46.53it/s] 90%|█████████ | 548/608 [00:11<00:01, 46.49it/s] 91%|█████████ | 553/608 [00:11<00:01, 46.47it/s] 92%|█████████▏| 558/608 [00:11<00:01, 46.58it/s] 93%|█████████▎| 563/608 [00:12<00:00, 46.59it/s] 93%|█████████▎| 568/608 [00:12<00:00, 46.53it/s] 94%|█████████▍| 573/608 [00:12<00:00, 46.51it/s] 95%|█████████▌| 578/608 [00:12<00:00, 46.46it/s] 96%|█████████▌| 583/608 [00:12<00:00, 46.45it/s] 97%|█████████▋| 588/608 [00:12<00:00, 46.43it/s] 98%|█████████▊| 593/608 [00:12<00:00, 46.38it/s] 98%|█████████▊| 598/608 [00:12<00:00, 46.53it/s] 99%|█████████▉| 603/608 [00:12<00:00, 46.55it/s]100%|██████████| 608/608 [00:13<00:00, 46.59it/s]100%|██████████| 608/608 [00:13<00:00, 46.68it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:33:57,793 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:57,793 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:57,793 >>   eval_loss               =     0.9655
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:57,793 >>   eval_runtime            = 0:00:13.04
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:57,793 >>   eval_samples            =       4860
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:57,793 >>   eval_samples_per_second =    372.463
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:57,794 >>   eval_steps_per_second   =     46.596
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:33:57,794 >>   perplexity              =     2.6262
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:04,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:04,705 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:04,705 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:04,706 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:04,706 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:34:05,358 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:34:05,359 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:34:05,931 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:34:06,965 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:34:06,965 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:09,790 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:09,794 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:09,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:09,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:34:09,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:34:10,447 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:34:10,448 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:34:10,997 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:34:11,142 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:34:11,142 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-354
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-236
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-590
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'labels': ['headquarters location', 'licensed to broadcast to', 'member of political party', 'narrative location', 'notable work'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14287
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14387, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.62it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.60it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.60it/s]Extractor Predicting: 38it [00:23,  1.57it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:25,  1.59it/s]Extractor Predicting: 42it [00:26,  1.61it/s]Extractor Predicting: 43it [00:27,  1.59it/s]Extractor Predicting: 44it [00:27,  1.54it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:29,  1.50it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:31,  1.51it/s]Extractor Predicting: 50it [00:31,  1.51it/s]Extractor Predicting: 51it [00:32,  1.48it/s]Extractor Predicting: 52it [00:33,  1.47it/s]Extractor Predicting: 53it [00:33,  1.48it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:35,  1.50it/s]Extractor Predicting: 57it [00:36,  1.49it/s]Extractor Predicting: 58it [00:37,  1.46it/s]Extractor Predicting: 59it [00:37,  1.48it/s]Extractor Predicting: 60it [00:38,  1.47it/s]Extractor Predicting: 61it [00:39,  1.52it/s]Extractor Predicting: 62it [00:39,  1.53it/s]Extractor Predicting: 63it [00:40,  1.50it/s]Extractor Predicting: 64it [00:41,  1.51it/s]Extractor Predicting: 65it [00:41,  1.53it/s]Extractor Predicting: 66it [00:42,  1.53it/s]Extractor Predicting: 67it [00:43,  1.37it/s]Extractor Predicting: 68it [00:43,  1.45it/s]Extractor Predicting: 69it [00:44,  1.45it/s]Extractor Predicting: 70it [00:45,  1.47it/s]Extractor Predicting: 71it [00:45,  1.50it/s]Extractor Predicting: 72it [00:46,  1.53it/s]Extractor Predicting: 73it [00:47,  1.57it/s]Extractor Predicting: 74it [00:47,  1.57it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:49,  1.60it/s]Extractor Predicting: 77it [00:49,  1.57it/s]Extractor Predicting: 78it [00:50,  1.58it/s]Extractor Predicting: 79it [00:50,  1.57it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:52,  1.57it/s]Extractor Predicting: 83it [00:53,  1.58it/s]Extractor Predicting: 84it [00:54,  1.58it/s]Extractor Predicting: 85it [00:54,  1.58it/s]Extractor Predicting: 86it [00:55,  1.60it/s]Extractor Predicting: 87it [00:56,  1.60it/s]Extractor Predicting: 88it [00:56,  1.57it/s]Extractor Predicting: 89it [00:57,  1.57it/s]Extractor Predicting: 90it [00:57,  1.57it/s]Extractor Predicting: 91it [00:58,  1.57it/s]Extractor Predicting: 92it [00:59,  1.55it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [01:00,  1.58it/s]Extractor Predicting: 95it [01:01,  1.56it/s]Extractor Predicting: 96it [01:01,  1.56it/s]Extractor Predicting: 97it [01:02,  1.57it/s]Extractor Predicting: 98it [01:03,  1.61it/s]Extractor Predicting: 99it [01:03,  1.57it/s]Extractor Predicting: 100it [01:04,  1.57it/s]Extractor Predicting: 101it [01:04,  1.59it/s]Extractor Predicting: 102it [01:05,  1.57it/s]Extractor Predicting: 103it [01:06,  1.55it/s]Extractor Predicting: 104it [01:06,  1.56it/s]Extractor Predicting: 105it [01:07,  1.57it/s]Extractor Predicting: 106it [01:08,  1.59it/s]Extractor Predicting: 107it [01:08,  1.60it/s]Extractor Predicting: 108it [01:09,  1.57it/s]Extractor Predicting: 109it [01:10,  1.56it/s]Extractor Predicting: 110it [01:10,  1.57it/s]Extractor Predicting: 111it [01:11,  1.60it/s]Extractor Predicting: 112it [01:11,  1.60it/s]Extractor Predicting: 113it [01:12,  1.62it/s]Extractor Predicting: 114it [01:13,  1.61it/s]Extractor Predicting: 115it [01:13,  1.60it/s]Extractor Predicting: 116it [01:14,  1.59it/s]Extractor Predicting: 117it [01:15,  1.61it/s]Extractor Predicting: 118it [01:15,  1.62it/s]Extractor Predicting: 119it [01:16,  1.62it/s]Extractor Predicting: 120it [01:16,  1.61it/s]Extractor Predicting: 121it [01:17,  1.58it/s]Extractor Predicting: 122it [01:18,  1.59it/s]Extractor Predicting: 123it [01:18,  1.60it/s]Extractor Predicting: 124it [01:19,  1.59it/s]Extractor Predicting: 125it [01:20,  1.53it/s]Extractor Predicting: 126it [01:20,  1.52it/s]Extractor Predicting: 127it [01:21,  1.54it/s]Extractor Predicting: 128it [01:22,  1.54it/s]Extractor Predicting: 129it [01:22,  1.54it/s]Extractor Predicting: 130it [01:23,  1.58it/s]Extractor Predicting: 131it [01:23,  1.56it/s]Extractor Predicting: 132it [01:24,  1.57it/s]Extractor Predicting: 133it [01:25,  1.52it/s]Extractor Predicting: 134it [01:25,  1.54it/s]Extractor Predicting: 135it [01:26,  1.53it/s]Extractor Predicting: 136it [01:27,  1.54it/s]Extractor Predicting: 137it [01:27,  1.54it/s]Extractor Predicting: 138it [01:28,  1.52it/s]Extractor Predicting: 139it [01:29,  1.53it/s]Extractor Predicting: 140it [01:29,  1.54it/s]Extractor Predicting: 141it [01:30,  1.52it/s]Extractor Predicting: 142it [01:31,  1.54it/s]Extractor Predicting: 143it [01:31,  1.54it/s]Extractor Predicting: 144it [01:32,  1.54it/s]Extractor Predicting: 145it [01:33,  1.55it/s]Extractor Predicting: 146it [01:33,  1.58it/s]Extractor Predicting: 147it [01:34,  1.52it/s]Extractor Predicting: 148it [01:35,  1.52it/s]Extractor Predicting: 149it [01:35,  1.52it/s]Extractor Predicting: 150it [01:36,  1.54it/s]Extractor Predicting: 151it [01:36,  1.56it/s]Extractor Predicting: 152it [01:37,  1.58it/s]Extractor Predicting: 153it [01:38,  1.63it/s]Extractor Predicting: 154it [01:38,  1.61it/s]Extractor Predicting: 155it [01:39,  1.57it/s]Extractor Predicting: 156it [01:40,  1.58it/s]Extractor Predicting: 157it [01:40,  1.59it/s]Extractor Predicting: 158it [01:41,  1.58it/s]Extractor Predicting: 159it [01:42,  1.55it/s]Extractor Predicting: 160it [01:42,  1.59it/s]Extractor Predicting: 161it [01:43,  1.56it/s]Extractor Predicting: 162it [01:43,  1.55it/s]Extractor Predicting: 163it [01:44,  1.55it/s]Extractor Predicting: 164it [01:45,  1.56it/s]Extractor Predicting: 165it [01:45,  1.56it/s]Extractor Predicting: 166it [01:46,  1.56it/s]Extractor Predicting: 167it [01:47,  1.54it/s]Extractor Predicting: 168it [01:47,  1.56it/s]Extractor Predicting: 169it [01:48,  1.54it/s]Extractor Predicting: 170it [01:49,  1.56it/s]Extractor Predicting: 171it [01:49,  1.58it/s]Extractor Predicting: 172it [01:50,  1.42it/s]Extractor Predicting: 173it [01:51,  1.43it/s]Extractor Predicting: 174it [01:51,  1.49it/s]Extractor Predicting: 175it [01:52,  1.50it/s]Extractor Predicting: 176it [01:53,  1.51it/s]Extractor Predicting: 177it [01:53,  1.47it/s]Extractor Predicting: 178it [01:54,  1.46it/s]Extractor Predicting: 179it [01:55,  1.48it/s]Extractor Predicting: 180it [01:55,  1.47it/s]Extractor Predicting: 181it [01:56,  1.47it/s]Extractor Predicting: 182it [01:57,  1.51it/s]Extractor Predicting: 183it [01:57,  1.48it/s]Extractor Predicting: 184it [01:58,  1.47it/s]Extractor Predicting: 185it [01:59,  1.44it/s]Extractor Predicting: 186it [02:00,  1.45it/s]Extractor Predicting: 187it [02:00,  1.45it/s]Extractor Predicting: 188it [02:01,  1.46it/s]Extractor Predicting: 189it [02:02,  1.48it/s]Extractor Predicting: 190it [02:02,  1.46it/s]Extractor Predicting: 191it [02:03,  1.47it/s]Extractor Predicting: 192it [02:04,  1.49it/s]Extractor Predicting: 193it [02:04,  1.53it/s]Extractor Predicting: 194it [02:05,  1.52it/s]Extractor Predicting: 195it [02:05,  1.94it/s]Extractor Predicting: 195it [02:05,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:27,439 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:27,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:27,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:27,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:27,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:36:28,047 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:36:28,048 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:36:28,629 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:36:29,665 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:36:29,665 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:32,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:32,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:32,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:32,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:36:32,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:36:33,243 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:36:33,244 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:36:33,817 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:36:33,973 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:36:33,973 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4779938587512794,
  "recall": 0.09609053497942387,
  "score": 0.16001370567072123,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21244
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21344, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.58it/s]Extractor Predicting: 26it [00:16,  1.48it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:19,  1.56it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.48it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:27,  1.49it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:31,  1.60it/s]Extractor Predicting: 50it [00:32,  1.60it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.61it/s]Extractor Predicting: 53it [00:34,  1.66it/s]Extractor Predicting: 54it [00:34,  1.68it/s]Extractor Predicting: 55it [00:35,  1.67it/s]Extractor Predicting: 56it [00:35,  1.63it/s]Extractor Predicting: 57it [00:36,  1.64it/s]Extractor Predicting: 58it [00:37,  1.68it/s]Extractor Predicting: 59it [00:37,  1.68it/s]Extractor Predicting: 60it [00:38,  1.67it/s]Extractor Predicting: 61it [00:38,  1.62it/s]Extractor Predicting: 62it [00:39,  1.65it/s]Extractor Predicting: 63it [00:40,  1.65it/s]Extractor Predicting: 64it [00:40,  1.61it/s]Extractor Predicting: 65it [00:41,  1.65it/s]Extractor Predicting: 66it [00:41,  1.64it/s]Extractor Predicting: 67it [00:42,  1.67it/s]Extractor Predicting: 68it [00:43,  1.67it/s]Extractor Predicting: 69it [00:43,  1.70it/s]Extractor Predicting: 70it [00:44,  1.68it/s]Extractor Predicting: 71it [00:44,  1.64it/s]Extractor Predicting: 72it [00:45,  1.63it/s]Extractor Predicting: 73it [00:46,  1.62it/s]Extractor Predicting: 74it [00:46,  1.61it/s]Extractor Predicting: 75it [00:47,  1.60it/s]Extractor Predicting: 76it [00:48,  1.58it/s]Extractor Predicting: 77it [00:48,  1.59it/s]Extractor Predicting: 78it [00:49,  1.59it/s]Extractor Predicting: 79it [00:49,  1.65it/s]Extractor Predicting: 80it [00:50,  1.66it/s]Extractor Predicting: 81it [00:51,  1.69it/s]Extractor Predicting: 82it [00:51,  1.68it/s]Extractor Predicting: 83it [00:52,  1.68it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:53,  1.62it/s]Extractor Predicting: 86it [00:54,  1.66it/s]Extractor Predicting: 87it [00:54,  1.67it/s]Extractor Predicting: 88it [00:55,  1.72it/s]Extractor Predicting: 89it [00:55,  1.71it/s]Extractor Predicting: 90it [00:56,  1.71it/s]Extractor Predicting: 91it [00:56,  1.79it/s]Extractor Predicting: 92it [00:57,  1.73it/s]Extractor Predicting: 93it [00:58,  1.74it/s]Extractor Predicting: 94it [00:58,  1.75it/s]Extractor Predicting: 95it [00:59,  1.74it/s]Extractor Predicting: 96it [00:59,  1.70it/s]Extractor Predicting: 97it [01:00,  1.67it/s]Extractor Predicting: 98it [01:01,  1.71it/s]Extractor Predicting: 99it [01:01,  1.72it/s]Extractor Predicting: 100it [01:02,  1.71it/s]Extractor Predicting: 101it [01:02,  1.72it/s]Extractor Predicting: 102it [01:03,  1.71it/s]Extractor Predicting: 103it [01:03,  1.72it/s]Extractor Predicting: 104it [01:04,  1.70it/s]Extractor Predicting: 105it [01:05,  1.76it/s]Extractor Predicting: 106it [01:05,  1.74it/s]Extractor Predicting: 107it [01:06,  1.77it/s]Extractor Predicting: 108it [01:06,  1.73it/s]Extractor Predicting: 109it [01:07,  1.78it/s]Extractor Predicting: 110it [01:07,  1.73it/s]Extractor Predicting: 111it [01:08,  1.75it/s]Extractor Predicting: 112it [01:09,  1.70it/s]Extractor Predicting: 113it [01:09,  1.68it/s]Extractor Predicting: 114it [01:10,  1.69it/s]Extractor Predicting: 115it [01:10,  1.68it/s]Extractor Predicting: 116it [01:11,  1.67it/s]Extractor Predicting: 117it [01:12,  1.67it/s]Extractor Predicting: 118it [01:12,  1.65it/s]Extractor Predicting: 119it [01:13,  1.64it/s]Extractor Predicting: 120it [01:13,  1.66it/s]Extractor Predicting: 121it [01:14,  1.69it/s]Extractor Predicting: 122it [01:15,  1.67it/s]Extractor Predicting: 123it [01:15,  1.52it/s]Extractor Predicting: 124it [01:16,  1.53it/s]Extractor Predicting: 125it [01:17,  1.55it/s]Extractor Predicting: 126it [01:17,  1.59it/s]Extractor Predicting: 127it [01:18,  1.62it/s]Extractor Predicting: 128it [01:19,  1.61it/s]Extractor Predicting: 129it [01:19,  1.63it/s]Extractor Predicting: 130it [01:20,  1.42it/s]Extractor Predicting: 131it [01:21,  1.45it/s]Extractor Predicting: 132it [01:21,  1.50it/s]Extractor Predicting: 133it [01:22,  1.53it/s]Extractor Predicting: 134it [01:23,  1.54it/s]Extractor Predicting: 135it [01:23,  1.59it/s]Extractor Predicting: 136it [01:24,  1.59it/s]Extractor Predicting: 137it [01:24,  1.64it/s]Extractor Predicting: 138it [01:25,  1.60it/s]Extractor Predicting: 139it [01:26,  1.64it/s]Extractor Predicting: 140it [01:26,  1.64it/s]Extractor Predicting: 141it [01:27,  1.62it/s]Extractor Predicting: 142it [01:27,  1.65it/s]Extractor Predicting: 143it [01:28,  1.62it/s]Extractor Predicting: 144it [01:29,  1.58it/s]Extractor Predicting: 145it [01:29,  1.55it/s]Extractor Predicting: 146it [01:30,  1.57it/s]Extractor Predicting: 147it [01:31,  1.60it/s]Extractor Predicting: 148it [01:31,  1.58it/s]Extractor Predicting: 149it [01:32,  1.57it/s]Extractor Predicting: 150it [01:33,  1.61it/s]Extractor Predicting: 151it [01:33,  1.61it/s]Extractor Predicting: 152it [01:34,  1.60it/s]Extractor Predicting: 153it [01:34,  1.63it/s]Extractor Predicting: 154it [01:35,  1.65it/s]Extractor Predicting: 155it [01:36,  1.61it/s]Extractor Predicting: 156it [01:36,  1.62it/s]Extractor Predicting: 157it [01:37,  1.60it/s]Extractor Predicting: 158it [01:37,  1.60it/s]Extractor Predicting: 159it [01:38,  1.58it/s]Extractor Predicting: 160it [01:39,  1.60it/s]Extractor Predicting: 161it [01:39,  1.57it/s]Extractor Predicting: 162it [01:40,  1.60it/s]Extractor Predicting: 163it [01:41,  1.60it/s]Extractor Predicting: 164it [01:41,  1.59it/s]Extractor Predicting: 165it [01:42,  1.57it/s]Extractor Predicting: 166it [01:43,  1.54it/s]Extractor Predicting: 167it [01:43,  1.55it/s]Extractor Predicting: 168it [01:44,  1.59it/s]Extractor Predicting: 169it [01:44,  1.58it/s]Extractor Predicting: 170it [01:45,  1.56it/s]Extractor Predicting: 171it [01:46,  1.54it/s]Extractor Predicting: 172it [01:46,  1.59it/s]Extractor Predicting: 173it [01:47,  1.58it/s]Extractor Predicting: 174it [01:48,  1.56it/s]Extractor Predicting: 175it [01:48,  1.56it/s]Extractor Predicting: 176it [01:49,  1.59it/s]Extractor Predicting: 177it [01:50,  1.60it/s]Extractor Predicting: 178it [01:50,  1.63it/s]Extractor Predicting: 179it [01:51,  1.66it/s]Extractor Predicting: 180it [01:51,  1.63it/s]Extractor Predicting: 181it [01:52,  1.54it/s]Extractor Predicting: 182it [01:53,  1.54it/s]Extractor Predicting: 183it [01:53,  1.57it/s]Extractor Predicting: 184it [01:54,  1.55it/s]Extractor Predicting: 185it [01:55,  1.55it/s]Extractor Predicting: 186it [01:55,  1.54it/s]Extractor Predicting: 187it [01:56,  1.57it/s]Extractor Predicting: 188it [01:56,  1.61it/s]Extractor Predicting: 189it [01:57,  1.63it/s]Extractor Predicting: 190it [01:58,  1.61it/s]Extractor Predicting: 191it [01:58,  1.62it/s]Extractor Predicting: 192it [01:59,  1.60it/s]Extractor Predicting: 193it [02:00,  1.65it/s]Extractor Predicting: 194it [02:00,  1.70it/s]Extractor Predicting: 195it [02:01,  1.73it/s]Extractor Predicting: 196it [02:01,  1.67it/s]Extractor Predicting: 197it [02:02,  1.65it/s]Extractor Predicting: 198it [02:03,  1.62it/s]Extractor Predicting: 199it [02:03,  1.69it/s]Extractor Predicting: 200it [02:04,  1.68it/s]Extractor Predicting: 201it [02:04,  1.66it/s]Extractor Predicting: 202it [02:05,  1.68it/s]Extractor Predicting: 203it [02:05,  1.68it/s]Extractor Predicting: 204it [02:06,  1.63it/s]Extractor Predicting: 205it [02:07,  1.64it/s]Extractor Predicting: 206it [02:07,  1.63it/s]Extractor Predicting: 207it [02:08,  1.62it/s]Extractor Predicting: 208it [02:09,  1.62it/s]Extractor Predicting: 209it [02:09,  1.63it/s]Extractor Predicting: 210it [02:10,  1.60it/s]Extractor Predicting: 211it [02:10,  1.59it/s]Extractor Predicting: 212it [02:11,  1.60it/s]Extractor Predicting: 213it [02:12,  1.64it/s]Extractor Predicting: 214it [02:12,  1.63it/s]Extractor Predicting: 215it [02:13,  1.65it/s]Extractor Predicting: 216it [02:13,  1.66it/s]Extractor Predicting: 217it [02:14,  1.67it/s]Extractor Predicting: 218it [02:15,  1.61it/s]Extractor Predicting: 219it [02:15,  1.60it/s]Extractor Predicting: 220it [02:16,  1.58it/s]Extractor Predicting: 221it [02:17,  1.53it/s]Extractor Predicting: 222it [02:17,  1.50it/s]Extractor Predicting: 223it [02:18,  1.57it/s]Extractor Predicting: 224it [02:19,  1.56it/s]Extractor Predicting: 225it [02:19,  1.56it/s]Extractor Predicting: 226it [02:20,  1.54it/s]Extractor Predicting: 227it [02:21,  1.56it/s]Extractor Predicting: 228it [02:21,  1.56it/s]Extractor Predicting: 229it [02:22,  1.60it/s]Extractor Predicting: 230it [02:22,  1.59it/s]Extractor Predicting: 231it [02:23,  1.56it/s]Extractor Predicting: 232it [02:24,  1.58it/s]Extractor Predicting: 233it [02:24,  1.60it/s]Extractor Predicting: 234it [02:25,  1.64it/s]Extractor Predicting: 235it [02:26,  1.65it/s]Extractor Predicting: 236it [02:26,  1.71it/s]Extractor Predicting: 237it [02:27,  1.72it/s]Extractor Predicting: 238it [02:27,  1.72it/s]Extractor Predicting: 239it [02:28,  1.71it/s]Extractor Predicting: 240it [02:28,  1.68it/s]Extractor Predicting: 241it [02:29,  1.63it/s]Extractor Predicting: 242it [02:30,  1.61it/s]Extractor Predicting: 243it [02:30,  1.65it/s]Extractor Predicting: 244it [02:31,  1.47it/s]Extractor Predicting: 245it [02:32,  1.51it/s]Extractor Predicting: 246it [02:32,  1.49it/s]Extractor Predicting: 247it [02:33,  1.50it/s]Extractor Predicting: 248it [02:34,  1.50it/s]Extractor Predicting: 249it [02:34,  1.51it/s]Extractor Predicting: 250it [02:35,  1.50it/s]Extractor Predicting: 251it [02:36,  1.50it/s]Extractor Predicting: 252it [02:36,  1.51it/s]Extractor Predicting: 253it [02:37,  1.52it/s]Extractor Predicting: 254it [02:38,  1.54it/s]Extractor Predicting: 255it [02:38,  1.58it/s]Extractor Predicting: 256it [02:39,  1.52it/s]Extractor Predicting: 257it [02:40,  1.52it/s]Extractor Predicting: 258it [02:40,  1.51it/s]Extractor Predicting: 259it [02:41,  1.52it/s]Extractor Predicting: 260it [02:42,  1.52it/s]Extractor Predicting: 261it [02:42,  1.49it/s]Extractor Predicting: 262it [02:43,  1.50it/s]Extractor Predicting: 263it [02:44,  1.51it/s]Extractor Predicting: 264it [02:44,  1.53it/s]Extractor Predicting: 265it [02:45,  1.51it/s]Extractor Predicting: 266it [02:46,  1.50it/s]Extractor Predicting: 267it [02:46,  1.50it/s]Extractor Predicting: 268it [02:47,  1.50it/s]Extractor Predicting: 269it [02:48,  1.50it/s]Extractor Predicting: 270it [02:48,  1.53it/s]Extractor Predicting: 271it [02:49,  1.53it/s]Extractor Predicting: 272it [02:50,  1.52it/s]Extractor Predicting: 273it [02:50,  1.52it/s]Extractor Predicting: 274it [02:51,  1.48it/s]Extractor Predicting: 275it [02:52,  1.49it/s]Extractor Predicting: 276it [02:52,  1.52it/s]Extractor Predicting: 277it [02:53,  1.53it/s]Extractor Predicting: 278it [02:54,  1.52it/s]Extractor Predicting: 279it [02:54,  1.52it/s]Extractor Predicting: 280it [02:55,  1.58it/s]Extractor Predicting: 281it [02:55,  1.60it/s]Extractor Predicting: 282it [02:56,  1.63it/s]Extractor Predicting: 283it [02:57,  1.60it/s]Extractor Predicting: 284it [02:57,  1.57it/s]Extractor Predicting: 285it [02:58,  1.58it/s]Extractor Predicting: 286it [02:59,  1.64it/s]Extractor Predicting: 287it [02:59,  1.65it/s]Extractor Predicting: 288it [03:00,  1.60it/s]Extractor Predicting: 289it [03:00,  1.60it/s]Extractor Predicting: 290it [03:01,  1.60it/s]Extractor Predicting: 291it [03:02,  1.60it/s]Extractor Predicting: 292it [03:02,  1.65it/s]Extractor Predicting: 293it [03:03,  1.65it/s]Extractor Predicting: 294it [03:03,  1.67it/s]Extractor Predicting: 295it [03:04,  1.73it/s]Extractor Predicting: 296it [03:05,  1.73it/s]Extractor Predicting: 297it [03:05,  1.72it/s]Extractor Predicting: 298it [03:06,  1.64it/s]Extractor Predicting: 299it [03:07,  1.53it/s]Extractor Predicting: 300it [03:07,  1.51it/s]Extractor Predicting: 301it [03:08,  1.50it/s]Extractor Predicting: 302it [03:08,  1.54it/s]Extractor Predicting: 303it [03:09,  1.57it/s]Extractor Predicting: 304it [03:10,  1.57it/s]Extractor Predicting: 305it [03:10,  1.53it/s]Extractor Predicting: 306it [03:11,  1.52it/s]Extractor Predicting: 307it [03:12,  1.52it/s]Extractor Predicting: 308it [03:12,  1.51it/s]Extractor Predicting: 309it [03:13,  1.49it/s]Extractor Predicting: 310it [03:14,  1.49it/s]Extractor Predicting: 311it [03:14,  1.50it/s]Extractor Predicting: 312it [03:15,  1.52it/s]Extractor Predicting: 313it [03:16,  1.52it/s]Extractor Predicting: 314it [03:16,  1.53it/s]Extractor Predicting: 315it [03:17,  1.58it/s]Extractor Predicting: 316it [03:18,  1.55it/s]Extractor Predicting: 317it [03:18,  1.52it/s]Extractor Predicting: 318it [03:19,  1.47it/s]Extractor Predicting: 319it [03:20,  1.48it/s]Extractor Predicting: 320it [03:20,  1.50it/s]Extractor Predicting: 321it [03:21,  1.49it/s]Extractor Predicting: 322it [03:22,  1.47it/s]Extractor Predicting: 323it [03:22,  1.47it/s]Extractor Predicting: 324it [03:23,  1.48it/s]Extractor Predicting: 325it [03:24,  1.47it/s]Extractor Predicting: 326it [03:24,  1.46it/s]Extractor Predicting: 327it [03:25,  1.45it/s]Extractor Predicting: 328it [03:26,  1.46it/s]Extractor Predicting: 329it [03:27,  1.46it/s]Extractor Predicting: 330it [03:27,  1.46it/s]Extractor Predicting: 331it [03:28,  1.45it/s]Extractor Predicting: 332it [03:29,  1.45it/s]Extractor Predicting: 333it [03:29,  1.44it/s]Extractor Predicting: 334it [03:30,  1.46it/s]Extractor Predicting: 335it [03:31,  1.44it/s]Extractor Predicting: 336it [03:31,  1.41it/s]Extractor Predicting: 337it [03:32,  1.42it/s]Extractor Predicting: 338it [03:33,  1.43it/s]Extractor Predicting: 339it [03:34,  1.43it/s]Extractor Predicting: 340it [03:34,  1.45it/s]Extractor Predicting: 341it [03:35,  1.46it/s]Extractor Predicting: 342it [03:36,  1.47it/s]Extractor Predicting: 343it [03:36,  1.49it/s]Extractor Predicting: 344it [03:37,  1.52it/s]Extractor Predicting: 345it [03:37,  1.53it/s]Extractor Predicting: 346it [03:38,  1.63it/s]Extractor Predicting: 346it [03:38,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:22,145 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:22,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:22,177 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:22,177 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:22,177 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:40:22,919 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:40:22,920 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:40:23,470 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:40:24,569 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:40:24,569 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:27,586 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:27,591 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:27,591 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:27,591 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:40:27,591 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:40:28,248 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:40:28,249 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:40:28,821 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:40:28,970 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:40:28,970 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.34834123222748814,
  "recall": 0.017723655654690137,
  "score": 0.033731069297843044,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6093
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6193, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.63it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.52it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.45it/s]Extractor Predicting: 19it [00:12,  1.45it/s]Extractor Predicting: 20it [00:13,  1.41it/s]Extractor Predicting: 21it [00:13,  1.44it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.39it/s]Extractor Predicting: 24it [00:15,  1.39it/s]Extractor Predicting: 25it [00:16,  1.44it/s]Extractor Predicting: 26it [00:17,  1.40it/s]Extractor Predicting: 27it [00:17,  1.45it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:19,  1.57it/s]Extractor Predicting: 30it [00:19,  1.64it/s]Extractor Predicting: 31it [00:20,  1.74it/s]Extractor Predicting: 32it [00:20,  1.79it/s]Extractor Predicting: 33it [00:21,  1.84it/s]Extractor Predicting: 34it [00:21,  1.84it/s]Extractor Predicting: 35it [00:22,  1.87it/s]Extractor Predicting: 36it [00:22,  1.86it/s]Extractor Predicting: 37it [00:23,  1.82it/s]Extractor Predicting: 38it [00:23,  1.85it/s]Extractor Predicting: 39it [00:24,  1.84it/s]Extractor Predicting: 40it [00:24,  1.85it/s]Extractor Predicting: 41it [00:25,  1.84it/s]Extractor Predicting: 42it [00:26,  1.82it/s]Extractor Predicting: 43it [00:26,  1.82it/s]Extractor Predicting: 44it [00:27,  1.84it/s]Extractor Predicting: 45it [00:27,  1.89it/s]Extractor Predicting: 46it [00:28,  1.90it/s]Extractor Predicting: 47it [00:28,  1.87it/s]Extractor Predicting: 48it [00:29,  1.86it/s]Extractor Predicting: 49it [00:29,  1.91it/s]Extractor Predicting: 50it [00:30,  1.85it/s]Extractor Predicting: 51it [00:30,  1.80it/s]Extractor Predicting: 52it [00:31,  1.83it/s]Extractor Predicting: 53it [00:32,  1.83it/s]Extractor Predicting: 54it [00:32,  1.83it/s]Extractor Predicting: 55it [00:33,  1.86it/s]Extractor Predicting: 56it [00:33,  1.85it/s]Extractor Predicting: 57it [00:34,  1.88it/s]Extractor Predicting: 58it [00:34,  1.76it/s]Extractor Predicting: 59it [00:35,  1.65it/s]Extractor Predicting: 60it [00:36,  1.56it/s]Extractor Predicting: 61it [00:36,  1.50it/s]Extractor Predicting: 62it [00:37,  1.49it/s]Extractor Predicting: 63it [00:38,  1.47it/s]Extractor Predicting: 64it [00:39,  1.46it/s]Extractor Predicting: 65it [00:39,  1.45it/s]Extractor Predicting: 66it [00:40,  1.45it/s]Extractor Predicting: 67it [00:41,  1.44it/s]Extractor Predicting: 68it [00:41,  1.43it/s]Extractor Predicting: 69it [00:42,  1.42it/s]Extractor Predicting: 70it [00:43,  1.45it/s]Extractor Predicting: 71it [00:43,  1.42it/s]Extractor Predicting: 72it [00:44,  1.43it/s]Extractor Predicting: 73it [00:45,  1.45it/s]Extractor Predicting: 74it [00:45,  1.47it/s]Extractor Predicting: 75it [00:46,  1.47it/s]Extractor Predicting: 76it [00:47,  1.49it/s]Extractor Predicting: 77it [00:47,  1.82it/s]Extractor Predicting: 77it [00:47,  1.62it/s]
[INFO|configuration_utils.py:515] 2023-08-29 08:41:17,896 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:41:17,899 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:41:17,904 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:41:17,905 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 08:41:17,908 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:41:27,022 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 08:41:27,026 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 08:41:27,048 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:41:27,049 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:41:27,061 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:27,067 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:27,067 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:27,067 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:27,067 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:27,067 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:41:27,067 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.9098497495826378,
  "recall": 0.13560587210748942,
  "score": 0.2360329146816804,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 08:41:27,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:27,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:28,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:29,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:29,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:30,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:31,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:31,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:32,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:33,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:33,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:34,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:35,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:36,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:36,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:37,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:38,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:38,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:39,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:40,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:41,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:41,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:30, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-29 08:41:42,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:43,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:43,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:44,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:45,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:45,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:46,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:47,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:47,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:48,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:49,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:49,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:50,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:51,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:51,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:52,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:52,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:53,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:54,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:54,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:55,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:28<03:06, 14.36s/it][WARNING|generation_utils.py:914] 2023-08-29 08:41:56,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:57,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:57,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:58,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:59,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:41:59,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:00,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:00,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:01,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:02,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:02,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:03,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:04,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:04,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:05,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:06,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:06,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:07,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:08,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:09,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:09,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:10,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:44<02:56, 14.71s/it][WARNING|generation_utils.py:914] 2023-08-29 08:42:11,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:12,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:12,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:13,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:14,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:15,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:15,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:16,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:17,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:18,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:18,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:19,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:20,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:21,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:22,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:22,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:23,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:24,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:24,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:25,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:26,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:27,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:00<02:49, 15.41s/it][WARNING|generation_utils.py:914] 2023-08-29 08:42:27,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:28,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:29,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:30,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:30,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:31,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:32,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:32,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:33,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:34,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:35,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:36,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:36,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:37,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:38,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:39,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:39,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:40,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:41,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:42,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:43,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:43,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:17<02:38, 15.84s/it][WARNING|generation_utils.py:914] 2023-08-29 08:42:44,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:45,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:45,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:46,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:47,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:48,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:48,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:49,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:50,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:51,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:52,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:52,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:53,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:54,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:54,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:55,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:56,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:57,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:57,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:58,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:42:59,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:00,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:00,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:34<02:26, 16.29s/it][WARNING|generation_utils.py:914] 2023-08-29 08:43:01,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:02,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:03,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:03,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:04,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:05,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:06,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:07,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:07,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:08,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:09,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:10,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:11,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:11,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:13,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:13,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:14,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:15,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:16,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:17,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:18,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:18,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:19,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:53<02:16, 17.11s/it][WARNING|generation_utils.py:914] 2023-08-29 08:43:20,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:21,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:21,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:22,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:23,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:23,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:24,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:25,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:25,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:26,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:27,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:27,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:28,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:29,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:30,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:30,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:31,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:32,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:33,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:33,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:35,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:35,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:36,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:37,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:38,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:11<02:02, 17.51s/it][WARNING|generation_utils.py:914] 2023-08-29 08:43:38,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:39,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:40,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:40,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:41,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:42,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:42,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:43,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:44,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:45,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:45,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:46,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:47,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:47,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:48,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:49,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:50,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:50,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:51,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:52,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:52,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:53,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:54,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:27<01:42, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-29 08:43:55,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:55,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:56,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:57,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:57,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:58,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:43:59,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:00,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:00,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:01,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:01,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:02,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:03,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:04,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:04,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:05,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:06,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:07,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:07,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:08,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:09,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:10,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:43<01:23, 16.74s/it][WARNING|generation_utils.py:914] 2023-08-29 08:44:10,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:11,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:12,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:13,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:13,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:14,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:15,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:15,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:16,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:17,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:17,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:18,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:18,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:19,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:20,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:20,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:21,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:22,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:22,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:23,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:23,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:24,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:57<01:03, 15.98s/it][WARNING|generation_utils.py:914] 2023-08-29 08:44:25,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:25,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:26,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:27,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:28,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:28,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:29,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:30,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:30,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:31,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:32,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:32,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:33,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:34,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:34,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:35,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:36,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:37,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:38,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:39,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:12<00:46, 15.56s/it][WARNING|generation_utils.py:914] 2023-08-29 08:44:39,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:41,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:41,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:42,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:43,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:43,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:44,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:45,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:46,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:46,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:47,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:48,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:49,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:50,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:51,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:52,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:53,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:54,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:55,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:56,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:29<00:32, 16.08s/it][WARNING|generation_utils.py:914] 2023-08-29 08:44:57,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:57,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:58,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:59,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:44:59,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:00,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:01,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:02,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:02,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:03,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:03,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:04,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:05,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:05,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:06,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:07,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:07,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:08,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:09,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:09,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:10,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:11,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:11,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:45<00:16, 16.12s/it][WARNING|generation_utils.py:914] 2023-08-29 08:45:13,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:14,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:14,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:15,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:16,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:16,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:17,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:18,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:19,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:20,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:20,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:21,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:22,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:22,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:23,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:24,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:25,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:25,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:26,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:27,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:27,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:45:28,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:02<00:00, 16.14s/it]Generating: 100%|██████████| 15/15 [04:02<00:00, 16.14s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:35,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:35,818 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:35,818 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:35,818 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:35,818 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:45:36,410 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:45:36,411 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:45:36,975 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:45:38,023 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:45:38,023 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:40,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:40,866 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:40,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:40,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:45:40,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:45:41,527 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:45:41,528 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:45:42,107 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:45:42,259 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:45:42,259 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.859375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9136904761904762, 'errors': {''}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : United Kingdom Independence Party .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : narrative location .', 'success_rate': 0.8707386363636364, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : notable work .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : applies to jurisdiction . Context : Later in 2008 , the federal court of appeals for the District of Columbia ruled that a 2011 ruling by the Supreme Court of the United States in the Western District of Pennsylvania did not violate the constitutional rights of the United States . Head Entity : U.S. Supreme Court , Tail Entity : District of Columbia .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : applies to jurisdiction .', 'success_rate': 0.8315217391304348, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : family name .', 'success_rate': 0.8315217391304348, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 380, 'raw': 512}
{'target': 600, 'success': 404, 'raw': 544}
{'target': 600, 'success': 428, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 600, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.75, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8288043478260869, 'errors': {'', "('country of origin', 'is a list of', '', 'The list is organized by country of origin .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.890625, 'errors': {''}}
['Relation : located on astronomical body . Context : This is the sixth most luminous moon , with a mass of 9 . 6 × 10 6 ” , with a magnitude of 5 . 26 . Head Entity : moon , Tail Entity : Earth .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8920454545454546, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.95625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('M8', 'manufacturer', '', 'The M8 , and later the M9 , were a line of .')"}}
['Relation : member of . Context : Later in the year , the United States appointed him a Vice President of the United States . Head Entity : George W. Bush , Tail Entity : United States .\n']
['Relation : member of . Context : Later in the year , the United States appointed him a Vice President of the United States . Head Entity : George W. Bush , Tail Entity : United States .\n', 'Relation : member of . Context : After he was elected to Prime Minister under his predecessor , Tony Blair , the Conservatives began a policy to end compulsory school testing . Head Entity : Tony Blair , Tail Entity : Tories .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : use .', 'success_rate': 0.8565340909090909, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 10854
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10954, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.66it/s]Extractor Estimating: 2it [00:01,  1.59it/s]Extractor Estimating: 3it [00:01,  1.62it/s]Extractor Estimating: 4it [00:02,  1.61it/s]Extractor Estimating: 5it [00:03,  1.67it/s]Extractor Estimating: 6it [00:03,  1.72it/s]Extractor Estimating: 7it [00:04,  1.73it/s]Extractor Estimating: 8it [00:04,  1.76it/s]Extractor Estimating: 9it [00:05,  1.71it/s]Extractor Estimating: 10it [00:05,  1.69it/s]Extractor Estimating: 11it [00:06,  1.71it/s]Extractor Estimating: 12it [00:07,  1.71it/s]Extractor Estimating: 13it [00:07,  1.67it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:08,  1.63it/s]Extractor Estimating: 16it [00:09,  1.65it/s]Extractor Estimating: 17it [00:10,  1.53it/s]Extractor Estimating: 18it [00:10,  1.55it/s]Extractor Estimating: 19it [00:11,  1.59it/s]Extractor Estimating: 20it [00:12,  1.64it/s]Extractor Estimating: 21it [00:12,  1.64it/s]Extractor Estimating: 22it [00:13,  1.60it/s]Extractor Estimating: 23it [00:13,  1.62it/s]Extractor Estimating: 24it [00:14,  1.61it/s]Extractor Estimating: 25it [00:15,  1.65it/s]Extractor Estimating: 26it [00:15,  1.66it/s]Extractor Estimating: 27it [00:16,  1.64it/s]Extractor Estimating: 28it [00:17,  1.53it/s]Extractor Estimating: 29it [00:17,  1.58it/s]Extractor Estimating: 30it [00:18,  1.61it/s]Extractor Estimating: 31it [00:18,  1.59it/s]Extractor Estimating: 32it [00:19,  1.63it/s]Extractor Estimating: 33it [00:20,  1.61it/s]Extractor Estimating: 34it [00:20,  1.64it/s]Extractor Estimating: 35it [00:21,  1.63it/s]Extractor Estimating: 36it [00:22,  1.61it/s]Extractor Estimating: 37it [00:22,  1.66it/s]Extractor Estimating: 38it [00:23,  1.64it/s]Extractor Estimating: 39it [00:23,  1.62it/s]Extractor Estimating: 40it [00:24,  1.61it/s]Extractor Estimating: 41it [00:25,  1.65it/s]Extractor Estimating: 42it [00:25,  1.63it/s]Extractor Estimating: 43it [00:26,  1.67it/s]Extractor Estimating: 44it [00:26,  1.66it/s]Extractor Estimating: 45it [00:27,  1.63it/s]Extractor Estimating: 46it [00:28,  1.59it/s]Extractor Estimating: 47it [00:28,  1.55it/s]Extractor Estimating: 48it [00:29,  1.55it/s]Extractor Estimating: 49it [00:30,  1.58it/s]Extractor Estimating: 50it [00:30,  1.59it/s]Extractor Estimating: 51it [00:31,  1.60it/s]Extractor Estimating: 52it [00:31,  1.65it/s]Extractor Estimating: 53it [00:32,  1.62it/s]Extractor Estimating: 54it [00:33,  1.63it/s]Extractor Estimating: 55it [00:33,  1.69it/s]Extractor Estimating: 56it [00:34,  1.72it/s]Extractor Estimating: 57it [00:34,  1.82it/s]Extractor Estimating: 58it [00:35,  1.81it/s]Extractor Estimating: 59it [00:35,  1.77it/s]Extractor Estimating: 60it [00:36,  1.80it/s]Extractor Estimating: 61it [00:36,  1.83it/s]Extractor Estimating: 62it [00:37,  1.80it/s]Extractor Estimating: 63it [00:38,  1.84it/s]Extractor Estimating: 64it [00:38,  1.70it/s]Extractor Estimating: 65it [00:39,  1.76it/s]Extractor Estimating: 66it [00:39,  1.77it/s]Extractor Estimating: 67it [00:40,  1.79it/s]Extractor Estimating: 68it [00:40,  1.79it/s]Extractor Estimating: 69it [00:41,  1.81it/s]Extractor Estimating: 70it [00:41,  1.84it/s]Extractor Estimating: 71it [00:42,  1.84it/s]Extractor Estimating: 72it [00:43,  1.88it/s]Extractor Estimating: 73it [00:43,  1.87it/s]Extractor Estimating: 74it [00:44,  1.85it/s]Extractor Estimating: 75it [00:44,  1.83it/s]Extractor Estimating: 76it [00:45,  1.75it/s]Extractor Estimating: 77it [00:45,  1.66it/s]Extractor Estimating: 78it [00:46,  1.60it/s]Extractor Estimating: 79it [00:47,  1.58it/s]Extractor Estimating: 80it [00:47,  1.58it/s]Extractor Estimating: 81it [00:48,  1.47it/s]Extractor Estimating: 82it [00:49,  1.48it/s]Extractor Estimating: 83it [00:50,  1.52it/s]Extractor Estimating: 84it [00:50,  1.49it/s]Extractor Estimating: 85it [00:51,  1.50it/s]Extractor Estimating: 86it [00:52,  1.52it/s]Extractor Estimating: 87it [00:52,  1.50it/s]Extractor Estimating: 88it [00:53,  1.44it/s]Extractor Estimating: 89it [00:54,  1.46it/s]Extractor Estimating: 90it [00:54,  1.43it/s]Extractor Estimating: 91it [00:55,  1.41it/s]Extractor Estimating: 92it [00:56,  1.44it/s]Extractor Estimating: 93it [00:56,  1.45it/s]Extractor Estimating: 94it [00:57,  1.46it/s]Extractor Estimating: 95it [00:58,  1.46it/s]Extractor Estimating: 96it [00:59,  1.37it/s]Extractor Estimating: 97it [00:59,  1.37it/s]Extractor Estimating: 98it [01:00,  1.38it/s]Extractor Estimating: 99it [01:01,  1.41it/s]Extractor Estimating: 100it [01:01,  1.47it/s]Extractor Estimating: 101it [01:02,  1.48it/s]Extractor Estimating: 102it [01:03,  1.45it/s]Extractor Estimating: 103it [01:03,  1.50it/s]Extractor Estimating: 104it [01:04,  1.54it/s]Extractor Estimating: 105it [01:05,  1.62it/s]Extractor Estimating: 106it [01:05,  1.57it/s]Extractor Estimating: 107it [01:06,  1.56it/s]Extractor Estimating: 108it [01:06,  1.61it/s]Extractor Estimating: 109it [01:07,  1.64it/s]Extractor Estimating: 110it [01:08,  1.61it/s]Extractor Estimating: 111it [01:08,  1.62it/s]Extractor Estimating: 112it [01:09,  1.54it/s]Extractor Estimating: 113it [01:10,  1.48it/s]Extractor Estimating: 114it [01:10,  1.47it/s]Extractor Estimating: 115it [01:11,  1.50it/s]Extractor Estimating: 116it [01:12,  1.51it/s]Extractor Estimating: 117it [01:12,  1.57it/s]Extractor Estimating: 118it [01:13,  1.59it/s]Extractor Estimating: 119it [01:14,  1.59it/s]Extractor Estimating: 120it [01:14,  1.51it/s]Extractor Estimating: 121it [01:15,  1.59it/s]Extractor Estimating: 122it [01:15,  1.56it/s]Extractor Estimating: 123it [01:16,  1.54it/s]Extractor Estimating: 124it [01:17,  1.55it/s]Extractor Estimating: 125it [01:17,  1.59it/s]Extractor Estimating: 126it [01:18,  1.61it/s]Extractor Estimating: 127it [01:19,  1.61it/s]Extractor Estimating: 128it [01:19,  1.62it/s]Extractor Estimating: 129it [01:20,  1.58it/s]Extractor Estimating: 130it [01:20,  1.59it/s]Extractor Estimating: 131it [01:21,  1.59it/s]Extractor Estimating: 132it [01:22,  1.56it/s]Extractor Estimating: 133it [01:22,  1.62it/s]Extractor Estimating: 134it [01:23,  1.62it/s]Extractor Estimating: 135it [01:24,  1.59it/s]Extractor Estimating: 136it [01:24,  1.52it/s]Extractor Estimating: 137it [01:25,  1.53it/s]Extractor Estimating: 138it [01:26,  1.54it/s]Extractor Estimating: 139it [01:26,  1.54it/s]Extractor Estimating: 140it [01:27,  1.58it/s]Extractor Estimating: 141it [01:28,  1.51it/s]Extractor Estimating: 142it [01:28,  1.51it/s]Extractor Estimating: 143it [01:29,  1.55it/s]Extractor Estimating: 144it [01:30,  1.52it/s]Extractor Estimating: 145it [01:30,  1.54it/s]Extractor Estimating: 146it [01:31,  1.57it/s]Extractor Estimating: 147it [01:31,  1.55it/s]Extractor Estimating: 148it [01:32,  1.57it/s]Extractor Estimating: 149it [01:33,  1.58it/s]Extractor Estimating: 150it [01:33,  1.58it/s]Extractor Estimating: 151it [01:34,  1.57it/s]Extractor Estimating: 152it [01:35,  1.49it/s]Extractor Estimating: 153it [01:35,  1.53it/s]Extractor Estimating: 154it [01:36,  1.53it/s]Extractor Estimating: 155it [01:37,  1.54it/s]Extractor Estimating: 156it [01:37,  1.58it/s]Extractor Estimating: 157it [01:38,  1.57it/s]Extractor Estimating: 158it [01:38,  1.59it/s]Extractor Estimating: 159it [01:39,  1.61it/s]Extractor Estimating: 160it [01:40,  1.57it/s]Extractor Estimating: 161it [01:40,  1.50it/s]Extractor Estimating: 162it [01:41,  1.51it/s]Extractor Estimating: 163it [01:42,  1.52it/s]Extractor Estimating: 164it [01:42,  1.51it/s]Extractor Estimating: 165it [01:43,  1.43it/s]Extractor Estimating: 166it [01:44,  1.45it/s]Extractor Estimating: 167it [01:45,  1.48it/s]Extractor Estimating: 168it [01:45,  1.40it/s]Extractor Estimating: 169it [01:46,  1.43it/s]Extractor Estimating: 170it [01:47,  1.48it/s]Extractor Estimating: 171it [01:47,  1.48it/s]Extractor Estimating: 172it [01:48,  1.51it/s]Extractor Estimating: 173it [01:49,  1.55it/s]Extractor Estimating: 174it [01:49,  1.54it/s]Extractor Estimating: 175it [01:50,  1.51it/s]Extractor Estimating: 176it [01:51,  1.49it/s]Extractor Estimating: 177it [01:51,  1.50it/s]Extractor Estimating: 178it [01:52,  1.56it/s]Extractor Estimating: 179it [01:53,  1.52it/s]Extractor Estimating: 180it [01:53,  1.50it/s]Extractor Estimating: 181it [01:54,  1.49it/s]Extractor Estimating: 182it [01:55,  1.48it/s]Extractor Estimating: 183it [01:55,  1.54it/s]Extractor Estimating: 184it [01:56,  1.56it/s]Extractor Estimating: 185it [01:56,  1.55it/s]Extractor Estimating: 186it [01:57,  1.61it/s]Extractor Estimating: 187it [01:58,  1.60it/s]Extractor Estimating: 188it [01:58,  1.55it/s]Extractor Estimating: 189it [01:59,  1.55it/s]Extractor Estimating: 190it [02:00,  1.58it/s]Extractor Estimating: 191it [02:00,  1.55it/s]Extractor Estimating: 192it [02:01,  1.44it/s]Extractor Estimating: 193it [02:02,  1.45it/s]Extractor Estimating: 194it [02:02,  1.45it/s]Extractor Estimating: 195it [02:03,  1.49it/s]Extractor Estimating: 196it [02:04,  1.51it/s]Extractor Estimating: 197it [02:04,  1.54it/s]Extractor Estimating: 198it [02:05,  1.51it/s]Extractor Estimating: 199it [02:06,  1.49it/s]Extractor Estimating: 200it [02:06,  1.50it/s]Extractor Estimating: 201it [02:07,  1.60it/s]Extractor Estimating: 202it [02:07,  1.67it/s]Extractor Estimating: 203it [02:08,  1.75it/s]Extractor Estimating: 204it [02:09,  1.72it/s]Extractor Estimating: 205it [02:09,  1.71it/s]Extractor Estimating: 206it [02:10,  1.76it/s]Extractor Estimating: 207it [02:10,  1.83it/s]Extractor Estimating: 208it [02:11,  1.87it/s]Extractor Estimating: 209it [02:11,  1.80it/s]Extractor Estimating: 210it [02:12,  1.69it/s]Extractor Estimating: 211it [02:13,  1.66it/s]Extractor Estimating: 212it [02:13,  1.71it/s]Extractor Estimating: 213it [02:14,  1.77it/s]Extractor Estimating: 214it [02:14,  1.61it/s]Extractor Estimating: 215it [02:15,  1.54it/s]Extractor Estimating: 216it [02:16,  1.62it/s]Extractor Estimating: 217it [02:16,  1.68it/s]Extractor Estimating: 218it [02:17,  1.66it/s]Extractor Estimating: 219it [02:17,  1.71it/s]Extractor Estimating: 220it [02:18,  1.71it/s]Extractor Estimating: 221it [02:19,  1.73it/s]Extractor Estimating: 222it [02:19,  1.74it/s]Extractor Estimating: 223it [02:20,  1.68it/s]Extractor Estimating: 224it [02:20,  1.74it/s]Extractor Estimating: 225it [02:21,  1.80it/s]Extractor Estimating: 226it [02:21,  1.84it/s]Extractor Estimating: 227it [02:22,  1.91it/s]Extractor Estimating: 228it [02:22,  1.88it/s]Extractor Estimating: 229it [02:23,  1.84it/s]Extractor Estimating: 230it [02:23,  1.82it/s]Extractor Estimating: 231it [02:24,  1.81it/s]Extractor Estimating: 232it [02:25,  1.86it/s]Extractor Estimating: 233it [02:25,  1.85it/s]Extractor Estimating: 234it [02:26,  1.82it/s]Extractor Estimating: 235it [02:26,  1.84it/s]Extractor Estimating: 236it [02:27,  1.84it/s]Extractor Estimating: 237it [02:27,  1.84it/s]Extractor Estimating: 238it [02:28,  1.75it/s]Extractor Estimating: 239it [02:28,  1.77it/s]Extractor Estimating: 240it [02:29,  1.80it/s]Extractor Estimating: 241it [02:30,  1.80it/s]Extractor Estimating: 242it [02:30,  1.84it/s]Extractor Estimating: 243it [02:31,  1.75it/s]Extractor Estimating: 244it [02:31,  1.75it/s]Extractor Estimating: 245it [02:32,  1.82it/s]Extractor Estimating: 246it [02:32,  1.80it/s]Extractor Estimating: 247it [02:33,  1.69it/s]Extractor Estimating: 248it [02:33,  1.78it/s]Extractor Estimating: 249it [02:34,  1.77it/s]Extractor Estimating: 250it [02:35,  1.82it/s]Extractor Estimating: 251it [02:35,  1.87it/s]Extractor Estimating: 252it [02:36,  1.84it/s]Extractor Estimating: 253it [02:36,  1.79it/s]Extractor Estimating: 254it [02:37,  1.81it/s]Extractor Estimating: 255it [02:37,  1.82it/s]Extractor Estimating: 256it [02:38,  1.80it/s]Extractor Estimating: 257it [02:38,  1.79it/s]Extractor Estimating: 258it [02:39,  1.84it/s]Extractor Estimating: 259it [02:39,  1.85it/s]Extractor Estimating: 260it [02:40,  1.62it/s]Extractor Estimating: 261it [02:41,  1.70it/s]Extractor Estimating: 262it [02:41,  1.75it/s]Extractor Estimating: 263it [02:42,  1.81it/s]Extractor Estimating: 264it [02:42,  1.80it/s]Extractor Estimating: 265it [02:43,  1.83it/s]Extractor Estimating: 266it [02:43,  1.86it/s]Extractor Estimating: 267it [02:44,  1.83it/s]Extractor Estimating: 268it [02:45,  1.80it/s]Extractor Estimating: 269it [02:45,  1.81it/s]Extractor Estimating: 270it [02:46,  1.82it/s]Extractor Estimating: 271it [02:46,  1.82it/s]Extractor Estimating: 272it [02:47,  1.84it/s]Extractor Estimating: 273it [02:47,  1.81it/s]Extractor Estimating: 274it [02:48,  1.83it/s]Extractor Estimating: 275it [02:49,  1.75it/s]Extractor Estimating: 276it [02:49,  1.67it/s]Extractor Estimating: 277it [02:50,  1.60it/s]Extractor Estimating: 278it [02:51,  1.55it/s]Extractor Estimating: 279it [02:51,  1.54it/s]Extractor Estimating: 280it [02:52,  1.55it/s]Extractor Estimating: 281it [02:52,  1.55it/s]Extractor Estimating: 282it [02:53,  1.52it/s]Extractor Estimating: 283it [02:54,  1.55it/s]Extractor Estimating: 284it [02:54,  1.52it/s]Extractor Estimating: 285it [02:55,  1.49it/s]Extractor Estimating: 286it [02:56,  1.51it/s]Extractor Estimating: 287it [02:56,  1.50it/s]Extractor Estimating: 288it [02:57,  1.54it/s]Extractor Estimating: 289it [02:58,  1.51it/s]Extractor Estimating: 290it [02:59,  1.45it/s]Extractor Estimating: 291it [02:59,  1.43it/s]Extractor Estimating: 292it [03:00,  1.46it/s]Extractor Estimating: 293it [03:01,  1.47it/s]Extractor Estimating: 294it [03:01,  1.48it/s]Extractor Estimating: 295it [03:02,  1.50it/s]Extractor Estimating: 296it [03:03,  1.49it/s]Extractor Estimating: 297it [03:03,  1.49it/s]Extractor Estimating: 298it [03:04,  1.50it/s]Extractor Estimating: 299it [03:05,  1.51it/s]Extractor Estimating: 300it [03:06,  1.33it/s]Extractor Estimating: 301it [03:06,  1.44it/s]Extractor Estimating: 302it [03:07,  1.48it/s]Extractor Estimating: 303it [03:07,  1.59it/s]Extractor Estimating: 304it [03:08,  1.62it/s]Extractor Estimating: 305it [03:08,  1.65it/s]Extractor Estimating: 306it [03:09,  1.71it/s]Extractor Estimating: 307it [03:10,  1.58it/s]Extractor Estimating: 308it [03:10,  1.66it/s]Extractor Estimating: 309it [03:11,  1.68it/s]Extractor Estimating: 310it [03:11,  1.74it/s]Extractor Estimating: 311it [03:12,  1.77it/s]Extractor Estimating: 312it [03:12,  1.79it/s]Extractor Estimating: 313it [03:13,  1.74it/s]Extractor Estimating: 314it [03:14,  1.76it/s]Extractor Estimating: 315it [03:14,  1.59it/s]Extractor Estimating: 316it [03:15,  1.64it/s]Extractor Estimating: 317it [03:16,  1.64it/s]Extractor Estimating: 318it [03:16,  1.55it/s]Extractor Estimating: 319it [03:17,  1.61it/s]Extractor Estimating: 320it [03:17,  1.62it/s]Extractor Estimating: 321it [03:18,  1.67it/s]Extractor Estimating: 322it [03:19,  1.69it/s]Extractor Estimating: 323it [03:19,  1.66it/s]Extractor Estimating: 324it [03:20,  1.68it/s]Extractor Estimating: 325it [03:20,  1.66it/s]Extractor Estimating: 326it [03:21,  1.70it/s]Extractor Estimating: 327it [03:22,  1.65it/s]Extractor Estimating: 328it [03:22,  1.71it/s]Extractor Estimating: 329it [03:23,  1.69it/s]Extractor Estimating: 330it [03:23,  1.69it/s]Extractor Estimating: 331it [03:24,  1.65it/s]Extractor Estimating: 332it [03:25,  1.65it/s]Extractor Estimating: 333it [03:25,  1.63it/s]Extractor Estimating: 334it [03:26,  1.61it/s]Extractor Estimating: 335it [03:26,  1.69it/s]Extractor Estimating: 336it [03:27,  1.70it/s]Extractor Estimating: 337it [03:27,  1.73it/s]Extractor Estimating: 338it [03:28,  1.67it/s]Extractor Estimating: 339it [03:29,  1.62it/s]Extractor Estimating: 340it [03:29,  1.69it/s]Extractor Estimating: 341it [03:30,  1.73it/s]Extractor Estimating: 342it [03:30,  1.70it/s]Extractor Estimating: 343it [03:31,  1.67it/s]Extractor Estimating: 344it [03:32,  1.67it/s]Extractor Estimating: 345it [03:32,  1.68it/s]Extractor Estimating: 346it [03:33,  1.71it/s]Extractor Estimating: 347it [03:33,  1.67it/s]Extractor Estimating: 348it [03:34,  1.73it/s]Extractor Estimating: 349it [03:35,  1.70it/s]Extractor Estimating: 350it [03:35,  1.69it/s]Extractor Estimating: 351it [03:36,  1.62it/s]Extractor Estimating: 352it [03:37,  1.62it/s]Extractor Estimating: 353it [03:37,  1.44it/s]Extractor Estimating: 354it [03:38,  1.48it/s]Extractor Estimating: 355it [03:39,  1.51it/s]Extractor Estimating: 356it [03:39,  1.52it/s]Extractor Estimating: 357it [03:40,  1.36it/s]Extractor Estimating: 358it [03:41,  1.44it/s]Extractor Estimating: 359it [03:42,  1.45it/s]Extractor Estimating: 360it [03:42,  1.46it/s]Extractor Estimating: 361it [03:43,  1.47it/s]Extractor Estimating: 362it [03:43,  1.53it/s]Extractor Estimating: 363it [03:44,  1.57it/s]Extractor Estimating: 364it [03:45,  1.66it/s]Extractor Estimating: 365it [03:45,  1.68it/s]Extractor Estimating: 366it [03:46,  1.62it/s]Extractor Estimating: 367it [03:46,  1.63it/s]Extractor Estimating: 368it [03:47,  1.53it/s]Extractor Estimating: 369it [03:48,  1.53it/s]Extractor Estimating: 370it [03:48,  1.57it/s]Extractor Estimating: 371it [03:49,  1.52it/s]Extractor Estimating: 372it [03:50,  1.57it/s]Extractor Estimating: 373it [03:50,  1.62it/s]Extractor Estimating: 374it [03:51,  1.58it/s]Extractor Estimating: 375it [03:51,  1.74it/s]Extractor Estimating: 375it [03:51,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:55,372 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:55,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:55,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:55,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:55,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:49:56,006 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:49:56,007 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:49:56,630 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:49:57,687 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:49:57,687 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:00,521 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:00,526 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:00,526 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:00,526 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:00,526 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:50:01,160 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:50:01,161 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:50:01,727 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:50:01,882 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:50:01,882 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 11:18:48,066 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 11:18:48,734 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7500 mean pseudo reward: 0.9244769431260438
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl'}
train vocab size: 21572
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21672, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21672, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.094, loss:759.3944
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.083, loss:733.7734
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.096, loss:756.5220
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.103, loss:683.3688
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.082, loss:697.4720
>> valid entity prec:0.5353, rec:0.5756, f1:0.5547
>> valid relation prec:0.4151, rec:0.0669, f1:0.1153
>> valid relation with NER prec:0.4151, rec:0.0669, f1:0.1153
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.866, loss:745.9143
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.077, loss:669.6885
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.091, loss:713.4627
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.086, loss:726.3357
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.110, loss:656.6003
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4813, rec:0.5526, f1:0.5145
>> valid relation prec:0.4097, rec:0.0678, f1:0.1163
>> valid relation with NER prec:0.4097, rec:0.0678, f1:0.1163
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.870, loss:691.5789
g_step 1200, step 261, avg_time 1.095, loss:688.8852
g_step 1300, step 48, avg_time 1.076, loss:666.7915
g_step 1400, step 148, avg_time 1.094, loss:659.0363
g_step 1500, step 248, avg_time 1.079, loss:658.7627
>> valid entity prec:0.4927, rec:0.6386, f1:0.5562
>> valid relation prec:0.5337, rec:0.0783, f1:0.1365
>> valid relation with NER prec:0.5337, rec:0.0783, f1:0.1365
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.867, loss:618.3403
g_step 1700, step 135, avg_time 1.084, loss:604.8764
g_step 1800, step 235, avg_time 1.078, loss:623.0970
g_step 1900, step 22, avg_time 1.070, loss:629.6729
g_step 2000, step 122, avg_time 1.085, loss:588.3921
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5639, rec:0.5531, f1:0.5585
>> valid relation prec:0.4110, rec:0.0803, f1:0.1344
>> valid relation with NER prec:0.4110, rec:0.0803, f1:0.1344
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 222, avg_time 2.858, loss:587.8155
g_step 2200, step 9, avg_time 1.099, loss:585.4294
g_step 2300, step 109, avg_time 1.080, loss:551.0791
g_step 2400, step 209, avg_time 1.093, loss:578.7648
g_step 2500, step 309, avg_time 1.087, loss:576.8400
>> valid entity prec:0.5603, rec:0.4966, f1:0.5265
>> valid relation prec:0.4112, rec:0.0949, f1:0.1543
>> valid relation with NER prec:0.4112, rec:0.0949, f1:0.1543
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.855, loss:531.3648
g_step 2700, step 196, avg_time 1.084, loss:552.7019
g_step 2800, step 296, avg_time 1.096, loss:541.4370
g_step 2900, step 83, avg_time 1.079, loss:520.2333
g_step 3000, step 183, avg_time 1.099, loss:522.1453
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6091, rec:0.3821, f1:0.4696
>> valid relation prec:0.4431, rec:0.0859, f1:0.1439
>> valid relation with NER prec:0.4431, rec:0.0859, f1:0.1439
g_step 3100, step 283, avg_time 2.825, loss:518.2837
g_step 3200, step 70, avg_time 1.058, loss:493.2232
g_step 3300, step 170, avg_time 1.084, loss:494.2182
g_step 3400, step 270, avg_time 1.088, loss:505.8434
g_step 3500, step 57, avg_time 1.092, loss:488.5919
>> valid entity prec:0.4977, rec:0.5620, f1:0.5279
>> valid relation prec:0.3529, rec:0.1054, f1:0.1624
>> valid relation with NER prec:0.3529, rec:0.1054, f1:0.1624
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 157, avg_time 2.872, loss:463.9119
g_step 3700, step 257, avg_time 1.084, loss:491.8227
g_step 3800, step 44, avg_time 1.074, loss:471.3701
g_step 3900, step 144, avg_time 1.079, loss:454.9506
g_step 4000, step 244, avg_time 1.097, loss:482.6712
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5595, rec:0.5087, f1:0.5329
>> valid relation prec:0.3987, rec:0.1042, f1:0.1652
>> valid relation with NER prec:0.3987, rec:0.1042, f1:0.1652
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 31, avg_time 2.857, loss:448.0647
g_step 4200, step 131, avg_time 1.081, loss:439.9606
g_step 4300, step 231, avg_time 1.098, loss:463.4145
g_step 4400, step 18, avg_time 1.071, loss:444.9378
g_step 4500, step 118, avg_time 1.074, loss:432.2171
>> valid entity prec:0.5746, rec:0.4438, f1:0.5008
>> valid relation prec:0.3801, rec:0.0741, f1:0.1241
>> valid relation with NER prec:0.3801, rec:0.0741, f1:0.1241
g_step 4600, step 218, avg_time 2.865, loss:434.1741
g_step 4700, step 5, avg_time 1.079, loss:425.4339
g_step 4800, step 105, avg_time 1.076, loss:387.5583
g_step 4900, step 205, avg_time 1.072, loss:407.0240
g_step 5000, step 305, avg_time 1.099, loss:451.6545
learning rate was adjusted to 0.0008
>> valid entity prec:0.5594, rec:0.4575, f1:0.5033
>> valid relation prec:0.3134, rec:0.0799, f1:0.1273
>> valid relation with NER prec:0.3134, rec:0.0799, f1:0.1273
g_step 5100, step 92, avg_time 2.885, loss:390.0269
g_step 5200, step 192, avg_time 1.081, loss:403.5478
g_step 5300, step 292, avg_time 1.072, loss:414.6854
g_step 5400, step 79, avg_time 1.084, loss:369.9241
g_step 5500, step 179, avg_time 1.079, loss:386.0585
>> valid entity prec:0.5494, rec:0.4391, f1:0.4881
>> valid relation prec:0.3137, rec:0.0517, f1:0.0888
>> valid relation with NER prec:0.3137, rec:0.0517, f1:0.0888
g_step 5600, step 279, avg_time 2.833, loss:420.5528
g_step 5700, step 66, avg_time 1.063, loss:355.0081
g_step 5800, step 166, avg_time 1.129, loss:377.4420
g_step 5900, step 266, avg_time 1.086, loss:378.3145
g_step 6000, step 53, avg_time 1.079, loss:362.3898
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5405, rec:0.4941, f1:0.5163
>> valid relation prec:0.3703, rec:0.0756, f1:0.1255
>> valid relation with NER prec:0.3703, rec:0.0756, f1:0.1255
g_step 6100, step 153, avg_time 2.842, loss:336.6899
g_step 6200, step 253, avg_time 1.093, loss:359.9869
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 11:18:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 11:18:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_11-18-48_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 11:18:49 - WARNING - datasets.builder -   Using custom data configuration default-c3ce1274f8c63d47
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c3ce1274f8c63d47/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 11:18:53,582 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:18:53,583 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:18:53,584 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:18:53,585 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:18:53,770 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,847 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,847 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,847 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,847 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,847 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,848 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 11:18:54,490 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:18:59,770 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 11:18:59,771 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c3ce1274f8c63d47/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.56ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.81ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.50ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.67ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.02ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.25ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.45ba/s]100%|██████████| 8/8 [00:01<00:00,  5.29ba/s]100%|██████████| 8/8 [00:01<00:00,  4.19ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.86ba/s] 40%|████      | 2/5 [00:00<00:00,  3.61ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.97ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.55ba/s]100%|██████████| 5/5 [00:01<00:00,  4.00ba/s]100%|██████████| 5/5 [00:01<00:00,  3.79ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  4.30ba/s] 38%|███▊      | 3/8 [00:00<00:00,  7.91ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.27ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.98ba/s]100%|██████████| 8/8 [00:00<00:00,  9.75ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.51ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.03ba/s]100%|██████████| 5/5 [00:00<00:00,  8.79ba/s]100%|██████████| 5/5 [00:00<00:00,  7.77ba/s]
[INFO|trainer.py:414] 2023-08-29 11:19:06,167 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 11:19:06,384 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 11:19:06,384 >>   Num examples = 7519
[INFO|trainer.py:1149] 2023-08-29 11:19:06,384 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 11:19:06,384 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 11:19:06,384 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 11:19:06,384 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 11:19:06,384 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:59,  3.26it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:48,  3.44it/s]  1%|          | 4/585 [00:01<02:47,  3.47it/s]  1%|          | 5/585 [00:01<02:46,  3.48it/s]  1%|          | 6/585 [00:01<02:46,  3.49it/s]  1%|          | 7/585 [00:02<02:45,  3.49it/s]  1%|▏         | 8/585 [00:02<02:45,  3.50it/s]  2%|▏         | 9/585 [00:02<02:44,  3.50it/s]  2%|▏         | 10/585 [00:02<02:44,  3.49it/s]  2%|▏         | 11/585 [00:03<02:44,  3.49it/s]  2%|▏         | 12/585 [00:03<02:43,  3.50it/s]  2%|▏         | 13/585 [00:03<02:43,  3.50it/s]  2%|▏         | 14/585 [00:04<02:43,  3.49it/s]  3%|▎         | 15/585 [00:04<02:43,  3.49it/s]  3%|▎         | 16/585 [00:04<02:42,  3.50it/s]  3%|▎         | 17/585 [00:04<02:57,  3.19it/s]  3%|▎         | 18/585 [00:05<02:53,  3.27it/s]  3%|▎         | 19/585 [00:05<02:49,  3.34it/s]  3%|▎         | 20/585 [00:05<02:46,  3.38it/s]  4%|▎         | 21/585 [00:06<02:45,  3.41it/s]  4%|▍         | 22/585 [00:06<02:43,  3.43it/s]  4%|▍         | 23/585 [00:06<02:42,  3.45it/s]  4%|▍         | 24/585 [00:07<03:00,  3.11it/s]  4%|▍         | 25/585 [00:07<02:54,  3.21it/s]  4%|▍         | 26/585 [00:07<02:49,  3.29it/s]  5%|▍         | 27/585 [00:07<02:46,  3.35it/s]  5%|▍         | 28/585 [00:08<02:44,  3.39it/s]  5%|▍         | 29/585 [00:08<02:42,  3.42it/s]  5%|▌         | 30/585 [00:08<02:41,  3.44it/s]  5%|▌         | 31/585 [00:09<02:40,  3.46it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:38,  3.48it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:46,  3.29it/s]  6%|▌         | 36/585 [00:10<02:43,  3.35it/s]  6%|▋         | 37/585 [00:10<02:41,  3.39it/s]  6%|▋         | 38/585 [00:11<02:39,  3.42it/s]  7%|▋         | 39/585 [00:11<02:38,  3.44it/s]  7%|▋         | 40/585 [00:11<02:37,  3.45it/s]  7%|▋         | 41/585 [00:12<02:36,  3.47it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:35,  3.48it/s]  8%|▊         | 44/585 [00:12<02:35,  3.48it/s]  8%|▊         | 45/585 [00:13<02:35,  3.48it/s]  8%|▊         | 46/585 [00:13<02:34,  3.48it/s]  8%|▊         | 47/585 [00:13<02:34,  3.48it/s]  8%|▊         | 48/585 [00:14<02:34,  3.48it/s]  8%|▊         | 49/585 [00:14<02:33,  3.48it/s]  9%|▊         | 50/585 [00:14<02:33,  3.49it/s]  9%|▊         | 51/585 [00:14<02:33,  3.49it/s]  9%|▉         | 52/585 [00:15<02:40,  3.32it/s]  9%|▉         | 53/585 [00:15<02:37,  3.37it/s]  9%|▉         | 54/585 [00:15<02:35,  3.41it/s]  9%|▉         | 55/585 [00:16<02:34,  3.43it/s] 10%|▉         | 56/585 [00:16<02:33,  3.45it/s] 10%|▉         | 57/585 [00:16<02:32,  3.46it/s] 10%|▉         | 58/585 [00:16<02:31,  3.47it/s] 10%|█         | 59/585 [00:17<02:43,  3.22it/s] 10%|█         | 60/585 [00:17<02:39,  3.29it/s] 10%|█         | 61/585 [00:17<02:36,  3.35it/s] 11%|█         | 62/585 [00:18<02:34,  3.39it/s] 11%|█         | 63/585 [00:18<02:32,  3.41it/s] 11%|█         | 64/585 [00:18<02:31,  3.43it/s] 11%|█         | 65/585 [00:19<02:30,  3.45it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 69/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 70/585 [00:20<02:33,  3.36it/s] 12%|█▏        | 71/585 [00:20<02:31,  3.39it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.42it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.45it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 76/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 78/585 [00:22<02:25,  3.47it/s] 14%|█▎        | 79/585 [00:23<02:25,  3.47it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.48it/s] 14%|█▍        | 81/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 82/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 83/585 [00:24<02:24,  3.48it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.48it/s] 15%|█▍        | 85/585 [00:24<02:23,  3.48it/s] 15%|█▍        | 86/585 [00:25<02:23,  3.48it/s] 15%|█▍        | 87/585 [00:25<02:30,  3.30it/s] 15%|█▌        | 88/585 [00:25<02:28,  3.35it/s] 15%|█▌        | 89/585 [00:26<02:26,  3.39it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.42it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 92/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 93/585 [00:27<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.46it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 97/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:28<02:19,  3.47it/s] 17%|█▋        | 100/585 [00:29<02:23,  3.38it/s] 17%|█▋        | 101/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 102/585 [00:29<02:21,  3.42it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.44it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:30<02:26,  3.28it/s] 18%|█▊        | 106/585 [00:30<02:23,  3.33it/s] 18%|█▊        | 107/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.41it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.43it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.44it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.47it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.47it/s] 20%|██        | 117/585 [00:34<02:14,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 11:19:40,660 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:19:40,660 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 11:19:40,660 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.59it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.80it/s][A
  3%|▎         | 18/608 [00:00<00:12, 49.04it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.30it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.80it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.63it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.34it/s][A
  7%|▋         | 43/608 [00:00<00:11, 47.13it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.97it/s][A
  9%|▊         | 53/608 [00:01<00:13, 40.85it/s][A
 10%|▉         | 58/608 [00:01<00:12, 42.55it/s][A
 10%|█         | 63/608 [00:01<00:12, 43.77it/s][A
 11%|█         | 68/608 [00:01<00:12, 44.76it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.41it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.89it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.21it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.46it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 46.47it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.62it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.72it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.75it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.85it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.85it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.88it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.91it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.97it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.96it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.89it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.85it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.90it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.89it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 47.02it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.90it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.93it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.92it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.98it/s][A
 31%|███       | 188/608 [00:04<00:08, 46.81it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.78it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.95it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.96it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.88it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.94it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.89it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.90it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.94it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.85it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.86it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.90it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.96it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.92it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.94it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.86it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.89it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.90it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.83it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.81it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.90it/s][A
 48%|████▊     | 293/608 [00:06<00:07, 43.75it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 44.69it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.41it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.77it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.18it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.41it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.52it/s][A
 54%|█████▍    | 328/608 [00:07<00:05, 46.75it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.52it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.63it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.73it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 43.93it/s][A
 58%|█████▊    | 353/608 [00:07<00:10, 24.10it/s][A
 59%|█████▊    | 357/608 [00:08<00:10, 23.70it/s][A
 60%|█████▉    | 362/608 [00:08<00:08, 28.34it/s][A
 60%|██████    | 367/608 [00:08<00:07, 32.30it/s][A
 61%|██████    | 372/608 [00:08<00:06, 35.74it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 38.56it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 40.80it/s][A
 64%|██████▎   | 387/608 [00:08<00:05, 42.48it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 43.68it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 44.62it/s][A
 66%|██████▌   | 402/608 [00:09<00:04, 45.07it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 45.60it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.99it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 46.32it/s][A
 69%|██████▉   | 422/608 [00:09<00:03, 46.55it/s][A
 70%|███████   | 427/608 [00:09<00:03, 46.67it/s][A
 71%|███████   | 432/608 [00:09<00:03, 46.77it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 46.79it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 46.83it/s][A
 74%|███████▎  | 447/608 [00:10<00:03, 46.66it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 46.66it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 46.81it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 46.90it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 46.81it/s][A
 78%|███████▊  | 472/608 [00:10<00:02, 46.87it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 46.86it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 46.77it/s][A
 80%|████████  | 487/608 [00:10<00:02, 46.83it/s][A
 81%|████████  | 492/608 [00:10<00:02, 46.87it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 46.90it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 46.89it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 46.83it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 40.76it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 42.46it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 43.78it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.65it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.33it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.81it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 46.18it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 46.47it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 46.31it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 46.36it/s][A
 92%|█████████▏| 562/608 [00:12<00:00, 46.55it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 46.69it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 46.83it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 46.81it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 46.82it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 46.89it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 46.91it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 46.78it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 46.61it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 42.81it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 42.81it/s][A 20%|██        | 117/585 [00:47<02:14,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:19:54,778 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 11:19:55,283 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:19:59,952 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:20:00,101 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:20:00,186 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:03<1:09:30,  8.93s/it] 20%|██        | 119/585 [01:03<49:23,  6.36s/it]   21%|██        | 120/585 [01:03<35:10,  4.54s/it] 21%|██        | 121/585 [01:04<25:13,  3.26s/it] 21%|██        | 122/585 [01:04<18:17,  2.37s/it] 21%|██        | 123/585 [01:04<13:26,  1.75s/it] 21%|██        | 124/585 [01:05<10:02,  1.31s/it] 21%|██▏       | 125/585 [01:05<07:40,  1.00s/it] 22%|██▏       | 126/585 [01:05<06:01,  1.27it/s] 22%|██▏       | 127/585 [01:05<04:51,  1.57it/s] 22%|██▏       | 128/585 [01:06<04:03,  1.88it/s] 22%|██▏       | 129/585 [01:06<03:29,  2.18it/s] 22%|██▏       | 130/585 [01:06<03:17,  2.30it/s] 22%|██▏       | 131/585 [01:07<02:56,  2.57it/s] 23%|██▎       | 132/585 [01:07<02:42,  2.79it/s] 23%|██▎       | 133/585 [01:07<02:32,  2.96it/s] 23%|██▎       | 134/585 [01:07<02:25,  3.10it/s] 23%|██▎       | 135/585 [01:08<02:20,  3.21it/s] 23%|██▎       | 136/585 [01:08<02:16,  3.29it/s] 23%|██▎       | 137/585 [01:08<02:14,  3.34it/s] 24%|██▎       | 138/585 [01:09<02:12,  3.38it/s] 24%|██▍       | 139/585 [01:09<02:10,  3.41it/s] 24%|██▍       | 140/585 [01:09<02:09,  3.43it/s] 24%|██▍       | 141/585 [01:10<02:16,  3.26it/s] 24%|██▍       | 142/585 [01:10<02:13,  3.32it/s] 24%|██▍       | 143/585 [01:10<02:11,  3.36it/s] 25%|██▍       | 144/585 [01:10<02:09,  3.40it/s] 25%|██▍       | 145/585 [01:11<02:08,  3.42it/s] 25%|██▍       | 146/585 [01:11<02:07,  3.44it/s] 25%|██▌       | 147/585 [01:11<02:07,  3.45it/s] 25%|██▌       | 148/585 [01:12<02:06,  3.46it/s] 25%|██▌       | 149/585 [01:12<02:05,  3.46it/s] 26%|██▌       | 150/585 [01:12<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:12<02:05,  3.47it/s] 26%|██▌       | 152/585 [01:13<02:13,  3.24it/s] 26%|██▌       | 153/585 [01:13<02:10,  3.31it/s] 26%|██▋       | 154/585 [01:13<02:08,  3.35it/s] 26%|██▋       | 155/585 [01:14<02:06,  3.39it/s] 27%|██▋       | 156/585 [01:14<02:05,  3.42it/s] 27%|██▋       | 157/585 [01:14<02:04,  3.43it/s] 27%|██▋       | 158/585 [01:15<02:03,  3.45it/s] 27%|██▋       | 159/585 [01:15<02:03,  3.46it/s] 27%|██▋       | 160/585 [01:15<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:15<02:02,  3.47it/s] 28%|██▊       | 162/585 [01:16<02:02,  3.47it/s] 28%|██▊       | 163/585 [01:16<02:06,  3.34it/s] 28%|██▊       | 164/585 [01:16<02:04,  3.38it/s] 28%|██▊       | 165/585 [01:17<02:03,  3.41it/s] 28%|██▊       | 166/585 [01:17<02:02,  3.43it/s] 29%|██▊       | 167/585 [01:17<02:01,  3.44it/s] 29%|██▊       | 168/585 [01:17<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:18<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:18<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:18<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:19<01:59,  3.47it/s] 30%|██▉       | 173/585 [01:19<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:19<01:58,  3.47it/s] 30%|██▉       | 175/585 [01:19<01:58,  3.47it/s] 30%|███       | 176/585 [01:20<01:57,  3.47it/s] 30%|███       | 177/585 [01:20<01:57,  3.47it/s] 30%|███       | 178/585 [01:20<01:57,  3.47it/s] 31%|███       | 179/585 [01:21<01:56,  3.47it/s] 31%|███       | 180/585 [01:21<01:56,  3.47it/s] 31%|███       | 181/585 [01:21<02:01,  3.33it/s] 31%|███       | 182/585 [01:22<01:59,  3.38it/s] 31%|███▏      | 183/585 [01:22<01:58,  3.41it/s] 31%|███▏      | 184/585 [01:22<01:57,  3.43it/s] 32%|███▏      | 185/585 [01:22<01:56,  3.44it/s] 32%|███▏      | 186/585 [01:23<01:55,  3.45it/s] 32%|███▏      | 187/585 [01:23<01:55,  3.45it/s] 32%|███▏      | 188/585 [01:23<01:54,  3.46it/s] 32%|███▏      | 189/585 [01:24<01:54,  3.46it/s] 32%|███▏      | 190/585 [01:24<01:53,  3.47it/s] 33%|███▎      | 191/585 [01:24<01:53,  3.47it/s] 33%|███▎      | 192/585 [01:24<01:55,  3.41it/s] 33%|███▎      | 193/585 [01:25<01:54,  3.42it/s] 33%|███▎      | 194/585 [01:25<01:53,  3.44it/s] 33%|███▎      | 195/585 [01:25<01:53,  3.45it/s] 34%|███▎      | 196/585 [01:26<01:52,  3.45it/s] 34%|███▎      | 197/585 [01:26<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:26<01:51,  3.46it/s] 34%|███▍      | 199/585 [01:26<01:51,  3.47it/s] 34%|███▍      | 200/585 [01:27<01:50,  3.47it/s] 34%|███▍      | 201/585 [01:27<01:50,  3.47it/s] 35%|███▍      | 202/585 [01:27<01:50,  3.47it/s] 35%|███▍      | 203/585 [01:28<02:00,  3.17it/s] 35%|███▍      | 204/585 [01:28<01:57,  3.25it/s] 35%|███▌      | 205/585 [01:28<01:54,  3.32it/s] 35%|███▌      | 206/585 [01:29<01:52,  3.37it/s] 35%|███▌      | 207/585 [01:29<01:57,  3.20it/s] 36%|███▌      | 208/585 [01:29<01:55,  3.27it/s] 36%|███▌      | 209/585 [01:29<01:52,  3.33it/s] 36%|███▌      | 210/585 [01:30<01:51,  3.37it/s] 36%|███▌      | 211/585 [01:30<01:49,  3.40it/s] 36%|███▌      | 212/585 [01:30<01:49,  3.42it/s] 36%|███▋      | 213/585 [01:31<01:48,  3.44it/s] 37%|███▋      | 214/585 [01:31<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:31<01:47,  3.46it/s] 37%|███▋      | 216/585 [01:32<01:54,  3.21it/s] 37%|███▋      | 217/585 [01:32<01:52,  3.29it/s] 37%|███▋      | 218/585 [01:32<01:49,  3.34it/s] 37%|███▋      | 219/585 [01:32<01:48,  3.38it/s] 38%|███▊      | 220/585 [01:33<01:47,  3.40it/s] 38%|███▊      | 221/585 [01:33<01:46,  3.43it/s] 38%|███▊      | 222/585 [01:33<01:45,  3.44it/s] 38%|███▊      | 223/585 [01:34<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:34<01:44,  3.46it/s] 38%|███▊      | 225/585 [01:34<01:43,  3.46it/s] 39%|███▊      | 226/585 [01:34<01:43,  3.47it/s] 39%|███▉      | 227/585 [01:35<01:48,  3.30it/s] 39%|███▉      | 228/585 [01:35<01:46,  3.35it/s] 39%|███▉      | 229/585 [01:35<01:45,  3.39it/s] 39%|███▉      | 230/585 [01:36<01:44,  3.41it/s] 39%|███▉      | 231/585 [01:36<01:43,  3.43it/s] 40%|███▉      | 232/585 [01:36<01:42,  3.44it/s] 40%|███▉      | 233/585 [01:36<01:41,  3.45it/s] 40%|████      | 234/585 [01:37<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 11:20:43,790 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:20:43,790 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 11:20:43,790 >>   Batch size = 8
{'eval_loss': 0.9966377019882202, 'eval_runtime': 13.5514, 'eval_samples_per_second': 358.636, 'eval_steps_per_second': 44.866, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.48it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.57it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.81it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.05it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.64it/s][A
  5%|▌         | 33/608 [00:00<00:13, 43.13it/s][A
  6%|▋         | 38/608 [00:00<00:12, 44.18it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.05it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.61it/s][A
  9%|▊         | 53/608 [00:01<00:12, 46.06it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.39it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.58it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.61it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.53it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.63it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.79it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.75it/s][A
 15%|█▌        | 93/608 [00:01<00:10, 46.82it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.84it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.91it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.95it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.99it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.84it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.82it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.84it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.84it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.79it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.91it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.86it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.88it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.95it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.88it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.85it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.41it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.90it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.23it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.42it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.58it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.68it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 43.59it/s][A
 34%|███▍      | 208/608 [00:04<00:14, 28.21it/s][A
 35%|███▍      | 212/608 [00:04<00:14, 27.30it/s][A
 36%|███▌      | 216/608 [00:04<00:13, 29.17it/s][A
 36%|███▋      | 221/608 [00:05<00:11, 33.29it/s][A
 37%|███▋      | 226/608 [00:05<00:10, 36.65it/s][A
 38%|███▊      | 231/608 [00:05<00:09, 39.35it/s][A
 39%|███▉      | 236/608 [00:05<00:08, 41.41it/s][A
 40%|███▉      | 241/608 [00:05<00:08, 43.01it/s][A
 40%|████      | 246/608 [00:05<00:08, 44.14it/s][A
 41%|████▏     | 251/608 [00:05<00:07, 45.01it/s][A
 42%|████▏     | 256/608 [00:05<00:07, 45.48it/s][A
 43%|████▎     | 261/608 [00:05<00:07, 45.66it/s][A
 44%|████▍     | 266/608 [00:06<00:07, 46.05it/s][A
 45%|████▍     | 271/608 [00:06<00:07, 46.22it/s][A
 45%|████▌     | 276/608 [00:06<00:07, 46.46it/s][A
 46%|████▌     | 281/608 [00:06<00:07, 46.53it/s][A
 47%|████▋     | 286/608 [00:06<00:06, 46.75it/s][A
 48%|████▊     | 291/608 [00:06<00:06, 46.81it/s][A
 49%|████▊     | 296/608 [00:06<00:06, 46.89it/s][A
 50%|████▉     | 301/608 [00:06<00:06, 45.23it/s][A
 50%|█████     | 306/608 [00:06<00:06, 45.75it/s][A
 51%|█████     | 311/608 [00:07<00:06, 46.08it/s][A
 52%|█████▏    | 316/608 [00:07<00:06, 46.30it/s][A
 53%|█████▎    | 321/608 [00:07<00:06, 46.47it/s][A
 54%|█████▎    | 326/608 [00:07<00:06, 46.59it/s][A
 54%|█████▍    | 331/608 [00:07<00:05, 46.74it/s][A
 55%|█████▌    | 336/608 [00:07<00:05, 46.78it/s][A
 56%|█████▌    | 341/608 [00:07<00:05, 46.71it/s][A
 57%|█████▋    | 346/608 [00:07<00:05, 46.71it/s][A
 58%|█████▊    | 351/608 [00:07<00:05, 46.75it/s][A
 59%|█████▊    | 356/608 [00:07<00:05, 46.79it/s][A
 59%|█████▉    | 361/608 [00:08<00:05, 46.87it/s][A
 60%|██████    | 366/608 [00:08<00:05, 46.79it/s][A
 61%|██████    | 371/608 [00:08<00:05, 46.77it/s][A
 62%|██████▏   | 376/608 [00:08<00:04, 46.86it/s][A
 63%|██████▎   | 381/608 [00:08<00:04, 46.94it/s][A
 63%|██████▎   | 386/608 [00:08<00:04, 46.79it/s][A
 64%|██████▍   | 391/608 [00:08<00:04, 46.81it/s][A
 65%|██████▌   | 396/608 [00:08<00:04, 46.75it/s][A
 66%|██████▌   | 401/608 [00:08<00:04, 46.83it/s][A
 67%|██████▋   | 406/608 [00:09<00:04, 46.86it/s][A
 68%|██████▊   | 411/608 [00:09<00:04, 46.89it/s][A
 68%|██████▊   | 416/608 [00:09<00:04, 46.78it/s][A
 69%|██████▉   | 421/608 [00:09<00:03, 46.83it/s][A
 70%|███████   | 426/608 [00:09<00:03, 46.78it/s][A
 71%|███████   | 431/608 [00:09<00:03, 46.77it/s][A
 72%|███████▏  | 436/608 [00:09<00:03, 46.77it/s][A
 73%|███████▎  | 441/608 [00:09<00:03, 43.59it/s][A
 73%|███████▎  | 446/608 [00:09<00:03, 44.53it/s][A
 74%|███████▍  | 451/608 [00:10<00:03, 45.12it/s][A
 75%|███████▌  | 456/608 [00:10<00:03, 45.69it/s][A
 76%|███████▌  | 461/608 [00:10<00:03, 46.09it/s][A
 77%|███████▋  | 466/608 [00:10<00:03, 46.31it/s][A
 77%|███████▋  | 471/608 [00:10<00:02, 46.49it/s][A
 78%|███████▊  | 476/608 [00:10<00:02, 46.63it/s][A
 79%|███████▉  | 481/608 [00:10<00:02, 46.48it/s][A
 80%|███████▉  | 486/608 [00:10<00:02, 46.66it/s][A
 81%|████████  | 491/608 [00:10<00:02, 46.72it/s][A
 82%|████████▏ | 496/608 [00:10<00:02, 46.69it/s][A
 82%|████████▏ | 501/608 [00:11<00:02, 46.74it/s][A
 83%|████████▎ | 506/608 [00:11<00:02, 46.74it/s][A
 84%|████████▍ | 511/608 [00:11<00:02, 46.80it/s][A
 85%|████████▍ | 516/608 [00:11<00:01, 46.83it/s][A
 86%|████████▌ | 521/608 [00:11<00:01, 46.94it/s][A
 87%|████████▋ | 526/608 [00:11<00:01, 46.80it/s][A
 87%|████████▋ | 531/608 [00:11<00:01, 46.75it/s][A
 88%|████████▊ | 536/608 [00:11<00:01, 46.83it/s][A
 89%|████████▉ | 541/608 [00:11<00:01, 46.78it/s][A
 90%|████████▉ | 546/608 [00:12<00:01, 46.78it/s][A
 91%|█████████ | 551/608 [00:12<00:01, 46.73it/s][A
 91%|█████████▏| 556/608 [00:12<00:01, 46.76it/s][A
 92%|█████████▏| 561/608 [00:12<00:01, 46.77it/s][A
 93%|█████████▎| 566/608 [00:12<00:00, 46.79it/s][A
 94%|█████████▍| 571/608 [00:12<00:00, 46.73it/s][A
 95%|█████████▍| 576/608 [00:12<00:00, 46.88it/s][A
 96%|█████████▌| 581/608 [00:12<00:00, 43.17it/s][A
 96%|█████████▋| 586/608 [00:12<00:00, 44.25it/s][A
 97%|█████████▋| 591/608 [00:13<00:00, 44.98it/s][A
 98%|█████████▊| 596/608 [00:13<00:00, 45.55it/s][A
 99%|█████████▉| 601/608 [00:13<00:00, 45.92it/s][A
100%|█████████▉| 606/608 [00:13<00:00, 46.20it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.20it/s][A 40%|████      | 234/585 [01:50<01:41,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:20:57,594 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 11:20:57,858 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:21:02,439 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:21:02,684 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:21:02,797 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:05<50:52,  8.72s/it] 40%|████      | 236/585 [02:05<36:04,  6.20s/it] 41%|████      | 237/585 [02:06<25:40,  4.43s/it] 41%|████      | 238/585 [02:06<18:25,  3.19s/it] 41%|████      | 239/585 [02:06<13:21,  2.32s/it] 41%|████      | 240/585 [02:07<09:48,  1.71s/it] 41%|████      | 241/585 [02:07<07:20,  1.28s/it] 41%|████▏     | 242/585 [02:07<05:37,  1.02it/s] 42%|████▏     | 243/585 [02:07<04:24,  1.29it/s] 42%|████▏     | 244/585 [02:08<03:34,  1.59it/s] 42%|████▏     | 245/585 [02:08<02:58,  1.90it/s] 42%|████▏     | 246/585 [02:08<02:33,  2.20it/s] 42%|████▏     | 247/585 [02:09<02:21,  2.38it/s] 42%|████▏     | 248/585 [02:09<02:08,  2.63it/s] 43%|████▎     | 249/585 [02:09<01:58,  2.84it/s] 43%|████▎     | 250/585 [02:10<01:51,  3.01it/s] 43%|████▎     | 251/585 [02:10<01:46,  3.14it/s] 43%|████▎     | 252/585 [02:10<01:42,  3.24it/s] 43%|████▎     | 253/585 [02:10<01:40,  3.31it/s] 43%|████▎     | 254/585 [02:11<01:38,  3.36it/s] 44%|████▎     | 255/585 [02:11<01:37,  3.40it/s] 44%|████▍     | 256/585 [02:11<01:36,  3.41it/s] 44%|████▍     | 257/585 [02:12<01:35,  3.44it/s] 44%|████▍     | 258/585 [02:12<01:39,  3.29it/s] 44%|████▍     | 259/585 [02:12<01:37,  3.34it/s] 44%|████▍     | 260/585 [02:12<01:36,  3.38it/s] 45%|████▍     | 261/585 [02:13<01:35,  3.41it/s] 45%|████▍     | 262/585 [02:13<01:34,  3.43it/s] 45%|████▍     | 263/585 [02:13<01:33,  3.45it/s] 45%|████▌     | 264/585 [02:14<01:32,  3.46it/s] 45%|████▌     | 265/585 [02:14<01:32,  3.46it/s] 45%|████▌     | 266/585 [02:14<01:31,  3.47it/s] 46%|████▌     | 267/585 [02:14<01:31,  3.47it/s] 46%|████▌     | 268/585 [02:15<01:31,  3.47it/s] 46%|████▌     | 269/585 [02:15<01:32,  3.41it/s] 46%|████▌     | 270/585 [02:15<01:31,  3.43it/s] 46%|████▋     | 271/585 [02:16<01:31,  3.45it/s] 46%|████▋     | 272/585 [02:16<01:30,  3.46it/s] 47%|████▋     | 273/585 [02:16<01:30,  3.46it/s] 47%|████▋     | 274/585 [02:17<01:29,  3.47it/s] 47%|████▋     | 275/585 [02:17<01:29,  3.47it/s] 47%|████▋     | 276/585 [02:17<01:29,  3.47it/s] 47%|████▋     | 277/585 [02:17<01:28,  3.47it/s] 48%|████▊     | 278/585 [02:18<01:28,  3.47it/s] 48%|████▊     | 279/585 [02:18<01:28,  3.48it/s] 48%|████▊     | 280/585 [02:18<01:29,  3.40it/s] 48%|████▊     | 281/585 [02:19<01:28,  3.42it/s] 48%|████▊     | 282/585 [02:19<01:28,  3.44it/s] 48%|████▊     | 283/585 [02:19<01:27,  3.45it/s] 49%|████▊     | 284/585 [02:19<01:27,  3.46it/s] 49%|████▊     | 285/585 [02:20<01:26,  3.46it/s] 49%|████▉     | 286/585 [02:20<01:26,  3.47it/s] 49%|████▉     | 287/585 [02:20<01:25,  3.47it/s] 49%|████▉     | 288/585 [02:21<01:25,  3.47it/s] 49%|████▉     | 289/585 [02:21<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:21<01:24,  3.48it/s] 50%|████▉     | 291/585 [02:21<01:25,  3.46it/s] 50%|████▉     | 292/585 [02:22<01:24,  3.47it/s] 50%|█████     | 293/585 [02:22<01:24,  3.47it/s] 50%|█████     | 294/585 [02:22<01:23,  3.47it/s] 50%|█████     | 295/585 [02:23<01:23,  3.47it/s] 51%|█████     | 296/585 [02:23<01:23,  3.47it/s] 51%|█████     | 297/585 [02:23<01:22,  3.47it/s] 51%|█████     | 298/585 [02:23<01:22,  3.47it/s] 51%|█████     | 299/585 [02:24<01:22,  3.47it/s] 51%|█████▏    | 300/585 [02:24<01:22,  3.48it/s] 51%|█████▏    | 301/585 [02:24<01:21,  3.47it/s] 52%|█████▏    | 302/585 [02:25<01:23,  3.37it/s] 52%|█████▏    | 303/585 [02:25<01:22,  3.40it/s] 52%|█████▏    | 304/585 [02:25<01:22,  3.42it/s] 52%|█████▏    | 305/585 [02:25<01:21,  3.44it/s] 52%|█████▏    | 306/585 [02:26<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:26<01:20,  3.46it/s] 53%|█████▎    | 308/585 [02:26<01:19,  3.46it/s] 53%|█████▎    | 309/585 [02:27<01:19,  3.47it/s] 53%|█████▎    | 310/585 [02:27<01:19,  3.47it/s] 53%|█████▎    | 311/585 [02:27<01:18,  3.47it/s] 53%|█████▎    | 312/585 [02:28<01:18,  3.47it/s] 54%|█████▎    | 313/585 [02:28<01:20,  3.40it/s] 54%|█████▎    | 314/585 [02:28<01:19,  3.42it/s] 54%|█████▍    | 315/585 [02:28<01:18,  3.44it/s] 54%|█████▍    | 316/585 [02:29<01:18,  3.45it/s] 54%|█████▍    | 317/585 [02:29<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:29<01:17,  3.44it/s] 55%|█████▍    | 319/585 [02:30<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:30<01:19,  3.32it/s] 55%|█████▍    | 321/585 [02:30<01:18,  3.37it/s] 55%|█████▌    | 322/585 [02:30<01:17,  3.40it/s] 55%|█████▌    | 323/585 [02:31<01:16,  3.42it/s] 55%|█████▌    | 324/585 [02:31<01:15,  3.44it/s] 56%|█████▌    | 325/585 [02:31<01:15,  3.44it/s] 56%|█████▌    | 326/585 [02:32<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:32<01:14,  3.46it/s] 56%|█████▌    | 328/585 [02:32<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:32<01:13,  3.46it/s] 56%|█████▋    | 330/585 [02:33<01:13,  3.47it/s] 57%|█████▋    | 331/585 [02:33<01:13,  3.43it/s] 57%|█████▋    | 332/585 [02:33<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:34<01:12,  3.46it/s] 57%|█████▋    | 334/585 [02:34<01:12,  3.46it/s] 57%|█████▋    | 335/585 [02:34<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:34<01:11,  3.47it/s] 58%|█████▊    | 337/585 [02:35<01:11,  3.47it/s] 58%|█████▊    | 338/585 [02:35<01:11,  3.47it/s] 58%|█████▊    | 339/585 [02:35<01:10,  3.47it/s] 58%|█████▊    | 340/585 [02:36<01:10,  3.47it/s] 58%|█████▊    | 341/585 [02:36<01:10,  3.47it/s] 58%|█████▊    | 342/585 [02:36<01:11,  3.41it/s] 59%|█████▊    | 343/585 [02:37<01:10,  3.43it/s] 59%|█████▉    | 344/585 [02:37<01:10,  3.44it/s] 59%|█████▉    | 345/585 [02:37<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:37<01:09,  3.46it/s] 59%|█████▉    | 347/585 [02:38<01:08,  3.46it/s] 59%|█████▉    | 348/585 [02:38<01:08,  3.47it/s] 60%|█████▉    | 349/585 [02:38<01:08,  3.47it/s] 60%|█████▉    | 350/585 [02:39<01:07,  3.47it/s] 60%|██████    | 351/585 [02:39<01:07,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 11:21:45,856 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:21:45,865 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 11:21:45,865 >>   Batch size = 8
{'eval_loss': 1.0091280937194824, 'eval_runtime': 13.4251, 'eval_samples_per_second': 362.008, 'eval_steps_per_second': 45.288, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.56it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.50it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.77it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.18it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.74it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.59it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.29it/s][A
  7%|▋         | 43/608 [00:00<00:11, 47.18it/s][A
  8%|▊         | 48/608 [00:01<00:11, 47.07it/s][A
  9%|▊         | 53/608 [00:01<00:11, 47.00it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.87it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.93it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.85it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.87it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.89it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.85it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.83it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.73it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.80it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.86it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.67it/s][A
 19%|█▉        | 114/608 [00:02<00:10, 46.60it/s][A
 20%|█▉        | 119/608 [00:02<00:10, 46.71it/s][A
 20%|██        | 124/608 [00:02<00:10, 47.23it/s][A
 21%|██        | 129/608 [00:02<00:10, 46.16it/s][A
 22%|██▏       | 134/608 [00:02<00:10, 45.60it/s][A
 23%|██▎       | 139/608 [00:02<00:10, 46.26it/s][A
 24%|██▎       | 144/608 [00:03<00:09, 46.53it/s][A
 25%|██▍       | 149/608 [00:03<00:09, 46.64it/s][A
 25%|██▌       | 154/608 [00:03<00:09, 46.66it/s][A
 26%|██▌       | 159/608 [00:03<00:09, 46.81it/s][A
 27%|██▋       | 164/608 [00:03<00:09, 46.84it/s][A
 28%|██▊       | 169/608 [00:03<00:09, 46.85it/s][A
 29%|██▊       | 174/608 [00:03<00:09, 46.74it/s][A
 29%|██▉       | 179/608 [00:03<00:09, 46.68it/s][A
 30%|███       | 184/608 [00:03<00:09, 46.66it/s][A
 31%|███       | 189/608 [00:04<00:08, 46.69it/s][A
 32%|███▏      | 194/608 [00:04<00:08, 46.72it/s][A
 33%|███▎      | 199/608 [00:04<00:08, 46.82it/s][A
 34%|███▎      | 204/608 [00:04<00:08, 46.84it/s][A
 34%|███▍      | 209/608 [00:04<00:08, 46.80it/s][A
 35%|███▌      | 214/608 [00:04<00:08, 46.87it/s][A
 36%|███▌      | 219/608 [00:04<00:08, 46.86it/s][A
 37%|███▋      | 224/608 [00:04<00:08, 46.81it/s][A
 38%|███▊      | 229/608 [00:04<00:08, 46.76it/s][A
 38%|███▊      | 234/608 [00:04<00:08, 46.69it/s][A
 39%|███▉      | 239/608 [00:05<00:07, 46.62it/s][A
 40%|████      | 244/608 [00:05<00:07, 46.73it/s][A
 41%|████      | 249/608 [00:05<00:07, 46.83it/s][A
 42%|████▏     | 254/608 [00:05<00:07, 46.92it/s][A
 43%|████▎     | 259/608 [00:05<00:07, 46.79it/s][A
 43%|████▎     | 264/608 [00:05<00:07, 46.91it/s][A
 44%|████▍     | 269/608 [00:05<00:07, 46.86it/s][A
 45%|████▌     | 274/608 [00:05<00:07, 46.80it/s][A
 46%|████▌     | 279/608 [00:05<00:07, 46.80it/s][A
 47%|████▋     | 284/608 [00:06<00:06, 46.70it/s][A
 48%|████▊     | 289/608 [00:06<00:06, 46.72it/s][A
 48%|████▊     | 294/608 [00:06<00:06, 45.78it/s][A
 49%|████▉     | 299/608 [00:06<00:06, 46.23it/s][A
 50%|█████     | 304/608 [00:06<00:06, 46.44it/s][A
 51%|█████     | 309/608 [00:06<00:06, 46.52it/s][A
 52%|█████▏    | 314/608 [00:06<00:06, 46.70it/s][A
 52%|█████▏    | 319/608 [00:06<00:06, 46.75it/s][A
 53%|█████▎    | 324/608 [00:06<00:06, 46.72it/s][A
 54%|█████▍    | 329/608 [00:07<00:05, 46.72it/s][A
 55%|█████▍    | 334/608 [00:07<00:05, 46.65it/s][A
 56%|█████▌    | 339/608 [00:07<00:05, 46.68it/s][A
 57%|█████▋    | 344/608 [00:07<00:05, 46.66it/s][A
 57%|█████▋    | 349/608 [00:07<00:05, 46.70it/s][A
 58%|█████▊    | 354/608 [00:07<00:05, 46.75it/s][A
 59%|█████▉    | 359/608 [00:07<00:05, 46.74it/s][A
 60%|█████▉    | 364/608 [00:07<00:05, 46.71it/s][A
 61%|██████    | 369/608 [00:07<00:05, 46.82it/s][A
 62%|██████▏   | 374/608 [00:07<00:05, 46.79it/s][A
 62%|██████▏   | 379/608 [00:08<00:04, 46.80it/s][A
 63%|██████▎   | 384/608 [00:08<00:04, 46.84it/s][A
 64%|██████▍   | 389/608 [00:08<00:04, 46.78it/s][A
 65%|██████▍   | 394/608 [00:08<00:04, 46.81it/s][A
 66%|██████▌   | 399/608 [00:08<00:04, 46.79it/s][A
 66%|██████▋   | 404/608 [00:08<00:04, 46.83it/s][A
 67%|██████▋   | 409/608 [00:08<00:04, 46.75it/s][A
 68%|██████▊   | 414/608 [00:08<00:04, 46.76it/s][A
 69%|██████▉   | 419/608 [00:08<00:04, 46.80it/s][A
 70%|██████▉   | 424/608 [00:09<00:03, 46.80it/s][A
 71%|███████   | 429/608 [00:09<00:03, 46.83it/s][A
 71%|███████▏  | 434/608 [00:09<00:03, 46.77it/s][A
 72%|███████▏  | 439/608 [00:09<00:03, 46.78it/s][A
 73%|███████▎  | 444/608 [00:09<00:03, 46.74it/s][A
 74%|███████▍  | 449/608 [00:09<00:03, 46.85it/s][A
 75%|███████▍  | 454/608 [00:09<00:03, 46.70it/s][A
 75%|███████▌  | 459/608 [00:09<00:03, 46.76it/s][A
 76%|███████▋  | 464/608 [00:09<00:03, 46.75it/s][A
 77%|███████▋  | 469/608 [00:10<00:02, 46.76it/s][A
 78%|███████▊  | 474/608 [00:10<00:02, 46.74it/s][A
 79%|███████▉  | 479/608 [00:10<00:02, 46.80it/s][A
 80%|███████▉  | 484/608 [00:10<00:02, 46.39it/s][A
 80%|████████  | 489/608 [00:10<00:02, 46.88it/s][A
 81%|████████▏ | 494/608 [00:10<00:02, 46.89it/s][A
 82%|████████▏ | 499/608 [00:10<00:02, 46.88it/s][A
 83%|████████▎ | 504/608 [00:10<00:02, 46.80it/s][A
 84%|████████▎ | 509/608 [00:10<00:02, 46.64it/s][A
 85%|████████▍ | 514/608 [00:10<00:02, 46.70it/s][A
 85%|████████▌ | 519/608 [00:11<00:01, 46.67it/s][A
 86%|████████▌ | 524/608 [00:11<00:01, 46.76it/s][A
 87%|████████▋ | 529/608 [00:11<00:01, 46.78it/s][A
 88%|████████▊ | 534/608 [00:11<00:01, 46.77it/s][A
 89%|████████▊ | 539/608 [00:11<00:01, 46.72it/s][A
 89%|████████▉ | 544/608 [00:11<00:01, 46.84it/s][A
 90%|█████████ | 549/608 [00:11<00:01, 46.78it/s][A
 91%|█████████ | 554/608 [00:11<00:01, 46.84it/s][A
 92%|█████████▏| 559/608 [00:11<00:01, 46.79it/s][A
 93%|█████████▎| 564/608 [00:12<00:00, 46.71it/s][A
 94%|█████████▎| 569/608 [00:12<00:00, 46.72it/s][A
 94%|█████████▍| 574/608 [00:12<00:00, 46.77it/s][A
 95%|█████████▌| 579/608 [00:12<00:00, 46.76it/s][A
 96%|█████████▌| 584/608 [00:12<00:00, 46.77it/s][A
 97%|█████████▋| 589/608 [00:12<00:00, 46.75it/s][A
 98%|█████████▊| 594/608 [00:12<00:00, 46.77it/s][A
 99%|█████████▊| 599/608 [00:12<00:00, 46.83it/s][A
 99%|█████████▉| 604/608 [00:12<00:00, 46.80it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:12<00:00, 46.80it/s][A 60%|██████    | 351/585 [02:52<01:07,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:21:58,900 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 11:21:58,936 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:22:01,763 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:22:01,803 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:22:01,813 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:01<26:09,  6.73s/it] 60%|██████    | 353/585 [03:01<18:33,  4.80s/it] 61%|██████    | 354/585 [03:01<13:16,  3.45s/it] 61%|██████    | 355/585 [03:01<09:34,  2.50s/it] 61%|██████    | 356/585 [03:02<07:00,  1.84s/it] 61%|██████    | 357/585 [03:02<05:12,  1.37s/it] 61%|██████    | 358/585 [03:02<03:57,  1.05s/it] 61%|██████▏   | 359/585 [03:03<03:05,  1.22it/s] 62%|██████▏   | 360/585 [03:03<02:28,  1.52it/s] 62%|██████▏   | 361/585 [03:03<02:02,  1.83it/s] 62%|██████▏   | 362/585 [03:03<01:44,  2.13it/s] 62%|██████▏   | 363/585 [03:04<01:36,  2.31it/s] 62%|██████▏   | 364/585 [03:04<01:26,  2.57it/s] 62%|██████▏   | 365/585 [03:04<01:19,  2.78it/s] 63%|██████▎   | 366/585 [03:05<01:13,  2.96it/s] 63%|██████▎   | 367/585 [03:05<01:10,  3.10it/s] 63%|██████▎   | 368/585 [03:05<01:07,  3.20it/s] 63%|██████▎   | 369/585 [03:06<01:05,  3.28it/s] 63%|██████▎   | 370/585 [03:06<01:04,  3.34it/s] 63%|██████▎   | 371/585 [03:06<01:03,  3.38it/s] 64%|██████▎   | 372/585 [03:06<01:02,  3.41it/s] 64%|██████▍   | 373/585 [03:07<01:01,  3.43it/s] 64%|██████▍   | 374/585 [03:07<01:05,  3.24it/s] 64%|██████▍   | 375/585 [03:07<01:03,  3.31it/s] 64%|██████▍   | 376/585 [03:08<01:02,  3.36it/s] 64%|██████▍   | 377/585 [03:08<01:01,  3.39it/s] 65%|██████▍   | 378/585 [03:08<01:00,  3.42it/s] 65%|██████▍   | 379/585 [03:09<01:00,  3.43it/s] 65%|██████▍   | 380/585 [03:09<00:59,  3.45it/s] 65%|██████▌   | 381/585 [03:09<00:59,  3.46it/s] 65%|██████▌   | 382/585 [03:09<00:58,  3.46it/s] 65%|██████▌   | 383/585 [03:10<00:58,  3.46it/s] 66%|██████▌   | 384/585 [03:10<00:57,  3.47it/s] 66%|██████▌   | 385/585 [03:10<00:58,  3.42it/s] 66%|██████▌   | 386/585 [03:11<00:57,  3.43it/s] 66%|██████▌   | 387/585 [03:11<00:57,  3.45it/s] 66%|██████▋   | 388/585 [03:11<00:57,  3.45it/s] 66%|██████▋   | 389/585 [03:11<00:56,  3.46it/s] 67%|██████▋   | 390/585 [03:12<00:56,  3.46it/s] 67%|██████▋   | 391/585 [03:12<00:55,  3.47it/s] 67%|██████▋   | 392/585 [03:12<00:55,  3.47it/s] 67%|██████▋   | 393/585 [03:13<00:55,  3.47it/s] 67%|██████▋   | 394/585 [03:13<00:54,  3.47it/s] 68%|██████▊   | 395/585 [03:13<00:54,  3.47it/s] 68%|██████▊   | 396/585 [03:13<00:55,  3.39it/s] 68%|██████▊   | 397/585 [03:14<00:55,  3.42it/s] 68%|██████▊   | 398/585 [03:14<00:54,  3.44it/s] 68%|██████▊   | 399/585 [03:14<00:53,  3.45it/s] 68%|██████▊   | 400/585 [03:15<00:53,  3.46it/s] 69%|██████▊   | 401/585 [03:15<00:53,  3.46it/s] 69%|██████▊   | 402/585 [03:15<00:52,  3.46it/s] 69%|██████▉   | 403/585 [03:15<00:52,  3.47it/s] 69%|██████▉   | 404/585 [03:16<00:52,  3.47it/s] 69%|██████▉   | 405/585 [03:16<00:51,  3.47it/s] 69%|██████▉   | 406/585 [03:16<00:51,  3.47it/s] 70%|██████▉   | 407/585 [03:17<00:52,  3.37it/s] 70%|██████▉   | 408/585 [03:17<00:52,  3.40it/s] 70%|██████▉   | 409/585 [03:17<00:51,  3.42it/s] 70%|███████   | 410/585 [03:17<00:50,  3.44it/s] 70%|███████   | 411/585 [03:18<00:50,  3.45it/s] 70%|███████   | 412/585 [03:18<00:50,  3.46it/s] 71%|███████   | 413/585 [03:18<00:49,  3.46it/s] 71%|███████   | 414/585 [03:19<00:49,  3.47it/s] 71%|███████   | 415/585 [03:19<00:49,  3.47it/s] 71%|███████   | 416/585 [03:19<00:48,  3.47it/s] 71%|███████▏  | 417/585 [03:20<00:48,  3.47it/s] 71%|███████▏  | 418/585 [03:20<00:49,  3.35it/s] 72%|███████▏  | 419/585 [03:20<00:48,  3.39it/s] 72%|███████▏  | 420/585 [03:20<00:48,  3.42it/s] 72%|███████▏  | 421/585 [03:21<00:47,  3.43it/s] 72%|███████▏  | 422/585 [03:21<00:47,  3.44it/s] 72%|███████▏  | 423/585 [03:21<00:46,  3.45it/s] 72%|███████▏  | 424/585 [03:22<00:46,  3.46it/s] 73%|███████▎  | 425/585 [03:22<00:46,  3.46it/s] 73%|███████▎  | 426/585 [03:22<00:45,  3.46it/s] 73%|███████▎  | 427/585 [03:22<00:45,  3.47it/s] 73%|███████▎  | 428/585 [03:23<00:45,  3.47it/s] 73%|███████▎  | 429/585 [03:23<00:47,  3.27it/s] 74%|███████▎  | 430/585 [03:23<00:46,  3.33it/s] 74%|███████▎  | 431/585 [03:24<00:45,  3.37it/s] 74%|███████▍  | 432/585 [03:24<00:44,  3.40it/s] 74%|███████▍  | 433/585 [03:24<00:44,  3.42it/s] 74%|███████▍  | 434/585 [03:24<00:43,  3.44it/s] 74%|███████▍  | 435/585 [03:25<00:43,  3.45it/s] 75%|███████▍  | 436/585 [03:25<00:43,  3.46it/s] 75%|███████▍  | 437/585 [03:25<00:42,  3.46it/s] 75%|███████▍  | 438/585 [03:26<00:42,  3.47it/s] 75%|███████▌  | 439/585 [03:26<00:42,  3.47it/s] 75%|███████▌  | 440/585 [03:26<00:43,  3.35it/s] 75%|███████▌  | 441/585 [03:27<00:42,  3.38it/s] 76%|███████▌  | 442/585 [03:27<00:41,  3.41it/s] 76%|███████▌  | 443/585 [03:27<00:41,  3.43it/s] 76%|███████▌  | 444/585 [03:27<00:40,  3.44it/s] 76%|███████▌  | 445/585 [03:28<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:28<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:28<00:39,  3.46it/s] 77%|███████▋  | 448/585 [03:29<00:39,  3.46it/s] 77%|███████▋  | 449/585 [03:29<00:39,  3.47it/s] 77%|███████▋  | 450/585 [03:29<00:40,  3.34it/s] 77%|███████▋  | 451/585 [03:29<00:41,  3.24it/s] 77%|███████▋  | 452/585 [03:30<00:40,  3.31it/s] 77%|███████▋  | 453/585 [03:30<00:39,  3.35it/s] 78%|███████▊  | 454/585 [03:30<00:38,  3.39it/s] 78%|███████▊  | 455/585 [03:31<00:38,  3.41it/s] 78%|███████▊  | 456/585 [03:31<00:37,  3.43it/s] 78%|███████▊  | 457/585 [03:31<00:37,  3.44it/s] 78%|███████▊  | 458/585 [03:32<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:32<00:36,  3.46it/s] 79%|███████▊  | 460/585 [03:32<00:36,  3.46it/s] 79%|███████▉  | 461/585 [03:32<00:35,  3.46it/s] 79%|███████▉  | 462/585 [03:33<00:35,  3.47it/s] 79%|███████▉  | 463/585 [03:33<00:35,  3.47it/s] 79%|███████▉  | 464/585 [03:33<00:34,  3.47it/s] 79%|███████▉  | 465/585 [03:34<00:34,  3.47it/s] 80%|███████▉  | 466/585 [03:34<00:34,  3.47it/s] 80%|███████▉  | 467/585 [03:34<00:33,  3.47it/s] 80%|████████  | 468/585 [03:34<00:35,  3.31it/s][INFO|trainer.py:2140] 2023-08-29 11:22:41,464 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:22:41,465 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 11:22:41,465 >>   Batch size = 8
{'eval_loss': 1.0219899415969849, 'eval_runtime': 13.0137, 'eval_samples_per_second': 373.453, 'eval_steps_per_second': 46.72, 'epoch': 3.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.69it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.69it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.87it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.26it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.72it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.42it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.05it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.80it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.78it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.83it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.86it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.92it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.94it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.93it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.91it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.72it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.62it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.64it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.66it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.62it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.72it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.79it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.79it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.87it/s][A
 21%|██        | 128/608 [00:02<00:11, 42.52it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 43.71it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 44.62it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.27it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.76it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.06it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.30it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.51it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.36it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.39it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.53it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.54it/s][A
 31%|███       | 188/608 [00:04<00:08, 46.70it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.72it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.75it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.85it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.84it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.72it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.59it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.68it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.68it/s][A
 38%|███▊      | 233/608 [00:04<00:08, 46.65it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.80it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.83it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.79it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.73it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.76it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.70it/s][A
 44%|████▍     | 268/608 [00:05<00:08, 41.57it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 42.97it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 44.08it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 44.85it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.42it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.95it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.21it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.42it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.34it/s][A
 51%|█████▏    | 313/608 [00:06<00:09, 32.33it/s][A
 52%|█████▏    | 317/608 [00:07<00:11, 25.74it/s][A
 53%|█████▎    | 321/608 [00:07<00:11, 25.46it/s][A
 54%|█████▎    | 326/608 [00:07<00:09, 29.92it/s][A
 54%|█████▍    | 331/608 [00:07<00:08, 33.83it/s][A
 55%|█████▌    | 336/608 [00:07<00:07, 37.04it/s][A
 56%|█████▌    | 341/608 [00:07<00:06, 39.63it/s][A
 57%|█████▋    | 346/608 [00:07<00:06, 41.64it/s][A
 58%|█████▊    | 351/608 [00:07<00:05, 43.13it/s][A
 59%|█████▊    | 356/608 [00:08<00:05, 44.18it/s][A
 59%|█████▉    | 361/608 [00:08<00:05, 44.89it/s][A
 60%|██████    | 366/608 [00:08<00:05, 45.11it/s][A
 61%|██████    | 371/608 [00:08<00:05, 45.49it/s][A
 62%|██████▏   | 376/608 [00:08<00:05, 45.92it/s][A
 63%|██████▎   | 381/608 [00:08<00:04, 46.25it/s][A
 63%|██████▎   | 386/608 [00:08<00:04, 46.42it/s][A
 64%|██████▍   | 391/608 [00:08<00:04, 43.95it/s][A
 65%|██████▌   | 396/608 [00:08<00:04, 44.77it/s][A
 66%|██████▌   | 401/608 [00:09<00:04, 45.35it/s][A
 67%|██████▋   | 406/608 [00:09<00:04, 45.87it/s][A
 68%|██████▊   | 411/608 [00:09<00:04, 46.18it/s][A
 68%|██████▊   | 416/608 [00:09<00:04, 46.38it/s][A
 69%|██████▉   | 421/608 [00:09<00:04, 46.42it/s][A
 70%|███████   | 426/608 [00:09<00:03, 46.58it/s][A
 71%|███████   | 431/608 [00:09<00:03, 46.38it/s][A
 72%|███████▏  | 436/608 [00:09<00:03, 46.44it/s][A
 73%|███████▎  | 441/608 [00:09<00:03, 46.54it/s][A
 73%|███████▎  | 446/608 [00:10<00:03, 46.62it/s][A
 74%|███████▍  | 451/608 [00:10<00:03, 46.70it/s][A
 75%|███████▌  | 456/608 [00:10<00:03, 46.78it/s][A
 76%|███████▌  | 461/608 [00:10<00:03, 46.78it/s][A
 77%|███████▋  | 466/608 [00:10<00:03, 46.79it/s][A
 77%|███████▋  | 471/608 [00:10<00:02, 46.65it/s][A
 78%|███████▊  | 476/608 [00:10<00:02, 46.60it/s][A
 79%|███████▉  | 481/608 [00:10<00:02, 46.54it/s][A
 80%|███████▉  | 486/608 [00:10<00:02, 46.54it/s][A
 81%|████████  | 491/608 [00:10<00:02, 46.55it/s][A
 82%|████████▏ | 496/608 [00:11<00:02, 46.70it/s][A
 82%|████████▏ | 501/608 [00:11<00:02, 46.76it/s][A
 83%|████████▎ | 506/608 [00:11<00:02, 46.79it/s][A
 84%|████████▍ | 511/608 [00:11<00:02, 46.87it/s][A
 85%|████████▍ | 516/608 [00:11<00:01, 46.74it/s][A
 86%|████████▌ | 521/608 [00:11<00:01, 46.55it/s][A
 87%|████████▋ | 526/608 [00:11<00:01, 46.65it/s][A
 87%|████████▋ | 531/608 [00:11<00:01, 44.13it/s][A
 88%|████████▊ | 536/608 [00:11<00:01, 44.98it/s][A
 89%|████████▉ | 541/608 [00:12<00:01, 45.53it/s][A
 90%|████████▉ | 546/608 [00:12<00:01, 45.90it/s][A
 91%|█████████ | 551/608 [00:12<00:01, 46.19it/s][A
 91%|█████████▏| 556/608 [00:12<00:01, 46.38it/s][A
 92%|█████████▏| 561/608 [00:12<00:01, 46.49it/s][A
 93%|█████████▎| 566/608 [00:12<00:00, 46.55it/s][A
 94%|█████████▍| 571/608 [00:12<00:00, 46.16it/s][A
 95%|█████████▍| 576/608 [00:12<00:00, 46.23it/s][A
 96%|█████████▌| 581/608 [00:12<00:00, 46.38it/s][A
 96%|█████████▋| 586/608 [00:13<00:00, 46.42it/s][A
 97%|█████████▋| 591/608 [00:13<00:00, 46.56it/s][A
 98%|█████████▊| 596/608 [00:13<00:00, 46.60it/s][A
 99%|█████████▉| 601/608 [00:13<00:00, 46.69it/s][A
100%|█████████▉| 606/608 [00:13<00:00, 46.74it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.74it/s][A 80%|████████  | 468/585 [03:48<00:35,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:22:55,136 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 11:22:55,576 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:23:00,095 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:23:00,606 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:23:00,729 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:10<20:45, 10.74s/it] 80%|████████  | 470/585 [04:10<14:41,  7.67s/it] 81%|████████  | 471/585 [04:10<10:21,  5.45s/it] 81%|████████  | 472/585 [04:11<07:21,  3.90s/it] 81%|████████  | 473/585 [04:11<05:15,  2.82s/it] 81%|████████  | 474/585 [04:11<03:48,  2.06s/it] 81%|████████  | 475/585 [04:11<02:48,  1.53s/it] 81%|████████▏ | 476/585 [04:12<02:05,  1.16s/it] 82%|████████▏ | 477/585 [04:12<01:36,  1.12it/s] 82%|████████▏ | 478/585 [04:12<01:16,  1.40it/s] 82%|████████▏ | 479/585 [04:13<01:02,  1.71it/s] 82%|████████▏ | 480/585 [04:13<00:55,  1.89it/s] 82%|████████▏ | 481/585 [04:13<00:47,  2.20it/s] 82%|████████▏ | 482/585 [04:14<00:41,  2.47it/s] 83%|████████▎ | 483/585 [04:14<00:37,  2.70it/s] 83%|████████▎ | 484/585 [04:14<00:34,  2.90it/s] 83%|████████▎ | 485/585 [04:14<00:32,  3.05it/s] 83%|████████▎ | 486/585 [04:15<00:31,  3.17it/s] 83%|████████▎ | 487/585 [04:15<00:30,  3.26it/s] 83%|████████▎ | 488/585 [04:15<00:29,  3.32it/s] 84%|████████▎ | 489/585 [04:16<00:28,  3.37it/s] 84%|████████▍ | 490/585 [04:16<00:27,  3.40it/s] 84%|████████▍ | 491/585 [04:16<00:31,  3.01it/s] 84%|████████▍ | 492/585 [04:17<00:29,  3.14it/s] 84%|████████▍ | 493/585 [04:17<00:28,  3.23it/s] 84%|████████▍ | 494/585 [04:17<00:27,  3.30it/s] 85%|████████▍ | 495/585 [04:17<00:26,  3.36it/s] 85%|████████▍ | 496/585 [04:18<00:26,  3.39it/s] 85%|████████▍ | 497/585 [04:18<00:25,  3.42it/s] 85%|████████▌ | 498/585 [04:18<00:25,  3.44it/s] 85%|████████▌ | 499/585 [04:19<00:24,  3.45it/s] 85%|████████▌ | 500/585 [04:19<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [04:19<00:24,  3.46it/s] 86%|████████▌ | 501/585 [04:19<00:25,  3.25it/s] 86%|████████▌ | 502/585 [04:20<00:25,  3.32it/s] 86%|████████▌ | 503/585 [04:20<00:24,  3.36it/s] 86%|████████▌ | 504/585 [04:20<00:23,  3.39it/s] 86%|████████▋ | 505/585 [04:20<00:23,  3.42it/s] 86%|████████▋ | 506/585 [04:21<00:22,  3.44it/s] 87%|████████▋ | 507/585 [04:21<00:22,  3.45it/s] 87%|████████▋ | 508/585 [04:21<00:22,  3.46it/s] 87%|████████▋ | 509/585 [04:22<00:21,  3.46it/s] 87%|████████▋ | 510/585 [04:22<00:21,  3.47it/s] 87%|████████▋ | 511/585 [04:22<00:21,  3.47it/s] 88%|████████▊ | 512/585 [04:22<00:22,  3.30it/s] 88%|████████▊ | 513/585 [04:23<00:21,  3.35it/s] 88%|████████▊ | 514/585 [04:23<00:20,  3.39it/s] 88%|████████▊ | 515/585 [04:23<00:20,  3.41it/s] 88%|████████▊ | 516/585 [04:24<00:20,  3.43it/s] 88%|████████▊ | 517/585 [04:24<00:19,  3.45it/s] 89%|████████▊ | 518/585 [04:24<00:19,  3.45it/s] 89%|████████▊ | 519/585 [04:24<00:19,  3.46it/s] 89%|████████▉ | 520/585 [04:25<00:18,  3.46it/s] 89%|████████▉ | 521/585 [04:25<00:18,  3.47it/s] 89%|████████▉ | 522/585 [04:25<00:18,  3.47it/s] 89%|████████▉ | 523/585 [04:26<00:18,  3.32it/s] 90%|████████▉ | 524/585 [04:26<00:18,  3.37it/s] 90%|████████▉ | 525/585 [04:26<00:17,  3.40it/s] 90%|████████▉ | 526/585 [04:27<00:17,  3.42it/s] 90%|█████████ | 527/585 [04:27<00:16,  3.44it/s] 90%|█████████ | 528/585 [04:27<00:16,  3.45it/s] 90%|█████████ | 529/585 [04:27<00:16,  3.46it/s] 91%|█████████ | 530/585 [04:28<00:15,  3.46it/s] 91%|█████████ | 531/585 [04:28<00:15,  3.47it/s] 91%|█████████ | 532/585 [04:28<00:15,  3.47it/s] 91%|█████████ | 533/585 [04:29<00:14,  3.47it/s] 91%|█████████▏| 534/585 [04:29<00:15,  3.31it/s] 91%|█████████▏| 535/585 [04:29<00:15,  3.25it/s] 92%|█████████▏| 536/585 [04:29<00:14,  3.30it/s] 92%|█████████▏| 537/585 [04:30<00:14,  3.35it/s] 92%|█████████▏| 538/585 [04:30<00:13,  3.39it/s] 92%|█████████▏| 539/585 [04:30<00:13,  3.41it/s] 92%|█████████▏| 540/585 [04:31<00:13,  3.43it/s] 92%|█████████▏| 541/585 [04:31<00:12,  3.44it/s] 93%|█████████▎| 542/585 [04:31<00:12,  3.45it/s] 93%|█████████▎| 543/585 [04:32<00:12,  3.46it/s] 93%|█████████▎| 544/585 [04:32<00:11,  3.46it/s] 93%|█████████▎| 545/585 [04:32<00:12,  3.31it/s] 93%|█████████▎| 546/585 [04:32<00:11,  3.35it/s] 94%|█████████▎| 547/585 [04:33<00:11,  3.39it/s] 94%|█████████▎| 548/585 [04:33<00:10,  3.41it/s] 94%|█████████▍| 549/585 [04:33<00:10,  3.43it/s] 94%|█████████▍| 550/585 [04:34<00:10,  3.44it/s] 94%|█████████▍| 551/585 [04:34<00:09,  3.45it/s] 94%|█████████▍| 552/585 [04:34<00:09,  3.46it/s] 95%|█████████▍| 553/585 [04:34<00:09,  3.46it/s] 95%|█████████▍| 554/585 [04:35<00:08,  3.47it/s] 95%|█████████▍| 555/585 [04:35<00:08,  3.47it/s] 95%|█████████▌| 556/585 [04:35<00:08,  3.47it/s] 95%|█████████▌| 557/585 [04:36<00:08,  3.47it/s] 95%|█████████▌| 558/585 [04:36<00:08,  3.29it/s] 96%|█████████▌| 559/585 [04:36<00:07,  3.34it/s] 96%|█████████▌| 560/585 [04:36<00:07,  3.38it/s] 96%|█████████▌| 561/585 [04:37<00:07,  3.41it/s] 96%|█████████▌| 562/585 [04:37<00:06,  3.43it/s] 96%|█████████▌| 563/585 [04:37<00:06,  3.44it/s] 96%|█████████▋| 564/585 [04:38<00:06,  3.45it/s] 97%|█████████▋| 565/585 [04:38<00:05,  3.45it/s] 97%|█████████▋| 566/585 [04:38<00:05,  3.46it/s] 97%|█████████▋| 567/585 [04:39<00:05,  3.46it/s] 97%|█████████▋| 568/585 [04:39<00:04,  3.47it/s] 97%|█████████▋| 569/585 [04:39<00:04,  3.34it/s] 97%|█████████▋| 570/585 [04:39<00:04,  3.38it/s] 98%|█████████▊| 571/585 [04:40<00:04,  3.41it/s] 98%|█████████▊| 572/585 [04:40<00:03,  3.43it/s] 98%|█████████▊| 573/585 [04:40<00:03,  3.44it/s] 98%|█████████▊| 574/585 [04:41<00:03,  3.45it/s] 98%|█████████▊| 575/585 [04:41<00:02,  3.45it/s] 98%|█████████▊| 576/585 [04:41<00:02,  3.46it/s] 99%|█████████▊| 577/585 [04:42<00:02,  2.78it/s] 99%|█████████▉| 578/585 [04:42<00:02,  2.58it/s] 99%|█████████▉| 579/585 [04:42<00:02,  2.66it/s] 99%|█████████▉| 580/585 [04:43<00:01,  2.86it/s] 99%|█████████▉| 581/585 [04:43<00:01,  3.02it/s] 99%|█████████▉| 582/585 [04:43<00:00,  3.15it/s]100%|█████████▉| 583/585 [04:44<00:00,  3.24it/s]100%|█████████▉| 584/585 [04:44<00:00,  3.31it/s]100%|██████████| 585/585 [04:44<00:00,  3.36it/s][INFO|trainer.py:2140] 2023-08-29 11:23:51,083 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:23:51,083 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 11:23:51,083 >>   Batch size = 8
{'eval_loss': 1.033044457435608, 'eval_runtime': 13.5173, 'eval_samples_per_second': 359.54, 'eval_steps_per_second': 44.98, 'epoch': 4.0}
{'loss': 0.5305, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.76it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.77it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.92it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.26it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.80it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.52it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.24it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.90it/s][A
  8%|▊         | 48/608 [00:01<00:11, 46.86it/s][A
  9%|▊         | 53/608 [00:01<00:12, 43.09it/s][A
 10%|▉         | 58/608 [00:01<00:12, 44.24it/s][A
 10%|█         | 63/608 [00:01<00:12, 45.18it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.64it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.05it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.40it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.62it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.71it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.36it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.39it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.53it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.65it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.81it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.84it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.86it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.95it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.96it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.79it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.69it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.65it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.74it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.83it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.86it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.94it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.93it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.88it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.83it/s][A
 31%|███       | 188/608 [00:04<00:08, 46.80it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 42.93it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 44.10it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 44.96it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.49it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.01it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.30it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.46it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.64it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.30it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.32it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.51it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.60it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.69it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.76it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.85it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.80it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.86it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.62it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.61it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.63it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.70it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.83it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.84it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.85it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.86it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.89it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.77it/s][A
 54%|█████▍    | 328/608 [00:07<00:05, 46.72it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 43.30it/s][A
 56%|█████▌    | 338/608 [00:07<00:06, 44.35it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.16it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.71it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.08it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.37it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.50it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.61it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.30it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.30it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.23it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.42it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.63it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.70it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.68it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.78it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.85it/s][A
 69%|██████▉   | 418/608 [00:08<00:04, 46.87it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.84it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.74it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.68it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.75it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.81it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.74it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.76it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.67it/s][A
 76%|███████▌  | 463/608 [00:09<00:03, 46.72it/s][A
 77%|███████▋  | 468/608 [00:10<00:02, 46.84it/s][A
 78%|███████▊  | 473/608 [00:10<00:03, 44.66it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.27it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.77it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.02it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.19it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.46it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.57it/s][A
 84%|████████▎ | 508/608 [00:10<00:02, 46.67it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.63it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.56it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.59it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.72it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.70it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.66it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.83it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.81it/s][A
 91%|█████████ | 553/608 [00:11<00:01, 46.78it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.86it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.65it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.71it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.75it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.70it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.75it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.72it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.72it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.73it/s][A
 99%|█████████▉| 603/608 [00:12<00:00, 46.79it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.78it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.78it/s][A100%|██████████| 585/585 [04:57<00:00,  3.36it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:24:04,347 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 11:24:04,515 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:24:08,240 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:24:08,403 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:24:08,484 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 11:24:18,436 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 11:24:18,480 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117 (score: 0.9966377019882202).
                                                 100%|██████████| 585/585 [05:24<00:00,  3.36it/s]100%|██████████| 585/585 [05:24<00:00,  1.80it/s]
[INFO|trainer.py:1894] 2023-08-29 11:24:31,405 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 11:24:31,590 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:24:35,540 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:24:35,681 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:24:35,751 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:24:36,334 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:36,334 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:36,334 >>   train_loss               =     0.5267
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:36,334 >>   train_runtime            = 0:05:24.95
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:36,334 >>   train_samples            =       7519
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:36,334 >>   train_samples_per_second =    115.693
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:36,334 >>   train_steps_per_second   =        1.8
{'eval_loss': 1.0354528427124023, 'eval_runtime': 13.1144, 'eval_samples_per_second': 370.584, 'eval_steps_per_second': 46.361, 'epoch': 5.0}
{'train_runtime': 324.9559, 'train_samples_per_second': 115.693, 'train_steps_per_second': 1.8, 'train_loss': 0.5266606713971521, 'epoch': 5.0}
08/29/2023 11:24:36 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 11:24:36,586 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:24:36,586 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 11:24:36,586 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 58.66it/s]  2%|▏         | 12/608 [00:00<00:11, 51.52it/s]  3%|▎         | 18/608 [00:00<00:11, 49.71it/s]  4%|▍         | 24/608 [00:00<00:11, 48.73it/s]  5%|▍         | 29/608 [00:00<00:12, 48.21it/s]  6%|▌         | 34/608 [00:00<00:11, 47.98it/s]  6%|▋         | 39/608 [00:00<00:11, 47.82it/s]  7%|▋         | 44/608 [00:00<00:11, 47.64it/s]  8%|▊         | 49/608 [00:01<00:11, 47.37it/s]  9%|▉         | 54/608 [00:01<00:11, 47.30it/s] 10%|▉         | 59/608 [00:01<00:11, 47.29it/s] 11%|█         | 64/608 [00:01<00:11, 47.35it/s] 11%|█▏        | 69/608 [00:01<00:11, 47.35it/s] 12%|█▏        | 74/608 [00:01<00:11, 47.32it/s] 13%|█▎        | 79/608 [00:01<00:11, 47.28it/s] 14%|█▍        | 84/608 [00:01<00:11, 47.31it/s] 15%|█▍        | 89/608 [00:01<00:10, 47.30it/s] 15%|█▌        | 94/608 [00:01<00:10, 47.21it/s] 16%|█▋        | 99/608 [00:02<00:11, 43.54it/s] 17%|█▋        | 104/608 [00:02<00:11, 44.63it/s] 18%|█▊        | 109/608 [00:02<00:11, 45.29it/s] 19%|█▉        | 114/608 [00:02<00:10, 45.88it/s] 20%|█▉        | 119/608 [00:02<00:10, 46.33it/s] 20%|██        | 124/608 [00:02<00:10, 46.62it/s] 21%|██        | 129/608 [00:02<00:10, 46.84it/s] 22%|██▏       | 134/608 [00:02<00:10, 46.92it/s] 23%|██▎       | 139/608 [00:02<00:10, 46.83it/s] 24%|██▎       | 144/608 [00:03<00:09, 46.89it/s] 25%|██▍       | 149/608 [00:03<00:09, 47.05it/s] 25%|██▌       | 154/608 [00:03<00:09, 47.15it/s] 26%|██▌       | 159/608 [00:03<00:09, 47.17it/s] 27%|██▋       | 164/608 [00:03<00:09, 47.26it/s] 28%|██▊       | 169/608 [00:03<00:09, 47.21it/s] 29%|██▊       | 174/608 [00:03<00:09, 47.21it/s] 29%|██▉       | 179/608 [00:03<00:09, 47.29it/s] 30%|███       | 184/608 [00:03<00:08, 47.15it/s] 31%|███       | 189/608 [00:04<00:08, 47.16it/s] 32%|███▏      | 194/608 [00:04<00:08, 47.17it/s] 33%|███▎      | 199/608 [00:04<00:08, 47.10it/s] 34%|███▎      | 204/608 [00:04<00:08, 47.15it/s] 34%|███▍      | 209/608 [00:04<00:08, 47.18it/s] 35%|███▌      | 214/608 [00:04<00:08, 47.24it/s] 36%|███▌      | 219/608 [00:04<00:08, 47.32it/s] 37%|███▋      | 224/608 [00:04<00:08, 47.19it/s] 38%|███▊      | 229/608 [00:04<00:08, 47.16it/s] 38%|███▊      | 234/608 [00:04<00:07, 47.12it/s] 39%|███▉      | 239/608 [00:05<00:07, 47.10it/s] 40%|████      | 244/608 [00:05<00:07, 47.00it/s] 41%|████      | 249/608 [00:05<00:07, 47.15it/s] 42%|████▏     | 254/608 [00:05<00:07, 47.13it/s] 43%|████▎     | 259/608 [00:05<00:07, 47.09it/s] 43%|████▎     | 264/608 [00:05<00:07, 47.18it/s] 44%|████▍     | 269/608 [00:05<00:07, 47.15it/s] 45%|████▌     | 274/608 [00:05<00:07, 47.11it/s] 46%|████▌     | 279/608 [00:05<00:06, 47.14it/s] 47%|████▋     | 284/608 [00:06<00:06, 47.05it/s] 48%|████▊     | 289/608 [00:06<00:06, 47.04it/s] 48%|████▊     | 294/608 [00:06<00:06, 47.12it/s] 49%|████▉     | 299/608 [00:06<00:06, 47.02it/s] 50%|█████     | 304/608 [00:06<00:06, 47.05it/s] 51%|█████     | 309/608 [00:06<00:06, 47.11it/s] 52%|█████▏    | 314/608 [00:06<00:06, 47.18it/s] 52%|█████▏    | 319/608 [00:06<00:06, 47.19it/s] 53%|█████▎    | 324/608 [00:06<00:06, 47.18it/s] 54%|█████▍    | 329/608 [00:06<00:05, 47.07it/s] 55%|█████▍    | 334/608 [00:07<00:05, 47.02it/s] 56%|█████▌    | 339/608 [00:07<00:05, 47.08it/s] 57%|█████▋    | 344/608 [00:07<00:05, 44.19it/s] 57%|█████▋    | 349/608 [00:07<00:05, 45.03it/s] 58%|█████▊    | 354/608 [00:07<00:05, 45.56it/s] 59%|█████▉    | 359/608 [00:07<00:05, 46.17it/s] 60%|█████▉    | 364/608 [00:07<00:05, 46.48it/s] 61%|██████    | 369/608 [00:07<00:05, 46.68it/s] 62%|██████▏   | 374/608 [00:07<00:04, 46.95it/s] 62%|██████▏   | 379/608 [00:08<00:04, 47.05it/s] 63%|██████▎   | 384/608 [00:08<00:04, 46.75it/s] 64%|██████▍   | 389/608 [00:08<00:04, 46.83it/s] 65%|██████▍   | 394/608 [00:08<00:04, 46.88it/s] 66%|██████▌   | 399/608 [00:08<00:04, 46.93it/s] 66%|██████▋   | 404/608 [00:08<00:04, 47.05it/s] 67%|██████▋   | 409/608 [00:08<00:04, 47.16it/s] 68%|██████▊   | 414/608 [00:08<00:04, 47.06it/s] 69%|██████▉   | 419/608 [00:08<00:04, 47.17it/s] 70%|██████▉   | 424/608 [00:09<00:03, 47.21it/s] 71%|███████   | 429/608 [00:09<00:03, 47.11it/s] 71%|███████▏  | 434/608 [00:09<00:03, 46.95it/s] 72%|███████▏  | 439/608 [00:09<00:03, 47.02it/s] 73%|███████▎  | 444/608 [00:09<00:03, 46.89it/s] 74%|███████▍  | 449/608 [00:09<00:03, 46.96it/s] 75%|███████▍  | 454/608 [00:09<00:03, 47.02it/s] 75%|███████▌  | 459/608 [00:09<00:03, 47.12it/s] 76%|███████▋  | 464/608 [00:09<00:03, 47.22it/s] 77%|███████▋  | 469/608 [00:09<00:02, 47.15it/s] 78%|███████▊  | 474/608 [00:10<00:02, 47.11it/s] 79%|███████▉  | 479/608 [00:10<00:02, 47.07it/s] 80%|███████▉  | 484/608 [00:10<00:02, 46.92it/s] 80%|████████  | 489/608 [00:10<00:02, 45.53it/s] 81%|████████▏ | 494/608 [00:10<00:02, 46.01it/s] 82%|████████▏ | 499/608 [00:10<00:02, 46.37it/s] 83%|████████▎ | 504/608 [00:10<00:02, 46.65it/s] 84%|████████▎ | 509/608 [00:10<00:02, 46.85it/s] 85%|████████▍ | 514/608 [00:10<00:01, 47.01it/s] 85%|████████▌ | 519/608 [00:11<00:01, 47.02it/s] 86%|████████▌ | 524/608 [00:11<00:01, 46.92it/s] 87%|████████▋ | 529/608 [00:11<00:01, 46.83it/s] 88%|████████▊ | 534/608 [00:11<00:01, 46.80it/s] 89%|████████▊ | 539/608 [00:11<00:01, 46.91it/s] 89%|████████▉ | 544/608 [00:11<00:01, 46.95it/s] 90%|█████████ | 549/608 [00:11<00:01, 47.11it/s] 91%|█████████ | 554/608 [00:12<00:01, 29.08it/s] 92%|█████████▏| 559/608 [00:12<00:01, 32.88it/s] 93%|█████████▎| 564/608 [00:12<00:01, 34.36it/s] 93%|█████████▎| 568/608 [00:12<00:01, 33.51it/s] 94%|█████████▍| 574/608 [00:12<00:00, 37.87it/s] 95%|█████████▌| 579/608 [00:12<00:00, 40.25it/s] 96%|█████████▌| 584/608 [00:12<00:00, 42.08it/s] 97%|█████████▋| 589/608 [00:12<00:00, 43.44it/s] 98%|█████████▊| 594/608 [00:12<00:00, 44.53it/s] 99%|█████████▊| 599/608 [00:13<00:00, 45.23it/s] 99%|█████████▉| 604/608 [00:13<00:00, 45.78it/s]100%|██████████| 608/608 [00:13<00:00, 46.00it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:24:49,826 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:49,826 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:49,826 >>   eval_loss               =     0.9966
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:49,826 >>   eval_runtime            = 0:00:13.24
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:49,826 >>   eval_samples            =       4860
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:49,826 >>   eval_samples_per_second =    367.059
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:49,826 >>   eval_steps_per_second   =      45.92
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:24:49,826 >>   perplexity              =     2.7092
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:01,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:01,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:01,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:01,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:01,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:25:02,034 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:25:02,035 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:25:02,594 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:25:03,613 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:25:03,613 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:07,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:07,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:07,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:07,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:25:07,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:25:07,842 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:25:07,843 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:25:08,458 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:25:08,612 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:25:08,612 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'labels': ['headquarters location', 'licensed to broadcast to', 'member of political party', 'narrative location', 'notable work'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14287
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14387, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:21,  1.59it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:22,  1.58it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:26,  1.60it/s]Extractor Predicting: 43it [00:27,  1.59it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:29,  1.52it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:32,  1.53it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.49it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:34,  1.52it/s]Extractor Predicting: 55it [00:35,  1.52it/s]Extractor Predicting: 56it [00:36,  1.52it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.46it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:38,  1.49it/s]Extractor Predicting: 61it [00:39,  1.54it/s]Extractor Predicting: 62it [00:40,  1.55it/s]Extractor Predicting: 63it [00:40,  1.50it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.53it/s]Extractor Predicting: 66it [00:42,  1.54it/s]Extractor Predicting: 67it [00:43,  1.48it/s]Extractor Predicting: 68it [00:44,  1.53it/s]Extractor Predicting: 69it [00:44,  1.52it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:46,  1.57it/s]Extractor Predicting: 73it [00:47,  1.57it/s]Extractor Predicting: 74it [00:47,  1.58it/s]Extractor Predicting: 75it [00:48,  1.58it/s]Extractor Predicting: 76it [00:49,  1.62it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:50,  1.59it/s]Extractor Predicting: 79it [00:50,  1.57it/s]Extractor Predicting: 80it [00:51,  1.57it/s]Extractor Predicting: 81it [00:52,  1.57it/s]Extractor Predicting: 82it [00:52,  1.59it/s]Extractor Predicting: 83it [00:53,  1.58it/s]Extractor Predicting: 84it [00:54,  1.59it/s]Extractor Predicting: 85it [00:54,  1.60it/s]Extractor Predicting: 86it [00:55,  1.62it/s]Extractor Predicting: 87it [00:55,  1.62it/s]Extractor Predicting: 88it [00:56,  1.58it/s]Extractor Predicting: 89it [00:57,  1.59it/s]Extractor Predicting: 90it [00:57,  1.59it/s]Extractor Predicting: 91it [00:58,  1.60it/s]Extractor Predicting: 92it [00:59,  1.58it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [01:00,  1.60it/s]Extractor Predicting: 95it [01:01,  1.59it/s]Extractor Predicting: 96it [01:01,  1.59it/s]Extractor Predicting: 97it [01:02,  1.59it/s]Extractor Predicting: 98it [01:02,  1.64it/s]Extractor Predicting: 99it [01:03,  1.60it/s]Extractor Predicting: 100it [01:04,  1.59it/s]Extractor Predicting: 101it [01:05,  1.02it/s]Extractor Predicting: 102it [01:06,  1.14it/s]Extractor Predicting: 103it [01:07,  1.23it/s]Extractor Predicting: 104it [01:07,  1.29it/s]Extractor Predicting: 105it [01:08,  1.38it/s]Extractor Predicting: 106it [01:09,  1.45it/s]Extractor Predicting: 107it [01:09,  1.51it/s]Extractor Predicting: 108it [01:10,  1.51it/s]Extractor Predicting: 109it [01:11,  1.50it/s]Extractor Predicting: 110it [01:11,  1.53it/s]Extractor Predicting: 111it [01:12,  1.58it/s]Extractor Predicting: 112it [01:13,  1.45it/s]Extractor Predicting: 113it [01:13,  1.51it/s]Extractor Predicting: 114it [01:14,  1.51it/s]Extractor Predicting: 115it [01:15,  1.54it/s]Extractor Predicting: 116it [01:15,  1.55it/s]Extractor Predicting: 117it [01:16,  1.58it/s]Extractor Predicting: 118it [01:16,  1.60it/s]Extractor Predicting: 119it [01:17,  1.58it/s]Extractor Predicting: 120it [01:18,  1.58it/s]Extractor Predicting: 121it [01:18,  1.55it/s]Extractor Predicting: 122it [01:19,  1.57it/s]Extractor Predicting: 123it [01:20,  1.58it/s]Extractor Predicting: 124it [01:20,  1.56it/s]Extractor Predicting: 125it [01:21,  1.51it/s]Extractor Predicting: 126it [01:22,  1.51it/s]Extractor Predicting: 127it [01:22,  1.52it/s]Extractor Predicting: 128it [01:23,  1.52it/s]Extractor Predicting: 129it [01:24,  1.50it/s]Extractor Predicting: 130it [01:24,  1.55it/s]Extractor Predicting: 131it [01:25,  1.53it/s]Extractor Predicting: 132it [01:25,  1.55it/s]Extractor Predicting: 133it [01:26,  1.50it/s]Extractor Predicting: 134it [01:27,  1.50it/s]Extractor Predicting: 135it [01:28,  1.50it/s]Extractor Predicting: 136it [01:28,  1.51it/s]Extractor Predicting: 137it [01:29,  1.51it/s]Extractor Predicting: 138it [01:30,  1.51it/s]Extractor Predicting: 139it [01:30,  1.49it/s]Extractor Predicting: 140it [01:31,  1.50it/s]Extractor Predicting: 141it [01:32,  1.49it/s]Extractor Predicting: 142it [01:32,  1.52it/s]Extractor Predicting: 143it [01:33,  1.51it/s]Extractor Predicting: 144it [01:33,  1.52it/s]Extractor Predicting: 145it [01:34,  1.54it/s]Extractor Predicting: 146it [01:35,  1.54it/s]Extractor Predicting: 147it [01:35,  1.49it/s]Extractor Predicting: 148it [01:36,  1.49it/s]Extractor Predicting: 149it [01:37,  1.49it/s]Extractor Predicting: 150it [01:37,  1.52it/s]Extractor Predicting: 151it [01:38,  1.52it/s]Extractor Predicting: 152it [01:39,  1.55it/s]Extractor Predicting: 153it [01:39,  1.60it/s]Extractor Predicting: 154it [01:40,  1.59it/s]Extractor Predicting: 155it [01:41,  1.55it/s]Extractor Predicting: 156it [01:41,  1.53it/s]Extractor Predicting: 157it [01:42,  1.55it/s]Extractor Predicting: 158it [01:43,  1.55it/s]Extractor Predicting: 159it [01:43,  1.53it/s]Extractor Predicting: 160it [01:44,  1.56it/s]Extractor Predicting: 161it [01:45,  1.52it/s]Extractor Predicting: 162it [01:45,  1.52it/s]Extractor Predicting: 163it [01:46,  1.52it/s]Extractor Predicting: 164it [01:46,  1.54it/s]Extractor Predicting: 165it [01:47,  1.53it/s]Extractor Predicting: 166it [01:48,  1.50it/s]Extractor Predicting: 167it [01:49,  1.49it/s]Extractor Predicting: 168it [01:49,  1.52it/s]Extractor Predicting: 169it [01:50,  1.51it/s]Extractor Predicting: 170it [01:50,  1.54it/s]Extractor Predicting: 171it [01:51,  1.54it/s]Extractor Predicting: 172it [01:52,  1.53it/s]Extractor Predicting: 173it [01:52,  1.51it/s]Extractor Predicting: 174it [01:53,  1.56it/s]Extractor Predicting: 175it [01:54,  1.54it/s]Extractor Predicting: 176it [01:54,  1.52it/s]Extractor Predicting: 177it [01:55,  1.49it/s]Extractor Predicting: 178it [01:56,  1.47it/s]Extractor Predicting: 179it [01:56,  1.49it/s]Extractor Predicting: 180it [01:57,  1.48it/s]Extractor Predicting: 181it [01:58,  1.47it/s]Extractor Predicting: 182it [01:58,  1.52it/s]Extractor Predicting: 183it [01:59,  1.49it/s]Extractor Predicting: 184it [02:00,  1.48it/s]Extractor Predicting: 185it [02:01,  1.46it/s]Extractor Predicting: 186it [02:01,  1.46it/s]Extractor Predicting: 187it [02:02,  1.46it/s]Extractor Predicting: 188it [02:03,  1.47it/s]Extractor Predicting: 189it [02:03,  1.49it/s]Extractor Predicting: 190it [02:04,  1.48it/s]Extractor Predicting: 191it [02:05,  1.49it/s]Extractor Predicting: 192it [02:05,  1.47it/s]Extractor Predicting: 193it [02:06,  1.52it/s]Extractor Predicting: 194it [02:06,  1.53it/s]Extractor Predicting: 195it [02:07,  1.95it/s]Extractor Predicting: 195it [02:07,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:31,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:31,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:31,350 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:31,350 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:31,350 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:27:32,358 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:27:32,359 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:27:32,995 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:27:34,091 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:27:34,091 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:37,259 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:37,312 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:37,312 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:37,312 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:27:37,312 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:27:38,196 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:27:38,197 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:27:38,892 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:27:39,161 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:27:39,162 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4971042471042471,
  "recall": 0.10596707818930041,
  "score": 0.1746947082767978,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21244
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21344, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.56it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.48it/s]Extractor Predicting: 37it [00:23,  1.53it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:28,  1.59it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:31,  1.61it/s]Extractor Predicting: 50it [00:32,  1.62it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:33,  1.64it/s]Extractor Predicting: 53it [00:33,  1.68it/s]Extractor Predicting: 54it [00:34,  1.70it/s]Extractor Predicting: 55it [00:35,  1.65it/s]Extractor Predicting: 56it [00:35,  1.63it/s]Extractor Predicting: 57it [00:36,  1.64it/s]Extractor Predicting: 58it [00:36,  1.69it/s]Extractor Predicting: 59it [00:37,  1.70it/s]Extractor Predicting: 60it [00:38,  1.69it/s]Extractor Predicting: 61it [00:38,  1.64it/s]Extractor Predicting: 62it [00:39,  1.67it/s]Extractor Predicting: 63it [00:39,  1.63it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:41,  1.64it/s]Extractor Predicting: 66it [00:41,  1.64it/s]Extractor Predicting: 67it [00:42,  1.67it/s]Extractor Predicting: 68it [00:42,  1.68it/s]Extractor Predicting: 69it [00:43,  1.71it/s]Extractor Predicting: 70it [00:44,  1.69it/s]Extractor Predicting: 71it [00:44,  1.65it/s]Extractor Predicting: 72it [00:45,  1.62it/s]Extractor Predicting: 73it [00:45,  1.63it/s]Extractor Predicting: 74it [00:46,  1.62it/s]Extractor Predicting: 75it [00:47,  1.61it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:48,  1.61it/s]Extractor Predicting: 78it [00:49,  1.60it/s]Extractor Predicting: 79it [00:49,  1.66it/s]Extractor Predicting: 80it [00:50,  1.65it/s]Extractor Predicting: 81it [00:50,  1.69it/s]Extractor Predicting: 82it [00:51,  1.68it/s]Extractor Predicting: 83it [00:52,  1.69it/s]Extractor Predicting: 84it [00:52,  1.62it/s]Extractor Predicting: 85it [00:53,  1.47it/s]Extractor Predicting: 86it [00:54,  1.55it/s]Extractor Predicting: 87it [00:54,  1.59it/s]Extractor Predicting: 88it [00:55,  1.63it/s]Extractor Predicting: 89it [00:55,  1.64it/s]Extractor Predicting: 90it [00:56,  1.66it/s]Extractor Predicting: 91it [00:56,  1.75it/s]Extractor Predicting: 92it [00:57,  1.71it/s]Extractor Predicting: 93it [00:58,  1.73it/s]Extractor Predicting: 94it [00:58,  1.73it/s]Extractor Predicting: 95it [00:59,  1.73it/s]Extractor Predicting: 96it [00:59,  1.68it/s]Extractor Predicting: 97it [01:00,  1.63it/s]Extractor Predicting: 98it [01:01,  1.68it/s]Extractor Predicting: 99it [01:01,  1.69it/s]Extractor Predicting: 100it [01:02,  1.69it/s]Extractor Predicting: 101it [01:02,  1.70it/s]Extractor Predicting: 102it [01:03,  1.69it/s]Extractor Predicting: 103it [01:04,  1.70it/s]Extractor Predicting: 104it [01:04,  1.69it/s]Extractor Predicting: 105it [01:05,  1.74it/s]Extractor Predicting: 106it [01:05,  1.71it/s]Extractor Predicting: 107it [01:06,  1.75it/s]Extractor Predicting: 108it [01:06,  1.71it/s]Extractor Predicting: 109it [01:07,  1.75it/s]Extractor Predicting: 110it [01:08,  1.70it/s]Extractor Predicting: 111it [01:08,  1.73it/s]Extractor Predicting: 112it [01:09,  1.68it/s]Extractor Predicting: 113it [01:09,  1.66it/s]Extractor Predicting: 114it [01:10,  1.65it/s]Extractor Predicting: 115it [01:11,  1.64it/s]Extractor Predicting: 116it [01:11,  1.64it/s]Extractor Predicting: 117it [01:12,  1.64it/s]Extractor Predicting: 118it [01:13,  1.62it/s]Extractor Predicting: 119it [01:13,  1.62it/s]Extractor Predicting: 120it [01:14,  1.63it/s]Extractor Predicting: 121it [01:14,  1.67it/s]Extractor Predicting: 122it [01:15,  1.65it/s]Extractor Predicting: 123it [01:16,  1.48it/s]Extractor Predicting: 124it [01:16,  1.50it/s]Extractor Predicting: 125it [01:17,  1.52it/s]Extractor Predicting: 126it [01:18,  1.57it/s]Extractor Predicting: 127it [01:18,  1.61it/s]Extractor Predicting: 128it [01:19,  1.60it/s]Extractor Predicting: 129it [01:19,  1.63it/s]Extractor Predicting: 130it [01:20,  1.61it/s]Extractor Predicting: 131it [01:21,  1.57it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.59it/s]Extractor Predicting: 134it [01:23,  1.59it/s]Extractor Predicting: 135it [01:23,  1.64it/s]Extractor Predicting: 136it [01:24,  1.63it/s]Extractor Predicting: 137it [01:24,  1.68it/s]Extractor Predicting: 138it [01:25,  1.65it/s]Extractor Predicting: 139it [01:26,  1.66it/s]Extractor Predicting: 140it [01:26,  1.67it/s]Extractor Predicting: 141it [01:27,  1.64it/s]Extractor Predicting: 142it [01:27,  1.68it/s]Extractor Predicting: 143it [01:28,  1.66it/s]Extractor Predicting: 144it [01:29,  1.61it/s]Extractor Predicting: 145it [01:29,  1.58it/s]Extractor Predicting: 146it [01:30,  1.60it/s]Extractor Predicting: 147it [01:31,  1.60it/s]Extractor Predicting: 148it [01:31,  1.58it/s]Extractor Predicting: 149it [01:32,  1.58it/s]Extractor Predicting: 150it [01:32,  1.62it/s]Extractor Predicting: 151it [01:33,  1.63it/s]Extractor Predicting: 152it [01:34,  1.61it/s]Extractor Predicting: 153it [01:34,  1.64it/s]Extractor Predicting: 154it [01:35,  1.66it/s]Extractor Predicting: 155it [01:35,  1.62it/s]Extractor Predicting: 156it [01:36,  1.59it/s]Extractor Predicting: 157it [01:37,  1.58it/s]Extractor Predicting: 158it [01:37,  1.59it/s]Extractor Predicting: 159it [01:38,  1.58it/s]Extractor Predicting: 160it [01:39,  1.60it/s]Extractor Predicting: 161it [01:39,  1.58it/s]Extractor Predicting: 162it [01:40,  1.60it/s]Extractor Predicting: 163it [01:41,  1.62it/s]Extractor Predicting: 164it [01:41,  1.57it/s]Extractor Predicting: 165it [01:42,  1.57it/s]Extractor Predicting: 166it [01:42,  1.54it/s]Extractor Predicting: 167it [01:43,  1.57it/s]Extractor Predicting: 168it [01:44,  1.61it/s]Extractor Predicting: 169it [01:44,  1.60it/s]Extractor Predicting: 170it [01:45,  1.58it/s]Extractor Predicting: 171it [01:46,  1.56it/s]Extractor Predicting: 172it [01:46,  1.56it/s]Extractor Predicting: 173it [01:47,  1.57it/s]Extractor Predicting: 174it [01:48,  1.56it/s]Extractor Predicting: 175it [01:48,  1.56it/s]Extractor Predicting: 176it [01:49,  1.61it/s]Extractor Predicting: 177it [01:49,  1.62it/s]Extractor Predicting: 178it [01:50,  1.65it/s]Extractor Predicting: 179it [01:51,  1.68it/s]Extractor Predicting: 180it [01:51,  1.64it/s]Extractor Predicting: 181it [01:52,  1.56it/s]Extractor Predicting: 182it [01:53,  1.57it/s]Extractor Predicting: 183it [01:53,  1.57it/s]Extractor Predicting: 184it [01:54,  1.56it/s]Extractor Predicting: 185it [01:54,  1.56it/s]Extractor Predicting: 186it [01:55,  1.57it/s]Extractor Predicting: 187it [01:56,  1.60it/s]Extractor Predicting: 188it [01:56,  1.63it/s]Extractor Predicting: 189it [01:57,  1.65it/s]Extractor Predicting: 190it [01:57,  1.63it/s]Extractor Predicting: 191it [01:58,  1.64it/s]Extractor Predicting: 192it [01:59,  1.63it/s]Extractor Predicting: 193it [01:59,  1.67it/s]Extractor Predicting: 194it [02:00,  1.73it/s]Extractor Predicting: 195it [02:00,  1.75it/s]Extractor Predicting: 196it [02:01,  1.69it/s]Extractor Predicting: 197it [02:02,  1.65it/s]Extractor Predicting: 198it [02:02,  1.62it/s]Extractor Predicting: 199it [02:03,  1.70it/s]Extractor Predicting: 200it [02:03,  1.69it/s]Extractor Predicting: 201it [02:04,  1.67it/s]Extractor Predicting: 202it [02:05,  1.67it/s]Extractor Predicting: 203it [02:05,  1.67it/s]Extractor Predicting: 204it [02:06,  1.63it/s]Extractor Predicting: 205it [02:06,  1.64it/s]Extractor Predicting: 206it [02:07,  1.46it/s]Extractor Predicting: 207it [02:08,  1.48it/s]Extractor Predicting: 208it [02:09,  1.52it/s]Extractor Predicting: 209it [02:09,  1.55it/s]Extractor Predicting: 210it [02:10,  1.55it/s]Extractor Predicting: 211it [02:10,  1.55it/s]Extractor Predicting: 212it [02:11,  1.55it/s]Extractor Predicting: 213it [02:12,  1.59it/s]Extractor Predicting: 214it [02:12,  1.60it/s]Extractor Predicting: 215it [02:13,  1.63it/s]Extractor Predicting: 216it [02:14,  1.64it/s]Extractor Predicting: 217it [02:14,  1.64it/s]Extractor Predicting: 218it [02:15,  1.58it/s]Extractor Predicting: 219it [02:15,  1.58it/s]Extractor Predicting: 220it [02:16,  1.56it/s]Extractor Predicting: 221it [02:17,  1.51it/s]Extractor Predicting: 222it [02:18,  1.47it/s]Extractor Predicting: 223it [02:18,  1.53it/s]Extractor Predicting: 224it [02:19,  1.53it/s]Extractor Predicting: 225it [02:19,  1.53it/s]Extractor Predicting: 226it [02:20,  1.52it/s]Extractor Predicting: 227it [02:21,  1.51it/s]Extractor Predicting: 228it [02:21,  1.52it/s]Extractor Predicting: 229it [02:22,  1.57it/s]Extractor Predicting: 230it [02:23,  1.57it/s]Extractor Predicting: 231it [02:23,  1.53it/s]Extractor Predicting: 232it [02:24,  1.56it/s]Extractor Predicting: 233it [02:25,  1.58it/s]Extractor Predicting: 234it [02:25,  1.62it/s]Extractor Predicting: 235it [02:26,  1.62it/s]Extractor Predicting: 236it [02:26,  1.69it/s]Extractor Predicting: 237it [02:27,  1.69it/s]Extractor Predicting: 238it [02:28,  1.69it/s]Extractor Predicting: 239it [02:28,  1.69it/s]Extractor Predicting: 240it [02:29,  1.66it/s]Extractor Predicting: 241it [02:29,  1.62it/s]Extractor Predicting: 242it [02:30,  1.61it/s]Extractor Predicting: 243it [02:31,  1.64it/s]Extractor Predicting: 244it [02:31,  1.66it/s]Extractor Predicting: 245it [02:32,  1.65it/s]Extractor Predicting: 246it [02:32,  1.57it/s]Extractor Predicting: 247it [02:33,  1.57it/s]Extractor Predicting: 248it [02:34,  1.56it/s]Extractor Predicting: 249it [02:34,  1.55it/s]Extractor Predicting: 250it [02:35,  1.53it/s]Extractor Predicting: 251it [02:36,  1.53it/s]Extractor Predicting: 252it [02:36,  1.53it/s]Extractor Predicting: 253it [02:37,  1.54it/s]Extractor Predicting: 254it [02:38,  1.52it/s]Extractor Predicting: 255it [02:38,  1.57it/s]Extractor Predicting: 256it [02:39,  1.52it/s]Extractor Predicting: 257it [02:40,  1.53it/s]Extractor Predicting: 258it [02:40,  1.52it/s]Extractor Predicting: 259it [02:41,  1.54it/s]Extractor Predicting: 260it [02:42,  1.53it/s]Extractor Predicting: 261it [02:42,  1.46it/s]Extractor Predicting: 262it [02:43,  1.49it/s]Extractor Predicting: 263it [02:44,  1.50it/s]Extractor Predicting: 264it [02:44,  1.53it/s]Extractor Predicting: 265it [02:45,  1.52it/s]Extractor Predicting: 266it [02:46,  1.51it/s]Extractor Predicting: 267it [02:46,  1.52it/s]Extractor Predicting: 268it [02:47,  1.53it/s]Extractor Predicting: 269it [02:48,  1.51it/s]Extractor Predicting: 270it [02:48,  1.54it/s]Extractor Predicting: 271it [02:49,  1.55it/s]Extractor Predicting: 272it [02:50,  1.49it/s]Extractor Predicting: 273it [02:50,  1.50it/s]Extractor Predicting: 274it [02:51,  1.50it/s]Extractor Predicting: 275it [02:52,  1.50it/s]Extractor Predicting: 276it [02:52,  1.54it/s]Extractor Predicting: 277it [02:53,  1.52it/s]Extractor Predicting: 278it [02:54,  1.52it/s]Extractor Predicting: 279it [02:54,  1.53it/s]Extractor Predicting: 280it [02:55,  1.59it/s]Extractor Predicting: 281it [02:55,  1.62it/s]Extractor Predicting: 282it [02:56,  1.65it/s]Extractor Predicting: 283it [02:57,  1.62it/s]Extractor Predicting: 284it [02:57,  1.59it/s]Extractor Predicting: 285it [02:58,  1.58it/s]Extractor Predicting: 286it [02:58,  1.64it/s]Extractor Predicting: 287it [02:59,  1.66it/s]Extractor Predicting: 288it [03:00,  1.61it/s]Extractor Predicting: 289it [03:00,  1.61it/s]Extractor Predicting: 290it [03:01,  1.61it/s]Extractor Predicting: 291it [03:02,  1.61it/s]Extractor Predicting: 292it [03:02,  1.66it/s]Extractor Predicting: 293it [03:03,  1.65it/s]Extractor Predicting: 294it [03:03,  1.68it/s]Extractor Predicting: 295it [03:04,  1.73it/s]Extractor Predicting: 296it [03:04,  1.73it/s]Extractor Predicting: 297it [03:05,  1.73it/s]Extractor Predicting: 298it [03:06,  1.64it/s]Extractor Predicting: 299it [03:06,  1.54it/s]Extractor Predicting: 300it [03:07,  1.52it/s]Extractor Predicting: 301it [03:08,  1.51it/s]Extractor Predicting: 302it [03:08,  1.51it/s]Extractor Predicting: 303it [03:09,  1.54it/s]Extractor Predicting: 304it [03:10,  1.55it/s]Extractor Predicting: 305it [03:10,  1.53it/s]Extractor Predicting: 306it [03:11,  1.52it/s]Extractor Predicting: 307it [03:12,  1.52it/s]Extractor Predicting: 308it [03:12,  1.51it/s]Extractor Predicting: 309it [03:13,  1.32it/s]Extractor Predicting: 310it [03:14,  1.36it/s]Extractor Predicting: 311it [03:15,  1.40it/s]Extractor Predicting: 312it [03:15,  1.45it/s]Extractor Predicting: 313it [03:16,  1.47it/s]Extractor Predicting: 314it [03:17,  1.50it/s]Extractor Predicting: 315it [03:17,  1.55it/s]Extractor Predicting: 316it [03:18,  1.52it/s]Extractor Predicting: 317it [03:19,  1.47it/s]Extractor Predicting: 318it [03:19,  1.43it/s]Extractor Predicting: 319it [03:20,  1.45it/s]Extractor Predicting: 320it [03:21,  1.47it/s]Extractor Predicting: 321it [03:21,  1.46it/s]Extractor Predicting: 322it [03:22,  1.39it/s]Extractor Predicting: 323it [03:23,  1.41it/s]Extractor Predicting: 324it [03:24,  1.42it/s]Extractor Predicting: 325it [03:24,  1.43it/s]Extractor Predicting: 326it [03:25,  1.42it/s]Extractor Predicting: 327it [03:26,  1.40it/s]Extractor Predicting: 328it [03:26,  1.42it/s]Extractor Predicting: 329it [03:27,  1.42it/s]Extractor Predicting: 330it [03:28,  1.44it/s]Extractor Predicting: 331it [03:28,  1.42it/s]Extractor Predicting: 332it [03:29,  1.39it/s]Extractor Predicting: 333it [03:30,  1.39it/s]Extractor Predicting: 334it [03:31,  1.42it/s]Extractor Predicting: 335it [03:31,  1.42it/s]Extractor Predicting: 336it [03:32,  1.40it/s]Extractor Predicting: 337it [03:33,  1.38it/s]Extractor Predicting: 338it [03:34,  1.39it/s]Extractor Predicting: 339it [03:34,  1.40it/s]Extractor Predicting: 340it [03:35,  1.43it/s]Extractor Predicting: 341it [03:36,  1.45it/s]Extractor Predicting: 342it [03:36,  1.42it/s]Extractor Predicting: 343it [03:37,  1.46it/s]Extractor Predicting: 344it [03:38,  1.50it/s]Extractor Predicting: 345it [03:38,  1.51it/s]Extractor Predicting: 346it [03:39,  1.62it/s]Extractor Predicting: 346it [03:39,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:34,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:34,838 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:34,838 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:34,838 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:34,838 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:31:35,603 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:31:35,604 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:31:36,252 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:31:37,367 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:31:37,367 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:40,488 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:40,524 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:40,525 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:40,525 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:31:40,525 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:31:41,402 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:31:41,403 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:31:42,056 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:31:42,283 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:31:42,283 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.23948220064724918,
  "recall": 0.017844224740776463,
  "score": 0.03321364452423698,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6093
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6193, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.46it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.59it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.45it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.42it/s]Extractor Predicting: 21it [00:13,  1.43it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.39it/s]Extractor Predicting: 24it [00:16,  1.40it/s]Extractor Predicting: 25it [00:16,  1.45it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.62it/s]Extractor Predicting: 30it [00:19,  1.68it/s]Extractor Predicting: 31it [00:20,  1.74it/s]Extractor Predicting: 32it [00:20,  1.79it/s]Extractor Predicting: 33it [00:21,  1.84it/s]Extractor Predicting: 34it [00:21,  1.84it/s]Extractor Predicting: 35it [00:22,  1.87it/s]Extractor Predicting: 36it [00:22,  1.86it/s]Extractor Predicting: 37it [00:23,  1.78it/s]Extractor Predicting: 38it [00:23,  1.82it/s]Extractor Predicting: 39it [00:24,  1.81it/s]Extractor Predicting: 40it [00:25,  1.83it/s]Extractor Predicting: 41it [00:25,  1.83it/s]Extractor Predicting: 42it [00:26,  1.82it/s]Extractor Predicting: 43it [00:26,  1.78it/s]Extractor Predicting: 44it [00:27,  1.82it/s]Extractor Predicting: 45it [00:27,  1.87it/s]Extractor Predicting: 46it [00:28,  1.89it/s]Extractor Predicting: 47it [00:28,  1.87it/s]Extractor Predicting: 48it [00:29,  1.84it/s]Extractor Predicting: 49it [00:29,  1.88it/s]Extractor Predicting: 50it [00:30,  1.84it/s]Extractor Predicting: 51it [00:31,  1.80it/s]Extractor Predicting: 52it [00:31,  1.84it/s]Extractor Predicting: 53it [00:32,  1.84it/s]Extractor Predicting: 54it [00:32,  1.84it/s]Extractor Predicting: 55it [00:33,  1.81it/s]Extractor Predicting: 56it [00:33,  1.83it/s]Extractor Predicting: 57it [00:34,  1.87it/s]Extractor Predicting: 58it [00:34,  1.76it/s]Extractor Predicting: 59it [00:35,  1.66it/s]Extractor Predicting: 60it [00:36,  1.57it/s]Extractor Predicting: 61it [00:37,  1.50it/s]Extractor Predicting: 62it [00:37,  1.49it/s]Extractor Predicting: 63it [00:38,  1.48it/s]Extractor Predicting: 64it [00:39,  1.48it/s]Extractor Predicting: 65it [00:39,  1.47it/s]Extractor Predicting: 66it [00:40,  1.42it/s]Extractor Predicting: 67it [00:41,  1.43it/s]Extractor Predicting: 68it [00:41,  1.43it/s]Extractor Predicting: 69it [00:42,  1.43it/s]Extractor Predicting: 70it [00:43,  1.46it/s]Extractor Predicting: 71it [00:44,  1.40it/s]Extractor Predicting: 72it [00:44,  1.41it/s]Extractor Predicting: 73it [00:45,  1.44it/s]Extractor Predicting: 74it [00:46,  1.46it/s]Extractor Predicting: 75it [00:46,  1.48it/s]Extractor Predicting: 76it [00:47,  1.46it/s]Extractor Predicting: 77it [00:47,  1.79it/s]Extractor Predicting: 77it [00:47,  1.61it/s]
[INFO|configuration_utils.py:515] 2023-08-29 11:32:33,776 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:32:33,777 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:32:33,846 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:32:33,847 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 11:32:33,922 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:32:45,641 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 11:32:45,667 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 11:32:45,812 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:32:45,812 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:32:45,905 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:32:45,947 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:32:45,947 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:32:45,947 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:32:45,947 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:32:45,947 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:32:45,947 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.9054290718038529,
  "recall": 0.12863896491664592,
  "score": 0.22527233115468406,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 11:32:46,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:47,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:47,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:48,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:49,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:49,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:50,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:50,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:51,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:52,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:53,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:53,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:54,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:55,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:55,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:56,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:57,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:57,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:58,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:32:59,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:00,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:21, 14.38s/it][WARNING|generation_utils.py:914] 2023-08-29 11:33:00,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:01,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:01,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:02,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:03,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:04,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:04,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:05,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:05,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:06,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:07,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:07,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:08,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:09,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:09,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:10,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:11,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:12,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:12,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:13,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:59, 13.81s/it][WARNING|generation_utils.py:914] 2023-08-29 11:33:14,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:14,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:15,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:16,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:16,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:17,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:18,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:18,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:19,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:20,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:20,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:21,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:21,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:22,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:23,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:23,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:24,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:24,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:25,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:26,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:26,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:27,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:41<02:46, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-29 11:33:28,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:28,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:29,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:30,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:31,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:32,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:32,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:33,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:34,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:35,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:35,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:36,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:37,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:38,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:39,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:40,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:40,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:41,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:42,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:43,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:44,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:58<02:45, 15.02s/it][WARNING|generation_utils.py:914] 2023-08-29 11:33:44,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:45,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:46,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:47,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:47,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:48,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:49,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:50,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:51,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:52,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:52,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:53,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:54,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:54,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:55,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:56,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:57,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:58,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:58,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:33:59,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:00,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:01,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:15<02:38, 15.87s/it][WARNING|generation_utils.py:914] 2023-08-29 11:34:02,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:03,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:03,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:04,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:05,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:05,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:06,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:07,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:07,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:08,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:09,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:09,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:10,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:11,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:11,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:12,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:13,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:14,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:14,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:15,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:16,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:17,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:31<02:21, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-29 11:34:17,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:18,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:19,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:20,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:20,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:22,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:23,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:24,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:25,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:25,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:26,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:27,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:28,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:29,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:29,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:30,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:31,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:31,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:32,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:33,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:34,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:35,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:35,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:36,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:50<02:15, 16.98s/it][WARNING|generation_utils.py:914] 2023-08-29 11:34:37,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:38,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:39,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:39,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:40,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:41,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:41,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:42,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:43,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:44,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:45,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:45,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:46,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:47,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:48,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:49,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:49,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:50,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:51,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:52,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:52,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:53,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:54,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:08<02:00, 17.21s/it][WARNING|generation_utils.py:914] 2023-08-29 11:34:54,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:55,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:56,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:56,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:58,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:58,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:34:59,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:00,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:01,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:01,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:02,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:03,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:03,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:04,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:05,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:06,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:06,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:07,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:07,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:08,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:09,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:10,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:24<01:40, 16.75s/it][WARNING|generation_utils.py:914] 2023-08-29 11:35:10,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:11,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:12,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:12,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:13,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:14,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:15,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:15,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:16,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:17,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:18,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:18,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:19,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:20,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:21,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:21,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:22,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:23,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:23,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:24,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:25,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:39<01:21, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-29 11:35:26,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:26,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:27,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:27,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:28,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:29,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:30,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:30,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:31,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:32,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:32,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:33,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:34,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:35,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:35,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:36,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:37,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:37,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:38,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:39,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:39,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:54<01:02, 15.69s/it][WARNING|generation_utils.py:914] 2023-08-29 11:35:40,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:40,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:41,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:42,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:43,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:44,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:44,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:45,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:46,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:47,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:47,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:48,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:49,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:50,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:50,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:51,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:52,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:53,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:53,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:54,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:08<00:46, 15.42s/it][WARNING|generation_utils.py:914] 2023-08-29 11:35:55,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:56,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:57,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:58,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:58,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:35:59,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:00,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:01,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:01,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:03,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:04,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:05,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:05,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:06,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:07,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:08,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:09,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:11,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:12,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:13,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:28<00:33, 16.57s/it][WARNING|generation_utils.py:914] 2023-08-29 11:36:14,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:15,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:16,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:16,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:17,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:18,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:18,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:19,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:20,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:20,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:21,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:22,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:23,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:23,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:24,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:25,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:25,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:26,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:27,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:27,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:28,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:29,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:43<00:16, 16.26s/it][WARNING|generation_utils.py:914] 2023-08-29 11:36:29,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:30,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:31,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:32,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:32,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:33,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:34,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:35,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:35,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:36,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:37,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:38,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:39,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:39,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:40,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:41,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:42,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:43,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:43,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:44,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:36:45,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:59<00:00, 16.24s/it]Generating: 100%|██████████| 15/15 [03:59<00:00, 15.98s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:53,239 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:53,241 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:53,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:53,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:53,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:36:53,861 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:36:53,863 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:36:54,467 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:36:55,512 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:36:55,512 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:58,525 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:58,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:58,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:58,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:58,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:36:59,243 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:36:59,244 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:36:59,968 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:37:00,123 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:37:00,123 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.90625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.946875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : narrative location .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : notable work .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : applies to jurisdiction .', 'success_rate': 0.8863636363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : family name .', 'success_rate': 0.8138020833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('21 ,979', 'is a list of', '', 'It has 21 ,979 entries , .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.95, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year , the United States appointed him a Vice President of the United States . Head Entity : George W. Bush , Tail Entity : United States .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8821022727272727, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : use .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 9346
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9446, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.59it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.60it/s]Extractor Estimating: 5it [00:03,  1.62it/s]Extractor Estimating: 6it [00:03,  1.59it/s]Extractor Estimating: 7it [00:04,  1.62it/s]Extractor Estimating: 8it [00:04,  1.64it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.63it/s]Extractor Estimating: 11it [00:06,  1.55it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.59it/s]Extractor Estimating: 14it [00:08,  1.64it/s]Extractor Estimating: 15it [00:09,  1.68it/s]Extractor Estimating: 16it [00:10,  1.56it/s]Extractor Estimating: 17it [00:10,  1.59it/s]Extractor Estimating: 18it [00:11,  1.62it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:13,  1.54it/s]Extractor Estimating: 22it [00:13,  1.52it/s]Extractor Estimating: 23it [00:14,  1.55it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:15,  1.58it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:16,  1.62it/s]Extractor Estimating: 28it [00:17,  1.64it/s]Extractor Estimating: 29it [00:18,  1.66it/s]Extractor Estimating: 30it [00:18,  1.63it/s]Extractor Estimating: 31it [00:19,  1.48it/s]Extractor Estimating: 32it [00:20,  1.58it/s]Extractor Estimating: 33it [00:20,  1.62it/s]Extractor Estimating: 34it [00:21,  1.65it/s]Extractor Estimating: 35it [00:21,  1.65it/s]Extractor Estimating: 36it [00:22,  1.69it/s]Extractor Estimating: 37it [00:22,  1.69it/s]Extractor Estimating: 38it [00:23,  1.62it/s]Extractor Estimating: 39it [00:24,  1.57it/s]Extractor Estimating: 40it [00:25,  1.53it/s]Extractor Estimating: 41it [00:25,  1.55it/s]Extractor Estimating: 42it [00:26,  1.56it/s]Extractor Estimating: 43it [00:26,  1.56it/s]Extractor Estimating: 44it [00:27,  1.55it/s]Extractor Estimating: 45it [00:28,  1.50it/s]Extractor Estimating: 46it [00:28,  1.53it/s]Extractor Estimating: 47it [00:29,  1.51it/s]Extractor Estimating: 48it [00:30,  1.62it/s]Extractor Estimating: 49it [00:30,  1.56it/s]Extractor Estimating: 50it [00:31,  1.59it/s]Extractor Estimating: 51it [00:31,  1.66it/s]Extractor Estimating: 52it [00:32,  1.71it/s]Extractor Estimating: 53it [00:33,  1.73it/s]Extractor Estimating: 54it [00:33,  1.79it/s]Extractor Estimating: 55it [00:34,  1.71it/s]Extractor Estimating: 56it [00:34,  1.72it/s]Extractor Estimating: 57it [00:35,  1.77it/s]Extractor Estimating: 58it [00:35,  1.82it/s]Extractor Estimating: 59it [00:36,  1.87it/s]Extractor Estimating: 60it [00:36,  1.88it/s]Extractor Estimating: 61it [00:37,  1.82it/s]Extractor Estimating: 62it [00:37,  1.84it/s]Extractor Estimating: 63it [00:38,  1.85it/s]Extractor Estimating: 64it [00:39,  1.87it/s]Extractor Estimating: 65it [00:39,  1.87it/s]Extractor Estimating: 66it [00:40,  1.83it/s]Extractor Estimating: 67it [00:40,  1.78it/s]Extractor Estimating: 68it [00:41,  1.81it/s]Extractor Estimating: 69it [00:41,  1.79it/s]Extractor Estimating: 70it [00:42,  1.82it/s]Extractor Estimating: 71it [00:42,  1.86it/s]Extractor Estimating: 72it [00:43,  1.91it/s]Extractor Estimating: 73it [00:43,  1.84it/s]Extractor Estimating: 74it [00:44,  1.75it/s]Extractor Estimating: 75it [00:45,  1.79it/s]Extractor Estimating: 76it [00:45,  1.63it/s]Extractor Estimating: 77it [00:46,  1.44it/s]Extractor Estimating: 78it [00:47,  1.46it/s]Extractor Estimating: 79it [00:48,  1.41it/s]Extractor Estimating: 80it [00:48,  1.43it/s]Extractor Estimating: 81it [00:49,  1.47it/s]Extractor Estimating: 82it [00:50,  1.48it/s]Extractor Estimating: 83it [00:50,  1.43it/s]Extractor Estimating: 84it [00:51,  1.50it/s]Extractor Estimating: 85it [00:52,  1.46it/s]Extractor Estimating: 86it [00:52,  1.48it/s]Extractor Estimating: 87it [00:53,  1.46it/s]Extractor Estimating: 88it [00:54,  1.47it/s]Extractor Estimating: 89it [00:55,  1.41it/s]Extractor Estimating: 90it [00:55,  1.31it/s]Extractor Estimating: 91it [00:56,  1.35it/s]Extractor Estimating: 92it [00:57,  1.32it/s]Extractor Estimating: 93it [00:58,  1.34it/s]Extractor Estimating: 94it [00:58,  1.36it/s]Extractor Estimating: 95it [00:59,  1.43it/s]Extractor Estimating: 96it [01:00,  1.44it/s]Extractor Estimating: 97it [01:00,  1.42it/s]Extractor Estimating: 98it [01:01,  1.37it/s]Extractor Estimating: 99it [01:02,  1.35it/s]Extractor Estimating: 100it [01:03,  1.37it/s]Extractor Estimating: 101it [01:03,  1.48it/s]Extractor Estimating: 102it [01:04,  1.49it/s]Extractor Estimating: 103it [01:05,  1.49it/s]Extractor Estimating: 104it [01:05,  1.54it/s]Extractor Estimating: 105it [01:06,  1.52it/s]Extractor Estimating: 106it [01:06,  1.54it/s]Extractor Estimating: 107it [01:07,  1.56it/s]Extractor Estimating: 108it [01:08,  1.52it/s]Extractor Estimating: 109it [01:08,  1.53it/s]Extractor Estimating: 110it [01:09,  1.55it/s]Extractor Estimating: 111it [01:10,  1.57it/s]Extractor Estimating: 112it [01:10,  1.55it/s]Extractor Estimating: 113it [01:11,  1.54it/s]Extractor Estimating: 114it [01:12,  1.55it/s]Extractor Estimating: 115it [01:12,  1.56it/s]Extractor Estimating: 116it [01:13,  1.56it/s]Extractor Estimating: 117it [01:14,  1.48it/s]Extractor Estimating: 118it [01:14,  1.49it/s]Extractor Estimating: 119it [01:15,  1.56it/s]Extractor Estimating: 120it [01:16,  1.50it/s]Extractor Estimating: 121it [01:16,  1.51it/s]Extractor Estimating: 122it [01:17,  1.54it/s]Extractor Estimating: 123it [01:17,  1.54it/s]Extractor Estimating: 124it [01:18,  1.45it/s]Extractor Estimating: 125it [01:19,  1.44it/s]Extractor Estimating: 126it [01:20,  1.49it/s]Extractor Estimating: 127it [01:20,  1.54it/s]Extractor Estimating: 128it [01:21,  1.47it/s]Extractor Estimating: 129it [01:22,  1.52it/s]Extractor Estimating: 130it [01:22,  1.53it/s]Extractor Estimating: 131it [01:23,  1.57it/s]Extractor Estimating: 132it [01:23,  1.62it/s]Extractor Estimating: 133it [01:24,  1.60it/s]Extractor Estimating: 134it [01:25,  1.63it/s]Extractor Estimating: 135it [01:25,  1.58it/s]Extractor Estimating: 136it [01:26,  1.56it/s]Extractor Estimating: 137it [01:27,  1.58it/s]Extractor Estimating: 138it [01:27,  1.58it/s]Extractor Estimating: 139it [01:28,  1.63it/s]Extractor Estimating: 140it [01:28,  1.58it/s]Extractor Estimating: 141it [01:29,  1.56it/s]Extractor Estimating: 142it [01:30,  1.55it/s]Extractor Estimating: 143it [01:30,  1.59it/s]Extractor Estimating: 144it [01:31,  1.59it/s]Extractor Estimating: 145it [01:32,  1.53it/s]Extractor Estimating: 146it [01:32,  1.54it/s]Extractor Estimating: 147it [01:33,  1.54it/s]Extractor Estimating: 148it [01:34,  1.51it/s]Extractor Estimating: 149it [01:34,  1.50it/s]Extractor Estimating: 150it [01:35,  1.45it/s]Extractor Estimating: 151it [01:36,  1.43it/s]Extractor Estimating: 152it [01:36,  1.46it/s]Extractor Estimating: 153it [01:37,  1.52it/s]Extractor Estimating: 154it [01:38,  1.51it/s]Extractor Estimating: 155it [01:38,  1.48it/s]Extractor Estimating: 156it [01:39,  1.50it/s]Extractor Estimating: 157it [01:40,  1.45it/s]Extractor Estimating: 158it [01:40,  1.47it/s]Extractor Estimating: 159it [01:41,  1.52it/s]Extractor Estimating: 160it [01:42,  1.39it/s]Extractor Estimating: 161it [01:43,  1.37it/s]Extractor Estimating: 162it [01:43,  1.43it/s]Extractor Estimating: 163it [01:44,  1.47it/s]Extractor Estimating: 164it [01:45,  1.49it/s]Extractor Estimating: 165it [01:45,  1.48it/s]Extractor Estimating: 166it [01:46,  1.51it/s]Extractor Estimating: 167it [01:47,  1.49it/s]Extractor Estimating: 168it [01:47,  1.54it/s]Extractor Estimating: 169it [01:48,  1.51it/s]Extractor Estimating: 170it [01:49,  1.50it/s]Extractor Estimating: 171it [01:49,  1.46it/s]Extractor Estimating: 172it [01:50,  1.51it/s]Extractor Estimating: 173it [01:51,  1.56it/s]Extractor Estimating: 174it [01:51,  1.47it/s]Extractor Estimating: 175it [01:52,  1.51it/s]Extractor Estimating: 176it [01:53,  1.40it/s]Extractor Estimating: 177it [01:54,  1.34it/s]Extractor Estimating: 178it [01:54,  1.38it/s]Extractor Estimating: 179it [01:55,  1.43it/s]Extractor Estimating: 180it [01:56,  1.46it/s]Extractor Estimating: 181it [01:56,  1.45it/s]Extractor Estimating: 182it [01:57,  1.50it/s]Extractor Estimating: 183it [01:57,  1.55it/s]Extractor Estimating: 184it [01:58,  1.57it/s]Extractor Estimating: 185it [01:59,  1.51it/s]Extractor Estimating: 186it [02:00,  1.39it/s]Extractor Estimating: 187it [02:00,  1.44it/s]Extractor Estimating: 188it [02:01,  1.41it/s]Extractor Estimating: 189it [02:02,  1.40it/s]Extractor Estimating: 190it [02:02,  1.44it/s]Extractor Estimating: 191it [02:03,  1.42it/s]Extractor Estimating: 192it [02:04,  1.46it/s]Extractor Estimating: 193it [02:04,  1.45it/s]Extractor Estimating: 194it [02:05,  1.49it/s]Extractor Estimating: 195it [02:06,  1.48it/s]Extractor Estimating: 196it [02:07,  1.40it/s]Extractor Estimating: 197it [02:07,  1.44it/s]Extractor Estimating: 198it [02:08,  1.45it/s]Extractor Estimating: 199it [02:08,  1.53it/s]Extractor Estimating: 200it [02:09,  1.56it/s]Extractor Estimating: 201it [02:10,  1.67it/s]Extractor Estimating: 202it [02:10,  1.69it/s]Extractor Estimating: 203it [02:11,  1.76it/s]Extractor Estimating: 204it [02:11,  1.83it/s]Extractor Estimating: 205it [02:12,  1.86it/s]Extractor Estimating: 206it [02:12,  1.78it/s]Extractor Estimating: 207it [02:13,  1.81it/s]Extractor Estimating: 208it [02:13,  1.91it/s]Extractor Estimating: 209it [02:14,  1.90it/s]Extractor Estimating: 210it [02:14,  1.94it/s]Extractor Estimating: 211it [02:15,  1.95it/s]Extractor Estimating: 212it [02:15,  1.89it/s]Extractor Estimating: 213it [02:16,  1.91it/s]Extractor Estimating: 214it [02:16,  1.85it/s]Extractor Estimating: 215it [02:17,  1.86it/s]Extractor Estimating: 216it [02:18,  1.79it/s]Extractor Estimating: 217it [02:18,  1.84it/s]Extractor Estimating: 218it [02:19,  1.86it/s]Extractor Estimating: 219it [02:19,  1.81it/s]Extractor Estimating: 220it [02:20,  1.82it/s]Extractor Estimating: 221it [02:20,  1.81it/s]Extractor Estimating: 222it [02:21,  1.89it/s]Extractor Estimating: 223it [02:21,  1.96it/s]Extractor Estimating: 224it [02:22,  1.79it/s]Extractor Estimating: 225it [02:22,  1.80it/s]Extractor Estimating: 226it [02:23,  1.85it/s]Extractor Estimating: 227it [02:24,  1.83it/s]Extractor Estimating: 228it [02:24,  1.92it/s]Extractor Estimating: 229it [02:25,  1.95it/s]Extractor Estimating: 230it [02:25,  1.89it/s]Extractor Estimating: 231it [02:26,  1.92it/s]Extractor Estimating: 232it [02:26,  1.93it/s]Extractor Estimating: 233it [02:27,  1.86it/s]Extractor Estimating: 234it [02:27,  1.89it/s]Extractor Estimating: 235it [02:28,  1.86it/s]Extractor Estimating: 236it [02:28,  1.82it/s]Extractor Estimating: 237it [02:29,  1.78it/s]Extractor Estimating: 238it [02:29,  1.82it/s]Extractor Estimating: 239it [02:30,  1.82it/s]Extractor Estimating: 240it [02:30,  1.89it/s]Extractor Estimating: 241it [02:31,  1.90it/s]Extractor Estimating: 242it [02:31,  1.93it/s]Extractor Estimating: 243it [02:32,  1.90it/s]Extractor Estimating: 244it [02:33,  1.89it/s]Extractor Estimating: 245it [02:33,  1.86it/s]Extractor Estimating: 246it [02:34,  1.86it/s]Extractor Estimating: 247it [02:34,  1.90it/s]Extractor Estimating: 248it [02:35,  1.93it/s]Extractor Estimating: 249it [02:35,  1.86it/s]Extractor Estimating: 250it [02:36,  1.92it/s]Extractor Estimating: 251it [02:36,  1.80it/s]Extractor Estimating: 252it [02:37,  1.89it/s]Extractor Estimating: 253it [02:37,  1.86it/s]Extractor Estimating: 254it [02:38,  1.66it/s]Extractor Estimating: 255it [02:39,  1.70it/s]Extractor Estimating: 256it [02:39,  1.78it/s]Extractor Estimating: 257it [02:40,  1.58it/s]Extractor Estimating: 258it [02:41,  1.65it/s]Extractor Estimating: 259it [02:41,  1.73it/s]Extractor Estimating: 260it [02:42,  1.76it/s]Extractor Estimating: 261it [02:42,  1.80it/s]Extractor Estimating: 262it [02:43,  1.81it/s]Extractor Estimating: 263it [02:43,  1.75it/s]Extractor Estimating: 264it [02:44,  1.82it/s]Extractor Estimating: 265it [02:44,  1.80it/s]Extractor Estimating: 266it [02:45,  1.78it/s]Extractor Estimating: 267it [02:45,  1.80it/s]Extractor Estimating: 268it [02:46,  1.79it/s]Extractor Estimating: 269it [02:47,  1.79it/s]Extractor Estimating: 270it [02:47,  1.78it/s]Extractor Estimating: 271it [02:48,  1.72it/s]Extractor Estimating: 272it [02:48,  1.82it/s]Extractor Estimating: 273it [02:49,  1.84it/s]Extractor Estimating: 274it [02:49,  1.83it/s]Extractor Estimating: 275it [02:50,  1.70it/s]Extractor Estimating: 276it [02:51,  1.62it/s]Extractor Estimating: 277it [02:51,  1.52it/s]Extractor Estimating: 278it [02:52,  1.51it/s]Extractor Estimating: 279it [02:53,  1.50it/s]Extractor Estimating: 280it [02:54,  1.45it/s]Extractor Estimating: 281it [02:54,  1.41it/s]Extractor Estimating: 282it [02:55,  1.41it/s]Extractor Estimating: 283it [02:56,  1.42it/s]Extractor Estimating: 284it [02:56,  1.45it/s]Extractor Estimating: 285it [02:57,  1.41it/s]Extractor Estimating: 286it [02:58,  1.41it/s]Extractor Estimating: 287it [02:58,  1.45it/s]Extractor Estimating: 288it [02:59,  1.43it/s]Extractor Estimating: 289it [03:00,  1.44it/s]Extractor Estimating: 290it [03:01,  1.45it/s]Extractor Estimating: 291it [03:01,  1.45it/s]Extractor Estimating: 292it [03:02,  1.46it/s]Extractor Estimating: 293it [03:03,  1.46it/s]Extractor Estimating: 294it [03:03,  1.46it/s]Extractor Estimating: 295it [03:04,  1.49it/s]Extractor Estimating: 296it [03:05,  1.48it/s]Extractor Estimating: 297it [03:05,  1.48it/s]Extractor Estimating: 298it [03:06,  1.50it/s]Extractor Estimating: 299it [03:07,  1.53it/s]Extractor Estimating: 300it [03:07,  1.57it/s]Extractor Estimating: 301it [03:08,  1.49it/s]Extractor Estimating: 302it [03:09,  1.48it/s]Extractor Estimating: 303it [03:09,  1.54it/s]Extractor Estimating: 304it [03:10,  1.65it/s]Extractor Estimating: 305it [03:10,  1.70it/s]Extractor Estimating: 306it [03:11,  1.67it/s]Extractor Estimating: 307it [03:11,  1.65it/s]Extractor Estimating: 308it [03:12,  1.66it/s]Extractor Estimating: 309it [03:13,  1.67it/s]Extractor Estimating: 310it [03:13,  1.70it/s]Extractor Estimating: 311it [03:14,  1.68it/s]Extractor Estimating: 312it [03:15,  1.56it/s]Extractor Estimating: 313it [03:15,  1.60it/s]Extractor Estimating: 314it [03:16,  1.60it/s]Extractor Estimating: 315it [03:16,  1.60it/s]Extractor Estimating: 316it [03:17,  1.63it/s]Extractor Estimating: 317it [03:18,  1.60it/s]Extractor Estimating: 318it [03:18,  1.53it/s]Extractor Estimating: 319it [03:19,  1.54it/s]Extractor Estimating: 320it [03:20,  1.59it/s]Extractor Estimating: 321it [03:20,  1.63it/s]Extractor Estimating: 322it [03:21,  1.64it/s]Extractor Estimating: 323it [03:21,  1.65it/s]Extractor Estimating: 324it [03:22,  1.48it/s]Extractor Estimating: 325it [03:23,  1.53it/s]Extractor Estimating: 326it [03:23,  1.60it/s]Extractor Estimating: 327it [03:24,  1.47it/s]Extractor Estimating: 328it [03:25,  1.50it/s]Extractor Estimating: 329it [03:25,  1.54it/s]Extractor Estimating: 330it [03:26,  1.61it/s]Extractor Estimating: 331it [03:27,  1.66it/s]Extractor Estimating: 332it [03:27,  1.58it/s]Extractor Estimating: 333it [03:28,  1.63it/s]Extractor Estimating: 334it [03:28,  1.62it/s]Extractor Estimating: 335it [03:29,  1.68it/s]Extractor Estimating: 336it [03:30,  1.70it/s]Extractor Estimating: 337it [03:30,  1.70it/s]Extractor Estimating: 338it [03:31,  1.70it/s]Extractor Estimating: 339it [03:31,  1.58it/s]Extractor Estimating: 340it [03:32,  1.60it/s]Extractor Estimating: 341it [03:33,  1.61it/s]Extractor Estimating: 342it [03:33,  1.60it/s]Extractor Estimating: 343it [03:34,  1.63it/s]Extractor Estimating: 344it [03:35,  1.64it/s]Extractor Estimating: 345it [03:35,  1.60it/s]Extractor Estimating: 346it [03:36,  1.63it/s]Extractor Estimating: 347it [03:36,  1.66it/s]Extractor Estimating: 348it [03:37,  1.61it/s]Extractor Estimating: 349it [03:38,  1.61it/s]Extractor Estimating: 350it [03:38,  1.57it/s]Extractor Estimating: 351it [03:39,  1.55it/s]Extractor Estimating: 352it [03:40,  1.55it/s]Extractor Estimating: 353it [03:40,  1.55it/s]Extractor Estimating: 354it [03:41,  1.55it/s]Extractor Estimating: 355it [03:42,  1.52it/s]Extractor Estimating: 356it [03:42,  1.58it/s]Extractor Estimating: 357it [03:43,  1.60it/s]Extractor Estimating: 358it [03:43,  1.61it/s]Extractor Estimating: 359it [03:44,  1.59it/s]Extractor Estimating: 360it [03:45,  1.55it/s]Extractor Estimating: 361it [03:45,  1.48it/s]Extractor Estimating: 362it [03:46,  1.53it/s]Extractor Estimating: 363it [03:47,  1.45it/s]Extractor Estimating: 364it [03:47,  1.52it/s]Extractor Estimating: 365it [03:48,  1.52it/s]Extractor Estimating: 366it [03:49,  1.58it/s]Extractor Estimating: 367it [03:49,  1.55it/s]Extractor Estimating: 368it [03:50,  1.56it/s]Extractor Estimating: 369it [03:51,  1.60it/s]Extractor Estimating: 370it [03:51,  1.56it/s]Extractor Estimating: 371it [03:52,  1.56it/s]Extractor Estimating: 372it [03:52,  1.59it/s]Extractor Estimating: 373it [03:53,  1.58it/s]Extractor Estimating: 374it [03:54,  1.55it/s]Extractor Estimating: 375it [03:54,  1.67it/s]Extractor Estimating: 375it [03:54,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:16,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:16,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:16,421 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:16,421 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:16,421 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:41:17,015 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:41:17,016 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:41:17,761 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:41:18,879 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:41:18,879 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:20,369 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:20,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:20,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:20,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:20,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:41:20,837 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:41:20,839 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:41:21,123 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:41:21,319 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:41:21,319 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 14:13:04,812 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 14:13:05,242 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7487 mean pseudo reward: 0.9199453861760878
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl'}
train vocab size: 18197
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18297, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=18297, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.095, loss:682.2437
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.135, loss:670.0894
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.103, loss:675.3642
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.111, loss:596.9039
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.125, loss:635.0071
>> valid entity prec:0.4990, rec:0.5685, f1:0.5315
>> valid relation prec:0.3628, rec:0.0855, f1:0.1383
>> valid relation with NER prec:0.3628, rec:0.0855, f1:0.1383
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.922, loss:603.2016
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.130, loss:593.9365
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.096, loss:594.4486
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.098, loss:606.0213
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.108, loss:585.1465
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4944, rec:0.5935, f1:0.5394
>> valid relation prec:0.3368, rec:0.1079, f1:0.1634
>> valid relation with NER prec:0.3368, rec:0.1079, f1:0.1634
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.913, loss:585.2023
g_step 1200, step 264, avg_time 1.125, loss:596.3924
g_step 1300, step 52, avg_time 1.113, loss:573.8008
g_step 1400, step 152, avg_time 1.108, loss:531.9412
g_step 1500, step 252, avg_time 1.122, loss:561.2274
>> valid entity prec:0.5414, rec:0.4530, f1:0.4933
>> valid relation prec:0.4165, rec:0.1145, f1:0.1796
>> valid relation with NER prec:0.4165, rec:0.1145, f1:0.1796
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 40, avg_time 2.890, loss:552.1098
g_step 1700, step 140, avg_time 1.109, loss:514.7857
g_step 1800, step 240, avg_time 1.113, loss:529.8032
g_step 1900, step 28, avg_time 1.127, loss:523.1799
g_step 2000, step 128, avg_time 1.128, loss:500.1694
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5563, rec:0.4552, f1:0.5007
>> valid relation prec:0.3798, rec:0.0869, f1:0.1414
>> valid relation with NER prec:0.3798, rec:0.0869, f1:0.1414
g_step 2100, step 228, avg_time 2.871, loss:500.6722
g_step 2200, step 16, avg_time 1.121, loss:513.4733
g_step 2300, step 116, avg_time 1.120, loss:453.3492
g_step 2400, step 216, avg_time 1.105, loss:478.3671
g_step 2500, step 4, avg_time 1.126, loss:515.0396
>> valid entity prec:0.5143, rec:0.5108, f1:0.5125
>> valid relation prec:0.2858, rec:0.0949, f1:0.1425
>> valid relation with NER prec:0.2858, rec:0.0949, f1:0.1425
g_step 2600, step 104, avg_time 2.903, loss:449.3380
g_step 2700, step 204, avg_time 1.104, loss:482.3715
g_step 2800, step 304, avg_time 1.110, loss:478.4339
g_step 2900, step 92, avg_time 1.127, loss:444.9711
g_step 3000, step 192, avg_time 1.107, loss:437.1839
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5479, rec:0.4312, f1:0.4826
>> valid relation prec:0.3609, rec:0.1023, f1:0.1595
>> valid relation with NER prec:0.3609, rec:0.1023, f1:0.1595
g_step 3100, step 292, avg_time 2.891, loss:473.7104
g_step 3200, step 80, avg_time 1.105, loss:431.9724
g_step 3300, step 180, avg_time 1.107, loss:445.8793
g_step 3400, step 280, avg_time 1.119, loss:424.4499
g_step 3500, step 68, avg_time 1.110, loss:429.0154
>> valid entity prec:0.5340, rec:0.4944, f1:0.5134
>> valid relation prec:0.3383, rec:0.1073, f1:0.1629
>> valid relation with NER prec:0.3383, rec:0.1073, f1:0.1629
g_step 3600, step 168, avg_time 2.890, loss:411.5514
g_step 3700, step 268, avg_time 1.133, loss:429.1594
g_step 3800, step 56, avg_time 1.111, loss:403.7993
g_step 3900, step 156, avg_time 1.123, loss:409.5321
g_step 4000, step 256, avg_time 1.107, loss:416.1626
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4936, rec:0.4881, f1:0.4908
>> valid relation prec:0.3378, rec:0.0943, f1:0.1475
>> valid relation with NER prec:0.3378, rec:0.0943, f1:0.1475
g_step 4100, step 44, avg_time 2.901, loss:410.0325
g_step 4200, step 144, avg_time 1.108, loss:381.3981
g_step 4300, step 244, avg_time 1.119, loss:399.1038
g_step 4400, step 32, avg_time 1.137, loss:402.9893
g_step 4500, step 132, avg_time 1.101, loss:382.1961
>> valid entity prec:0.5280, rec:0.4575, f1:0.4902
>> valid relation prec:0.1926, rec:0.0564, f1:0.0873
>> valid relation with NER prec:0.1926, rec:0.0564, f1:0.0873
g_step 4600, step 232, avg_time 2.881, loss:387.1901
g_step 4700, step 20, avg_time 1.135, loss:363.9497
g_step 4800, step 120, avg_time 1.106, loss:355.6218
g_step 4900, step 220, avg_time 1.127, loss:385.6418
g_step 5000, step 8, avg_time 1.109, loss:383.0210
learning rate was adjusted to 0.0008
>> valid entity prec:0.4986, rec:0.5153, f1:0.5068
>> valid relation prec:0.2724, rec:0.0927, f1:0.1383
>> valid relation with NER prec:0.2724, rec:0.0927, f1:0.1383
g_step 5100, step 108, avg_time 2.883, loss:352.0772
g_step 5200, step 208, avg_time 1.124, loss:349.6673
g_step 5300, step 308, avg_time 1.131, loss:371.3536
g_step 5400, step 96, avg_time 1.114, loss:332.5631
g_step 5500, step 196, avg_time 1.112, loss:347.7686
>> valid entity prec:0.5189, rec:0.4983, f1:0.5084
>> valid relation prec:0.3009, rec:0.1159, f1:0.1674
>> valid relation with NER prec:0.3009, rec:0.1159, f1:0.1674
g_step 5600, step 296, avg_time 2.910, loss:345.3515
g_step 5700, step 84, avg_time 1.124, loss:349.9179
g_step 5800, step 184, avg_time 1.103, loss:324.7600
g_step 5900, step 284, avg_time 1.120, loss:335.1001
g_step 6000, step 72, avg_time 1.094, loss:316.3597
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5278, rec:0.5024, f1:0.5148
>> valid relation prec:0.2694, rec:0.1032, f1:0.1492
>> valid relation with NER prec:0.2694, rec:0.1032, f1:0.1492
g_step 6100, step 172, avg_time 2.889, loss:316.6586
g_step 6200, step 272, avg_time 1.116, loss:325.5324
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 14:13:05 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 14:13:05 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_14-13-04_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 14:13:06 - WARNING - datasets.builder -   Using custom data configuration default-9dad316620c619a5
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9dad316620c619a5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 14:13:09,220 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 14:13:09,221 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 14:13:09,221 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 14:13:09,222 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 14:13:09,358 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:13:09,408 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:13:09,408 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:13:09,408 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:13:09,408 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:13:09,408 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:13:09,408 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 14:13:09,984 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 14:13:13,424 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 14:13:13,424 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9dad316620c619a5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  1.81ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.86ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.55ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.00ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.30ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.51ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.63ba/s]100%|██████████| 8/8 [00:01<00:00,  5.54ba/s]100%|██████████| 8/8 [00:01<00:00,  4.27ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.09ba/s] 40%|████      | 2/5 [00:00<00:00,  3.04ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.55ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.85ba/s]100%|██████████| 5/5 [00:01<00:00,  4.22ba/s]100%|██████████| 5/5 [00:01<00:00,  3.70ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.87ba/s] 38%|███▊      | 3/8 [00:00<00:00,  6.37ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  8.06ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.10ba/s]100%|██████████| 8/8 [00:00<00:00,  8.51ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.46ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.99ba/s]100%|██████████| 5/5 [00:00<00:00,  8.80ba/s]100%|██████████| 5/5 [00:00<00:00,  7.75ba/s]
[INFO|trainer.py:414] 2023-08-29 14:13:21,814 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 14:13:22,060 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 14:13:22,060 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 14:13:22,060 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 14:13:22,060 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 14:13:22,060 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 14:13:22,060 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 14:13:22,060 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:02<21:44,  2.23s/it]  0%|          | 2/585 [00:02<13:02,  1.34s/it]  1%|          | 3/585 [00:03<10:21,  1.07s/it]  1%|          | 4/585 [00:04<07:44,  1.25it/s]  1%|          | 5/585 [00:04<06:12,  1.56it/s]  1%|          | 6/585 [00:04<05:08,  1.88it/s]  1%|          | 7/585 [00:05<04:21,  2.21it/s]  1%|▏         | 8/585 [00:05<03:58,  2.42it/s]  2%|▏         | 9/585 [00:05<03:35,  2.68it/s]  2%|▏         | 10/585 [00:05<03:19,  2.88it/s]  2%|▏         | 11/585 [00:06<03:08,  3.04it/s]  2%|▏         | 12/585 [00:06<03:01,  3.17it/s]  2%|▏         | 13/585 [00:06<02:55,  3.26it/s]  2%|▏         | 14/585 [00:07<02:51,  3.32it/s]  3%|▎         | 15/585 [00:07<02:49,  3.37it/s]  3%|▎         | 16/585 [00:07<02:47,  3.40it/s]  3%|▎         | 17/585 [00:07<02:45,  3.42it/s]  3%|▎         | 18/585 [00:08<02:44,  3.44it/s]  3%|▎         | 19/585 [00:08<02:43,  3.46it/s]  3%|▎         | 20/585 [00:08<02:42,  3.47it/s]  4%|▎         | 21/585 [00:09<02:42,  3.47it/s]  4%|▍         | 22/585 [00:09<02:42,  3.48it/s]  4%|▍         | 23/585 [00:09<02:41,  3.48it/s]  4%|▍         | 24/585 [00:09<02:41,  3.48it/s]  4%|▍         | 25/585 [00:10<02:40,  3.48it/s]  4%|▍         | 26/585 [00:10<02:40,  3.48it/s]  5%|▍         | 27/585 [00:10<02:40,  3.48it/s]  5%|▍         | 28/585 [00:11<02:39,  3.48it/s]  5%|▍         | 29/585 [00:11<02:39,  3.48it/s]  5%|▌         | 30/585 [00:11<02:39,  3.48it/s]  5%|▌         | 31/585 [00:11<02:39,  3.48it/s]  5%|▌         | 32/585 [00:12<02:38,  3.48it/s]  6%|▌         | 33/585 [00:12<02:38,  3.48it/s]  6%|▌         | 34/585 [00:12<02:38,  3.48it/s]  6%|▌         | 35/585 [00:13<02:37,  3.48it/s]  6%|▌         | 36/585 [00:13<02:37,  3.48it/s]  6%|▋         | 37/585 [00:13<02:37,  3.48it/s]  6%|▋         | 38/585 [00:13<02:37,  3.48it/s]  7%|▋         | 39/585 [00:14<02:36,  3.48it/s]  7%|▋         | 40/585 [00:14<02:36,  3.48it/s]  7%|▋         | 41/585 [00:14<02:36,  3.48it/s]  7%|▋         | 42/585 [00:15<02:35,  3.48it/s]  7%|▋         | 43/585 [00:15<02:35,  3.48it/s]  8%|▊         | 44/585 [00:15<02:35,  3.48it/s]  8%|▊         | 45/585 [00:16<02:35,  3.48it/s]  8%|▊         | 46/585 [00:16<02:35,  3.48it/s]  8%|▊         | 47/585 [00:16<02:34,  3.48it/s]  8%|▊         | 48/585 [00:16<02:34,  3.48it/s]  8%|▊         | 49/585 [00:17<02:34,  3.48it/s]  9%|▊         | 50/585 [00:17<02:33,  3.48it/s]  9%|▊         | 51/585 [00:17<02:33,  3.48it/s]  9%|▉         | 52/585 [00:18<02:33,  3.48it/s]  9%|▉         | 53/585 [00:18<02:33,  3.48it/s]  9%|▉         | 54/585 [00:18<02:32,  3.48it/s]  9%|▉         | 55/585 [00:18<02:32,  3.48it/s] 10%|▉         | 56/585 [00:19<02:32,  3.47it/s] 10%|▉         | 57/585 [00:19<02:31,  3.48it/s] 10%|▉         | 58/585 [00:19<02:31,  3.48it/s] 10%|█         | 59/585 [00:20<02:31,  3.48it/s] 10%|█         | 60/585 [00:20<02:30,  3.48it/s] 10%|█         | 61/585 [00:20<02:30,  3.48it/s] 11%|█         | 62/585 [00:20<02:30,  3.48it/s] 11%|█         | 63/585 [00:21<02:30,  3.48it/s] 11%|█         | 64/585 [00:21<02:29,  3.48it/s] 11%|█         | 65/585 [00:21<02:29,  3.48it/s] 11%|█▏        | 66/585 [00:22<02:29,  3.48it/s] 11%|█▏        | 67/585 [00:22<02:28,  3.48it/s] 12%|█▏        | 68/585 [00:22<02:28,  3.48it/s] 12%|█▏        | 69/585 [00:22<02:28,  3.48it/s] 12%|█▏        | 70/585 [00:23<02:28,  3.48it/s] 12%|█▏        | 71/585 [00:23<02:39,  3.22it/s] 12%|█▏        | 72/585 [00:23<02:35,  3.30it/s] 12%|█▏        | 73/585 [00:24<02:32,  3.35it/s] 13%|█▎        | 74/585 [00:24<02:30,  3.39it/s] 13%|█▎        | 75/585 [00:24<02:29,  3.41it/s] 13%|█▎        | 76/585 [00:24<02:28,  3.43it/s] 13%|█▎        | 77/585 [00:25<02:27,  3.45it/s] 13%|█▎        | 78/585 [00:25<02:26,  3.45it/s] 14%|█▎        | 79/585 [00:25<02:26,  3.46it/s] 14%|█▎        | 80/585 [00:26<02:25,  3.46it/s] 14%|█▍        | 81/585 [00:26<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:26<02:25,  3.47it/s] 14%|█▍        | 83/585 [00:27<02:24,  3.47it/s] 14%|█▍        | 84/585 [00:27<02:24,  3.47it/s] 15%|█▍        | 85/585 [00:27<02:23,  3.47it/s] 15%|█▍        | 86/585 [00:27<02:23,  3.47it/s] 15%|█▍        | 87/585 [00:28<02:23,  3.47it/s] 15%|█▌        | 88/585 [00:28<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:28<02:22,  3.48it/s] 15%|█▌        | 90/585 [00:29<02:22,  3.47it/s] 16%|█▌        | 91/585 [00:29<02:22,  3.48it/s] 16%|█▌        | 92/585 [00:29<02:21,  3.47it/s] 16%|█▌        | 93/585 [00:29<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:30<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:30<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:30<02:20,  3.47it/s] 17%|█▋        | 97/585 [00:31<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:31<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:31<02:20,  3.47it/s] 17%|█▋        | 100/585 [00:31<02:19,  3.47it/s] 17%|█▋        | 101/585 [00:32<02:19,  3.47it/s] 17%|█▋        | 102/585 [00:32<02:19,  3.47it/s] 18%|█▊        | 103/585 [00:32<02:18,  3.47it/s] 18%|█▊        | 104/585 [00:33<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:33<02:18,  3.47it/s] 18%|█▊        | 106/585 [00:33<02:18,  3.47it/s] 18%|█▊        | 107/585 [00:33<02:17,  3.47it/s] 18%|█▊        | 108/585 [00:34<02:17,  3.47it/s] 19%|█▊        | 109/585 [00:34<02:17,  3.47it/s] 19%|█▉        | 110/585 [00:34<02:16,  3.47it/s] 19%|█▉        | 111/585 [00:35<02:16,  3.47it/s] 19%|█▉        | 112/585 [00:35<02:16,  3.47it/s] 19%|█▉        | 113/585 [00:35<02:15,  3.47it/s] 19%|█▉        | 114/585 [00:35<02:15,  3.47it/s] 20%|█▉        | 115/585 [00:36<02:15,  3.47it/s] 20%|█▉        | 116/585 [00:36<02:15,  3.47it/s] 20%|██        | 117/585 [00:36<02:14,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 14:13:58,913 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:13:58,913 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 14:13:58,913 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.98it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.60it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.66it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.93it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.50it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.16it/s][A
  6%|▋         | 38/608 [00:00<00:12, 47.04it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.75it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.55it/s][A
  9%|▊         | 53/608 [00:01<00:12, 44.05it/s][A
 10%|▉         | 58/608 [00:01<00:12, 44.85it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.42it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.80it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.98it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.25it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.39it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.43it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.47it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.43it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.52it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.60it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.67it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.66it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.63it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.59it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.66it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.70it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.68it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.69it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.66it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.66it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.61it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.61it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.71it/s][A
 29%|██▉       | 178/608 [00:03<00:10, 39.73it/s][A
 30%|███       | 183/608 [00:03<00:10, 41.63it/s][A
 31%|███       | 188/608 [00:04<00:09, 43.02it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.12it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 44.84it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.34it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.68it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.98it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.17it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.27it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.42it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.51it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.48it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.58it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.62it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.61it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.53it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.54it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.48it/s][A
 45%|████▍     | 273/608 [00:05<00:08, 39.08it/s][A
 46%|████▌     | 278/608 [00:06<00:12, 25.53it/s][A
 47%|████▋     | 283/608 [00:06<00:10, 29.59it/s][A
 47%|████▋     | 288/608 [00:06<00:09, 33.17it/s][A
 48%|████▊     | 293/608 [00:06<00:08, 36.32it/s][A
 49%|████▉     | 298/608 [00:06<00:08, 36.25it/s][A
 50%|████▉     | 303/608 [00:06<00:07, 39.40it/s][A
 51%|█████     | 308/608 [00:07<00:07, 41.28it/s][A
 51%|█████▏    | 313/608 [00:07<00:06, 42.79it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 43.84it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 44.71it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.25it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.62it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.94it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.00it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.21it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.38it/s][A
 59%|█████▉    | 358/608 [00:08<00:05, 46.46it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 46.43it/s][A
 61%|██████    | 368/608 [00:08<00:05, 46.54it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.54it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.66it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.57it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.41it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.52it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.63it/s][A
 66%|██████▋   | 403/608 [00:09<00:04, 46.59it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 46.62it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 46.60it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.51it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.43it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.04it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.45it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.81it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.02it/s][A
 74%|███████▎  | 448/608 [00:10<00:03, 46.21it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 46.36it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 46.43it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.50it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.45it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.31it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.44it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.55it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.46it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.53it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 46.59it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 46.55it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 46.55it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.51it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.45it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.42it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.42it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.48it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.50it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.93it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 46.19it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 46.31it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.42it/s][A
 93%|█████████▎| 563/608 [00:12<00:01, 41.75it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 43.01it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 44.03it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 44.82it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.33it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.72it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 41.55it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 42.94it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 44.02it/s][A
100%|██████████| 608/608 [00:13<00:00, 44.68it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.68it/s][A 20%|██        | 117/585 [00:50<02:14,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:14:13,030 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 14:14:13,359 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:14:17,827 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:14:18,106 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:14:18,233 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:05<1:08:42,  8.83s/it] 20%|██        | 119/585 [01:05<48:43,  6.27s/it]   21%|██        | 120/585 [01:06<34:42,  4.48s/it] 21%|██        | 121/585 [01:06<24:54,  3.22s/it] 21%|██        | 122/585 [01:06<18:03,  2.34s/it] 21%|██        | 123/585 [01:07<13:16,  1.72s/it] 21%|██        | 124/585 [01:07<09:56,  1.29s/it] 21%|██▏       | 125/585 [01:07<07:36,  1.01it/s] 22%|██▏       | 126/585 [01:07<05:58,  1.28it/s] 22%|██▏       | 127/585 [01:08<04:49,  1.58it/s] 22%|██▏       | 128/585 [01:08<04:01,  1.89it/s] 22%|██▏       | 129/585 [01:08<03:28,  2.19it/s] 22%|██▏       | 130/585 [01:09<03:07,  2.42it/s] 22%|██▏       | 131/585 [01:09<02:50,  2.67it/s] 23%|██▎       | 132/585 [01:09<02:38,  2.87it/s] 23%|██▎       | 133/585 [01:09<02:29,  3.03it/s] 23%|██▎       | 134/585 [01:10<02:23,  3.15it/s] 23%|██▎       | 135/585 [01:10<02:18,  3.24it/s] 23%|██▎       | 136/585 [01:10<02:15,  3.31it/s] 23%|██▎       | 137/585 [01:11<02:13,  3.36it/s] 24%|██▎       | 138/585 [01:11<02:11,  3.39it/s] 24%|██▍       | 139/585 [01:11<02:10,  3.42it/s] 24%|██▍       | 140/585 [01:11<02:09,  3.43it/s] 24%|██▍       | 141/585 [01:12<02:12,  3.35it/s] 24%|██▍       | 142/585 [01:12<02:10,  3.39it/s] 24%|██▍       | 143/585 [01:12<02:09,  3.42it/s] 25%|██▍       | 144/585 [01:13<02:08,  3.43it/s] 25%|██▍       | 145/585 [01:13<02:07,  3.45it/s] 25%|██▍       | 146/585 [01:13<02:07,  3.46it/s] 25%|██▌       | 147/585 [01:13<02:06,  3.46it/s] 25%|██▌       | 148/585 [01:14<02:06,  3.47it/s] 25%|██▌       | 149/585 [01:14<02:05,  3.47it/s] 26%|██▌       | 150/585 [01:14<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:15<02:04,  3.47it/s] 26%|██▌       | 152/585 [01:15<02:09,  3.35it/s] 26%|██▌       | 153/585 [01:15<02:07,  3.39it/s] 26%|██▋       | 154/585 [01:16<02:06,  3.41it/s] 26%|██▋       | 155/585 [01:16<02:05,  3.43it/s] 27%|██▋       | 156/585 [01:16<02:04,  3.44it/s] 27%|██▋       | 157/585 [01:16<02:04,  3.45it/s] 27%|██▋       | 158/585 [01:17<02:03,  3.46it/s] 27%|██▋       | 159/585 [01:17<02:03,  3.46it/s] 27%|██▋       | 160/585 [01:17<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:18<02:02,  3.47it/s] 28%|██▊       | 162/585 [01:18<02:01,  3.47it/s] 28%|██▊       | 163/585 [01:18<02:05,  3.35it/s] 28%|██▊       | 164/585 [01:18<02:04,  3.39it/s] 28%|██▊       | 165/585 [01:19<02:02,  3.42it/s] 28%|██▊       | 166/585 [01:19<02:02,  3.43it/s] 29%|██▊       | 167/585 [01:19<02:01,  3.44it/s] 29%|██▊       | 168/585 [01:20<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:20<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:20<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:20<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:21<01:59,  3.47it/s] 30%|██▉       | 173/585 [01:21<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:21<01:58,  3.47it/s] 30%|██▉       | 175/585 [01:22<01:58,  3.47it/s] 30%|███       | 176/585 [01:22<01:57,  3.47it/s] 30%|███       | 177/585 [01:22<01:57,  3.47it/s] 30%|███       | 178/585 [01:22<01:59,  3.40it/s] 31%|███       | 179/585 [01:23<01:58,  3.42it/s] 31%|███       | 180/585 [01:23<01:57,  3.44it/s] 31%|███       | 181/585 [01:23<01:57,  3.45it/s] 31%|███       | 182/585 [01:24<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:24<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:24<01:55,  3.46it/s] 32%|███▏      | 185/585 [01:25<01:55,  3.46it/s] 32%|███▏      | 186/585 [01:25<01:55,  3.47it/s] 32%|███▏      | 187/585 [01:25<01:54,  3.47it/s] 32%|███▏      | 188/585 [01:25<01:54,  3.47it/s] 32%|███▏      | 189/585 [01:26<01:59,  3.31it/s] 32%|███▏      | 190/585 [01:26<01:57,  3.36it/s] 33%|███▎      | 191/585 [01:26<01:56,  3.39it/s] 33%|███▎      | 192/585 [01:27<01:55,  3.42it/s] 33%|███▎      | 193/585 [01:27<01:54,  3.43it/s] 33%|███▎      | 194/585 [01:27<01:53,  3.44it/s] 33%|███▎      | 195/585 [01:27<01:52,  3.45it/s] 34%|███▎      | 196/585 [01:28<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:28<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:28<01:51,  3.47it/s] 34%|███▍      | 199/585 [01:29<01:51,  3.46it/s] 34%|███▍      | 200/585 [01:29<01:53,  3.40it/s] 34%|███▍      | 201/585 [01:29<01:52,  3.42it/s] 35%|███▍      | 202/585 [01:29<01:51,  3.44it/s] 35%|███▍      | 203/585 [01:30<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:30<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:30<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:31<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:31<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:31<01:48,  3.47it/s] 36%|███▌      | 209/585 [01:31<01:48,  3.47it/s] 36%|███▌      | 210/585 [01:32<01:48,  3.47it/s] 36%|███▌      | 211/585 [01:32<01:50,  3.39it/s] 36%|███▌      | 212/585 [01:32<01:49,  3.41it/s] 36%|███▋      | 213/585 [01:33<01:48,  3.43it/s] 37%|███▋      | 214/585 [01:33<01:47,  3.44it/s] 37%|███▋      | 215/585 [01:33<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:34<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:34<01:46,  3.46it/s] 37%|███▋      | 218/585 [01:34<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:34<01:45,  3.46it/s] 38%|███▊      | 220/585 [01:35<01:45,  3.46it/s] 38%|███▊      | 221/585 [01:35<01:45,  3.46it/s] 38%|███▊      | 222/585 [01:35<01:48,  3.36it/s] 38%|███▊      | 223/585 [01:36<01:46,  3.39it/s] 38%|███▊      | 224/585 [01:36<01:45,  3.41it/s] 38%|███▊      | 225/585 [01:36<01:44,  3.43it/s] 39%|███▊      | 226/585 [01:36<01:44,  3.44it/s] 39%|███▉      | 227/585 [01:37<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:37<01:43,  3.46it/s] 39%|███▉      | 229/585 [01:37<01:42,  3.46it/s] 39%|███▉      | 230/585 [01:38<01:42,  3.46it/s] 39%|███▉      | 231/585 [01:38<01:42,  3.46it/s] 40%|███▉      | 232/585 [01:38<01:41,  3.46it/s] 40%|███▉      | 233/585 [01:39<01:46,  3.31it/s] 40%|████      | 234/585 [01:39<01:44,  3.36it/s][INFO|trainer.py:2140] 2023-08-29 14:15:01,400 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:15:01,400 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 14:15:01,400 >>   Batch size = 8
{'eval_loss': 1.0451455116271973, 'eval_runtime': 13.5725, 'eval_samples_per_second': 358.077, 'eval_steps_per_second': 44.796, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.27it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.36it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.52it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.92it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.41it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.17it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.91it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.52it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.52it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.51it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.58it/s][A
 10%|█         | 63/608 [00:01<00:12, 43.72it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.05it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.49it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.93it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.04it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.20it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.35it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.38it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.50it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.19it/s][A
 19%|█▊        | 113/608 [00:02<00:11, 44.39it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.12it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.64it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.90it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.07it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.24it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.36it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.40it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.18it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.07it/s][A
 27%|██▋       | 163/608 [00:03<00:11, 39.14it/s][A
 28%|██▊       | 168/608 [00:03<00:13, 33.44it/s][A
 28%|██▊       | 173/608 [00:03<00:12, 35.77it/s][A
 29%|██▉       | 178/608 [00:04<00:11, 38.40it/s][A
 30%|███       | 183/608 [00:04<00:10, 40.56it/s][A
 31%|███       | 188/608 [00:04<00:09, 42.21it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 43.46it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 44.35it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 44.99it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.37it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.50it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.75it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.86it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 46.11it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.37it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.43it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.51it/s][A
 41%|████      | 248/608 [00:05<00:08, 44.19it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 44.82it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.40it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.66it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.85it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 46.07it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 46.21it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.35it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.29it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.31it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.36it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.42it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.40it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.45it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 46.51it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 46.40it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.46it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.49it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.37it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.44it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.46it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.44it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.48it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 46.52it/s][A
 61%|██████    | 368/608 [00:08<00:05, 46.38it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.46it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.41it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.40it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 44.35it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 44.99it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.43it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.81it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.05it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 46.25it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.41it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.32it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.11it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.18it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.24it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.29it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.39it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.45it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 46.55it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.56it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.49it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.38it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.38it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.21it/s][A
 80%|████████  | 488/608 [00:10<00:02, 44.22it/s][A
 81%|████████  | 493/608 [00:10<00:02, 44.95it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 45.45it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.86it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 46.10it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.26it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.33it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.37it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.28it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.10it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.16it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.21it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 46.37it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 46.46it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.58it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.54it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.48it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.45it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.35it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.19it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.20it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.20it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 46.41it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.54it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.57it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.57it/s][A 40%|████      | 234/585 [01:52<01:44,  3.36it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:15:15,200 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 14:15:15,630 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:15:20,347 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:15:20,676 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:15:20,816 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:16<1:05:58, 11.31s/it] 40%|████      | 236/585 [02:16<46:38,  8.02s/it]   41%|████      | 237/585 [02:16<33:03,  5.70s/it] 41%|████      | 238/585 [02:17<23:34,  4.08s/it] 41%|████      | 239/585 [02:17<16:56,  2.94s/it] 41%|████      | 240/585 [02:17<12:19,  2.14s/it] 41%|████      | 241/585 [02:18<09:05,  1.59s/it] 41%|████▏     | 242/585 [02:18<06:50,  1.20s/it] 42%|████▏     | 243/585 [02:18<05:16,  1.08it/s] 42%|████▏     | 244/585 [02:18<04:09,  1.36it/s] 42%|████▏     | 245/585 [02:19<03:23,  1.67it/s] 42%|████▏     | 246/585 [02:19<02:51,  1.98it/s] 42%|████▏     | 247/585 [02:19<02:33,  2.20it/s] 42%|████▏     | 248/585 [02:20<02:16,  2.47it/s] 43%|████▎     | 249/585 [02:20<02:04,  2.71it/s] 43%|████▎     | 250/585 [02:20<01:55,  2.90it/s] 43%|████▎     | 251/585 [02:20<01:49,  3.05it/s] 43%|████▎     | 252/585 [02:21<01:56,  2.85it/s] 43%|████▎     | 253/585 [02:21<01:50,  3.02it/s] 43%|████▎     | 254/585 [02:21<01:45,  3.14it/s] 44%|████▎     | 255/585 [02:22<01:41,  3.24it/s] 44%|████▍     | 256/585 [02:22<01:39,  3.31it/s] 44%|████▍     | 257/585 [02:22<01:37,  3.35it/s] 44%|████▍     | 258/585 [02:23<01:36,  3.39it/s] 44%|████▍     | 259/585 [02:23<01:35,  3.42it/s] 44%|████▍     | 260/585 [02:23<01:34,  3.43it/s] 45%|████▍     | 261/585 [02:23<01:34,  3.45it/s] 45%|████▍     | 262/585 [02:24<01:39,  3.24it/s] 45%|████▍     | 263/585 [02:24<01:37,  3.30it/s] 45%|████▌     | 264/585 [02:24<01:35,  3.36it/s] 45%|████▌     | 265/585 [02:25<01:34,  3.39it/s] 45%|████▌     | 266/585 [02:25<01:33,  3.41it/s] 46%|████▌     | 267/585 [02:25<01:32,  3.43it/s] 46%|████▌     | 268/585 [02:26<01:31,  3.45it/s] 46%|████▌     | 269/585 [02:26<01:31,  3.45it/s] 46%|████▌     | 270/585 [02:26<01:30,  3.46it/s] 46%|████▋     | 271/585 [02:26<01:30,  3.47it/s] 46%|████▋     | 272/585 [02:27<01:30,  3.47it/s] 47%|████▋     | 273/585 [02:27<01:34,  3.30it/s] 47%|████▋     | 274/585 [02:27<01:32,  3.35it/s] 47%|████▋     | 275/585 [02:28<01:31,  3.39it/s] 47%|████▋     | 276/585 [02:28<01:30,  3.41it/s] 47%|████▋     | 277/585 [02:28<01:29,  3.43it/s] 48%|████▊     | 278/585 [02:28<01:29,  3.44it/s] 48%|████▊     | 279/585 [02:29<01:28,  3.45it/s] 48%|████▊     | 280/585 [02:29<01:28,  3.46it/s] 48%|████▊     | 281/585 [02:29<01:27,  3.46it/s] 48%|████▊     | 282/585 [02:30<01:27,  3.47it/s] 48%|████▊     | 283/585 [02:30<01:27,  3.47it/s] 49%|████▊     | 284/585 [02:30<01:33,  3.22it/s] 49%|████▊     | 285/585 [02:31<01:31,  3.29it/s] 49%|████▉     | 286/585 [02:31<01:29,  3.35it/s] 49%|████▉     | 287/585 [02:31<01:28,  3.39it/s] 49%|████▉     | 288/585 [02:31<01:27,  3.41it/s] 49%|████▉     | 289/585 [02:32<01:26,  3.43it/s] 50%|████▉     | 290/585 [02:32<01:25,  3.44it/s] 50%|████▉     | 291/585 [02:32<01:25,  3.45it/s] 50%|████▉     | 292/585 [02:33<01:24,  3.46it/s] 50%|█████     | 293/585 [02:33<01:24,  3.47it/s] 50%|█████     | 294/585 [02:33<01:23,  3.47it/s] 50%|█████     | 295/585 [02:33<01:25,  3.38it/s] 51%|█████     | 296/585 [02:34<01:24,  3.40it/s] 51%|█████     | 297/585 [02:34<01:24,  3.43it/s] 51%|█████     | 298/585 [02:34<01:23,  3.44it/s] 51%|█████     | 299/585 [02:35<01:22,  3.45it/s] 51%|█████▏    | 300/585 [02:35<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:35<01:22,  3.46it/s] 52%|█████▏    | 302/585 [02:35<01:21,  3.47it/s] 52%|█████▏    | 303/585 [02:36<01:21,  3.47it/s] 52%|█████▏    | 304/585 [02:36<01:20,  3.47it/s] 52%|█████▏    | 305/585 [02:36<01:20,  3.47it/s] 52%|█████▏    | 306/585 [02:37<01:24,  3.32it/s] 52%|█████▏    | 307/585 [02:37<01:22,  3.37it/s] 53%|█████▎    | 308/585 [02:37<01:21,  3.40it/s] 53%|█████▎    | 309/585 [02:38<01:20,  3.42it/s] 53%|█████▎    | 310/585 [02:38<01:20,  3.44it/s] 53%|█████▎    | 311/585 [02:38<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:38<01:19,  3.46it/s] 54%|█████▎    | 313/585 [02:39<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:39<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:39<01:17,  3.46it/s] 54%|█████▍    | 316/585 [02:40<01:17,  3.47it/s] 54%|█████▍    | 317/585 [02:40<01:20,  3.31it/s] 54%|█████▍    | 318/585 [02:40<01:19,  3.36it/s] 55%|█████▍    | 319/585 [02:40<01:18,  3.39it/s] 55%|█████▍    | 320/585 [02:41<01:17,  3.42it/s] 55%|█████▍    | 321/585 [02:41<01:16,  3.43it/s] 55%|█████▌    | 322/585 [02:41<01:16,  3.44it/s] 55%|█████▌    | 323/585 [02:42<01:15,  3.45it/s] 55%|█████▌    | 324/585 [02:42<01:15,  3.46it/s] 56%|█████▌    | 325/585 [02:42<01:15,  3.46it/s] 56%|█████▌    | 326/585 [02:42<01:14,  3.47it/s] 56%|█████▌    | 327/585 [02:43<01:19,  3.25it/s] 56%|█████▌    | 328/585 [02:43<01:20,  3.18it/s] 56%|█████▌    | 329/585 [02:43<01:18,  3.26it/s] 56%|█████▋    | 330/585 [02:44<01:16,  3.32it/s] 57%|█████▋    | 331/585 [02:44<01:15,  3.37it/s] 57%|█████▋    | 332/585 [02:44<01:14,  3.40it/s] 57%|█████▋    | 333/585 [02:45<01:13,  3.42it/s] 57%|█████▋    | 334/585 [02:45<01:13,  3.43it/s] 57%|█████▋    | 335/585 [02:45<01:12,  3.44it/s] 57%|█████▋    | 336/585 [02:45<01:12,  3.45it/s] 58%|█████▊    | 337/585 [02:46<01:11,  3.46it/s] 58%|█████▊    | 338/585 [02:46<01:11,  3.46it/s] 58%|█████▊    | 339/585 [02:46<01:13,  3.34it/s] 58%|█████▊    | 340/585 [02:47<01:12,  3.38it/s] 58%|█████▊    | 341/585 [02:47<01:11,  3.41it/s] 58%|█████▊    | 342/585 [02:47<01:10,  3.42it/s] 59%|█████▊    | 343/585 [02:48<01:10,  3.44it/s] 59%|█████▉    | 344/585 [02:48<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:48<01:09,  3.46it/s] 59%|█████▉    | 346/585 [02:48<01:09,  3.46it/s] 59%|█████▉    | 347/585 [02:49<01:08,  3.46it/s] 59%|█████▉    | 348/585 [02:49<01:08,  3.47it/s] 60%|█████▉    | 349/585 [02:49<01:08,  3.47it/s] 60%|█████▉    | 350/585 [02:50<01:08,  3.41it/s] 60%|██████    | 351/585 [02:50<01:08,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 14:16:12,467 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:16:12,467 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 14:16:12,467 >>   Batch size = 8
{'eval_loss': 1.0561567544937134, 'eval_runtime': 13.3448, 'eval_samples_per_second': 364.188, 'eval_steps_per_second': 45.561, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.37it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.45it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.72it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.04it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.66it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.33it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.95it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.72it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.64it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.74it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.69it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.69it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.72it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.73it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.72it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.68it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.54it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 46.55it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.60it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.60it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.62it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.64it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.67it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.73it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.67it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.46it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.61it/s][A
 24%|██▎       | 143/608 [00:03<00:09, 46.66it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.59it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.60it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.66it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.70it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.73it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.58it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.53it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.52it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.56it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.63it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.53it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.62it/s][A
 34%|███▍      | 208/608 [00:04<00:09, 40.13it/s][A
 35%|███▌      | 213/608 [00:04<00:09, 41.88it/s][A
 36%|███▌      | 218/608 [00:04<00:09, 43.24it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 44.27it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 44.82it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.49it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.86it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.07it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.00it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.06it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.26it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.31it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.27it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.42it/s][A
 46%|████▌     | 278/608 [00:05<00:07, 46.44it/s][A
 47%|████▋     | 283/608 [00:06<00:06, 46.55it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.64it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.45it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.36it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.35it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.40it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.50it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.59it/s][A
 53%|█████▎    | 323/608 [00:06<00:06, 46.49it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.65it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.62it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.53it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.54it/s][A
 57%|█████▋    | 348/608 [00:07<00:06, 41.42it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 42.83it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 43.92it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 44.76it/s][A
 61%|██████    | 368/608 [00:07<00:05, 45.28it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.74it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.07it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.24it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.01it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.04it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.19it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.29it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.49it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.54it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.59it/s][A
 70%|██████▉   | 423/608 [00:09<00:03, 46.44it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.51it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.43it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.27it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.33it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.33it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.31it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.42it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.50it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.57it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.58it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.37it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.32it/s][A
 80%|████████  | 488/608 [00:10<00:03, 38.57it/s][A
 81%|████████  | 493/608 [00:10<00:02, 40.58it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 42.26it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 43.53it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 44.35it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.06it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.57it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.82it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.76it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.83it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.97it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.11it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.23it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 46.41it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.56it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.49it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.48it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.38it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.30it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.35it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.31it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.37it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 46.39it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.55it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.53it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.53it/s][A 60%|██████    | 351/585 [03:03<01:08,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:16:25,793 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 14:16:26,075 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:16:29,659 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:16:29,878 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:16:29,976 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:16<31:23,  8.08s/it] 60%|██████    | 353/585 [03:16<22:15,  5.75s/it] 61%|██████    | 354/585 [03:17<15:50,  4.11s/it] 61%|██████    | 355/585 [03:17<11:22,  2.97s/it] 61%|██████    | 356/585 [03:17<08:15,  2.16s/it] 61%|██████    | 357/585 [03:18<06:04,  1.60s/it] 61%|██████    | 358/585 [03:18<04:33,  1.21s/it] 61%|██████▏   | 359/585 [03:18<03:30,  1.07it/s] 62%|██████▏   | 360/585 [03:18<02:46,  1.35it/s] 62%|██████▏   | 361/585 [03:19<02:15,  1.66it/s] 62%|██████▏   | 362/585 [03:19<01:53,  1.97it/s] 62%|██████▏   | 363/585 [03:19<01:38,  2.26it/s] 62%|██████▏   | 364/585 [03:20<01:29,  2.47it/s] 62%|██████▏   | 365/585 [03:20<01:21,  2.71it/s] 63%|██████▎   | 366/585 [03:20<01:15,  2.90it/s] 63%|██████▎   | 367/585 [03:21<01:11,  3.05it/s] 63%|██████▎   | 368/585 [03:21<01:08,  3.17it/s] 63%|██████▎   | 369/585 [03:21<01:06,  3.25it/s] 63%|██████▎   | 370/585 [03:21<01:04,  3.32it/s] 63%|██████▎   | 371/585 [03:22<01:03,  3.36it/s] 64%|██████▎   | 372/585 [03:22<01:02,  3.40it/s] 64%|██████▍   | 373/585 [03:22<01:02,  3.42it/s] 64%|██████▍   | 374/585 [03:23<01:01,  3.43it/s] 64%|██████▍   | 375/585 [03:23<01:00,  3.45it/s] 64%|██████▍   | 376/585 [03:23<01:00,  3.45it/s] 64%|██████▍   | 377/585 [03:23<01:00,  3.46it/s] 65%|██████▍   | 378/585 [03:24<00:59,  3.47it/s] 65%|██████▍   | 379/585 [03:24<00:59,  3.47it/s] 65%|██████▍   | 380/585 [03:24<00:59,  3.47it/s] 65%|██████▌   | 381/585 [03:25<00:58,  3.47it/s] 65%|██████▌   | 382/585 [03:25<00:58,  3.47it/s] 65%|██████▌   | 383/585 [03:25<00:58,  3.47it/s] 66%|██████▌   | 384/585 [03:25<01:00,  3.31it/s] 66%|██████▌   | 385/585 [03:26<00:59,  3.34it/s] 66%|██████▌   | 386/585 [03:26<00:58,  3.38it/s] 66%|██████▌   | 387/585 [03:26<00:58,  3.40it/s] 66%|██████▋   | 388/585 [03:27<00:57,  3.42it/s] 66%|██████▋   | 389/585 [03:27<00:57,  3.44it/s] 67%|██████▋   | 390/585 [03:27<00:56,  3.45it/s] 67%|██████▋   | 391/585 [03:27<00:56,  3.45it/s] 67%|██████▋   | 392/585 [03:28<00:55,  3.46it/s] 67%|██████▋   | 393/585 [03:28<00:55,  3.46it/s] 67%|██████▋   | 394/585 [03:28<00:55,  3.47it/s] 68%|██████▊   | 395/585 [03:29<00:56,  3.35it/s] 68%|██████▊   | 396/585 [03:29<00:55,  3.39it/s] 68%|██████▊   | 397/585 [03:29<00:55,  3.41it/s] 68%|██████▊   | 398/585 [03:30<00:54,  3.43it/s] 68%|██████▊   | 399/585 [03:30<00:54,  3.44it/s] 68%|██████▊   | 400/585 [03:30<00:53,  3.45it/s] 69%|██████▊   | 401/585 [03:30<00:53,  3.46it/s] 69%|██████▊   | 402/585 [03:31<00:52,  3.46it/s] 69%|██████▉   | 403/585 [03:31<00:52,  3.47it/s] 69%|██████▉   | 404/585 [03:31<00:52,  3.47it/s] 69%|██████▉   | 405/585 [03:32<00:51,  3.47it/s] 69%|██████▉   | 406/585 [03:32<00:54,  3.26it/s] 70%|██████▉   | 407/585 [03:32<00:53,  3.32it/s] 70%|██████▉   | 408/585 [03:32<00:52,  3.36it/s] 70%|██████▉   | 409/585 [03:33<00:51,  3.39it/s] 70%|███████   | 410/585 [03:33<00:51,  3.42it/s] 70%|███████   | 411/585 [03:33<00:50,  3.43it/s] 70%|███████   | 412/585 [03:34<00:50,  3.44it/s] 71%|███████   | 413/585 [03:34<00:49,  3.45it/s] 71%|███████   | 414/585 [03:34<00:49,  3.46it/s] 71%|███████   | 415/585 [03:34<00:49,  3.46it/s] 71%|███████   | 416/585 [03:35<00:48,  3.46it/s] 71%|███████▏  | 417/585 [03:35<00:51,  3.28it/s] 71%|███████▏  | 418/585 [03:35<00:50,  3.34it/s] 72%|███████▏  | 419/585 [03:36<00:49,  3.38it/s] 72%|███████▏  | 420/585 [03:36<00:48,  3.40it/s] 72%|███████▏  | 421/585 [03:36<00:47,  3.42it/s] 72%|███████▏  | 422/585 [03:37<00:47,  3.44it/s] 72%|███████▏  | 423/585 [03:37<00:46,  3.45it/s] 72%|███████▏  | 424/585 [03:37<00:46,  3.46it/s] 73%|███████▎  | 425/585 [03:37<00:46,  3.46it/s] 73%|███████▎  | 426/585 [03:38<00:45,  3.46it/s] 73%|███████▎  | 427/585 [03:38<00:45,  3.47it/s] 73%|███████▎  | 428/585 [03:38<00:46,  3.36it/s] 73%|███████▎  | 429/585 [03:39<00:45,  3.39it/s] 74%|███████▎  | 430/585 [03:39<00:45,  3.42it/s] 74%|███████▎  | 431/585 [03:39<00:44,  3.43it/s] 74%|███████▍  | 432/585 [03:39<00:44,  3.44it/s] 74%|███████▍  | 433/585 [03:40<00:44,  3.45it/s] 74%|███████▍  | 434/585 [03:40<00:43,  3.46it/s] 74%|███████▍  | 435/585 [03:40<00:43,  3.46it/s] 75%|███████▍  | 436/585 [03:41<00:43,  3.46it/s] 75%|███████▍  | 437/585 [03:41<00:42,  3.47it/s] 75%|███████▍  | 438/585 [03:41<00:42,  3.47it/s] 75%|███████▌  | 439/585 [03:42<00:43,  3.33it/s] 75%|███████▌  | 440/585 [03:42<00:43,  3.36it/s] 75%|███████▌  | 441/585 [03:42<00:42,  3.40it/s] 76%|███████▌  | 442/585 [03:42<00:41,  3.42it/s] 76%|███████▌  | 443/585 [03:43<00:41,  3.43it/s] 76%|███████▌  | 444/585 [03:43<00:43,  3.27it/s] 76%|███████▌  | 445/585 [03:43<00:42,  3.31it/s] 76%|███████▌  | 446/585 [03:44<00:41,  3.36it/s] 76%|███████▋  | 447/585 [03:44<00:40,  3.39it/s] 77%|███████▋  | 448/585 [03:44<00:40,  3.41it/s] 77%|███████▋  | 449/585 [03:44<00:39,  3.43it/s] 77%|███████▋  | 450/585 [03:45<00:40,  3.36it/s] 77%|███████▋  | 451/585 [03:45<00:39,  3.39it/s] 77%|███████▋  | 452/585 [03:45<00:38,  3.42it/s] 77%|███████▋  | 453/585 [03:46<00:38,  3.43it/s] 78%|███████▊  | 454/585 [03:46<00:38,  3.44it/s] 78%|███████▊  | 455/585 [03:46<00:37,  3.45it/s] 78%|███████▊  | 456/585 [03:46<00:37,  3.45it/s] 78%|███████▊  | 457/585 [03:47<00:37,  3.46it/s] 78%|███████▊  | 458/585 [03:47<00:36,  3.46it/s] 78%|███████▊  | 459/585 [03:47<00:36,  3.46it/s] 79%|███████▊  | 460/585 [03:48<00:36,  3.46it/s] 79%|███████▉  | 461/585 [03:48<00:37,  3.28it/s] 79%|███████▉  | 462/585 [03:48<00:36,  3.33it/s] 79%|███████▉  | 463/585 [03:49<00:36,  3.37it/s] 79%|███████▉  | 464/585 [03:49<00:35,  3.40it/s] 79%|███████▉  | 465/585 [03:49<00:35,  3.42it/s] 80%|███████▉  | 466/585 [03:49<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:50<00:34,  3.44it/s] 80%|████████  | 468/585 [03:50<00:33,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 14:17:12,625 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:17:12,625 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 14:17:12,625 >>   Batch size = 8
{'eval_loss': 1.0734291076660156, 'eval_runtime': 13.2453, 'eval_samples_per_second': 366.922, 'eval_steps_per_second': 45.903, 'epoch': 3.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.08it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.45it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.69it/s][A
  4%|▍         | 23/608 [00:00<00:12, 48.00it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.61it/s][A
  5%|▌         | 33/608 [00:00<00:13, 43.56it/s][A
  6%|▋         | 38/608 [00:00<00:12, 44.42it/s][A
  7%|▋         | 43/608 [00:00<00:12, 44.99it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.52it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.88it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.11it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.32it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.43it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.17it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.29it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.29it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.25it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 46.34it/s][A
 16%|█▌        | 98/608 [00:02<00:10, 46.50it/s][A
 17%|█▋        | 103/608 [00:02<00:10, 46.58it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 46.60it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.50it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.48it/s][A
 20%|██        | 123/608 [00:02<00:10, 44.98it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.54it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.74it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 45.99it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.20it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.30it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.39it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.43it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.40it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.24it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.28it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.25it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.31it/s][A
 31%|███       | 188/608 [00:04<00:09, 46.46it/s][A
 32%|███▏      | 193/608 [00:04<00:08, 46.48it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 46.45it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 46.54it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 46.49it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.39it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.33it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.39it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.41it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.51it/s][A
 39%|███▉      | 238/608 [00:05<00:07, 46.53it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.47it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.48it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.52it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.41it/s][A
 43%|████▎     | 263/608 [00:05<00:08, 39.42it/s][A
 44%|████▍     | 268/608 [00:05<00:08, 41.31it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 42.71it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 43.90it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 44.69it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.24it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.69it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.02it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.78it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.89it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.00it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.09it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 46.21it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 46.37it/s][A
 55%|█████▍    | 333/608 [00:07<00:05, 46.44it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 46.50it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 46.55it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 46.27it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 46.27it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.21it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.26it/s][A
 61%|██████    | 368/608 [00:07<00:05, 46.38it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 46.39it/s][A
 62%|██████▏   | 378/608 [00:08<00:04, 46.46it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.47it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.43it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.43it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.41it/s][A
 66%|██████▋   | 403/608 [00:08<00:05, 40.94it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 42.45it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 43.55it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 44.48it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.13it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.57it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.91it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.15it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.84it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.79it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.01it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.21it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.32it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 46.46it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 46.48it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 46.47it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 46.43it/s][A
 80%|████████  | 488/608 [00:10<00:02, 46.31it/s][A
 81%|████████  | 493/608 [00:10<00:02, 46.18it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 46.10it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.30it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 46.32it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 46.42it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 46.48it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.52it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.59it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.37it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.22it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 43.95it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 44.67it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.19it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.68it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.03it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.22it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.28it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.23it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.00it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 45.98it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.09it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 46.23it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.31it/s][A
100%|██████████| 608/608 [00:13<00:00, 46.44it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 46.44it/s][A 80%|████████  | 468/585 [04:03<00:33,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:17:25,931 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 14:17:26,097 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:17:29,735 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:17:30,140 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:17:30,206 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:16<15:29,  8.02s/it] 80%|████████  | 470/585 [04:16<10:56,  5.71s/it] 81%|████████  | 471/585 [04:17<07:45,  4.09s/it] 81%|████████  | 472/585 [04:17<05:32,  2.95s/it] 81%|████████  | 473/585 [04:17<04:00,  2.15s/it] 81%|████████  | 474/585 [04:18<02:56,  1.59s/it] 81%|████████  | 475/585 [04:18<02:11,  1.20s/it] 81%|████████▏ | 476/585 [04:18<01:40,  1.08it/s] 82%|████████▏ | 477/585 [04:18<01:19,  1.36it/s] 82%|████████▏ | 478/585 [04:19<01:04,  1.67it/s] 82%|████████▏ | 479/585 [04:19<00:53,  1.97it/s] 82%|████████▏ | 480/585 [04:19<00:53,  1.98it/s] 82%|████████▏ | 481/585 [04:20<00:46,  2.22it/s] 82%|████████▏ | 482/585 [04:20<00:41,  2.49it/s] 83%|████████▎ | 483/585 [04:20<00:37,  2.72it/s] 83%|████████▎ | 484/585 [04:21<00:34,  2.91it/s] 83%|████████▎ | 485/585 [04:21<00:32,  3.06it/s] 83%|████████▎ | 486/585 [04:21<00:31,  3.17it/s] 83%|████████▎ | 487/585 [04:22<00:30,  3.26it/s] 83%|████████▎ | 488/585 [04:22<00:29,  3.32it/s] 84%|████████▎ | 489/585 [04:22<00:28,  3.36it/s] 84%|████████▍ | 490/585 [04:22<00:27,  3.40it/s] 84%|████████▍ | 491/585 [04:23<00:27,  3.42it/s] 84%|████████▍ | 492/585 [04:23<00:28,  3.32it/s] 84%|████████▍ | 493/585 [04:23<00:27,  3.36it/s] 84%|████████▍ | 494/585 [04:24<00:26,  3.39it/s] 85%|████████▍ | 495/585 [04:24<00:26,  3.42it/s] 85%|████████▍ | 496/585 [04:24<00:25,  3.44it/s] 85%|████████▍ | 497/585 [04:24<00:25,  3.44it/s] 85%|████████▌ | 498/585 [04:25<00:25,  3.45it/s] 85%|████████▌ | 499/585 [04:25<00:24,  3.46it/s] 85%|████████▌ | 500/585 [04:25<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [04:25<00:24,  3.47it/s] 86%|████████▌ | 501/585 [04:26<00:24,  3.47it/s] 86%|████████▌ | 502/585 [04:26<00:23,  3.47it/s] 86%|████████▌ | 503/585 [04:26<00:23,  3.47it/s] 86%|████████▌ | 504/585 [04:26<00:23,  3.47it/s] 86%|████████▋ | 505/585 [04:27<00:23,  3.40it/s] 86%|████████▋ | 506/585 [04:27<00:23,  3.42it/s] 87%|████████▋ | 507/585 [04:27<00:22,  3.44it/s] 87%|████████▋ | 508/585 [04:28<00:22,  3.45it/s] 87%|████████▋ | 509/585 [04:28<00:22,  3.45it/s] 87%|████████▋ | 510/585 [04:28<00:21,  3.46it/s] 87%|████████▋ | 511/585 [04:29<00:21,  3.46it/s] 88%|████████▊ | 512/585 [04:29<00:21,  3.46it/s] 88%|████████▊ | 513/585 [04:29<00:20,  3.47it/s] 88%|████████▊ | 514/585 [04:29<00:20,  3.47it/s] 88%|████████▊ | 515/585 [04:30<00:20,  3.47it/s] 88%|████████▊ | 516/585 [04:30<00:20,  3.35it/s] 88%|████████▊ | 517/585 [04:30<00:20,  3.38it/s] 89%|████████▊ | 518/585 [04:31<00:19,  3.41it/s] 89%|████████▊ | 519/585 [04:31<00:19,  3.43it/s] 89%|████████▉ | 520/585 [04:31<00:18,  3.44it/s] 89%|████████▉ | 521/585 [04:31<00:18,  3.45it/s] 89%|████████▉ | 522/585 [04:32<00:18,  3.46it/s] 89%|████████▉ | 523/585 [04:32<00:17,  3.46it/s] 90%|████████▉ | 524/585 [04:32<00:17,  3.46it/s] 90%|████████▉ | 525/585 [04:33<00:17,  3.47it/s] 90%|████████▉ | 526/585 [04:33<00:17,  3.47it/s] 90%|█████████ | 527/585 [04:33<00:17,  3.31it/s] 90%|█████████ | 528/585 [04:33<00:16,  3.36it/s] 90%|█████████ | 529/585 [04:34<00:16,  3.39it/s] 91%|█████████ | 530/585 [04:34<00:16,  3.41it/s] 91%|█████████ | 531/585 [04:34<00:15,  3.43it/s] 91%|█████████ | 532/585 [04:35<00:15,  3.44it/s] 91%|█████████ | 533/585 [04:35<00:15,  3.45it/s] 91%|█████████▏| 534/585 [04:35<00:14,  3.45it/s] 91%|█████████▏| 535/585 [04:36<00:14,  3.46it/s] 92%|█████████▏| 536/585 [04:36<00:14,  3.46it/s] 92%|█████████▏| 537/585 [04:36<00:13,  3.46it/s] 92%|█████████▏| 538/585 [04:36<00:14,  3.34it/s] 92%|█████████▏| 539/585 [04:37<00:13,  3.38it/s] 92%|█████████▏| 540/585 [04:37<00:13,  3.40it/s] 92%|█████████▏| 541/585 [04:37<00:12,  3.42it/s] 93%|█████████▎| 542/585 [04:38<00:12,  3.44it/s] 93%|█████████▎| 543/585 [04:38<00:12,  3.45it/s] 93%|█████████▎| 544/585 [04:38<00:11,  3.45it/s] 93%|█████████▎| 545/585 [04:38<00:11,  3.46it/s] 93%|█████████▎| 546/585 [04:39<00:11,  3.46it/s] 94%|█████████▎| 547/585 [04:39<00:10,  3.47it/s] 94%|█████████▎| 548/585 [04:39<00:10,  3.46it/s] 94%|█████████▍| 549/585 [04:40<00:10,  3.34it/s] 94%|█████████▍| 550/585 [04:40<00:10,  3.38it/s] 94%|█████████▍| 551/585 [04:40<00:09,  3.40it/s] 94%|█████████▍| 552/585 [04:40<00:09,  3.42it/s] 95%|█████████▍| 553/585 [04:41<00:09,  3.44it/s] 95%|█████████▍| 554/585 [04:41<00:08,  3.45it/s] 95%|█████████▍| 555/585 [04:41<00:08,  3.45it/s] 95%|█████████▌| 556/585 [04:42<00:08,  3.46it/s] 95%|█████████▌| 557/585 [04:42<00:08,  3.46it/s] 95%|█████████▌| 558/585 [04:42<00:07,  3.46it/s] 96%|█████████▌| 559/585 [04:42<00:07,  3.46it/s] 96%|█████████▌| 560/585 [04:43<00:07,  3.38it/s] 96%|█████████▌| 561/585 [04:43<00:07,  3.22it/s] 96%|█████████▌| 562/585 [04:43<00:07,  3.28it/s] 96%|█████████▌| 563/585 [04:44<00:06,  3.33it/s] 96%|█████████▋| 564/585 [04:44<00:06,  3.37it/s] 97%|█████████▋| 565/585 [04:44<00:05,  3.40it/s] 97%|█████████▋| 566/585 [04:45<00:05,  3.42it/s] 97%|█████████▋| 567/585 [04:45<00:05,  3.43it/s] 97%|█████████▋| 568/585 [04:45<00:04,  3.44it/s] 97%|█████████▋| 569/585 [04:45<00:04,  3.45it/s] 97%|█████████▋| 570/585 [04:46<00:04,  3.45it/s] 98%|█████████▊| 571/585 [04:46<00:04,  3.38it/s] 98%|█████████▊| 572/585 [04:46<00:03,  3.40it/s] 98%|█████████▊| 573/585 [04:47<00:03,  3.42it/s] 98%|█████████▊| 574/585 [04:47<00:03,  3.43it/s] 98%|█████████▊| 575/585 [04:47<00:02,  3.44it/s] 98%|█████████▊| 576/585 [04:48<00:02,  3.45it/s] 99%|█████████▊| 577/585 [04:48<00:02,  3.45it/s] 99%|█████████▉| 578/585 [04:48<00:02,  3.45it/s] 99%|█████████▉| 579/585 [04:48<00:01,  3.46it/s] 99%|█████████▉| 580/585 [04:49<00:01,  3.46it/s] 99%|█████████▉| 581/585 [04:49<00:01,  3.46it/s] 99%|█████████▉| 582/585 [04:49<00:00,  3.36it/s]100%|█████████▉| 583/585 [04:50<00:00,  3.39it/s]100%|█████████▉| 584/585 [04:50<00:00,  3.41it/s]100%|██████████| 585/585 [04:50<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 14:18:12,707 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:18:12,707 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 14:18:12,707 >>   Batch size = 8
{'eval_loss': 1.0854442119598389, 'eval_runtime': 13.2662, 'eval_samples_per_second': 366.344, 'eval_steps_per_second': 45.831, 'epoch': 4.0}
{'loss': 0.4127, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.63it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.21it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.58it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.84it/s][A
  5%|▍         | 28/608 [00:00<00:12, 47.45it/s][A
  5%|▌         | 33/608 [00:00<00:12, 47.21it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.94it/s][A
  7%|▋         | 43/608 [00:00<00:12, 46.46it/s][A
  8%|▊         | 48/608 [00:01<00:12, 46.43it/s][A
  9%|▊         | 53/608 [00:01<00:11, 46.43it/s][A
 10%|▉         | 58/608 [00:01<00:11, 46.54it/s][A
 10%|█         | 63/608 [00:01<00:11, 46.55it/s][A
 11%|█         | 68/608 [00:01<00:11, 46.58it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 46.54it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 46.62it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 46.61it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 46.50it/s][A
 15%|█▌        | 93/608 [00:01<00:11, 44.44it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.05it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.56it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.89it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 46.10it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 46.31it/s][A
 20%|██        | 123/608 [00:02<00:10, 46.42it/s][A
 21%|██        | 128/608 [00:02<00:10, 46.42it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 46.28it/s][A
 23%|██▎       | 138/608 [00:02<00:10, 46.05it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 46.26it/s][A
 24%|██▍       | 148/608 [00:03<00:09, 46.34it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 46.40it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 46.52it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 46.60it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 46.64it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 46.51it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 46.41it/s][A
 30%|███       | 183/608 [00:03<00:09, 46.28it/s][A
 31%|███       | 188/608 [00:04<00:09, 43.32it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.32it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 44.97it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.46it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.79it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 46.01it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 46.12it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 46.35it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 46.13it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 46.01it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 46.20it/s][A
 40%|███▉      | 243/608 [00:05<00:07, 46.39it/s][A
 41%|████      | 248/608 [00:05<00:07, 46.45it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 46.46it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 46.50it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 46.45it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 46.49it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 46.41it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 46.22it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 46.24it/s][A
 47%|████▋     | 288/608 [00:06<00:06, 46.31it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 46.39it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 46.50it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 46.50it/s][A
 51%|█████     | 308/608 [00:06<00:06, 46.32it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 46.46it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 46.34it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 46.30it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 41.16it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 42.65it/s][A
 56%|█████▌    | 338/608 [00:07<00:06, 43.78it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 44.68it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.24it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.63it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 46.00it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 46.21it/s][A
 61%|██████    | 368/608 [00:07<00:05, 45.81it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.88it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.90it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 46.14it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 46.30it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 46.41it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 46.47it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 46.53it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 46.51it/s][A
 68%|██████▊   | 413/608 [00:08<00:04, 46.31it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 46.24it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 46.15it/s][A
 70%|███████   | 428/608 [00:09<00:03, 46.19it/s][A
 71%|███████   | 433/608 [00:09<00:03, 46.34it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 46.46it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 46.51it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 46.55it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 46.52it/s][A
 75%|███████▌  | 458/608 [00:09<00:03, 46.28it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 46.26it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 41.16it/s][A
 78%|███████▊  | 473/608 [00:10<00:03, 42.66it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 43.81it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 44.59it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.20it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.60it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 45.97it/s][A
 83%|████████▎ | 503/608 [00:10<00:02, 46.18it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.86it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.93it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.97it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 46.14it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 46.34it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 46.50it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 46.46it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 46.52it/s][A
 90%|█████████ | 548/608 [00:11<00:01, 46.47it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 46.26it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 46.26it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 46.18it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 46.19it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 46.32it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 46.38it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 46.44it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 46.54it/s][A
 98%|█████████▊| 593/608 [00:12<00:00, 46.52it/s][A
 98%|█████████▊| 598/608 [00:12<00:00, 46.40it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 46.25it/s][A
100%|██████████| 608/608 [00:13<00:00, 40.03it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 40.03it/s][A100%|██████████| 585/585 [05:03<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:18:26,561 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 14:18:26,913 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:18:30,531 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:18:30,670 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:18:30,732 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 14:18:38,789 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 14:18:38,802 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117 (score: 1.0451455116271973).
                                                 100%|██████████| 585/585 [05:25<00:00,  3.42it/s]100%|██████████| 585/585 [05:25<00:00,  1.80it/s]
[INFO|trainer.py:1894] 2023-08-29 14:18:47,915 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 14:18:48,092 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:18:51,609 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:18:51,765 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:18:51,862 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 14:18:52,434 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:18:52,434 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:18:52,434 >>   train_loss               =     0.4096
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:18:52,434 >>   train_runtime            = 0:05:25.34
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:18:52,434 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:18:52,434 >>   train_samples_per_second =    115.262
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:18:52,434 >>   train_steps_per_second   =      1.798
{'eval_loss': 1.0879120826721191, 'eval_runtime': 13.2768, 'eval_samples_per_second': 366.051, 'eval_steps_per_second': 45.794, 'epoch': 5.0}
{'train_runtime': 325.3445, 'train_samples_per_second': 115.262, 'train_steps_per_second': 1.798, 'train_loss': 0.40957244481795874, 'epoch': 5.0}
08/29/2023 14:18:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 14:18:52,679 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:18:52,679 >>   Num examples = 4860
[INFO|trainer.py:2145] 2023-08-29 14:18:52,679 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 58.00it/s]  2%|▏         | 12/608 [00:00<00:11, 51.18it/s]  3%|▎         | 18/608 [00:00<00:12, 49.15it/s]  4%|▍         | 23/608 [00:00<00:12, 48.26it/s]  5%|▍         | 28/608 [00:00<00:12, 47.73it/s]  5%|▌         | 33/608 [00:00<00:12, 47.49it/s]  6%|▋         | 38/608 [00:00<00:12, 47.29it/s]  7%|▋         | 43/608 [00:00<00:11, 47.17it/s]  8%|▊         | 48/608 [00:01<00:11, 46.89it/s]  9%|▊         | 53/608 [00:01<00:11, 46.90it/s] 10%|▉         | 58/608 [00:01<00:11, 46.92it/s] 10%|█         | 63/608 [00:01<00:11, 46.95it/s] 11%|█         | 68/608 [00:01<00:11, 46.94it/s] 12%|█▏        | 73/608 [00:01<00:11, 46.94it/s] 13%|█▎        | 78/608 [00:01<00:11, 46.87it/s] 14%|█▎        | 83/608 [00:01<00:11, 46.84it/s] 14%|█▍        | 88/608 [00:01<00:11, 46.91it/s] 15%|█▌        | 93/608 [00:01<00:11, 46.80it/s] 16%|█▌        | 98/608 [00:02<00:11, 44.81it/s] 17%|█▋        | 103/608 [00:02<00:11, 45.38it/s] 18%|█▊        | 108/608 [00:02<00:10, 45.86it/s] 19%|█▊        | 113/608 [00:02<00:10, 46.15it/s] 19%|█▉        | 118/608 [00:02<00:10, 46.32it/s] 20%|██        | 123/608 [00:02<00:10, 46.58it/s] 21%|██        | 128/608 [00:02<00:10, 46.65it/s] 22%|██▏       | 133/608 [00:02<00:10, 46.72it/s] 23%|██▎       | 138/608 [00:02<00:10, 46.74it/s] 24%|██▎       | 143/608 [00:03<00:09, 46.71it/s] 24%|██▍       | 148/608 [00:03<00:09, 46.66it/s] 25%|██▌       | 153/608 [00:03<00:09, 46.84it/s] 26%|██▌       | 158/608 [00:03<00:09, 46.86it/s] 27%|██▋       | 163/608 [00:03<00:09, 46.85it/s] 28%|██▊       | 168/608 [00:03<00:09, 46.85it/s] 28%|██▊       | 173/608 [00:03<00:09, 46.89it/s] 29%|██▉       | 178/608 [00:03<00:09, 46.91it/s] 30%|███       | 183/608 [00:03<00:09, 46.91it/s] 31%|███       | 188/608 [00:04<00:08, 46.88it/s] 32%|███▏      | 193/608 [00:04<00:08, 46.81it/s] 33%|███▎      | 198/608 [00:04<00:08, 46.79it/s] 33%|███▎      | 203/608 [00:04<00:08, 46.84it/s] 34%|███▍      | 208/608 [00:04<00:08, 46.82it/s] 35%|███▌      | 213/608 [00:04<00:08, 46.89it/s] 36%|███▌      | 218/608 [00:04<00:08, 46.86it/s] 37%|███▋      | 223/608 [00:04<00:08, 46.81it/s] 38%|███▊      | 228/608 [00:04<00:08, 46.86it/s] 38%|███▊      | 233/608 [00:04<00:07, 46.90it/s] 39%|███▉      | 238/608 [00:05<00:08, 44.97it/s] 40%|███▉      | 243/608 [00:05<00:08, 45.38it/s] 41%|████      | 248/608 [00:05<00:07, 45.79it/s] 42%|████▏     | 253/608 [00:05<00:07, 46.06it/s] 42%|████▏     | 258/608 [00:05<00:07, 46.27it/s] 43%|████▎     | 263/608 [00:05<00:07, 46.42it/s] 44%|████▍     | 268/608 [00:05<00:07, 46.60it/s] 45%|████▍     | 273/608 [00:05<00:07, 46.62it/s] 46%|████▌     | 278/608 [00:05<00:07, 46.55it/s] 47%|████▋     | 283/608 [00:06<00:06, 46.75it/s] 47%|████▋     | 288/608 [00:06<00:06, 46.64it/s] 48%|████▊     | 293/608 [00:06<00:06, 46.61it/s] 49%|████▉     | 298/608 [00:06<00:06, 46.59it/s] 50%|████▉     | 303/608 [00:06<00:06, 46.70it/s] 51%|█████     | 308/608 [00:06<00:06, 46.78it/s] 51%|█████▏    | 313/608 [00:06<00:06, 46.74it/s] 52%|█████▏    | 318/608 [00:06<00:06, 46.81it/s] 53%|█████▎    | 323/608 [00:06<00:06, 46.85it/s] 54%|█████▍    | 328/608 [00:07<00:05, 46.88it/s] 55%|█████▍    | 333/608 [00:07<00:05, 46.92it/s] 56%|█████▌    | 338/608 [00:07<00:05, 46.82it/s] 56%|█████▋    | 343/608 [00:07<00:05, 46.84it/s] 57%|█████▋    | 348/608 [00:07<00:05, 46.77it/s] 58%|█████▊    | 353/608 [00:07<00:05, 46.87it/s] 59%|█████▉    | 358/608 [00:07<00:05, 46.88it/s] 60%|█████▉    | 363/608 [00:07<00:05, 46.89it/s] 61%|██████    | 368/608 [00:07<00:05, 46.87it/s] 61%|██████▏   | 373/608 [00:07<00:05, 46.88it/s] 62%|██████▏   | 378/608 [00:08<00:05, 45.40it/s] 63%|██████▎   | 383/608 [00:08<00:04, 45.87it/s] 64%|██████▍   | 388/608 [00:08<00:04, 46.18it/s] 65%|██████▍   | 393/608 [00:08<00:04, 46.36it/s] 65%|██████▌   | 398/608 [00:08<00:04, 46.41it/s] 66%|██████▋   | 403/608 [00:08<00:04, 46.61it/s] 67%|██████▋   | 408/608 [00:08<00:04, 46.66it/s] 68%|██████▊   | 413/608 [00:08<00:04, 46.81it/s] 69%|██████▉   | 418/608 [00:08<00:04, 46.80it/s] 70%|██████▉   | 423/608 [00:09<00:03, 46.66it/s] 70%|███████   | 428/608 [00:09<00:03, 46.69it/s] 71%|███████   | 433/608 [00:09<00:03, 46.64it/s] 72%|███████▏  | 438/608 [00:09<00:03, 46.66it/s] 73%|███████▎  | 443/608 [00:09<00:03, 46.73it/s] 74%|███████▎  | 448/608 [00:09<00:03, 46.74it/s] 75%|███████▍  | 453/608 [00:09<00:03, 46.73it/s] 75%|███████▌  | 458/608 [00:09<00:03, 46.78it/s] 76%|███████▌  | 463/608 [00:09<00:03, 46.82it/s] 77%|███████▋  | 468/608 [00:10<00:02, 46.75it/s] 78%|███████▊  | 473/608 [00:10<00:02, 46.81it/s] 79%|███████▊  | 478/608 [00:10<00:02, 46.83it/s] 79%|███████▉  | 483/608 [00:10<00:02, 46.75it/s] 80%|████████  | 488/608 [00:10<00:02, 46.73it/s] 81%|████████  | 493/608 [00:10<00:02, 46.85it/s] 82%|████████▏ | 498/608 [00:10<00:02, 46.83it/s] 83%|████████▎ | 503/608 [00:10<00:02, 46.90it/s] 84%|████████▎ | 508/608 [00:10<00:02, 46.82it/s] 84%|████████▍ | 513/608 [00:10<00:02, 46.86it/s] 85%|████████▌ | 518/608 [00:11<00:02, 44.85it/s] 86%|████████▌ | 523/608 [00:11<00:01, 45.47it/s] 87%|████████▋ | 528/608 [00:11<00:01, 45.83it/s] 88%|████████▊ | 533/608 [00:11<00:01, 46.11it/s] 88%|████████▊ | 538/608 [00:11<00:01, 46.29it/s] 89%|████████▉ | 543/608 [00:11<00:01, 46.52it/s] 90%|█████████ | 548/608 [00:11<00:01, 46.67it/s] 91%|█████████ | 553/608 [00:11<00:01, 46.64it/s] 92%|█████████▏| 558/608 [00:11<00:01, 46.50it/s] 93%|█████████▎| 563/608 [00:12<00:00, 46.64it/s] 93%|█████████▎| 568/608 [00:12<00:00, 46.62it/s] 94%|█████████▍| 573/608 [00:12<00:00, 46.70it/s] 95%|█████████▌| 578/608 [00:12<00:00, 46.69it/s] 96%|█████████▌| 583/608 [00:12<00:00, 46.76it/s] 97%|█████████▋| 588/608 [00:12<00:00, 46.76it/s] 98%|█████████▊| 593/608 [00:12<00:00, 46.77it/s] 98%|█████████▊| 598/608 [00:12<00:00, 46.85it/s] 99%|█████████▉| 603/608 [00:12<00:00, 46.83it/s]100%|██████████| 608/608 [00:13<00:00, 40.77it/s]100%|██████████| 608/608 [00:13<00:00, 46.50it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 14:19:05,775 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:19:05,775 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:19:05,775 >>   eval_loss               =     1.0451
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:19:05,775 >>   eval_runtime            = 0:00:13.09
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:19:05,775 >>   eval_samples            =       4860
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:19:05,775 >>   eval_samples_per_second =    371.113
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:19:05,775 >>   eval_steps_per_second   =     46.427
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:19:05,775 >>   perplexity              =     2.8438
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:17,129 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:17,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:17,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:17,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:17,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:19:17,727 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:19:17,729 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:19:18,177 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:19:19,339 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:19:19,339 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:21,676 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:21,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:21,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:21,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:19:21,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:19:22,475 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:19:22,476 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:19:22,997 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:19:23,276 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:19:23,276 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/dev.jsonl', 'labels': ['headquarters location', 'licensed to broadcast to', 'member of political party', 'narrative location', 'notable work'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14287
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14387, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.59it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.50it/s]Extractor Predicting: 18it [00:11,  1.45it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.56it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:20,  1.57it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:21,  1.58it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.58it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.59it/s]Extractor Predicting: 43it [00:27,  1.58it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:29,  1.49it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:31,  1.50it/s]Extractor Predicting: 50it [00:32,  1.50it/s]Extractor Predicting: 51it [00:33,  1.45it/s]Extractor Predicting: 52it [00:33,  1.45it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:35,  1.49it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:36,  1.48it/s]Extractor Predicting: 57it [00:37,  1.48it/s]Extractor Predicting: 58it [00:37,  1.45it/s]Extractor Predicting: 59it [00:38,  1.47it/s]Extractor Predicting: 60it [00:39,  1.47it/s]Extractor Predicting: 61it [00:39,  1.50it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:41,  1.51it/s]Extractor Predicting: 65it [00:42,  1.53it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:43,  1.45it/s]Extractor Predicting: 68it [00:44,  1.51it/s]Extractor Predicting: 69it [00:45,  1.51it/s]Extractor Predicting: 70it [00:45,  1.51it/s]Extractor Predicting: 71it [00:46,  1.50it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:47,  1.57it/s]Extractor Predicting: 74it [00:48,  1.57it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:49,  1.58it/s]Extractor Predicting: 77it [00:50,  1.56it/s]Extractor Predicting: 78it [00:50,  1.58it/s]Extractor Predicting: 79it [00:51,  1.57it/s]Extractor Predicting: 80it [00:52,  1.56it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:53,  1.57it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:54,  1.56it/s]Extractor Predicting: 85it [00:55,  1.57it/s]Extractor Predicting: 86it [00:55,  1.60it/s]Extractor Predicting: 87it [00:56,  1.60it/s]Extractor Predicting: 88it [00:57,  1.54it/s]Extractor Predicting: 89it [00:57,  1.55it/s]Extractor Predicting: 90it [00:58,  1.57it/s]Extractor Predicting: 91it [00:59,  1.57it/s]Extractor Predicting: 92it [01:00,  1.44it/s]Extractor Predicting: 93it [01:00,  1.46it/s]Extractor Predicting: 94it [01:01,  1.49it/s]Extractor Predicting: 95it [01:01,  1.50it/s]Extractor Predicting: 96it [01:02,  1.51it/s]Extractor Predicting: 97it [01:03,  1.53it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:04,  1.52it/s]Extractor Predicting: 100it [01:05,  1.52it/s]Extractor Predicting: 101it [01:05,  1.56it/s]Extractor Predicting: 102it [01:06,  1.54it/s]Extractor Predicting: 103it [01:07,  1.49it/s]Extractor Predicting: 104it [01:07,  1.51it/s]Extractor Predicting: 105it [01:08,  1.52it/s]Extractor Predicting: 106it [01:09,  1.55it/s]Extractor Predicting: 107it [01:09,  1.57it/s]Extractor Predicting: 108it [01:10,  1.52it/s]Extractor Predicting: 109it [01:11,  1.52it/s]Extractor Predicting: 110it [01:11,  1.53it/s]Extractor Predicting: 111it [01:12,  1.57it/s]Extractor Predicting: 112it [01:13,  1.56it/s]Extractor Predicting: 113it [01:13,  1.56it/s]Extractor Predicting: 114it [01:14,  1.56it/s]Extractor Predicting: 115it [01:14,  1.57it/s]Extractor Predicting: 116it [01:15,  1.55it/s]Extractor Predicting: 117it [01:16,  1.57it/s]Extractor Predicting: 118it [01:16,  1.56it/s]Extractor Predicting: 119it [01:17,  1.57it/s]Extractor Predicting: 120it [01:18,  1.57it/s]Extractor Predicting: 121it [01:18,  1.53it/s]Extractor Predicting: 122it [01:19,  1.55it/s]Extractor Predicting: 123it [01:20,  1.53it/s]Extractor Predicting: 124it [01:20,  1.52it/s]Extractor Predicting: 125it [01:21,  1.47it/s]Extractor Predicting: 126it [01:22,  1.48it/s]Extractor Predicting: 127it [01:22,  1.50it/s]Extractor Predicting: 128it [01:23,  1.51it/s]Extractor Predicting: 129it [01:24,  1.51it/s]Extractor Predicting: 130it [01:24,  1.54it/s]Extractor Predicting: 131it [01:25,  1.53it/s]Extractor Predicting: 132it [01:26,  1.54it/s]Extractor Predicting: 133it [01:26,  1.50it/s]Extractor Predicting: 134it [01:27,  1.51it/s]Extractor Predicting: 135it [01:28,  1.49it/s]Extractor Predicting: 136it [01:28,  1.50it/s]Extractor Predicting: 137it [01:29,  1.51it/s]Extractor Predicting: 138it [01:30,  1.49it/s]Extractor Predicting: 139it [01:30,  1.50it/s]Extractor Predicting: 140it [01:31,  1.50it/s]Extractor Predicting: 141it [01:32,  1.49it/s]Extractor Predicting: 142it [01:32,  1.52it/s]Extractor Predicting: 143it [01:33,  1.52it/s]Extractor Predicting: 144it [01:34,  1.52it/s]Extractor Predicting: 145it [01:34,  1.53it/s]Extractor Predicting: 146it [01:35,  1.56it/s]Extractor Predicting: 147it [01:36,  1.51it/s]Extractor Predicting: 148it [01:36,  1.50it/s]Extractor Predicting: 149it [01:37,  1.51it/s]Extractor Predicting: 150it [01:38,  1.51it/s]Extractor Predicting: 151it [01:38,  1.54it/s]Extractor Predicting: 152it [01:39,  1.56it/s]Extractor Predicting: 153it [01:39,  1.61it/s]Extractor Predicting: 154it [01:40,  1.59it/s]Extractor Predicting: 155it [01:41,  1.54it/s]Extractor Predicting: 156it [01:41,  1.55it/s]Extractor Predicting: 157it [01:42,  1.57it/s]Extractor Predicting: 158it [01:43,  1.57it/s]Extractor Predicting: 159it [01:43,  1.54it/s]Extractor Predicting: 160it [01:44,  1.56it/s]Extractor Predicting: 161it [01:45,  1.54it/s]Extractor Predicting: 162it [01:45,  1.53it/s]Extractor Predicting: 163it [01:46,  1.53it/s]Extractor Predicting: 164it [01:46,  1.55it/s]Extractor Predicting: 165it [01:47,  1.53it/s]Extractor Predicting: 166it [01:48,  1.53it/s]Extractor Predicting: 167it [01:48,  1.52it/s]Extractor Predicting: 168it [01:49,  1.55it/s]Extractor Predicting: 169it [01:50,  1.53it/s]Extractor Predicting: 170it [01:50,  1.53it/s]Extractor Predicting: 171it [01:51,  1.55it/s]Extractor Predicting: 172it [01:52,  1.54it/s]Extractor Predicting: 173it [01:52,  1.52it/s]Extractor Predicting: 174it [01:53,  1.57it/s]Extractor Predicting: 175it [01:54,  1.54it/s]Extractor Predicting: 176it [01:54,  1.54it/s]Extractor Predicting: 177it [01:55,  1.47it/s]Extractor Predicting: 178it [01:56,  1.46it/s]Extractor Predicting: 179it [01:56,  1.48it/s]Extractor Predicting: 180it [01:57,  1.35it/s]Extractor Predicting: 181it [01:58,  1.34it/s]Extractor Predicting: 182it [01:59,  1.41it/s]Extractor Predicting: 183it [01:59,  1.40it/s]Extractor Predicting: 184it [02:00,  1.41it/s]Extractor Predicting: 185it [02:01,  1.40it/s]Extractor Predicting: 186it [02:02,  1.38it/s]Extractor Predicting: 187it [02:02,  1.39it/s]Extractor Predicting: 188it [02:03,  1.41it/s]Extractor Predicting: 189it [02:04,  1.44it/s]Extractor Predicting: 190it [02:04,  1.43it/s]Extractor Predicting: 191it [02:05,  1.42it/s]Extractor Predicting: 192it [02:06,  1.44it/s]Extractor Predicting: 193it [02:06,  1.49it/s]Extractor Predicting: 194it [02:07,  1.50it/s]Extractor Predicting: 195it [02:07,  1.91it/s]Extractor Predicting: 195it [02:07,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:48,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:48,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:48,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:48,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:48,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:21:48,708 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:21:48,709 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:21:49,026 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:21:50,131 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:21:50,131 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:51,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:51,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:51,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:51,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:21:51,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:21:52,513 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:21:52,514 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:21:52,996 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:21:53,231 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:21:53,231 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.46482213438735176,
  "recall": 0.12098765432098765,
  "score": 0.19199999999999998,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21244
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21344, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:20,  1.48it/s]Extractor Predicting: 33it [00:21,  1.46it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:22,  1.47it/s]Extractor Predicting: 36it [00:23,  1.43it/s]Extractor Predicting: 37it [00:24,  1.49it/s]Extractor Predicting: 38it [00:24,  1.50it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.55it/s]Extractor Predicting: 41it [00:26,  1.50it/s]Extractor Predicting: 42it [00:27,  1.48it/s]Extractor Predicting: 43it [00:28,  1.43it/s]Extractor Predicting: 44it [00:29,  1.47it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.53it/s]Extractor Predicting: 49it [00:32,  1.58it/s]Extractor Predicting: 50it [00:32,  1.58it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:34,  1.60it/s]Extractor Predicting: 54it [00:35,  1.63it/s]Extractor Predicting: 55it [00:35,  1.62it/s]Extractor Predicting: 56it [00:36,  1.59it/s]Extractor Predicting: 57it [00:37,  1.60it/s]Extractor Predicting: 58it [00:37,  1.66it/s]Extractor Predicting: 59it [00:38,  1.57it/s]Extractor Predicting: 60it [00:39,  1.59it/s]Extractor Predicting: 61it [00:39,  1.55it/s]Extractor Predicting: 62it [00:40,  1.60it/s]Extractor Predicting: 63it [00:40,  1.61it/s]Extractor Predicting: 64it [00:41,  1.58it/s]Extractor Predicting: 65it [00:42,  1.61it/s]Extractor Predicting: 66it [00:42,  1.60it/s]Extractor Predicting: 67it [00:43,  1.61it/s]Extractor Predicting: 68it [00:44,  1.62it/s]Extractor Predicting: 69it [00:44,  1.65it/s]Extractor Predicting: 70it [00:45,  1.64it/s]Extractor Predicting: 71it [00:45,  1.60it/s]Extractor Predicting: 72it [00:46,  1.60it/s]Extractor Predicting: 73it [00:47,  1.60it/s]Extractor Predicting: 74it [00:47,  1.59it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:49,  1.55it/s]Extractor Predicting: 77it [00:49,  1.56it/s]Extractor Predicting: 78it [00:50,  1.56it/s]Extractor Predicting: 79it [00:50,  1.61it/s]Extractor Predicting: 80it [00:51,  1.63it/s]Extractor Predicting: 81it [00:52,  1.66it/s]Extractor Predicting: 82it [00:52,  1.65it/s]Extractor Predicting: 83it [00:53,  1.66it/s]Extractor Predicting: 84it [00:54,  1.56it/s]Extractor Predicting: 85it [00:54,  1.59it/s]Extractor Predicting: 86it [00:55,  1.64it/s]Extractor Predicting: 87it [00:55,  1.65it/s]Extractor Predicting: 88it [00:56,  1.71it/s]Extractor Predicting: 89it [00:56,  1.69it/s]Extractor Predicting: 90it [00:57,  1.70it/s]Extractor Predicting: 91it [00:58,  1.78it/s]Extractor Predicting: 92it [00:58,  1.70it/s]Extractor Predicting: 93it [00:59,  1.71it/s]Extractor Predicting: 94it [00:59,  1.72it/s]Extractor Predicting: 95it [01:00,  1.72it/s]Extractor Predicting: 96it [01:01,  1.68it/s]Extractor Predicting: 97it [01:01,  1.65it/s]Extractor Predicting: 98it [01:02,  1.69it/s]Extractor Predicting: 99it [01:02,  1.70it/s]Extractor Predicting: 100it [01:03,  1.70it/s]Extractor Predicting: 101it [01:04,  1.69it/s]Extractor Predicting: 102it [01:04,  1.68it/s]Extractor Predicting: 103it [01:05,  1.70it/s]Extractor Predicting: 104it [01:05,  1.66it/s]Extractor Predicting: 105it [01:06,  1.71it/s]Extractor Predicting: 106it [01:06,  1.70it/s]Extractor Predicting: 107it [01:07,  1.74it/s]Extractor Predicting: 108it [01:08,  1.71it/s]Extractor Predicting: 109it [01:08,  1.76it/s]Extractor Predicting: 110it [01:09,  1.66it/s]Extractor Predicting: 111it [01:09,  1.70it/s]Extractor Predicting: 112it [01:10,  1.66it/s]Extractor Predicting: 113it [01:11,  1.65it/s]Extractor Predicting: 114it [01:11,  1.67it/s]Extractor Predicting: 115it [01:12,  1.65it/s]Extractor Predicting: 116it [01:12,  1.65it/s]Extractor Predicting: 117it [01:13,  1.65it/s]Extractor Predicting: 118it [01:14,  1.60it/s]Extractor Predicting: 119it [01:14,  1.61it/s]Extractor Predicting: 120it [01:15,  1.63it/s]Extractor Predicting: 121it [01:16,  1.64it/s]Extractor Predicting: 122it [01:16,  1.62it/s]Extractor Predicting: 123it [01:17,  1.49it/s]Extractor Predicting: 124it [01:18,  1.51it/s]Extractor Predicting: 125it [01:18,  1.52it/s]Extractor Predicting: 126it [01:19,  1.55it/s]Extractor Predicting: 127it [01:19,  1.59it/s]Extractor Predicting: 128it [01:20,  1.58it/s]Extractor Predicting: 129it [01:21,  1.60it/s]Extractor Predicting: 130it [01:21,  1.59it/s]Extractor Predicting: 131it [01:22,  1.57it/s]Extractor Predicting: 132it [01:23,  1.58it/s]Extractor Predicting: 133it [01:23,  1.59it/s]Extractor Predicting: 134it [01:24,  1.57it/s]Extractor Predicting: 135it [01:25,  1.61it/s]Extractor Predicting: 136it [01:25,  1.60it/s]Extractor Predicting: 137it [01:26,  1.64it/s]Extractor Predicting: 138it [01:26,  1.62it/s]Extractor Predicting: 139it [01:27,  1.65it/s]Extractor Predicting: 140it [01:28,  1.66it/s]Extractor Predicting: 141it [01:28,  1.63it/s]Extractor Predicting: 142it [01:29,  1.48it/s]Extractor Predicting: 143it [01:30,  1.45it/s]Extractor Predicting: 144it [01:30,  1.46it/s]Extractor Predicting: 145it [01:31,  1.45it/s]Extractor Predicting: 146it [01:32,  1.49it/s]Extractor Predicting: 147it [01:32,  1.54it/s]Extractor Predicting: 148it [01:33,  1.53it/s]Extractor Predicting: 149it [01:34,  1.54it/s]Extractor Predicting: 150it [01:34,  1.55it/s]Extractor Predicting: 151it [01:35,  1.57it/s]Extractor Predicting: 152it [01:36,  1.57it/s]Extractor Predicting: 153it [01:36,  1.60it/s]Extractor Predicting: 154it [01:37,  1.62it/s]Extractor Predicting: 155it [01:37,  1.59it/s]Extractor Predicting: 156it [01:38,  1.60it/s]Extractor Predicting: 157it [01:39,  1.58it/s]Extractor Predicting: 158it [01:39,  1.57it/s]Extractor Predicting: 159it [01:40,  1.55it/s]Extractor Predicting: 160it [01:41,  1.57it/s]Extractor Predicting: 161it [01:41,  1.54it/s]Extractor Predicting: 162it [01:42,  1.56it/s]Extractor Predicting: 163it [01:42,  1.58it/s]Extractor Predicting: 164it [01:43,  1.57it/s]Extractor Predicting: 165it [01:44,  1.55it/s]Extractor Predicting: 166it [01:45,  1.51it/s]Extractor Predicting: 167it [01:45,  1.53it/s]Extractor Predicting: 168it [01:46,  1.57it/s]Extractor Predicting: 169it [01:46,  1.54it/s]Extractor Predicting: 170it [01:47,  1.53it/s]Extractor Predicting: 171it [01:48,  1.51it/s]Extractor Predicting: 172it [01:48,  1.56it/s]Extractor Predicting: 173it [01:49,  1.56it/s]Extractor Predicting: 174it [01:50,  1.52it/s]Extractor Predicting: 175it [01:50,  1.52it/s]Extractor Predicting: 176it [01:51,  1.57it/s]Extractor Predicting: 177it [01:52,  1.58it/s]Extractor Predicting: 178it [01:52,  1.61it/s]Extractor Predicting: 179it [01:53,  1.64it/s]Extractor Predicting: 180it [01:53,  1.61it/s]Extractor Predicting: 181it [01:54,  1.52it/s]Extractor Predicting: 182it [01:55,  1.51it/s]Extractor Predicting: 183it [01:55,  1.54it/s]Extractor Predicting: 184it [01:56,  1.52it/s]Extractor Predicting: 185it [01:57,  1.51it/s]Extractor Predicting: 186it [01:57,  1.52it/s]Extractor Predicting: 187it [01:58,  1.55it/s]Extractor Predicting: 188it [01:59,  1.59it/s]Extractor Predicting: 189it [01:59,  1.60it/s]Extractor Predicting: 190it [02:00,  1.56it/s]Extractor Predicting: 191it [02:01,  1.58it/s]Extractor Predicting: 192it [02:01,  1.57it/s]Extractor Predicting: 193it [02:02,  1.61it/s]Extractor Predicting: 194it [02:02,  1.66it/s]Extractor Predicting: 195it [02:03,  1.69it/s]Extractor Predicting: 196it [02:04,  1.63it/s]Extractor Predicting: 197it [02:04,  1.62it/s]Extractor Predicting: 198it [02:05,  1.57it/s]Extractor Predicting: 199it [02:05,  1.64it/s]Extractor Predicting: 200it [02:06,  1.64it/s]Extractor Predicting: 201it [02:07,  1.63it/s]Extractor Predicting: 202it [02:07,  1.66it/s]Extractor Predicting: 203it [02:08,  1.66it/s]Extractor Predicting: 204it [02:08,  1.61it/s]Extractor Predicting: 205it [02:09,  1.61it/s]Extractor Predicting: 206it [02:10,  1.60it/s]Extractor Predicting: 207it [02:10,  1.58it/s]Extractor Predicting: 208it [02:11,  1.59it/s]Extractor Predicting: 209it [02:12,  1.60it/s]Extractor Predicting: 210it [02:12,  1.58it/s]Extractor Predicting: 211it [02:13,  1.57it/s]Extractor Predicting: 212it [02:14,  1.59it/s]Extractor Predicting: 213it [02:14,  1.62it/s]Extractor Predicting: 214it [02:15,  1.61it/s]Extractor Predicting: 215it [02:15,  1.61it/s]Extractor Predicting: 216it [02:16,  1.63it/s]Extractor Predicting: 217it [02:17,  1.65it/s]Extractor Predicting: 218it [02:17,  1.59it/s]Extractor Predicting: 219it [02:18,  1.58it/s]Extractor Predicting: 220it [02:19,  1.56it/s]Extractor Predicting: 221it [02:19,  1.51it/s]Extractor Predicting: 222it [02:20,  1.49it/s]Extractor Predicting: 223it [02:21,  1.53it/s]Extractor Predicting: 224it [02:21,  1.53it/s]Extractor Predicting: 225it [02:22,  1.54it/s]Extractor Predicting: 226it [02:23,  1.53it/s]Extractor Predicting: 227it [02:23,  1.55it/s]Extractor Predicting: 228it [02:24,  1.55it/s]Extractor Predicting: 229it [02:24,  1.58it/s]Extractor Predicting: 230it [02:25,  1.58it/s]Extractor Predicting: 231it [02:26,  1.52it/s]Extractor Predicting: 232it [02:26,  1.55it/s]Extractor Predicting: 233it [02:27,  1.57it/s]Extractor Predicting: 234it [02:28,  1.62it/s]Extractor Predicting: 235it [02:28,  1.63it/s]Extractor Predicting: 236it [02:29,  1.70it/s]Extractor Predicting: 237it [02:29,  1.71it/s]Extractor Predicting: 238it [02:30,  1.71it/s]Extractor Predicting: 239it [02:31,  1.64it/s]Extractor Predicting: 240it [02:31,  1.62it/s]Extractor Predicting: 241it [02:32,  1.60it/s]Extractor Predicting: 242it [02:33,  1.52it/s]Extractor Predicting: 243it [02:33,  1.58it/s]Extractor Predicting: 244it [02:34,  1.61it/s]Extractor Predicting: 245it [02:34,  1.62it/s]Extractor Predicting: 246it [02:35,  1.56it/s]Extractor Predicting: 247it [02:36,  1.53it/s]Extractor Predicting: 248it [02:36,  1.52it/s]Extractor Predicting: 249it [02:37,  1.52it/s]Extractor Predicting: 250it [02:38,  1.51it/s]Extractor Predicting: 251it [02:38,  1.52it/s]Extractor Predicting: 252it [02:39,  1.52it/s]Extractor Predicting: 253it [02:40,  1.53it/s]Extractor Predicting: 254it [02:40,  1.55it/s]Extractor Predicting: 255it [02:41,  1.53it/s]Extractor Predicting: 256it [02:42,  1.34it/s]Extractor Predicting: 257it [02:43,  1.38it/s]Extractor Predicting: 258it [02:43,  1.40it/s]Extractor Predicting: 259it [02:44,  1.44it/s]Extractor Predicting: 260it [02:45,  1.45it/s]Extractor Predicting: 261it [02:45,  1.44it/s]Extractor Predicting: 262it [02:46,  1.44it/s]Extractor Predicting: 263it [02:47,  1.46it/s]Extractor Predicting: 264it [02:47,  1.49it/s]Extractor Predicting: 265it [02:48,  1.48it/s]Extractor Predicting: 266it [02:49,  1.46it/s]Extractor Predicting: 267it [02:49,  1.48it/s]Extractor Predicting: 268it [02:50,  1.49it/s]Extractor Predicting: 269it [02:51,  1.50it/s]Extractor Predicting: 270it [02:51,  1.50it/s]Extractor Predicting: 271it [02:52,  1.51it/s]Extractor Predicting: 272it [02:53,  1.51it/s]Extractor Predicting: 273it [02:53,  1.51it/s]Extractor Predicting: 274it [02:54,  1.49it/s]Extractor Predicting: 275it [02:55,  1.49it/s]Extractor Predicting: 276it [02:55,  1.51it/s]Extractor Predicting: 277it [02:56,  1.50it/s]Extractor Predicting: 278it [02:57,  1.50it/s]Extractor Predicting: 279it [02:57,  1.49it/s]Extractor Predicting: 280it [02:58,  1.55it/s]Extractor Predicting: 281it [02:59,  1.57it/s]Extractor Predicting: 282it [02:59,  1.60it/s]Extractor Predicting: 283it [03:00,  1.58it/s]Extractor Predicting: 284it [03:00,  1.56it/s]Extractor Predicting: 285it [03:01,  1.55it/s]Extractor Predicting: 286it [03:02,  1.61it/s]Extractor Predicting: 287it [03:02,  1.63it/s]Extractor Predicting: 288it [03:03,  1.57it/s]Extractor Predicting: 289it [03:04,  1.57it/s]Extractor Predicting: 290it [03:04,  1.58it/s]Extractor Predicting: 291it [03:05,  1.57it/s]Extractor Predicting: 292it [03:05,  1.62it/s]Extractor Predicting: 293it [03:06,  1.62it/s]Extractor Predicting: 294it [03:07,  1.62it/s]Extractor Predicting: 295it [03:07,  1.68it/s]Extractor Predicting: 296it [03:08,  1.69it/s]Extractor Predicting: 297it [03:08,  1.69it/s]Extractor Predicting: 298it [03:09,  1.62it/s]Extractor Predicting: 299it [03:10,  1.52it/s]Extractor Predicting: 300it [03:10,  1.50it/s]Extractor Predicting: 301it [03:11,  1.50it/s]Extractor Predicting: 302it [03:12,  1.53it/s]Extractor Predicting: 303it [03:12,  1.55it/s]Extractor Predicting: 304it [03:13,  1.56it/s]Extractor Predicting: 305it [03:14,  1.53it/s]Extractor Predicting: 306it [03:14,  1.52it/s]Extractor Predicting: 307it [03:15,  1.52it/s]Extractor Predicting: 308it [03:16,  1.51it/s]Extractor Predicting: 309it [03:16,  1.48it/s]Extractor Predicting: 310it [03:17,  1.46it/s]Extractor Predicting: 311it [03:18,  1.48it/s]Extractor Predicting: 312it [03:18,  1.50it/s]Extractor Predicting: 313it [03:19,  1.51it/s]Extractor Predicting: 314it [03:20,  1.53it/s]Extractor Predicting: 315it [03:20,  1.57it/s]Extractor Predicting: 316it [03:21,  1.53it/s]Extractor Predicting: 317it [03:22,  1.49it/s]Extractor Predicting: 318it [03:22,  1.45it/s]Extractor Predicting: 319it [03:23,  1.47it/s]Extractor Predicting: 320it [03:24,  1.48it/s]Extractor Predicting: 321it [03:24,  1.47it/s]Extractor Predicting: 322it [03:25,  1.46it/s]Extractor Predicting: 323it [03:26,  1.46it/s]Extractor Predicting: 324it [03:26,  1.47it/s]Extractor Predicting: 325it [03:27,  1.45it/s]Extractor Predicting: 326it [03:28,  1.44it/s]Extractor Predicting: 327it [03:29,  1.44it/s]Extractor Predicting: 328it [03:29,  1.45it/s]Extractor Predicting: 329it [03:30,  1.45it/s]Extractor Predicting: 330it [03:31,  1.45it/s]Extractor Predicting: 331it [03:31,  1.44it/s]Extractor Predicting: 332it [03:32,  1.39it/s]Extractor Predicting: 333it [03:33,  1.40it/s]Extractor Predicting: 334it [03:34,  1.42it/s]Extractor Predicting: 335it [03:34,  1.42it/s]Extractor Predicting: 336it [03:35,  1.40it/s]Extractor Predicting: 337it [03:36,  1.41it/s]Extractor Predicting: 338it [03:36,  1.41it/s]Extractor Predicting: 339it [03:37,  1.38it/s]Extractor Predicting: 340it [03:38,  1.41it/s]Extractor Predicting: 341it [03:38,  1.44it/s]Extractor Predicting: 342it [03:39,  1.45it/s]Extractor Predicting: 343it [03:40,  1.47it/s]Extractor Predicting: 344it [03:40,  1.50it/s]Extractor Predicting: 345it [03:41,  1.51it/s]Extractor Predicting: 346it [03:42,  1.61it/s]Extractor Predicting: 346it [03:42,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:49,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:49,217 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:49,217 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:49,217 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:49,217 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:25:49,906 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:25:49,907 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:25:50,400 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:25:51,492 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:25:51,492 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:54,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:54,410 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:54,410 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:54,410 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:25:54,410 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:25:55,566 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:25:55,567 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:25:56,243 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:25:56,530 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:25:56,531 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.16455053326561705,
  "recall": 0.039064383891970096,
  "score": 0.06313943291435253,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6093
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6193, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.43it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.40it/s]Extractor Predicting: 19it [00:12,  1.41it/s]Extractor Predicting: 20it [00:13,  1.39it/s]Extractor Predicting: 21it [00:14,  1.36it/s]Extractor Predicting: 22it [00:15,  1.33it/s]Extractor Predicting: 23it [00:15,  1.30it/s]Extractor Predicting: 24it [00:16,  1.32it/s]Extractor Predicting: 25it [00:17,  1.32it/s]Extractor Predicting: 26it [00:18,  1.35it/s]Extractor Predicting: 27it [00:18,  1.41it/s]Extractor Predicting: 28it [00:19,  1.43it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:20,  1.62it/s]Extractor Predicting: 31it [00:21,  1.63it/s]Extractor Predicting: 32it [00:21,  1.70it/s]Extractor Predicting: 33it [00:22,  1.77it/s]Extractor Predicting: 34it [00:22,  1.78it/s]Extractor Predicting: 35it [00:23,  1.83it/s]Extractor Predicting: 36it [00:23,  1.82it/s]Extractor Predicting: 37it [00:24,  1.72it/s]Extractor Predicting: 38it [00:24,  1.76it/s]Extractor Predicting: 39it [00:25,  1.77it/s]Extractor Predicting: 40it [00:25,  1.80it/s]Extractor Predicting: 41it [00:26,  1.80it/s]Extractor Predicting: 42it [00:27,  1.78it/s]Extractor Predicting: 43it [00:27,  1.71it/s]Extractor Predicting: 44it [00:28,  1.75it/s]Extractor Predicting: 45it [00:28,  1.82it/s]Extractor Predicting: 46it [00:29,  1.84it/s]Extractor Predicting: 47it [00:29,  1.83it/s]Extractor Predicting: 48it [00:30,  1.82it/s]Extractor Predicting: 49it [00:31,  1.77it/s]Extractor Predicting: 50it [00:31,  1.75it/s]Extractor Predicting: 51it [00:32,  1.73it/s]Extractor Predicting: 52it [00:32,  1.78it/s]Extractor Predicting: 53it [00:33,  1.79it/s]Extractor Predicting: 54it [00:33,  1.79it/s]Extractor Predicting: 55it [00:34,  1.75it/s]Extractor Predicting: 56it [00:34,  1.77it/s]Extractor Predicting: 57it [00:35,  1.81it/s]Extractor Predicting: 58it [00:36,  1.70it/s]Extractor Predicting: 59it [00:36,  1.61it/s]Extractor Predicting: 60it [00:37,  1.46it/s]Extractor Predicting: 61it [00:38,  1.43it/s]Extractor Predicting: 62it [00:39,  1.43it/s]Extractor Predicting: 63it [00:39,  1.43it/s]Extractor Predicting: 64it [00:40,  1.43it/s]Extractor Predicting: 65it [00:41,  1.38it/s]Extractor Predicting: 66it [00:42,  1.39it/s]Extractor Predicting: 67it [00:42,  1.40it/s]Extractor Predicting: 68it [00:43,  1.39it/s]Extractor Predicting: 69it [00:44,  1.39it/s]Extractor Predicting: 70it [00:44,  1.36it/s]Extractor Predicting: 71it [00:45,  1.33it/s]Extractor Predicting: 72it [00:46,  1.36it/s]Extractor Predicting: 73it [00:47,  1.39it/s]Extractor Predicting: 74it [00:47,  1.41it/s]Extractor Predicting: 75it [00:48,  1.43it/s]Extractor Predicting: 76it [00:49,  1.40it/s]Extractor Predicting: 77it [00:49,  1.73it/s]Extractor Predicting: 77it [00:49,  1.56it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7403055229142186,
  "recall": 0.1567554117939786,
  "score": 0.2587268993839836,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/', 'labels': ['applies to jurisdiction', 'family name', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'use'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
